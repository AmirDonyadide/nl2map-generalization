{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5118d89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Repo root: /Users/amirdonyadide/Documents/GitHub/IMGOFUP\n",
      "ðŸ“¦ Using src from: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/src\n",
      "ðŸ”§ PROJ_ROOT env set to: /Users/amirdonyadide/Documents/GitHub/IMGOFUP\n"
     ]
    }
   ],
   "source": [
    "# ===================== 03_train_models â€” CELL 0: Bootstrap =====================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "p = Path.cwd().resolve()\n",
    "REPO_ROOT = None\n",
    "for candidate in [p, *p.parents]:\n",
    "    if (candidate / \"src\" / \"imgofup\").is_dir():\n",
    "        REPO_ROOT = candidate\n",
    "        break\n",
    "if REPO_ROOT is None:\n",
    "    raise RuntimeError(\"Could not find repo root (no 'src/imgofup' found).\")\n",
    "\n",
    "SRC_DIR = REPO_ROOT / \"src\"\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "os.environ[\"PROJ_ROOT\"] = str(REPO_ROOT)\n",
    "\n",
    "print(\"ðŸ“¦ Repo root:\", REPO_ROOT)\n",
    "print(\"ðŸ“¦ Using src from:\", SRC_DIR)\n",
    "print(\"ðŸ”§ PROJ_ROOT env set to:\", os.environ[\"PROJ_ROOT\"])\n",
    "\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b66722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded EXPERIMENTS: ['openai_prompt_only', 'use_prompt_only', 'map_only', 'use_map', 'openai_map']\n"
     ]
    }
   ],
   "source": [
    "# ===================== 03_train_models â€” CELL 1: Load experiment registry =====================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# repo_root is already computed in your bootstrap cell\n",
    "# Import from notebooks/experiments.py (same folder as notebooks)\n",
    "NOTEBOOKS_DIR = Path.cwd().resolve()\n",
    "if (NOTEBOOKS_DIR / \"experiments.py\").is_file():\n",
    "    from experiments import make_experiments\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing notebooks/experiments.py. Create it so all notebooks share the same EXPERIMENTS registry.\"\n",
    "    )\n",
    "\n",
    "EXPERIMENTS = make_experiments(REPO_ROOT)\n",
    "\n",
    "# normalize paths and ensure dirs exist\n",
    "for cfg in EXPERIMENTS.values():\n",
    "    cfg[\"train_out\"] = Path(cfg[\"train_out\"]).resolve()\n",
    "    cfg[\"model_out\"] = Path(cfg[\"model_out\"]).resolve()\n",
    "    cfg[\"train_out\"].mkdir(parents=True, exist_ok=True)\n",
    "    cfg[\"model_out\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ… Loaded EXPERIMENTS:\", list(EXPERIMENTS.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee4cb408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Stage-2 cache produced by notebook 02 ===\n",
      "âœ… openai_prompt_only: loaded cache from /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/models/exp_openai_prompt_only/cache_stage2\n",
      "âœ… use_prompt_only: loaded cache from /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/models/exp_use_prompt_only/cache_stage2\n",
      "âœ… map_only: loaded cache from /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/models/exp_map_only/cache_stage2\n",
      "âœ… use_map: loaded cache from /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/models/exp_use_map/cache_stage2\n",
      "âœ… openai_map: loaded cache from /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/models/exp_openai_map/cache_stage2\n",
      "\n",
      "âœ… Stage-2 cache loaded. You can now run classifier + regressor training.\n"
     ]
    }
   ],
   "source": [
    "# ===================== 03_train_models â€” CELL: Load Stage-2 cache from disk =====================\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from imgofup.config import paths as CONFIG\n",
    "from imgofup.config.constants import MAPS_ID_COL\n",
    "\n",
    "STAGE2_DIRNAME = \"cache_stage2\"\n",
    "\n",
    "SPLITS = {}\n",
    "PREPROC = {}\n",
    "LABELS = {}\n",
    "\n",
    "print(\"\\n=== Loading Stage-2 cache produced by notebook 02 ===\")\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    cache_dir = Path(cfg[\"model_out\"]).expanduser().resolve() / STAGE2_DIRNAME\n",
    "    if not cache_dir.is_dir():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing cache for {exp_name} at {cache_dir}\\n\"\n",
    "            \"Fix: run 02_build_training_data.ipynb (with cache-saving cell) first.\"\n",
    "        )\n",
    "\n",
    "    # ---- load scaled arrays\n",
    "    zX = np.load(cache_dir / \"X_scaled.npz\", allow_pickle=True)\n",
    "    X_train_s = np.asarray(zX[\"X_train_s\"], dtype=np.float64)\n",
    "    X_val_s   = np.asarray(zX[\"X_val_s\"], dtype=np.float64)\n",
    "    X_test_s  = np.asarray(zX[\"X_test_s\"], dtype=np.float64)\n",
    "\n",
    "    # ---- load labels\n",
    "    zL = np.load(cache_dir / \"labels.npz\", allow_pickle=True)\n",
    "    y_train_cls = np.asarray(zL[\"y_train_cls\"], dtype=int)\n",
    "    y_val_cls   = np.asarray(zL[\"y_val_cls\"], dtype=int)\n",
    "    y_test_cls  = np.asarray(zL[\"y_test_cls\"], dtype=int)\n",
    "    sample_w    = np.asarray(zL[\"sample_w\"], dtype=np.float64)\n",
    "\n",
    "    # ---- load class names\n",
    "    class_names = json.loads((cache_dir / \"class_names.json\").read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    # ---- load dfs (needed for grouped CV + regressor targets)\n",
    "    df_train = __import__(\"pandas\").read_parquet(cache_dir / \"df_train.parquet\")\n",
    "    df_val   = __import__(\"pandas\").read_parquet(cache_dir / \"df_val.parquet\")\n",
    "    df_test  = __import__(\"pandas\").read_parquet(cache_dir / \"df_test.parquet\")\n",
    "\n",
    "    # minimal SPLITS structure expected by your training code\n",
    "    SPLITS[exp_name] = {\n",
    "        \"df_train\": df_train,\n",
    "        \"df_val\": df_val,\n",
    "        \"df_test\": df_test,\n",
    "    }\n",
    "\n",
    "    PREPROC[exp_name] = {\n",
    "        \"X_train_s\": X_train_s,\n",
    "        \"X_val_s\": X_val_s,\n",
    "        \"X_test_s\": X_test_s,\n",
    "        \"bundle_path\": str(Path(cfg[\"model_out\"]) / \"preproc.joblib\"),  # already saved earlier\n",
    "    }\n",
    "\n",
    "    LABELS[exp_name] = {\n",
    "        \"class_names\": class_names,\n",
    "        \"y_train_cls\": y_train_cls,\n",
    "        \"y_val_cls\": y_val_cls,\n",
    "        \"y_test_cls\": y_test_cls,\n",
    "        \"sample_w\": sample_w,\n",
    "    }\n",
    "\n",
    "    # quick sanity\n",
    "    if MAPS_ID_COL not in df_train.columns:\n",
    "        raise ValueError(f\"{exp_name}: df_train missing '{MAPS_ID_COL}' (needed for grouped CV).\")\n",
    "    if X_train_s.shape[0] != len(y_train_cls) or X_train_s.shape[0] != len(df_train):\n",
    "        raise ValueError(\n",
    "            f\"{exp_name}: mismatch lengths: X_train_s={X_train_s.shape[0]}, \"\n",
    "            f\"y_train={len(y_train_cls)}, df_train={len(df_train)}\"\n",
    "        )\n",
    "\n",
    "    print(f\"âœ… {exp_name}: loaded cache from {cache_dir}\")\n",
    "\n",
    "# reload these from config each time (not from cache)\n",
    "DISTANCE_OPS = CONFIG.DISTANCE_OPS\n",
    "AREA_OPS = CONFIG.AREA_OPS\n",
    "CFG = CONFIG.CFG\n",
    "\n",
    "print(\"\\nâœ… Stage-2 cache loaded. You can now run classifier + regressor training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd2280f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found required variables from notebook 02.\n",
      "Experiments: ['openai_prompt_only', 'use_prompt_only', 'map_only', 'use_map', 'openai_map']\n"
     ]
    }
   ],
   "source": [
    "# ===================== 03_train_models â€” CELL 1: Ensure notebook 02 has been run =====================\n",
    "\n",
    "required_globals = [\"EXPERIMENTS\", \"SPLITS\", \"PREPROC\", \"LABELS\", \"DISTANCE_OPS\", \"AREA_OPS\"]\n",
    "\n",
    "missing = [k for k in required_globals if k not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(\n",
    "        \"This notebook expects outputs from 02_build_training_data.ipynb in the SAME kernel.\\n\"\n",
    "        f\"Missing variables: {missing}\\n\\n\"\n",
    "        \"Fix: run notebooks/02_build_training_data.ipynb first (same kernel), then run this notebook.\"\n",
    "    )\n",
    "\n",
    "print(\"âœ… Found required variables from notebook 02.\")\n",
    "print(\"Experiments:\", list(EXPERIMENTS.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f82955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training operator classifiers for all experiments ===\n",
      "\n",
      "ðŸ§ª Experiment: openai_prompt_only\n",
      "   Classes   : ['simplify', 'select', 'aggregate', 'displace']\n",
      "   Train X   : (448, 1536)\n",
      "   Val X     : (57, 1536)\n",
      "   Test X    : (57, 1536)\n",
      "   Model out : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/models/exp_openai_prompt_only\n",
      "\n",
      "Searching 50 MLP configs...\n",
      "[01/50] cvF1=0.929Â±0.018 | VAL F1=0.923 acc=0.930 | (128, 64), Î±=2.02e-02, lr=1.2e-03, bs=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:788: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/50] cvF1=0.918Â±0.038 | VAL F1=0.939 acc=0.947 | (256, 128), Î±=3.49e-05, lr=1.7e-04, bs=64\n"
     ]
    }
   ],
   "source": [
    "# ===================== 03_train_models â€” CELL 2: Train classifier (per experiment) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from dataclasses import asdict, is_dataclass\n",
    "\n",
    "from imgofup.config.paths import CFG\n",
    "from imgofup.config.constants import (\n",
    "    MAPS_ID_COL,\n",
    "    CLS_SEARCH_N_ITER_DEFAULT,\n",
    "    CLS_SEARCH_N_SPLITS_DEFAULT,\n",
    "    CLS_SEARCH_SEED_DEFAULT,\n",
    ")\n",
    "from imgofup.models.train_classifier import train_mlp_classifier_with_search\n",
    "\n",
    "CLF_RESULTS = {}\n",
    "\n",
    "def _safe_get(obj, *names, default=None):\n",
    "    for n in names:\n",
    "        if hasattr(obj, n):\n",
    "            return getattr(obj, n)\n",
    "    return default\n",
    "\n",
    "print(\"\\n=== Training operator classifiers for all experiments ===\")\n",
    "\n",
    "printed_debug_fields = False\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    split = SPLITS[exp_name]\n",
    "    pre   = PREPROC[exp_name]\n",
    "    lab   = LABELS[exp_name]\n",
    "\n",
    "    X_train_s = pre[\"X_train_s\"]\n",
    "    X_val_s   = pre[\"X_val_s\"]\n",
    "    X_test_s  = pre[\"X_test_s\"]\n",
    "\n",
    "    y_train  = lab[\"y_train_cls\"]\n",
    "    y_val    = lab[\"y_val_cls\"]\n",
    "    y_test   = lab[\"y_test_cls\"]\n",
    "    sample_w = lab[\"sample_w\"]\n",
    "\n",
    "    class_names = [str(x) for x in lab[\"class_names\"]]\n",
    "\n",
    "    # Sanity checks\n",
    "    if X_train_s.shape[0] != len(y_train):\n",
    "        raise ValueError(f\"{exp_name}: X_train rows {X_train_s.shape[0]} != y_train {len(y_train)}\")\n",
    "    if X_val_s.shape[0] != len(y_val):\n",
    "        raise ValueError(f\"{exp_name}: X_val rows {X_val_s.shape[0]} != y_val {len(y_val)}\")\n",
    "    if X_test_s.shape[0] != len(y_test):\n",
    "        raise ValueError(f\"{exp_name}: X_test rows {X_test_s.shape[0]} != y_test {len(y_test)}\")\n",
    "\n",
    "    # Grouped CV: group by map_id to avoid leakage across folds\n",
    "    if MAPS_ID_COL not in split[\"df_train\"].columns:\n",
    "        raise ValueError(f\"{exp_name}: df_train missing '{MAPS_ID_COL}' for grouped CV.\")\n",
    "    groups_tr = split[\"df_train\"][MAPS_ID_COL].astype(str).to_numpy()\n",
    "\n",
    "    model_out_dir = Path(cfg[\"model_out\"]).expanduser().resolve()\n",
    "    model_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   Classes   : {class_names}\")\n",
    "    print(f\"   Train X   : {X_train_s.shape}\")\n",
    "    print(f\"   Val X     : {X_val_s.shape}\")\n",
    "    print(f\"   Test X    : {X_test_s.shape}\")\n",
    "    print(f\"   Model out : {model_out_dir}\")\n",
    "\n",
    "    res_clf = train_mlp_classifier_with_search(\n",
    "        exp_name=exp_name,\n",
    "        X_train=X_train_s,\n",
    "        y_train=y_train,\n",
    "        groups_train=groups_tr,\n",
    "        sample_w=sample_w,\n",
    "        X_val=X_val_s,\n",
    "        y_val=y_val,\n",
    "        X_test=X_test_s,\n",
    "        y_test=y_test,\n",
    "        class_names=class_names,\n",
    "        out_dir=model_out_dir,\n",
    "        n_iter=int(CLS_SEARCH_N_ITER_DEFAULT),\n",
    "        n_splits=int(CLS_SEARCH_N_SPLITS_DEFAULT),\n",
    "        seed=int(getattr(CFG, \"SEED\", CLS_SEARCH_SEED_DEFAULT)),\n",
    "        verbose=True,\n",
    "        save_name=\"classifier.joblib\",\n",
    "    )\n",
    "\n",
    "    CLF_RESULTS[exp_name] = res_clf\n",
    "\n",
    "    # Robust reporting (no assumptions about field names)\n",
    "    model_path    = _safe_get(res_clf, \"model_path\", \"path\", default=str(model_out_dir / \"classifier.joblib\"))\n",
    "    best_val_f1   = _safe_get(res_clf, \"val_f1_macro\", \"best_val_f1\", \"val_f1\", \"best_f1\", default=None)\n",
    "    best_val_acc  = _safe_get(res_clf, \"val_acc\", \"best_val_acc\", \"best_accuracy\", default=None)\n",
    "    test_f1       = _safe_get(res_clf, \"test_f1_macro\", \"test_f1\", default=None)\n",
    "    test_acc      = _safe_get(res_clf, \"test_acc\", \"accuracy_test\", default=None)\n",
    "\n",
    "    print(\"   âœ… Classifier training done.\")\n",
    "    print(\"   Saved to:\", model_path)\n",
    "    if best_val_f1 is not None or best_val_acc is not None:\n",
    "        print(\"   Best VAL:\", {\"macro_f1\": best_val_f1, \"acc\": best_val_acc})\n",
    "    if test_f1 is not None or test_acc is not None:\n",
    "        print(\"   TEST     :\", {\"macro_f1\": test_f1, \"acc\": test_acc})\n",
    "\n",
    "    # Save lightweight meta for evaluation / reporting\n",
    "    clf_meta = {\n",
    "        \"experiment\": exp_name,\n",
    "        \"feature_mode\": cfg[\"feature_mode\"],\n",
    "        \"class_names\": class_names,\n",
    "        \"best_val\": {\"macro_f1\": best_val_f1, \"acc\": best_val_acc},\n",
    "        \"test\": {\"macro_f1\": test_f1, \"acc\": test_acc},\n",
    "        \"model_path\": str(model_path),\n",
    "    }\n",
    "    (model_out_dir / \"classifier_meta.json\").write_text(json.dumps(clf_meta, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # Print available fields once for debugging\n",
    "    if not printed_debug_fields:\n",
    "        printed_debug_fields = True\n",
    "        if is_dataclass(res_clf):\n",
    "            print(\"   (debug) Result fields:\", list(asdict(res_clf).keys()))\n",
    "        else:\n",
    "            print(\"   (debug) Result attrs :\", [a for a in dir(res_clf) if not a.startswith(\"_\")])\n",
    "\n",
    "print(\"\\nâœ… All classifiers trained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e8435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 03_train_models â€” CELL 3: Train regressors + save final bundle =====================\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from imgofup.config.paths import CFG\n",
    "from imgofup.config.constants import (\n",
    "    MAPS_ID_COL,\n",
    "    PARAM_TARGET_NAME,\n",
    "    EXTENT_DIAG_COL,\n",
    "    EXTENT_AREA_COL,\n",
    "    REG_USE_LOG1P_DEFAULT,\n",
    "    REG_N_SPLITS_DEFAULT,\n",
    "    REG_N_ITER_DEFAULT,\n",
    "    REG_RANDOM_STATE_DEFAULT,\n",
    "    REG_VERBOSE_DEFAULT,\n",
    ")\n",
    "from imgofup.models.train_regressors import train_regressors_per_operator\n",
    "from imgofup.models.save_bundle import save_cls_plus_regressors_bundle\n",
    "\n",
    "BUNDLES = {}     # exp_name -> bundle path\n",
    "REG_RESULTS = {} # exp_name -> regressor training result\n",
    "\n",
    "def _safe_get(obj, *names, default=None):\n",
    "    for n in names:\n",
    "        if hasattr(obj, n):\n",
    "            return getattr(obj, n)\n",
    "    return default\n",
    "\n",
    "print(\"\\n=== Training per-operator regressors and saving final bundles ===\")\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    split = SPLITS[exp_name]\n",
    "    pre   = PREPROC[exp_name]\n",
    "    lab   = LABELS[exp_name]\n",
    "    res_clf = CLF_RESULTS[exp_name]\n",
    "\n",
    "    X_train_s = pre[\"X_train_s\"]\n",
    "    df_train  = split[\"df_train\"]\n",
    "    y_train_cls = lab[\"y_train_cls\"]\n",
    "    sample_w = lab[\"sample_w\"]\n",
    "\n",
    "    cn = [str(x) for x in lab[\"class_names\"]]\n",
    "\n",
    "    model_out_dir = Path(cfg[\"model_out\"]).expanduser().resolve()\n",
    "    model_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   Model out: {model_out_dir}\")\n",
    "    print(f\"   Train X  : {X_train_s.shape} | df_train: {df_train.shape}\")\n",
    "\n",
    "    # (1) Train per-operator regressors on TRAIN only\n",
    "    reg_res = train_regressors_per_operator(\n",
    "        X_train_s=X_train_s,\n",
    "        df_train=df_train,\n",
    "        y_train_cls=y_train_cls,\n",
    "        class_names=cn,\n",
    "        sample_w=sample_w,\n",
    "        group_col=MAPS_ID_COL,\n",
    "        target_col=PARAM_TARGET_NAME,\n",
    "        use_log1p=bool(REG_USE_LOG1P_DEFAULT),\n",
    "        n_splits=int(REG_N_SPLITS_DEFAULT),\n",
    "        n_iter=int(REG_N_ITER_DEFAULT),\n",
    "        random_state=int(getattr(CFG, \"SEED\", REG_RANDOM_STATE_DEFAULT)),\n",
    "        verbose=int(REG_VERBOSE_DEFAULT),\n",
    "    )\n",
    "\n",
    "    REG_RESULTS[exp_name] = reg_res\n",
    "\n",
    "    # (2) Load the trained classifier model from disk\n",
    "    clf_model_path = _safe_get(res_clf, \"model_path\", \"path\", default=str(model_out_dir / \"classifier.joblib\"))\n",
    "    clf_pack = joblib.load(Path(clf_model_path))\n",
    "    final_clf = clf_pack[\"model\"] if isinstance(clf_pack, dict) and \"model\" in clf_pack else clf_pack\n",
    "\n",
    "    # (3) Save combined bundle for evaluation\n",
    "    bundle_res = save_cls_plus_regressors_bundle(\n",
    "        exp_name=exp_name,\n",
    "        out_dir=model_out_dir,\n",
    "        classifier=final_clf,\n",
    "        regressors_by_class=reg_res.regressors_by_class,\n",
    "        class_names=cn,\n",
    "        use_log1p=reg_res.use_log1p,\n",
    "        cv_summary=reg_res.cv_summary,\n",
    "        distance_ops=DISTANCE_OPS,\n",
    "        area_ops=AREA_OPS,\n",
    "        diag_col=EXTENT_DIAG_COL,\n",
    "        area_col=EXTENT_AREA_COL,\n",
    "        save_name=\"cls_plus_regressors.joblib\",\n",
    "    )\n",
    "\n",
    "    BUNDLES[exp_name] = bundle_res.bundle_path\n",
    "\n",
    "    print(\"   âœ… Saved bundle:\", bundle_res.bundle_path)\n",
    "    print(\"   âœ… Regressors trained for:\", sorted(list(reg_res.regressors_by_class.keys())))\n",
    "\n",
    "print(\"\\nâœ… All bundles saved.\")\n",
    "for k, v in BUNDLES.items():\n",
    "    print(f\" - {k:18s}: {v}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
