{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ccd509",
   "metadata": {},
   "source": [
    "### Cell 0 â€” Repository Bootstrap & Experiment Registry (Required)\n",
    "\n",
    "This cell ensures that the project repository is discoverable by Python **and**\n",
    "defines the registry of experiments that will be executed in this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why this is needed\n",
    "\n",
    "- The notebook lives inside the `notebooks/` directory  \n",
    "- Python does not automatically know where the project root is  \n",
    "- All project code lives under the `src/` directory  \n",
    "- Multiple experimental configurations (prompt-only, USE + map, OpenAI + map)\n",
    "  must be executed in a single, reproducible workflow  \n",
    "\n",
    "---\n",
    "\n",
    "#### What this cell does\n",
    "\n",
    "- Walks up the directory tree starting from the current notebook location  \n",
    "- Finds the repository root (identified by the presence of a `src/` folder)  \n",
    "- Adds that directory to `sys.path` so imports such as  \n",
    "  `from src.config import ...` work correctly  \n",
    "- Defines a central **experiment registry** describing:\n",
    "  - which feature representation is used\n",
    "  - where training data is read from\n",
    "  - where trained models and metadata are saved  \n",
    "\n",
    "---\n",
    "\n",
    "#### Design principles\n",
    "\n",
    "- Executed once at the very top of the notebook  \n",
    "- Contains no learning or model-specific logic  \n",
    "- Provides a single source of truth for all experiment configurations  \n",
    "- Enables fair and controlled comparison between different feature setups  \n",
    "\n",
    "---\n",
    "\n",
    "This cell must be executed **before any imports from `src.*`**.  \n",
    "All subsequent cells rely on the repository path and experiment definitions\n",
    "established here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2152ca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Repo root: /Users/amirdonyadide/Documents/GitHub/IMGOFUP\n",
      "ðŸ”§ PROJ_ROOT env set to: /Users/amirdonyadide/Documents/GitHub/IMGOFUP\n",
      "ðŸ§ª Will run experiments:\n",
      " - openai_prompt_only | mode=prompt_only    | prompt=openai-small   | train_out=train_out_openai_prompt_only | model_out=exp_openai_prompt_only\n",
      " - use_prompt_only    | mode=prompt_only    | prompt=dan            | train_out=train_out_use_prompt_only | model_out=exp_use_prompt_only\n",
      " - map_only           | mode=map_only       | prompt=-              | train_out=train_out_map_only | model_out=exp_map_only\n",
      " - use_map            | mode=prompt_plus_map | prompt=dan            | train_out=train_out_use_map | model_out=exp_use_map\n",
      " - openai_map         | mode=prompt_plus_map | prompt=openai-small   | train_out=train_out_openai_map | model_out=exp_openai_map\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 0 â€” Bootstrap + experiment registry =====================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Find repo root (folder that contains \"src/imgofup\")\n",
    "# -------------------------------------------------\n",
    "p = Path.cwd().resolve()\n",
    "REPO_ROOT = None\n",
    "\n",
    "for candidate in [p, *p.parents]:\n",
    "    if (candidate / \"src\" / \"imgofup\").is_dir():\n",
    "        REPO_ROOT = candidate\n",
    "        break\n",
    "\n",
    "if REPO_ROOT is None:\n",
    "    raise RuntimeError(\"Could not find repo root (no 'src/imgofup' found).\")\n",
    "\n",
    "SRC_DIR = REPO_ROOT / \"src\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Make src/ importable (NOT the repo root)\n",
    "# -------------------------------------------------\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "# -------------------------------------------------\n",
    "# IMPORTANT: tell config the repo root (so paths.py is stable)\n",
    "# -------------------------------------------------\n",
    "os.environ[\"PROJ_ROOT\"] = str(REPO_ROOT)\n",
    "\n",
    "print(\"ðŸ“¦ Repo root:\", REPO_ROOT)\n",
    "print(\"ðŸ”§ PROJ_ROOT env set to:\", os.environ[\"PROJ_ROOT\"])\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Experiment registry (ALL experiments will run)\n",
    "# -------------------------------------------------\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "\n",
    "# âœ… NEW: models go to repo-level /models (webapp expects this)\n",
    "MODELS_DIR = REPO_ROOT / \"models\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    \"openai_prompt_only\": {\n",
    "        \"train_out\": DATA_DIR / \"output\" / \"train_out_openai_prompt_only\",\n",
    "        \"model_out\": MODELS_DIR / \"exp_openai_prompt_only\",\n",
    "        \"feature_mode\": \"prompt_only\",\n",
    "        \"prompt_encoder_kind\": \"openai-small\",\n",
    "    },\n",
    "    \"use_prompt_only\": {\n",
    "        \"train_out\": DATA_DIR / \"output\" / \"train_out_use_prompt_only\",\n",
    "        \"model_out\": MODELS_DIR / \"exp_use_prompt_only\",\n",
    "        \"feature_mode\": \"prompt_only\",\n",
    "        \"prompt_encoder_kind\": \"dan\",\n",
    "    },\n",
    "    \"map_only\": {\n",
    "        \"train_out\": DATA_DIR / \"output\" / \"train_out_map_only\",\n",
    "        \"model_out\": MODELS_DIR / \"exp_map_only\",\n",
    "        \"feature_mode\": \"map_only\",\n",
    "    },\n",
    "    \"use_map\": {\n",
    "        \"train_out\": DATA_DIR / \"output\" / \"train_out_use_map\",\n",
    "        \"model_out\": MODELS_DIR / \"exp_use_map\",\n",
    "        \"feature_mode\": \"prompt_plus_map\",\n",
    "        \"prompt_encoder_kind\": \"dan\",\n",
    "    },\n",
    "    \"openai_map\": {\n",
    "        \"train_out\": DATA_DIR / \"output\" / \"train_out_openai_map\",\n",
    "        \"model_out\": MODELS_DIR / \"exp_openai_map\",\n",
    "        \"feature_mode\": \"prompt_plus_map\",\n",
    "        \"prompt_encoder_kind\": \"openai-small\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Ensure output dirs exist\n",
    "for exp_cfg in EXPERIMENTS.values():\n",
    "    exp_cfg[\"train_out\"] = Path(exp_cfg[\"train_out\"])\n",
    "    exp_cfg[\"model_out\"] = Path(exp_cfg[\"model_out\"])\n",
    "    exp_cfg[\"train_out\"].mkdir(parents=True, exist_ok=True)\n",
    "    exp_cfg[\"model_out\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Summary\n",
    "# -------------------------------------------------\n",
    "print(\"ðŸ§ª Will run experiments:\")\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    pe = cfg.get(\"prompt_encoder_kind\", \"-\")\n",
    "    print(\n",
    "        f\" - {exp_name:18s} | \"\n",
    "        f\"mode={cfg['feature_mode']:14s} | \"\n",
    "        f\"prompt={pe:14s} | \"\n",
    "        f\"train_out={cfg['train_out'].name} | \"\n",
    "        f\"model_out={cfg['model_out'].name}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f56a2c",
   "metadata": {},
   "source": [
    "### Cell 1 â€” Experiment Setup & Global Configuration\n",
    "\n",
    "This cell initializes the experiment environment and loads the global configuration required\n",
    "for training and evaluation. It is designed so the notebook can run **all experiment\n",
    "configurations in one pass** (prompt-only, USE + map, OpenAI + map) without manual edits.\n",
    "\n",
    "**What this cell does:**\n",
    "\n",
    "- **Loads global project configuration**\n",
    "  - Paths (`PATHS`)\n",
    "  - Runtime settings (`CFG`)\n",
    "  - Operator groups (`DISTANCE_OPS`, `AREA_OPS`)\n",
    "  - Dynamic extent configuration flags (`USE_DYNAMIC_EXTENT_REFS`, `ALLOW_FALLBACK_EXTENT`)\n",
    "  - Extent reference column names (`EXTENT_DIAG_COL`, `EXTENT_AREA_COL`)\n",
    "\n",
    "- **Defines a central experiment registry**\n",
    "  - `EXPERIMENTS` â€” a dictionary where each experiment specifies:\n",
    "    - `feature_mode`:\n",
    "      - `prompt_only` (uses prompt embeddings only)\n",
    "      - `fused` (uses concatenated map + prompt embeddings)\n",
    "    - `train_out` â€” where the prepared matrices (`X_*`) and `train_pairs.parquet` are read from\n",
    "    - `model_out` â€” where trained artifacts are saved\n",
    "\n",
    "- **Sets embedding dimensions**\n",
    "  - `MAP_DIM`, `PROMPT_DIM`\n",
    "  - `FUSED_DIM = MAP_DIM + PROMPT_DIM`\n",
    "  - The effective input dimension is derived per experiment:\n",
    "    - `prompt_only` â†’ `PROMPT_DIM`\n",
    "    - `fused` â†’ `FUSED_DIM`\n",
    "\n",
    "- **Validates experiment folders**\n",
    "  - Ensures each experimentâ€™s `train_out` and `model_out` directories exist\n",
    "  - Performs basic schema checks (required keys, valid feature modes)\n",
    "\n",
    "**Design principles**\n",
    "\n",
    "- Executed once near the top of the notebook (before data loading/training)\n",
    "- Contains no model training logic\n",
    "- Provides a single source of truth for experiment configuration\n",
    "- Prevents accidental overwrites by saving each experiment into its own output folder\n",
    "\n",
    "This cell must be executed **before any training or evaluation cells**. All experiment\n",
    "comparisons depend on the consistent configuration established here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "367f89f6ac45439b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:21:53.302406Z",
     "start_time": "2025-10-27T11:21:53.298709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFIG SUMMARY ===\n",
      "PROJ_ROOT  : /Users/amirdonyadide/Documents/GitHub/IMGOFUP\n",
      "DATA_DIR   : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data\n",
      "INPUT_DIR  : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/input\n",
      "OUTPUT_DIR : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output\n",
      "MAPS_ROOT  : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/input/samples/pairs\n",
      "INPUT PAT. : *_input.geojson\n",
      "--- User Study ---\n",
      "USER_STUDY_XLSX : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/userstudy/UserStudy.xlsx\n",
      "RESPONSES_SHEET : Responses\n",
      "TILE_ID_COL     : tile_id\n",
      "COMPLETE_COL    : complete\n",
      "REMOVE_COL      : remove\n",
      "TEXT_COL        : cleaned_text\n",
      "PROMPT_ID_COL   : prompt_id\n",
      "PARAM_VALUE_COL : param_value\n",
      "OPERATOR_COL    : operator\n",
      "INTENSITY_COL   : intensity\n",
      "--- Filters ---\n",
      "ONLY_COMPLETE   : True\n",
      "EXCLUDE_REMOVED : True\n",
      "--- Outputs ---\n",
      "PROMPT_OUT : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/prompt_out\n",
      "MAP_OUT    : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/map_out\n",
      "TRAIN_OUT  : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out\n",
      "MODEL_OUT  : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/models\n",
      "SPLIT_OUT  : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out/splits\n",
      "PRM_NPZ    : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/prompt_out/prompts_embeddings.npz\n",
      "PROMPTS_PQ : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/prompt_out/prompts.parquet\n",
      "MAPS_NPZ   : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/map_out/maps_embeddings.npz\n",
      "MAPS_PQ    : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/map_out/maps.parquet\n",
      "PAIRS_PQ   : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out/train_pairs.parquet\n",
      "--- Model ---\n",
      "PROMPT_ENCODER: openai-small\n",
      "MAP_DIM       : 165\n",
      "PROMPT_DIM    : 512\n",
      "FUSED_DIM     : 677\n",
      "BATCH_SIZE    : 512\n",
      "VAL/TEST      : 0.15 0.15\n",
      "SEED          : 42\n",
      "--- Normalization ---\n",
      "USE_DYNAMIC_EXTENT_REFS : True\n",
      "ALLOW_FALLBACK_EXTENT   : True\n",
      "Extent cols             : extent_diag_m extent_area_m2\n",
      "--- Fallback tile scale (ONLY if dynamic refs missing) ---\n",
      "DEFAULT_TILE_W/H (m) : 400.0 400.0\n",
      "DEFAULT_TILE_DIAG_M  : 565.685424949238\n",
      "DEFAULT_TILE_AREA_M2 : 160000.0\n",
      "--- Operator groups ---\n",
      "DISTANCE_OPS : ('aggregate', 'displace', 'simplify')\n",
      "AREA_OPS     : ('select',)\n",
      "--- Param estimation (inference policy) ---\n",
      "PARAM_STRATEGY : mlp\n",
      "\n",
      "ðŸ”Ž Sanity checks:\n",
      "USER_STUDY_XLSX: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/userstudy/UserStudy.xlsx\n",
      "is_file: True\n",
      "USE_DYNAMIC_EXTENT_REFS: True\n",
      "ALLOW_FALLBACK_EXTENT  : True\n",
      "EXTENT_DIAG_COL: extent_diag_m | EXTENT_AREA_COL: extent_area_m2\n",
      "\n",
      "CFG dims -> MAP_DIM: 165 | PROMPT_DIM: 512 | FUSED_DIM: 677\n",
      "BATCH_SIZE: 512\n",
      "\n",
      "ðŸ§ª Experiments to be executed:\n",
      " - openai_prompt_only | mode=prompt_only    | prompt=openai-small   | train_out=train_out_openai_prompt_only | model_out=exp_openai_prompt_only\n",
      " - use_prompt_only    | mode=prompt_only    | prompt=dan            | train_out=train_out_use_prompt_only | model_out=exp_use_prompt_only\n",
      " - map_only           | mode=map_only       | prompt=-              | train_out=train_out_map_only | model_out=exp_map_only\n",
      " - use_map            | mode=prompt_plus_map | prompt=dan            | train_out=train_out_use_map | model_out=exp_use_map\n",
      " - openai_map         | mode=prompt_plus_map | prompt=openai-small   | train_out=train_out_openai_map | model_out=exp_openai_map\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 1 â€” PARAMETERS =====================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from imgofup.config.paths import (\n",
    "    PATHS, CFG, print_summary,\n",
    "    DISTANCE_OPS, AREA_OPS,\n",
    "    USE_DYNAMIC_EXTENT_REFS, ALLOW_FALLBACK_EXTENT,\n",
    ")\n",
    "from imgofup.config.constants import EXTENT_DIAG_COL, EXTENT_AREA_COL\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Config summary + quick sanity checks\n",
    "# -------------------------------------------------\n",
    "print_summary()\n",
    "\n",
    "print(\"\\nðŸ”Ž Sanity checks:\")\n",
    "print(\"USER_STUDY_XLSX:\", PATHS.USER_STUDY_XLSX)\n",
    "print(\"is_file:\", PATHS.USER_STUDY_XLSX.is_file())\n",
    "\n",
    "print(\"USE_DYNAMIC_EXTENT_REFS:\", USE_DYNAMIC_EXTENT_REFS)\n",
    "print(\"ALLOW_FALLBACK_EXTENT  :\", ALLOW_FALLBACK_EXTENT)\n",
    "print(\"EXTENT_DIAG_COL:\", EXTENT_DIAG_COL, \"| EXTENT_AREA_COL:\", EXTENT_AREA_COL)\n",
    "\n",
    "MAP_DIM_CFG = int(CFG.MAP_DIM)\n",
    "PROMPT_DIM_CFG = int(CFG.PROMPT_DIM)\n",
    "FUSED_DIM_CFG = MAP_DIM_CFG + PROMPT_DIM_CFG\n",
    "BATCH_SIZE = int(CFG.BATCH_SIZE)\n",
    "\n",
    "print(\"\\nCFG dims -> MAP_DIM:\", MAP_DIM_CFG, \"| PROMPT_DIM:\", PROMPT_DIM_CFG, \"| FUSED_DIM:\", FUSED_DIM_CFG)\n",
    "print(\"BATCH_SIZE:\", BATCH_SIZE)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Validate experiment registry (from Cell 0)\n",
    "# -------------------------------------------------\n",
    "required_keys = {\"train_out\", \"model_out\", \"feature_mode\"}\n",
    "allowed_modes = {\"prompt_only\", \"map_only\", \"prompt_plus_map\"}\n",
    "\n",
    "for exp_name, exp_cfg in EXPERIMENTS.items():\n",
    "    missing = required_keys - set(exp_cfg.keys())\n",
    "    if missing:\n",
    "        raise ValueError(f\"Experiment '{exp_name}' is missing keys: {missing}\")\n",
    "\n",
    "    mode = str(exp_cfg[\"feature_mode\"]).strip().lower()\n",
    "    if mode not in allowed_modes:\n",
    "        raise ValueError(\n",
    "            f\"Experiment '{exp_name}' has invalid feature_mode='{exp_cfg['feature_mode']}'. \"\n",
    "            f\"Allowed: {sorted(allowed_modes)}\"\n",
    "        )\n",
    "    exp_cfg[\"feature_mode\"] = mode\n",
    "\n",
    "    exp_cfg[\"train_out\"] = Path(exp_cfg[\"train_out\"])\n",
    "    exp_cfg[\"model_out\"] = Path(exp_cfg[\"model_out\"])\n",
    "\n",
    "    exp_cfg[\"train_out\"].mkdir(parents=True, exist_ok=True)\n",
    "    exp_cfg[\"model_out\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Prompt encoder required whenever prompts are part of features\n",
    "    if mode in {\"prompt_only\", \"prompt_plus_map\"}:\n",
    "        if \"prompt_encoder_kind\" not in exp_cfg:\n",
    "            raise ValueError(\n",
    "                f\"Experiment '{exp_name}' needs 'prompt_encoder_kind' because feature_mode='{mode}'.\"\n",
    "            )\n",
    "\n",
    "    # No prompt encoder needed for map_only\n",
    "    if mode == \"map_only\":\n",
    "        exp_cfg.pop(\"prompt_encoder_kind\", None)\n",
    "\n",
    "print(\"\\nðŸ§ª Experiments to be executed:\")\n",
    "for exp_name, exp_cfg in EXPERIMENTS.items():\n",
    "    pe = exp_cfg.get(\"prompt_encoder_kind\", \"-\")\n",
    "    print(\n",
    "        f\" - {exp_name:18s} | \"\n",
    "        f\"mode={exp_cfg['feature_mode']:14s} | \"\n",
    "        f\"prompt={pe:14s} | \"\n",
    "        f\"train_out={exp_cfg['train_out'].name} | \"\n",
    "        f\"model_out={exp_cfg['model_out'].name}\"\n",
    "    )\n",
    "\n",
    "def get_feature_dims_from_cfg(feature_mode: str):\n",
    "    fm = str(feature_mode).strip().lower()\n",
    "    if fm == \"prompt_only\":\n",
    "        return 0, PROMPT_DIM_CFG, PROMPT_DIM_CFG\n",
    "    if fm == \"map_only\":\n",
    "        return MAP_DIM_CFG, 0, MAP_DIM_CFG\n",
    "    if fm == \"prompt_plus_map\":\n",
    "        return MAP_DIM_CFG, PROMPT_DIM_CFG, FUSED_DIM_CFG\n",
    "    raise ValueError(f\"Unknown feature_mode: {feature_mode}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a2350e",
   "metadata": {},
   "source": [
    "## Step 2 â€” Prompt Embedding Generation (All Experiments)\n",
    "\n",
    "In this step, we generate vector embeddings for all user prompts for **each experiment\n",
    "configuration** (e.g., prompt-only, USE + map, OpenAI + map). The embedding backend is\n",
    "selected per experiment via `PROMPT_ENCODER` (e.g., **USE-DAN/Transformer** or **OpenAI\n",
    "text-embedding-3-* models**).\n",
    "\n",
    "### What happens here?\n",
    "\n",
    "For each entry in the experiment registry:\n",
    "\n",
    "- Prompts are loaded from the user study source file.\n",
    "- Only valid prompts are kept (`complete == True`, `remove == False`).\n",
    "- The prompt embedding model is selected from the experiment configuration\n",
    "  (mapped to `CFG.PROMPT_ENCODER`).\n",
    "- All prompts are embedded in batches (with optional L2 normalization).\n",
    "- The resulting embeddings and metadata are saved to an **experiment-specific folder**\n",
    "  so no configuration overwrites another.\n",
    "\n",
    "**Outputs per experiment** (written under `PATHS.PROMPT_OUT/<experiment_name>/`):\n",
    "\n",
    "- `prompts_embeddings.npz` â€” matrix `E` and `ids`\n",
    "- `prompts.parquet` â€” prompt_id, text, tile_id\n",
    "- `meta.json` â€” model label, dimensionality, and export info\n",
    "\n",
    "### Why this is encapsulated in a helper\n",
    "\n",
    "To keep the notebook clean and reproducible, all logic related to:\n",
    "\n",
    "- loading and filtering prompts,\n",
    "- selecting the embedding backend,\n",
    "- batching and normalization,\n",
    "- saving outputs in a consistent format,\n",
    "\n",
    "is encapsulated in `src/train/run_prompt_embeddings.py`.\n",
    "\n",
    "The notebook only *orchestrates* experiments by calling this helper with an\n",
    "experiment-specific configuration.\n",
    "\n",
    "This design ensures:\n",
    "\n",
    "- consistent prompt embeddings across training and evaluation,\n",
    "- easy comparison between USE and OpenAI backends,\n",
    "- clean separation between experiment orchestration (notebook) and implementation (src),\n",
    "- safe parallel storage of artifacts for multiple experiment runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed0df45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:21:55.572701Z",
     "start_time": "2025-10-27T11:21:55.570071Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 21:33:43 | INFO | Reading Excel: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/userstudy/UserStudy.xlsx (sheet=Responses)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running prompt embeddings for experiments that require prompts ===\n",
      "\n",
      "ðŸ“Œ USER_STUDY_XLSX = /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/userstudy/UserStudy.xlsx\n",
      "   is_file: True\n",
      "\n",
      "ðŸ§ª Experiment: openai_prompt_only\n",
      "   feature_mode   : prompt_only\n",
      "   PROMPT_ENCODER : openai-small\n",
      "   Output dir     : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_openai_prompt_only/prompt_out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 21:33:44 | INFO | Embedding 562 prompts with OpenAI model=text-embedding-3-small (batch_size=512, l2=True)â€¦\n",
      "2026-02-01 21:33:46 | INFO | HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 21:33:48 | INFO | HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 21:33:48 | INFO | Done OpenAI embedding in 3.63s (dim=1536).\n",
      "2026-02-01 21:33:48 | INFO | Writing outputs to /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_openai_prompt_only/prompt_out\n",
      "2026-02-01 21:33:48 | INFO |   saved prompts_embeddings.npz (shape=(562, 1536))\n",
      "2026-02-01 21:33:48 | INFO |   saved prompts.parquet (rows=562)\n",
      "2026-02-01 21:33:48 | INFO |   saved meta.json\n",
      "2026-02-01 21:33:48 | INFO | Reading Excel: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/userstudy/UserStudy.xlsx (sheet=Responses)\n",
      "2026-02-01 21:33:48 | INFO | Using local USE-dan at /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/input/model_dan\n",
      "2026-02-01 21:33:48 | INFO | Loading USE-dan from local path: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/input/model_dan â€¦\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Prompt embeddings completed.\n",
      "\n",
      "ðŸ§ª Experiment: use_prompt_only\n",
      "   feature_mode   : prompt_only\n",
      "   PROMPT_ENCODER : dan\n",
      "   Output dir     : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_use_prompt_only/prompt_out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 21:33:52 | INFO | Fingerprint not found. Saved model loading will continue.\n",
      "2026-02-01 21:33:52 | INFO | path_and_singleprint metric could not be logged. Saved model loading will continue.\n",
      "2026-02-01 21:33:52 | INFO | USE model loaded in 3.69s\n",
      "2026-02-01 21:33:52 | INFO | Embedding 562 prompts with USE (batch_size=512, l2=True)â€¦\n",
      "2026-02-01 21:33:52 | INFO | Done USE embedding in 0.16s (dim=512).\n",
      "2026-02-01 21:33:52 | INFO | Writing outputs to /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_use_prompt_only/prompt_out\n",
      "2026-02-01 21:33:52 | INFO |   saved prompts_embeddings.npz (shape=(562, 512))\n",
      "2026-02-01 21:33:52 | INFO |   saved prompts.parquet (rows=562)\n",
      "2026-02-01 21:33:52 | INFO |   saved meta.json\n",
      "2026-02-01 21:33:52 | INFO | Reading Excel: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/userstudy/UserStudy.xlsx (sheet=Responses)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Prompt embeddings completed.\n",
      "\n",
      "ðŸ§ª Experiment: map_only\n",
      "   (skip) feature_mode=map_only â†’ no prompt embeddings required.\n",
      "\n",
      "ðŸ§ª Experiment: use_map\n",
      "   feature_mode   : prompt_plus_map\n",
      "   PROMPT_ENCODER : dan\n",
      "   Output dir     : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_use_map/prompt_out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 21:33:52 | INFO | Using local USE-dan at /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/input/model_dan\n",
      "2026-02-01 21:33:52 | INFO | Loading USE-dan from local path: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/input/model_dan â€¦\n",
      "2026-02-01 21:33:56 | INFO | Fingerprint not found. Saved model loading will continue.\n",
      "2026-02-01 21:33:56 | INFO | path_and_singleprint metric could not be logged. Saved model loading will continue.\n",
      "2026-02-01 21:33:56 | INFO | USE model loaded in 3.64s\n",
      "2026-02-01 21:33:56 | INFO | Embedding 562 prompts with USE (batch_size=512, l2=True)â€¦\n",
      "2026-02-01 21:33:56 | INFO | Done USE embedding in 0.14s (dim=512).\n",
      "2026-02-01 21:33:56 | INFO | Writing outputs to /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_use_map/prompt_out\n",
      "2026-02-01 21:33:56 | INFO |   saved prompts_embeddings.npz (shape=(562, 512))\n",
      "2026-02-01 21:33:56 | INFO |   saved prompts.parquet (rows=562)\n",
      "2026-02-01 21:33:56 | INFO |   saved meta.json\n",
      "2026-02-01 21:33:56 | INFO | Reading Excel: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/userstudy/UserStudy.xlsx (sheet=Responses)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Prompt embeddings completed.\n",
      "\n",
      "ðŸ§ª Experiment: openai_map\n",
      "   feature_mode   : prompt_plus_map\n",
      "   PROMPT_ENCODER : openai-small\n",
      "   Output dir     : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_openai_map/prompt_out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-01 21:33:57 | INFO | Embedding 562 prompts with OpenAI model=text-embedding-3-small (batch_size=512, l2=True)â€¦\n",
      "2026-02-01 21:34:00 | INFO | HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 21:34:02 | INFO | HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-01 21:34:02 | INFO | Done OpenAI embedding in 5.47s (dim=1536).\n",
      "2026-02-01 21:34:02 | INFO | Writing outputs to /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_openai_map/prompt_out\n",
      "2026-02-01 21:34:02 | INFO |   saved prompts_embeddings.npz (shape=(562, 1536))\n",
      "2026-02-01 21:34:02 | INFO |   saved prompts.parquet (rows=562)\n",
      "2026-02-01 21:34:02 | INFO |   saved meta.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Prompt embeddings completed.\n",
      "\n",
      "âœ… Prompt embedding step finished.\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 2 â€” Prompt embeddings (experiment-scoped) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "from dataclasses import replace\n",
    "\n",
    "from imgofup.config import paths\n",
    "from imgofup.config.constants import (\n",
    "    PROMPT_EMBEDDINGS_NPZ_NAME,\n",
    "    PROMPTS_PARQUET_NAME,\n",
    "    PROMPT_EMBED_VERBOSITY_DEFAULT,\n",
    "    PROMPT_EMBED_L2_NORMALIZE_DEFAULT,\n",
    "    PROMPT_EMBED_SAVE_CSV_DEFAULT,\n",
    ")\n",
    "from imgofup.pipelines.run_prompt_embeddings import run_prompt_embeddings_from_config\n",
    "\n",
    "print(\"\\n=== Running prompt embeddings for experiments that require prompts ===\")\n",
    "\n",
    "prompt_meta_by_experiment = {}\n",
    "\n",
    "# IMPORTANT: because prompt_id is now read from Excel, old artifacts may be stale.\n",
    "FORCE_REBUILD_PROMPTS = False  # set True to recompute even if artifacts exist\n",
    "\n",
    "# --- validate Excel input path early ---\n",
    "input_xlsx = Path(paths.PATHS.USER_STUDY_XLSX).expanduser().resolve()\n",
    "print(f\"\\nðŸ“Œ USER_STUDY_XLSX = {input_xlsx}\")\n",
    "print(\"   is_file:\", input_xlsx.is_file())\n",
    "\n",
    "if not input_xlsx.is_file():\n",
    "    raise FileNotFoundError(\n",
    "        f\"USER_STUDY_XLSX is not a file: {input_xlsx}\\n\"\n",
    "        \"Fix: set PROJ_ROOT correctly in Cell 0 OR set USER_STUDY_XLSX env var.\"\n",
    "    )\n",
    "\n",
    "# --- run per experiment ---\n",
    "for exp_name, exp_cfg in EXPERIMENTS.items():\n",
    "    feature_mode = exp_cfg[\"feature_mode\"]\n",
    "\n",
    "    if feature_mode == \"map_only\":\n",
    "        print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "        print(\"   (skip) feature_mode=map_only â†’ no prompt embeddings required.\")\n",
    "        continue\n",
    "\n",
    "    # Allow per-experiment override, otherwise use global config default from paths.CFG\n",
    "    prompt_encoder_kind = exp_cfg.get(\"prompt_encoder_kind\", paths.CFG.PROMPT_ENCODER)\n",
    "    CFG_EXP = replace(paths.CFG, PROMPT_ENCODER=str(prompt_encoder_kind))\n",
    "\n",
    "    # âœ… Truly experiment-scoped: store prompt artifacts under each experiment's train_out\n",
    "    prompt_out_dir = Path(exp_cfg[\"train_out\"]) / \"prompt_out\"\n",
    "    prompt_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    emb_npz = prompt_out_dir / PROMPT_EMBEDDINGS_NPZ_NAME\n",
    "    prm_pq  = prompt_out_dir / PROMPTS_PARQUET_NAME\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   feature_mode   : {feature_mode}\")\n",
    "    print(f\"   PROMPT_ENCODER : {CFG_EXP.PROMPT_ENCODER}\")\n",
    "    print(f\"   Output dir     : {prompt_out_dir}\")\n",
    "\n",
    "    if (not FORCE_REBUILD_PROMPTS) and emb_npz.is_file() and prm_pq.is_file():\n",
    "        print(\"   âœ… Prompt embeddings already exist â€” skipping recomputation.\")\n",
    "        meta = {\n",
    "            \"out_dir\": str(prompt_out_dir),\n",
    "            \"embeddings_path\": str(emb_npz),\n",
    "            \"prompts_parquet_path\": str(prm_pq),\n",
    "            \"skipped\": True,\n",
    "        }\n",
    "    else:\n",
    "        meta = run_prompt_embeddings_from_config(\n",
    "            input_path=input_xlsx,\n",
    "            out_dir=prompt_out_dir,\n",
    "            cfg=CFG_EXP,\n",
    "            paths=paths.PATHS,  # sheet/columns/filters come from config\n",
    "            verbosity=PROMPT_EMBED_VERBOSITY_DEFAULT,\n",
    "            l2_normalize=PROMPT_EMBED_L2_NORMALIZE_DEFAULT,\n",
    "            also_save_embeddings_csv=PROMPT_EMBED_SAVE_CSV_DEFAULT,\n",
    "        )\n",
    "        print(\"   âœ… Prompt embeddings completed.\")\n",
    "\n",
    "    prompt_meta_by_experiment[exp_name] = meta\n",
    "\n",
    "print(\"\\nâœ… Prompt embedding step finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c0b51",
   "metadata": {},
   "source": [
    "## Step 3 â€” Map Embeddings (Dynamic, Shared Across Experiments)\n",
    "\n",
    "In this step, we compute **map embeddings** for all GeoJSON tiles that are eligible for the user study.  \n",
    "Because map embeddings depend only on the input map data (not on the prompt encoder), they are computed **once** and stored in a **shared output folder**, then reused across all experiments.\n",
    "\n",
    "### What this step does\n",
    "\n",
    "1. **Filters tiles using the user study Excel**\n",
    "   - Keeps only rows marked as *complete*\n",
    "   - Excludes rows marked as *removed*\n",
    "   - Extracts the set of allowed `tile_id`s (used to select which map folders to embed)\n",
    "\n",
    "2. **Discovers and embeds GeoJSON maps**\n",
    "   - Finds all GeoJSON files under `PATHS.MAPS_ROOT`\n",
    "   - Keeps only those whose `map_id` is in the allowed set\n",
    "   - Counts valid polygons per map to determine a dataset-wide `max_polygons`\n",
    "     (used to normalize the `poly_count` feature safely)\n",
    "\n",
    "3. **Computes map embeddings**\n",
    "   - Uses `norm=\"extent\"` for **dynamic per-map normalization**\n",
    "   - Ensures all vectors have consistent dimensionality\n",
    "   - Skips maps with invalid geometries or degenerate extents\n",
    "\n",
    "4. **Stores dynamic extent references (required for parameter scaling)**\n",
    "   - `extent_diag_m`\n",
    "   - `extent_area_m2`\n",
    "\n",
    "   These are saved alongside embeddings and are later used to convert\n",
    "   normalized parameters (`param_norm`) into real-world units:\n",
    "   - distance operators â†’ meters via `extent_diag_m`\n",
    "   - area operators â†’ mÂ² via `extent_area_m2`\n",
    "\n",
    "5. **Writes outputs once to a shared directory**\n",
    "   - Prevents redundant computation across experiments\n",
    "   - Guarantees every experiment uses the **same map representation**\n",
    "   - Avoids accidental overwrites while keeping artifacts reusable\n",
    "\n",
    "### Why this is important\n",
    "\n",
    "- Ensures **consistent normalization** between training and evaluation  \n",
    "- Provides the necessary per-map reference scales for parameter un-normalization  \n",
    "- Improves reproducibility and efficiency by reusing identical map embeddings  \n",
    "- Supports fair comparison between:\n",
    "  - prompt-only baselines (which ignore map embeddings)\n",
    "  - fused prompt + map hybrids\n",
    "  - different prompt backends (USE vs OpenAI)\n",
    "\n",
    "At the end of this step, the repository contains a self-contained set of map embeddings\n",
    "ready to be concatenated with prompt embeddings in the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ca0c3d8b71fc70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:26:59.687350Z",
     "start_time": "2025-10-27T11:26:19.901557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Map embeddings (shared across all experiments) ===\n",
      "MAPS_ROOT      : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/input/samples/pairs\n",
      "  is_dir       : True\n",
      "USER_STUDY_XLSX: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/userstudy/UserStudy.xlsx\n",
      "  is_file      : True\n",
      "Target dir: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/map_out/shared_extent\n",
      "Artifacts : maps_embeddings.npz | maps.parquet\n",
      "âœ… Map embeddings completed.\n",
      "âœ… Map embedding artifacts ready:\n",
      " - /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/map_out/shared_extent/maps_embeddings.npz\n",
      " - /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/map_out/shared_extent/maps.parquet\n",
      "MAP_EMB_DIR: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/map_out/shared_extent\n",
      "MapEmbeddingRunMeta(n_tiles_allowed=551, n_maps_found=824, n_maps_used=551, max_polygons=731, out_dir='/Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/map_out/shared_extent', embeddings_path='/Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/map_out/shared_extent/maps_embeddings.npz', maps_parquet_path='/Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/map_out/shared_extent/maps.parquet', n_skipped_bad_extent=0, n_skipped_dim_mismatch=0, n_failed_embed=0)\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 3 â€” Map embeddings (shared) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from imgofup.config import paths\n",
    "from imgofup.config.constants import MAP_EMBEDDINGS_NPZ_NAME, MAPS_PARQUET_NAME\n",
    "from imgofup.pipelines.run_map_embeddings import run_map_embeddings_from_config\n",
    "\n",
    "print(\"\\n=== Map embeddings (shared across all experiments) ===\")\n",
    "\n",
    "# --- validate inputs early ---\n",
    "maps_root = Path(paths.PATHS.MAPS_ROOT).expanduser().resolve()\n",
    "xlsx_path = Path(paths.PATHS.USER_STUDY_XLSX).expanduser().resolve()\n",
    "\n",
    "print(\"MAPS_ROOT      :\", maps_root)\n",
    "print(\"  is_dir       :\", maps_root.is_dir())\n",
    "print(\"USER_STUDY_XLSX:\", xlsx_path)\n",
    "print(\"  is_file      :\", xlsx_path.is_file())\n",
    "\n",
    "if not maps_root.is_dir():\n",
    "    raise NotADirectoryError(f\"MAPS_ROOT is not a directory: {maps_root}\")\n",
    "if not xlsx_path.is_file():\n",
    "    raise FileNotFoundError(f\"USER_STUDY_XLSX is not a file: {xlsx_path}\")\n",
    "\n",
    "# Map embeddings do NOT depend on prompt backend, so compute once and reuse.\n",
    "MAP_EMB_DIR = Path(paths.PATHS.MAP_OUT) / \"shared_extent\"\n",
    "MAP_EMB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "maps_npz = MAP_EMB_DIR / MAP_EMBEDDINGS_NPZ_NAME\n",
    "maps_pq  = MAP_EMB_DIR / MAPS_PARQUET_NAME\n",
    "\n",
    "print(\"Target dir:\", MAP_EMB_DIR)\n",
    "print(\"Artifacts :\", maps_npz.name, \"|\", maps_pq.name)\n",
    "\n",
    "# If you changed anything about map embedding logic, set this True once.\n",
    "FORCE_REBUILD_MAPS = False\n",
    "\n",
    "if (not FORCE_REBUILD_MAPS) and maps_npz.is_file() and maps_pq.is_file():\n",
    "    print(\"âœ… Map embeddings already exist â€” skipping recomputation.\")\n",
    "    map_meta = {\"out_dir\": str(MAP_EMB_DIR), \"skipped\": True}\n",
    "else:\n",
    "    map_meta = run_map_embeddings_from_config(\n",
    "        maps_root=maps_root,\n",
    "        input_pattern=paths.PATHS.INPUT_MAPS_PATTERN,\n",
    "\n",
    "        # used for filtering which tiles are considered (you set no filtering below)\n",
    "        user_study_xlsx=xlsx_path,\n",
    "        responses_sheet=paths.PATHS.RESPONSES_SHEET,\n",
    "        tile_id_col=paths.PATHS.TILE_ID_COL,\n",
    "        complete_col=paths.PATHS.COMPLETE_COL,\n",
    "        remove_col=paths.PATHS.REMOVE_COL,\n",
    "\n",
    "        # explicit: shared across all experiments, no filtering\n",
    "        only_complete=False,\n",
    "        exclude_removed=False,\n",
    "\n",
    "        out_dir=MAP_EMB_DIR,\n",
    "        verbosity=1,\n",
    "        norm=\"extent\",\n",
    "    )\n",
    "    print(\"âœ… Map embeddings completed.\")\n",
    "\n",
    "if not maps_npz.is_file():\n",
    "    raise FileNotFoundError(f\"Missing {MAP_EMBEDDINGS_NPZ_NAME} at: {maps_npz}\")\n",
    "if not maps_pq.is_file():\n",
    "    raise FileNotFoundError(f\"Missing {MAPS_PARQUET_NAME} at: {maps_pq}\")\n",
    "\n",
    "print(\"âœ… Map embedding artifacts ready:\")\n",
    "print(\" -\", maps_npz)\n",
    "print(\" -\", maps_pq)\n",
    "print(\"MAP_EMB_DIR:\", MAP_EMB_DIR)\n",
    "print(map_meta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaf32c5",
   "metadata": {},
   "source": [
    "### ðŸ”¢ Inferring Embedding Dimensions (Experiment-Aware)\n",
    "\n",
    "In this step, we **infer embedding dimensionalities directly from the saved embedding files**\n",
    "rather than relying on configuration defaults. Dimensions are inferred **per experiment** to\n",
    "account for different prompt embedding backends (e.g., USE vs OpenAI), while map embeddings\n",
    "are shared across experiments.\n",
    "\n",
    "This ensures:\n",
    "- A **single source of truth** for feature dimensions\n",
    "- **Consistency** between training and evaluation pipelines\n",
    "- Robustness to changes in embedding models or backend configurations\n",
    "- Correct handling of mixed feature modes (prompt-only vs. fused prompt + map)\n",
    "\n",
    "Specifically, we:\n",
    "\n",
    "- Load **prompt embeddings** from  \n",
    "  `PATHS.PROMPT_OUT/<experiment_name>/prompts_embeddings.npz`\n",
    "- Load **map embeddings** from the shared map embedding directory  \n",
    "  `PATHS.MAP_OUT/<shared_folder>/maps_embeddings.npz`\n",
    "- Infer dimensions as follows:\n",
    "  - `PROMPT_DIM` â€” from prompt embeddings (per experiment)\n",
    "  - `MAP_DIM` â€” from map embeddings (shared)\n",
    "  - `FUSED_DIM` â€” computed per experiment:\n",
    "    - `prompt_only` â†’ `PROMPT_DIM`\n",
    "    - `fused` â†’ `MAP_DIM + PROMPT_DIM`\n",
    "\n",
    "If inferred dimensions differ from those defined in the global configuration (`CFG`),\n",
    "the inferred values take precedence for all downstream processing.\n",
    "\n",
    "The inferred dimensions are stored in the experiment registry and used consistently by:\n",
    "- feature preprocessing\n",
    "- operator classification\n",
    "- parameter regression\n",
    "- evaluation and inference\n",
    "\n",
    "This guarantees that all downstream models operate on **correctly shaped feature vectors**\n",
    "and that comparisons between experiments remain valid and reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e8cdf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Inferred MAP_DIM from shared maps: 165\n",
      "\n",
      "âœ… Inferred dims per experiment:\n",
      " - openai_prompt_only | mode=prompt_only    | MAP_DIM=   0 | PROMPT_DIM=1536 | FUSED_DIM=1536\n",
      " - use_prompt_only    | mode=prompt_only    | MAP_DIM=   0 | PROMPT_DIM= 512 | FUSED_DIM= 512\n",
      " - map_only           | mode=map_only       | MAP_DIM= 165 | PROMPT_DIM=   0 | FUSED_DIM= 165\n",
      " - use_map            | mode=prompt_plus_map | MAP_DIM= 165 | PROMPT_DIM= 512 | FUSED_DIM= 677\n",
      " - openai_map         | mode=prompt_plus_map | MAP_DIM= 165 | PROMPT_DIM=1536 | FUSED_DIM=1701\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 4 â€” Infer embedding dimensions (multi-experiment, incl. map-only) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from imgofup.config.constants import MAP_EMBEDDINGS_NPZ_NAME, PROMPT_EMBEDDINGS_NPZ_NAME\n",
    "\n",
    "def _infer_dim_from_npz(npz_path: Path) -> int:\n",
    "    npz_path = Path(npz_path).expanduser().resolve()\n",
    "    if not npz_path.is_file():\n",
    "        raise FileNotFoundError(f\"Missing embeddings file: {npz_path}\")\n",
    "    with np.load(npz_path, allow_pickle=True) as z:\n",
    "        if \"E\" not in z:\n",
    "            raise ValueError(f\"{npz_path} missing array 'E'\")\n",
    "        E = z[\"E\"]\n",
    "    if E.ndim != 2 or E.shape[1] <= 0:\n",
    "        raise ValueError(f\"Invalid embedding matrix in {npz_path}: shape={E.shape}\")\n",
    "    return int(E.shape[1])\n",
    "\n",
    "# -------------------------------\n",
    "# Map dim (shared across all experiments)\n",
    "# -------------------------------\n",
    "maps_npz = Path(MAP_EMB_DIR) / MAP_EMBEDDINGS_NPZ_NAME\n",
    "MAP_DIM_INF = _infer_dim_from_npz(maps_npz)\n",
    "print(\"âœ… Inferred MAP_DIM from shared maps:\", MAP_DIM_INF)\n",
    "\n",
    "# -------------------------------\n",
    "# Prompt dim per experiment + final input dim per experiment\n",
    "# -------------------------------\n",
    "dims_by_experiment = {}\n",
    "\n",
    "PROMPT_BASED_MODES = {\"prompt_only\", \"prompt_plus_map\"}\n",
    "\n",
    "for exp_name, exp_cfg in EXPERIMENTS.items():\n",
    "    feature_mode = str(exp_cfg[\"feature_mode\"]).strip().lower()\n",
    "\n",
    "    # infer prompt dim if this mode uses prompts\n",
    "    PROMPT_DIM_INF = 0\n",
    "    if feature_mode in PROMPT_BASED_MODES:\n",
    "        # âœ… new location (from revised Cell 2)\n",
    "        prm_npz = Path(exp_cfg[\"train_out\"]) / \"prompt_out\" / PROMPT_EMBEDDINGS_NPZ_NAME\n",
    "        PROMPT_DIM_INF = _infer_dim_from_npz(prm_npz)\n",
    "\n",
    "    if feature_mode == \"prompt_only\":\n",
    "        map_dim = 0\n",
    "        prompt_dim = PROMPT_DIM_INF\n",
    "        fused_dim = PROMPT_DIM_INF\n",
    "\n",
    "    elif feature_mode == \"map_only\":\n",
    "        map_dim = MAP_DIM_INF\n",
    "        prompt_dim = 0\n",
    "        fused_dim = MAP_DIM_INF\n",
    "\n",
    "    elif feature_mode == \"prompt_plus_map\":\n",
    "        map_dim = MAP_DIM_INF\n",
    "        prompt_dim = PROMPT_DIM_INF\n",
    "        fused_dim = MAP_DIM_INF + PROMPT_DIM_INF\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown feature_mode for {exp_name}: {feature_mode}\")\n",
    "\n",
    "    exp_cfg[\"map_dim\"] = int(map_dim)\n",
    "    exp_cfg[\"prompt_dim\"] = int(prompt_dim)\n",
    "    exp_cfg[\"fused_dim\"] = int(fused_dim)\n",
    "\n",
    "    dims_by_experiment[exp_name] = {\n",
    "        \"feature_mode\": feature_mode,\n",
    "        \"MAP_DIM\": int(map_dim),\n",
    "        \"PROMPT_DIM\": int(prompt_dim),\n",
    "        \"FUSED_DIM\": int(fused_dim),\n",
    "    }\n",
    "\n",
    "print(\"\\nâœ… Inferred dims per experiment:\")\n",
    "for exp_name, d in dims_by_experiment.items():\n",
    "    print(\n",
    "        f\" - {exp_name:18s} | mode={d['feature_mode']:14s} | \"\n",
    "        f\"MAP_DIM={d['MAP_DIM']:4d} | PROMPT_DIM={d['PROMPT_DIM']:4d} | FUSED_DIM={d['FUSED_DIM']:4d}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd186319e89f445",
   "metadata": {},
   "source": [
    "### ðŸ”— Feature Construction (Prompt-Only vs. Fused Map + Prompt Embeddings)\n",
    "\n",
    "In this step, we construct the final feature matrices used for training and evaluation by\n",
    "aligning prompt embeddings with map embeddings and exporting **experiment-scoped** artifacts.\n",
    "\n",
    "#### What this step does (per experiment)\n",
    "\n",
    "- Loads **prompt embeddings** from  \n",
    "  `PATHS.PROMPT_OUT/<experiment_name>/prompts_embeddings.npz`\n",
    "- Loads **map embeddings** from the shared map embedding directory  \n",
    "  `PATHS.MAP_OUT/<shared_folder>/maps_embeddings.npz`\n",
    "- Aligns samples via the authoritative pairing table (`prompts.parquet` â†’ `map_id/tile_id` + `prompt_id`)\n",
    "- Merges **dynamic map extent metadata** (e.g., `extent_diag_m`, `extent_area_m2`) from `maps.parquet`\n",
    "  so downstream regression can convert normalized parameters into real-world units.\n",
    "\n",
    "#### Feature modes supported\n",
    "\n",
    "- **`prompt_only`**  \n",
    "  Uses only prompt vectors:  \n",
    "  `X = prompt_embedding`\n",
    "\n",
    "- **`fused`**  \n",
    "  Concatenates map and prompt vectors:  \n",
    "  `X = [map_embedding | prompt_embedding]`\n",
    "\n",
    "#### Outputs written per experiment\n",
    "\n",
    "All artifacts are saved into the experimentâ€™s `train_out` directory (to prevent overwrites):\n",
    "\n",
    "- `X_prompt.npy` or `X_concat.npy` â€” final feature matrix (depending on feature mode)\n",
    "- `train_pairs.parquet` â€” aligned metadata (including `operator`, `param_value`, and extent references)\n",
    "- `meta.json` â€” provenance (sources, options) and shape information\n",
    "\n",
    "#### Why this design\n",
    "\n",
    "- Keeps **training and evaluation perfectly aligned** by exporting a single, consistent pairing table\n",
    "- Avoids hard-coded dimensions by relying on the saved embedding files\n",
    "- Supports **multiple experiments side-by-side** without overwriting artifacts\n",
    "- Enables **dynamic extent-aware** parameter regression (meters / mÂ² scaling) downstream\n",
    "- Ensures fair comparison: prompt-only baselines vs. fused map+prompt models\n",
    "\n",
    "After this step, each experiment has a complete, self-contained dataset ready for:\n",
    "1. Operator classification  \n",
    "2. Per-operator parameter regression  \n",
    "3. End-to-end evaluation in the evaluation notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa2b07a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Building feature matrices for all experiments ===\n",
      "\n",
      "ðŸ§ª Experiment: openai_prompt_only\n",
      "   Feature mode : prompt_only\n",
      "   Prompt out   : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_openai_prompt_only/prompt_out\n",
      "   Map out      : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/map_out/shared_extent\n",
      "   Train out    : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_openai_prompt_only\n",
      "   âœ… Feature construction completed.\n",
      "\n",
      "ðŸ§ª Experiment: use_prompt_only\n",
      "   Feature mode : prompt_only\n",
      "   Prompt out   : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_use_prompt_only/prompt_out\n",
      "   Map out      : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/map_out/shared_extent\n",
      "   Train out    : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_use_prompt_only\n",
      "   âœ… Feature construction completed.\n",
      "\n",
      "ðŸ§ª Experiment: map_only\n",
      "   Feature mode : map_only\n",
      "   Prompt out   : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_map_only/prompt_out\n",
      "   Pairs parquet: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_use_prompt_only/prompt_out/prompts.parquet  (shared)\n",
      "   Map out      : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/map_out/shared_extent\n",
      "   Train out    : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_map_only\n",
      "   âœ… Feature construction completed.\n",
      "\n",
      "ðŸ§ª Experiment: use_map\n",
      "   Feature mode : prompt_plus_map\n",
      "   Prompt out   : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_use_map/prompt_out\n",
      "   Map out      : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/map_out/shared_extent\n",
      "   Train out    : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_use_map\n",
      "   âœ… Feature construction completed.\n",
      "\n",
      "ðŸ§ª Experiment: openai_map\n",
      "   Feature mode : prompt_plus_map\n",
      "   Prompt out   : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_openai_map/prompt_out\n",
      "   Map out      : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/map_out/shared_extent\n",
      "   Train out    : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_openai_map\n",
      "   âœ… Feature construction completed.\n",
      "\n",
      "âœ… All feature construction finished.\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 5 â€” Feature construction (multi-experiment) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from imgofup.pipelines.run_concat_features import run_concat_features_from_dirs\n",
    "from imgofup.config.constants import PROMPTS_PARQUET_NAME\n",
    "\n",
    "print(\"\\n=== Building feature matrices for all experiments ===\")\n",
    "\n",
    "concat_meta_by_experiment = {}\n",
    "\n",
    "# Because prompt_id changed, you should rebuild features at least once.\n",
    "FORCE_REBUILD_FEATURES = True  # set False later\n",
    "\n",
    "# Choose a canonical source of prompts.parquet (pairs table) for map_only.\n",
    "# Any prompt-based experiment works as long as it produced prompts.parquet.\n",
    "PAIRS_SOURCE_EXP = \"use_prompt_only\"\n",
    "PAIRS_PARQUET_CANON = (\n",
    "    Path(EXPERIMENTS[PAIRS_SOURCE_EXP][\"train_out\"]) / \"prompt_out\" / PROMPTS_PARQUET_NAME\n",
    ")\n",
    "\n",
    "if not PAIRS_PARQUET_CANON.is_file():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Expected prompts parquet for map_only at:\\n  {PAIRS_PARQUET_CANON}\\n\"\n",
    "        f\"Run Cell 2 (prompt embeddings) for '{PAIRS_SOURCE_EXP}' first.\"\n",
    "    )\n",
    "\n",
    "for exp_name, exp_cfg in EXPERIMENTS.items():\n",
    "    feature_mode = exp_cfg[\"feature_mode\"]\n",
    "\n",
    "    train_out_dir = Path(exp_cfg[\"train_out\"])\n",
    "    train_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    map_out_dir = Path(MAP_EMB_DIR)\n",
    "\n",
    "    # âœ… NEW location (from revised Cell 2)\n",
    "    prompt_out_dir = train_out_dir / \"prompt_out\"\n",
    "\n",
    "    # For map_only we still need a prompts table, but not prompt embeddings\n",
    "    pairs_parquet = PAIRS_PARQUET_CANON if feature_mode == \"map_only\" else None\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   Feature mode : {feature_mode}\")\n",
    "    print(f\"   Prompt out   : {prompt_out_dir}\")\n",
    "    if pairs_parquet is not None:\n",
    "        print(f\"   Pairs parquet: {pairs_parquet}  (shared)\")\n",
    "    print(f\"   Map out      : {map_out_dir}\")\n",
    "    print(f\"   Train out    : {train_out_dir}\")\n",
    "\n",
    "    # Expected outputs for this experiment\n",
    "    X_expected = train_out_dir / f\"X_{exp_name}.npy\"\n",
    "    pairs_expected = train_out_dir / f\"train_pairs_{exp_name}.parquet\"\n",
    "\n",
    "    if (not FORCE_REBUILD_FEATURES) and X_expected.is_file() and pairs_expected.is_file():\n",
    "        print(\"   âœ… Features already exist â€” skipping recomputation.\")\n",
    "        meta = {\n",
    "            \"skipped\": True,\n",
    "            \"X_path\": str(X_expected),\n",
    "            \"pairs_path\": str(pairs_expected),\n",
    "        }\n",
    "    else:\n",
    "        meta = run_concat_features_from_dirs(\n",
    "            prompt_out_dir=prompt_out_dir,\n",
    "            map_out_dir=map_out_dir,\n",
    "            out_dir=train_out_dir,\n",
    "            exp_name=exp_name,\n",
    "            feature_mode=feature_mode,\n",
    "            verbosity=1,\n",
    "            prompt_id_width=4,\n",
    "            pairs_parquet=pairs_parquet,\n",
    "        )\n",
    "        print(\"   âœ… Feature construction completed.\")\n",
    "\n",
    "    concat_meta_by_experiment[exp_name] = meta\n",
    "\n",
    "print(\"\\nâœ… All feature construction finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5142d4b68c273d37",
   "metadata": {},
   "source": [
    "## Step 6 â€” Load Training Data and Build Normalized Regression Target (Experiment-Aware)\n",
    "\n",
    "In this step, we load the **experiment-specific training data** produced by the feature\n",
    "construction stage and build the final learning targets for both classification and regression.\n",
    "\n",
    "Because this notebook runs **multiple experiments**, the same procedure is applied\n",
    "**independently for each experiment**, using its own `train_out` directory.\n",
    "\n",
    "### What happens here (per experiment)\n",
    "\n",
    "1. **Load the feature matrix**\n",
    "   - `prompt_only` experiments load prompt-only features (e.g., `X_prompt.npy`)\n",
    "   - fused experiments load concatenated features (e.g., `X_concat.npy`)\n",
    "\n",
    "2. **Load the paired metadata table**\n",
    "   - `train_pairs.parquet` containing aligned `(map_id, prompt_id)` rows and dynamic extent references\n",
    "\n",
    "3. **Attach labels and apply consistent filtering**\n",
    "   - Ensures only valid user study rows are included (e.g., `complete == True`, `remove == False`)\n",
    "   - Ensures `operator` and `param_value` are present (and prompt text if required)\n",
    "\n",
    "4. **Validate dynamic extent references**\n",
    "   - Confirms the presence of per-map reference scales required for normalization:\n",
    "     - `extent_diag_m`\n",
    "     - `extent_area_m2`\n",
    "\n",
    "5. **Compute the normalized regression target `param_norm`**\n",
    "   Normalization depends on the operator group:\n",
    "\n",
    "   - **Distance-based operators** (`aggregate`, `displace`, `simplify`):  \n",
    "     `param_norm = param_value / extent_diag_m`\n",
    "\n",
    "   - **Area-based operators** (`select`):  \n",
    "     `param_norm = param_value / extent_area_m2`\n",
    "\n",
    "### Why this normalization is used\n",
    "\n",
    "This step converts parameters from heterogeneous, map-scale-dependent units into a\n",
    "scale-aware normalized target. It allows per-operator regressors to generalize across\n",
    "maps of different extents while preserving physical meaning during inference\n",
    "(when `param_norm` is converted back to meters or mÂ² using the same extent references).\n",
    "\n",
    "### Outputs\n",
    "\n",
    "For each experiment, we obtain:\n",
    "\n",
    "- `X` â€” feature matrix aligned with labels  \n",
    "- `df` â€” cleaned metadata table including `operator`, `param_value`, `param_norm`, and extent references  \n",
    "\n",
    "These outputs feed directly into the subsequent training stages:\n",
    "1. Operator classification  \n",
    "2. Per-operator parameter regression  \n",
    "3. End-to-end evaluation across experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a494fd27dfe7681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading training data for all experiments (unified loader) ===\n",
      "\n",
      "ðŸ§ª Experiment: openai_prompt_only\n",
      "   train_out : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_openai_prompt_only\n",
      "   mode      : prompt_only\n",
      "   âœ… Loaded: X=(562, 1536) | df=(562, 15)\n",
      "   Operators: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "ðŸ§ª Experiment: use_prompt_only\n",
      "   train_out : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_use_prompt_only\n",
      "   mode      : prompt_only\n",
      "   âœ… Loaded: X=(562, 512) | df=(562, 15)\n",
      "   Operators: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "ðŸ§ª Experiment: map_only\n",
      "   train_out : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_map_only\n",
      "   mode      : map_only\n",
      "   âœ… Loaded: X=(562, 165) | df=(562, 15)\n",
      "   Operators: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "ðŸ§ª Experiment: use_map\n",
      "   train_out : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_use_map\n",
      "   mode      : prompt_plus_map\n",
      "   âœ… Loaded: X=(562, 677) | df=(562, 15)\n",
      "   Operators: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "ðŸ§ª Experiment: openai_map\n",
      "   train_out : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_openai_map\n",
      "   mode      : prompt_plus_map\n",
      "   âœ… Loaded: X=(562, 1701) | df=(562, 15)\n",
      "   Operators: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "First loaded experiment: openai_prompt_only\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 6 â€” Load training data + compute param_norm (collision-proof) =====================\n",
    "\n",
    "from dataclasses import replace\n",
    "from pathlib import Path\n",
    "\n",
    "from imgofup.config import paths\n",
    "from imgofup.datasets.load_training_data import load_training_data_with_dynamic_param_norm\n",
    "\n",
    "TRAIN_DATA = {}\n",
    "\n",
    "print(\"\\n=== Loading training data for all experiments (unified loader) ===\")\n",
    "\n",
    "for exp_name, exp_cfg in EXPERIMENTS.items():\n",
    "    train_out_dir = Path(exp_cfg[\"train_out\"]).expanduser().resolve()\n",
    "    if not train_out_dir.is_dir():\n",
    "        raise FileNotFoundError(f\"Missing train_out directory for {exp_name}: {train_out_dir}\")\n",
    "\n",
    "    feature_mode = str(exp_cfg[\"feature_mode\"]).strip().lower()\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   train_out : {train_out_dir}\")\n",
    "    print(f\"   mode      : {feature_mode}\")\n",
    "\n",
    "    # âœ… IMPORTANT: ProjectPaths fields are Paths, so pass Path (not str)\n",
    "    PATHS_EXP = replace(paths.PATHS, TRAIN_OUT=train_out_dir)\n",
    "\n",
    "    # âœ… Only require text when prompts are part of the feature space\n",
    "    require_text = feature_mode in {\"prompt_only\", \"prompt_plus_map\"}\n",
    "\n",
    "    data = load_training_data_with_dynamic_param_norm(\n",
    "        exp_name=exp_name,\n",
    "        feature_mode=feature_mode,\n",
    "        paths=PATHS_EXP,\n",
    "        cfg=paths.CFG,\n",
    "        distance_ops=paths.DISTANCE_OPS,\n",
    "        area_ops=paths.AREA_OPS,\n",
    "        require_text=require_text,\n",
    "    )\n",
    "\n",
    "    X = data.X\n",
    "    df = data.df\n",
    "\n",
    "    print(f\"   âœ… Loaded: X={X.shape} | df={df.shape}\")\n",
    "    if PATHS_EXP.OPERATOR_COL in df.columns:\n",
    "        print(\"   Operators:\", sorted(df[PATHS_EXP.OPERATOR_COL].dropna().unique().tolist()))\n",
    "\n",
    "    TRAIN_DATA[exp_name] = {\"X\": X, \"df\": df, \"paths\": PATHS_EXP}\n",
    "\n",
    "first_key = next(iter(TRAIN_DATA.keys()))\n",
    "print(\"\\nFirst loaded experiment:\", first_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997cab7",
   "metadata": {},
   "source": [
    "## Step 7 â€” Train/Validation/Test Split (Shared, Leakage-Free by Map)\n",
    "\n",
    "In this step, we construct a **reproducible and fair** train/validation/test split that is\n",
    "**shared across all experiments** (prompt-only, USE + map, OpenAI + map).  \n",
    "The split is computed **once** and then applied to every experiment to ensure that\n",
    "performance differences are attributable solely to the feature representation.\n",
    "\n",
    "---\n",
    "\n",
    "### Constraints enforced\n",
    "\n",
    "- **No leakage by map**  \n",
    "  The same `map_id` never appears in more than one split (train / validation / test).\n",
    "\n",
    "- **Multi-prompt maps are forced into TRAIN**  \n",
    "  If a map has multiple prompts, *all* corresponding samples are assigned to the\n",
    "  training set.  \n",
    "  As a result, validation and test sets contain **single-prompt maps only**.\n",
    "\n",
    "- **Consistency across experiments**  \n",
    "  The exact same samples (identified by `(map_id, prompt_id)`) are used for\n",
    "  train/validation/test in every experiment.\n",
    "\n",
    "---\n",
    "\n",
    "### Stratification strategy\n",
    "\n",
    "To obtain balanced splits while respecting the above constraints, stratification is applied as:\n",
    "\n",
    "- Primary: `operator Ã— intensity` (if sufficient samples exist), otherwise\n",
    "- Fallback: `operator` only (automatically selected if finer stratification is infeasible)\n",
    "\n",
    "---\n",
    "\n",
    "### Coverage requirement\n",
    "\n",
    "Each split is required to contain **all operators** in the fixed class set:\n",
    "\n",
    "- `simplify`\n",
    "- `select`\n",
    "- `aggregate`\n",
    "- `displace`\n",
    "\n",
    "This guarantees that classification and regression models can be trained and evaluated\n",
    "for every operator.\n",
    "\n",
    "---\n",
    "\n",
    "### Outputs\n",
    "\n",
    "- A single shared split definition is saved to disk as:  \n",
    "  `splits_shared.json`\n",
    "- For each experiment, the split is applied to slice:\n",
    "  - `X` â€” the feature matrix\n",
    "  - `df` â€” the aligned metadata table\n",
    "\n",
    "The resulting subsets (`train`, `val`, `test`) are then used in all downstream\n",
    "training and evaluation steps.\n",
    "\n",
    "---\n",
    "\n",
    "This design ensures:\n",
    "- **Leakage-free evaluation**\n",
    "- **Fair, apples-to-apples comparison** between experiments\n",
    "- **Reproducibility**, since the split is deterministic and persisted to disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bc0897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Computing shared split using reference experiment: use_prompt_only ===\n",
      "ref_df: (562, 16) | ref_X: (562, 512)\n",
      "Saving split to: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out/splits/splits_shared.json\n",
      "=== DATASET SUMMARY ===\n",
      "Total rows (prompts): 562\n",
      "Unique maps: 399\n",
      "Multi-prompt maps (>1 prompt): 22\n",
      "Single-prompt maps (=1 prompt): 377\n",
      "\n",
      "Top 10 maps by prompt count:\n",
      "map_id\n",
      "1646    30\n",
      "1304    29\n",
      "1755    26\n",
      "1532    13\n",
      "0127    10\n",
      "0168     8\n",
      "0142     7\n",
      "0078     6\n",
      "0080     6\n",
      "0001     6\n",
      "dtype: int64\n",
      "\n",
      "âœ… Saved splits to /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out/splits/splits_shared.json\n",
      "\n",
      "âœ… Shared split created:\n",
      "   Train keys: 448 | Val keys: 57 | Test keys: 57\n",
      "   Saved to  : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out/splits/splits_shared.json\n",
      "\n",
      "ðŸ§ª openai_prompt_only\n",
      "Rows -> Train: (448, 1536) Val: (57, 1536) Test: (57, 1536)\n",
      "\n",
      "ðŸ§ª use_prompt_only\n",
      "Rows -> Train: (448, 512) Val: (57, 512) Test: (57, 512)\n",
      "\n",
      "ðŸ§ª map_only\n",
      "Rows -> Train: (448, 165) Val: (57, 165) Test: (57, 165)\n",
      "\n",
      "ðŸ§ª use_map\n",
      "Rows -> Train: (448, 677) Val: (57, 677) Test: (57, 677)\n",
      "\n",
      "ðŸ§ª openai_map\n",
      "Rows -> Train: (448, 1701) Val: (57, 1701) Test: (57, 1701)\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 7 â€” Shared Train/Val/Test Split (fair across experiments, incl. map-only) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from imgofup.config import paths\n",
    "from imgofup.config.constants import MAPS_ID_COL, PROMPTS_PROMPT_ID_COL\n",
    "from imgofup.datasets.splitting import make_splits_multi_prompt_to_train\n",
    "\n",
    "FIXED_CLASSES = [\"simplify\", \"select\", \"aggregate\", \"displace\"]\n",
    "USE_INTENSITY_FOR_STRAT = True\n",
    "\n",
    "OP_COL  = paths.PATHS.OPERATOR_COL\n",
    "INT_COL = paths.PATHS.INTENSITY_COL\n",
    "\n",
    "MAP_ID_COL = MAPS_ID_COL              # \"map_id\"\n",
    "PROMPT_ID_COL = PROMPTS_PROMPT_ID_COL # \"prompt_id\"\n",
    "\n",
    "# Where to save ONE shared split (used by all experiments)\n",
    "SPLITS_DIR = Path(paths.PATHS.SPLIT_OUT).expanduser().resolve()\n",
    "SPLITS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "split_path = SPLITS_DIR / \"splits_shared.json\"\n",
    "\n",
    "# -------------------------------\n",
    "# Choose a stable reference experiment\n",
    "# Prefer prompt-based; avoid map_only if possible.\n",
    "# -------------------------------\n",
    "preferred_order = [\"use_prompt_only\", \"use_map\", \"openai_map\", \"map_only\"]\n",
    "ref_exp = next((name for name in preferred_order if name in TRAIN_DATA), None)\n",
    "if ref_exp is None:\n",
    "    ref_exp = next(iter(TRAIN_DATA.keys()))\n",
    "\n",
    "ref_df = TRAIN_DATA[ref_exp][\"df\"].copy().reset_index(drop=True)\n",
    "ref_X  = TRAIN_DATA[ref_exp][\"X\"]\n",
    "\n",
    "# Must have keys for stable matching across experiments\n",
    "if not {MAP_ID_COL, PROMPT_ID_COL}.issubset(ref_df.columns):\n",
    "    raise ValueError(f\"Expected columns {{{MAP_ID_COL!r},{PROMPT_ID_COL!r}}} in df for split mapping.\")\n",
    "\n",
    "# Must have operator for stratification constraints\n",
    "if OP_COL not in ref_df.columns:\n",
    "    raise ValueError(f\"Reference df missing operator column '{OP_COL}'. Cannot build stratified split.\")\n",
    "\n",
    "# Build a stable key per row for mapping splits across experiments\n",
    "# (pad map_id for stability; keep prompt_id as-is)\n",
    "ref_df[\"row_key\"] = (\n",
    "    ref_df[MAP_ID_COL].astype(str).str.zfill(4) + \"::\" + ref_df[PROMPT_ID_COL].astype(str)\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Computing shared split using reference experiment: {ref_exp} ===\")\n",
    "print(\"ref_df:\", ref_df.shape, \"| ref_X:\", ref_X.shape)\n",
    "print(\"Saving split to:\", split_path)\n",
    "\n",
    "split = make_splits_multi_prompt_to_train(\n",
    "    df=ref_df,\n",
    "    X=ref_X,\n",
    "    op_col=OP_COL,\n",
    "    intensity_col=INT_COL if (USE_INTENSITY_FOR_STRAT and INT_COL in ref_df.columns) else None,\n",
    "    map_id_col=MAP_ID_COL,\n",
    "    fixed_classes=FIXED_CLASSES,\n",
    "    use_intensity_for_strat=USE_INTENSITY_FOR_STRAT,\n",
    "    seed=int(paths.CFG.SEED),\n",
    "    val_ratio=float(paths.CFG.VAL_RATIO),\n",
    "    test_ratio=float(paths.CFG.TEST_RATIO),\n",
    "    max_attempts=500,\n",
    "    save_splits_json=split_path,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "train_idx_ref, val_idx_ref, test_idx_ref = split.train_idx, split.val_idx, split.test_idx\n",
    "\n",
    "# Convert indices -> row_key sets (transfer across experiments)\n",
    "train_keys = set(ref_df.loc[train_idx_ref, \"row_key\"].tolist())\n",
    "val_keys   = set(ref_df.loc[val_idx_ref,   \"row_key\"].tolist()) if len(val_idx_ref) else set()\n",
    "test_keys  = set(ref_df.loc[test_idx_ref,  \"row_key\"].tolist()) if len(test_idx_ref) else set()\n",
    "\n",
    "# Sanity: no overlap\n",
    "assert train_keys.isdisjoint(val_keys)\n",
    "assert train_keys.isdisjoint(test_keys)\n",
    "assert val_keys.isdisjoint(test_keys)\n",
    "\n",
    "print(\"\\nâœ… Shared split created:\")\n",
    "print(f\"   Train keys: {len(train_keys)} | Val keys: {len(val_keys)} | Test keys: {len(test_keys)}\")\n",
    "print(f\"   Saved to  : {split_path}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Apply the SAME split to every experiment by mapping row_key -> positional indices\n",
    "# -------------------------------\n",
    "SPLITS = {}  # exp_name -> dict with X_train/X_val/X_test and df_train/df_val/df_test\n",
    "\n",
    "needed_keys = train_keys | val_keys | test_keys\n",
    "\n",
    "for exp_name, pack in TRAIN_DATA.items():\n",
    "    df = pack[\"df\"].copy().reset_index(drop=True)\n",
    "    X  = pack[\"X\"]\n",
    "\n",
    "    if not {MAP_ID_COL, PROMPT_ID_COL}.issubset(df.columns):\n",
    "        raise ValueError(f\"Experiment '{exp_name}' df missing {MAP_ID_COL}/{PROMPT_ID_COL} needed for split mapping.\")\n",
    "\n",
    "    df[\"row_key\"] = df[MAP_ID_COL].astype(str).str.zfill(4) + \"::\" + df[PROMPT_ID_COL].astype(str)\n",
    "\n",
    "    # Strict coverage check: all experiments must share the same row universe\n",
    "    missing = needed_keys - set(df[\"row_key\"].tolist())\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"Experiment '{exp_name}' is missing {len(missing)} rows from the shared split \"\n",
    "            f\"(first few: {list(sorted(missing))[:5]}).\\n\"\n",
    "            \"This usually means the pairs table differs between experiments.\\n\"\n",
    "            \"Fix: ensure map_only uses the same prompts.parquet as the reference experiment, \"\n",
    "            \"and ensure concat produced the same (map_id,prompt_id) universe.\"\n",
    "        )\n",
    "\n",
    "    # positional indices (0..n-1) after reset_index(drop=True)\n",
    "    train_idx = df.index[df[\"row_key\"].isin(train_keys)].to_numpy()\n",
    "    val_idx   = df.index[df[\"row_key\"].isin(val_keys)].to_numpy() if val_keys else df.index[:0].to_numpy()\n",
    "    test_idx  = df.index[df[\"row_key\"].isin(test_keys)].to_numpy() if test_keys else df.index[:0].to_numpy()\n",
    "\n",
    "    X_train, X_val, X_test = X[train_idx], X[val_idx], X[test_idx]\n",
    "    df_train = df.loc[train_idx].reset_index(drop=True)\n",
    "    df_val   = df.loc[val_idx].reset_index(drop=True)\n",
    "    df_test  = df.loc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    SPLITS[exp_name] = {\n",
    "        \"train_idx\": train_idx,\n",
    "        \"val_idx\": val_idx,\n",
    "        \"test_idx\": test_idx,\n",
    "        \"X_train\": X_train, \"X_val\": X_val, \"X_test\": X_test,\n",
    "        \"df_train\": df_train, \"df_val\": df_val, \"df_test\": df_test,\n",
    "    }\n",
    "\n",
    "    print(f\"\\nðŸ§ª {exp_name}\")\n",
    "    print(\"Rows -> Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187473d",
   "metadata": {},
   "source": [
    "## Step 8 â€” Modality-Aware Preprocessing (Experiment-Aware)\n",
    "\n",
    "This step applies preprocessing tailored to the input modalities **separately for each\n",
    "experiment**, using **training data only** to fit preprocessing parameters.  \n",
    "A preprocessing bundle is saved per experiment so the exact same transformations can be\n",
    "reused during evaluation and inference.\n",
    "\n",
    "---\n",
    "\n",
    "### Prompt embeddings (all feature modes)\n",
    "\n",
    "Prompt vectors are normalized using **row-wise L2 normalization** to ensure consistent scale\n",
    "across samples and embedding backends (e.g., USE vs OpenAI).\n",
    "\n",
    "---\n",
    "\n",
    "### Map embeddings (only for fused prompt + map experiments)\n",
    "\n",
    "When the feature mode includes map vectors (`prompt_plus_map` / fused), the map block is\n",
    "processed using a robust pipeline:\n",
    "\n",
    "1. Replace non-finite values (`Â±inf`) with `NaN`\n",
    "2. Impute missing values using the **median** (fit on training data only)\n",
    "3. Clip each feature to training-set **5thâ€“95th percentiles** to reduce outlier impact\n",
    "4. Drop zero-variance (or near-constant) features based on training data\n",
    "5. Apply **RobustScaler** using quantile range **(5, 95)**\n",
    "\n",
    "The prompt block remains L2-normalized and is concatenated with the processed map block.\n",
    "\n",
    "---\n",
    "\n",
    "### Outputs (per experiment)\n",
    "\n",
    "For each experiment we obtain:\n",
    "\n",
    "- `X_train_s`, `X_val_s`, `X_test_s` â€” preprocessed matrices ready for training\n",
    "\n",
    "A preprocessing bundle is saved into the experimentâ€™s model output folder as:\n",
    "\n",
    "- `preproc.joblib`\n",
    "\n",
    "This ensures the exact same preprocessing can be reused for:\n",
    "- reproducible training\n",
    "- consistent evaluation across experiments\n",
    "- deployment-time inference (operator + parameter prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41942b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fitting modality-aware preprocessing per experiment ===\n",
      "\n",
      "ðŸ§ª Experiment: openai_prompt_only\n",
      "   Feature mode : prompt_only -> preproc_mode=prompt_only\n",
      "   map_dim      : 0\n",
      "   prompt_dim   : 1536\n",
      "   Save preproc : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_openai_prompt_only/preproc.joblib\n",
      "   âœ… Preprocessing complete.\n",
      "   Shapes: (448, 1536) (57, 1536) (57, 1536)\n",
      "\n",
      "ðŸ§ª Experiment: use_prompt_only\n",
      "   Feature mode : prompt_only -> preproc_mode=prompt_only\n",
      "   map_dim      : 0\n",
      "   prompt_dim   : 512\n",
      "   Save preproc : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_use_prompt_only/preproc.joblib\n",
      "   âœ… Preprocessing complete.\n",
      "   Shapes: (448, 512) (57, 512) (57, 512)\n",
      "\n",
      "ðŸ§ª Experiment: map_only\n",
      "   Feature mode : map_only -> preproc_mode=prompt_plus_map\n",
      "   map_dim      : 165\n",
      "   prompt_dim   : 0\n",
      "   Save preproc : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_map_only/preproc.joblib\n",
      "   âœ… Preprocessing complete.\n",
      "   Shapes: (448, 165) (57, 165) (57, 165)\n",
      "\n",
      "ðŸ§ª Experiment: use_map\n",
      "   Feature mode : prompt_plus_map -> preproc_mode=prompt_plus_map\n",
      "   map_dim      : 165\n",
      "   prompt_dim   : 512\n",
      "   Save preproc : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_use_map/preproc.joblib\n",
      "   âœ… Preprocessing complete.\n",
      "   Shapes: (448, 677) (57, 677) (57, 677)\n",
      "\n",
      "ðŸ§ª Experiment: openai_map\n",
      "   Feature mode : prompt_plus_map -> preproc_mode=prompt_plus_map\n",
      "   map_dim      : 165\n",
      "   prompt_dim   : 1536\n",
      "   Save preproc : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_openai_map/preproc.joblib\n",
      "   âœ… Preprocessing complete.\n",
      "   Shapes: (448, 1701) (57, 1701) (57, 1701)\n",
      "\n",
      "âœ… All preprocessing finished.\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 8 â€” Modality-aware preprocessing (per experiment, incl. map-only) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from imgofup.preprocessing.preprocessing import fit_transform_modality_preproc\n",
    "from imgofup.config.constants import (\n",
    "    MAP_CLIP_Q_DEFAULT,\n",
    "    MAP_IMPUTE_STRATEGY_DEFAULT,\n",
    "    MAP_ROBUST_QRANGE_DEFAULT,\n",
    "    MAP_VAR_EPS_DEFAULT,\n",
    ")\n",
    "\n",
    "PREPROC = {}  # exp_name -> dict with scaled arrays + bundle path\n",
    "\n",
    "print(\"\\n=== Fitting modality-aware preprocessing per experiment ===\")\n",
    "\n",
    "def _to_preproc_mode(feature_mode: str) -> str:\n",
    "    \"\"\"\n",
    "    fit_transform_modality_preproc expects:\n",
    "      - \"prompt_only\"\n",
    "      - \"prompt_plus_map\"\n",
    "    For map_only we use \"prompt_plus_map\" semantics with prompt_dim=0.\n",
    "    \"\"\"\n",
    "    fm = str(feature_mode).strip().lower()\n",
    "    if fm == \"prompt_only\":\n",
    "        return \"prompt_only\"\n",
    "    if fm in {\"prompt_plus_map\", \"map_only\"}:\n",
    "        return \"prompt_plus_map\"\n",
    "    raise ValueError(f\"Unsupported feature_mode for preprocessing: {feature_mode}\")\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    split = SPLITS[exp_name]\n",
    "    feature_mode = cfg[\"feature_mode\"]\n",
    "    preproc_mode = _to_preproc_mode(feature_mode)\n",
    "\n",
    "    # dims inferred in Cell 4\n",
    "    map_dim    = int(cfg[\"map_dim\"])\n",
    "    prompt_dim = int(cfg[\"prompt_dim\"])\n",
    "\n",
    "    model_out_dir = Path(cfg[\"model_out\"]).expanduser().resolve()\n",
    "    model_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    preproc_path = model_out_dir / \"preproc.joblib\"\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   Feature mode : {feature_mode} -> preproc_mode={preproc_mode}\")\n",
    "    print(f\"   map_dim      : {map_dim}\")\n",
    "    print(f\"   prompt_dim   : {prompt_dim}\")\n",
    "    print(f\"   Save preproc : {preproc_path}\")\n",
    "\n",
    "    # Safety checks: X dims must match the experiment dims\n",
    "    Xtr = split[\"X_train\"]\n",
    "    if Xtr.shape[1] != (map_dim + prompt_dim):\n",
    "        raise ValueError(\n",
    "            f\"Dim mismatch in {exp_name}: X_train has {Xtr.shape[1]} cols, \"\n",
    "            f\"but map_dim+prompt_dim={map_dim + prompt_dim} (map_dim={map_dim}, prompt_dim={prompt_dim}).\"\n",
    "        )\n",
    "\n",
    "    res = fit_transform_modality_preproc(\n",
    "        X_train=split[\"X_train\"],\n",
    "        X_val=split[\"X_val\"],\n",
    "        X_test=split[\"X_test\"],\n",
    "        feature_mode=preproc_mode,\n",
    "        map_dim=map_dim,\n",
    "        prompt_dim=prompt_dim,\n",
    "        eps=float(MAP_VAR_EPS_DEFAULT),\n",
    "        clip_q=tuple(MAP_CLIP_Q_DEFAULT),\n",
    "        impute_strategy=str(MAP_IMPUTE_STRATEGY_DEFAULT),\n",
    "        robust_qrange=tuple(MAP_ROBUST_QRANGE_DEFAULT),\n",
    "        save_path=preproc_path,\n",
    "    )\n",
    "\n",
    "    PREPROC[exp_name] = {\n",
    "        \"X_train_s\": res.X_train_s,\n",
    "        \"X_val_s\":   res.X_val_s,\n",
    "        \"X_test_s\":  res.X_test_s,\n",
    "        \"bundle_path\": res.bundle_path,\n",
    "    }\n",
    "\n",
    "    print(\"   âœ… Preprocessing complete.\")\n",
    "    print(\"   Shapes:\", res.X_train_s.shape, res.X_val_s.shape, res.X_test_s.shape)\n",
    "\n",
    "print(\"\\nâœ… All preprocessing finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c32c2",
   "metadata": {},
   "source": [
    "## Step 9 â€” Build Class Labels and Sample Weights (Experiment-Aware)\n",
    "\n",
    "In this step, we construct the classification labels and training sample weights **for each\n",
    "experiment**, using the same fixed class order and the same split definition. This guarantees\n",
    "that differences in performance across experiments are due to the feature representation,\n",
    "not label encoding or sampling artifacts.\n",
    "\n",
    "---\n",
    "\n",
    "### Fixed class encoding\n",
    "\n",
    "Operator labels are encoded using a fixed global class order:\n",
    "\n",
    "`[simplify, select, aggregate, displace]`\n",
    "\n",
    "This guarantees consistent label indices across:\n",
    "- training\n",
    "- saved model bundles\n",
    "- evaluation and inference code\n",
    "\n",
    "Because the split is shared across experiments, this encoding remains stable and comparable.\n",
    "\n",
    "---\n",
    "\n",
    "### Sample weighting\n",
    "\n",
    "Training samples are weighted to address two common sources of bias:\n",
    "\n",
    "1. **Class imbalance**\n",
    "   - Balanced class weights are computed from the training distribution to prevent majority\n",
    "     classes from dominating learning.\n",
    "\n",
    "2. **Map-level prompt multiplicity**\n",
    "   - Some `map_id`s have multiple prompts.\n",
    "   - To prevent such maps from contributing disproportionately, each map contributes\n",
    "     approximately equal total weight by assigning each prompt a map-weight of:\n",
    "\n",
    "   `map_weight = 1 / (#prompts for that map_id)`\n",
    "\n",
    "---\n",
    "\n",
    "### Final weight definition\n",
    "\n",
    "The final per-sample weight used during training is:\n",
    "\n",
    "`sample_w = class_weight(operator) Ã— map_weight(map_id)`\n",
    "\n",
    "These weights are used during classifier training (and optionally regression training)\n",
    "to improve robustness and ensure fair learning across operators and maps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0df2b2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Building labels and sample weights per experiment ===\n",
      "\n",
      "ðŸ§ª openai_prompt_only\n",
      "Classes (fixed order): [np.str_('simplify'), np.str_('select'), np.str_('aggregate'), np.str_('displace')]\n",
      "Class weights: {'simplify': 1.0275229357798166, 'select': 0.7777777777777778, 'aggregate': 0.835820895522388, 'displace': 1.8360655737704918}\n",
      "y_train/y_val/y_test shapes: (448,) (57,) (57,)\n",
      "Sample weight summary: {'min': 0.025925925925925925, 'max': 1.8360655737704918, 'mean': 0.6487687942076353}\n",
      "\n",
      "ðŸ§ª use_prompt_only\n",
      "Classes (fixed order): [np.str_('simplify'), np.str_('select'), np.str_('aggregate'), np.str_('displace')]\n",
      "Class weights: {'simplify': 1.0275229357798166, 'select': 0.7777777777777778, 'aggregate': 0.835820895522388, 'displace': 1.8360655737704918}\n",
      "y_train/y_val/y_test shapes: (448,) (57,) (57,)\n",
      "Sample weight summary: {'min': 0.025925925925925925, 'max': 1.8360655737704918, 'mean': 0.6487687942076353}\n",
      "\n",
      "ðŸ§ª map_only\n",
      "Classes (fixed order): [np.str_('simplify'), np.str_('select'), np.str_('aggregate'), np.str_('displace')]\n",
      "Class weights: {'simplify': 1.0275229357798166, 'select': 0.7777777777777778, 'aggregate': 0.835820895522388, 'displace': 1.8360655737704918}\n",
      "y_train/y_val/y_test shapes: (448,) (57,) (57,)\n",
      "Sample weight summary: {'min': 0.025925925925925925, 'max': 1.8360655737704918, 'mean': 0.6487687942076353}\n",
      "\n",
      "ðŸ§ª use_map\n",
      "Classes (fixed order): [np.str_('simplify'), np.str_('select'), np.str_('aggregate'), np.str_('displace')]\n",
      "Class weights: {'simplify': 1.0275229357798166, 'select': 0.7777777777777778, 'aggregate': 0.835820895522388, 'displace': 1.8360655737704918}\n",
      "y_train/y_val/y_test shapes: (448,) (57,) (57,)\n",
      "Sample weight summary: {'min': 0.025925925925925925, 'max': 1.8360655737704918, 'mean': 0.6487687942076353}\n",
      "\n",
      "ðŸ§ª openai_map\n",
      "Classes (fixed order): [np.str_('simplify'), np.str_('select'), np.str_('aggregate'), np.str_('displace')]\n",
      "Class weights: {'simplify': 1.0275229357798166, 'select': 0.7777777777777778, 'aggregate': 0.835820895522388, 'displace': 1.8360655737704918}\n",
      "y_train/y_val/y_test shapes: (448,) (57,) (57,)\n",
      "Sample weight summary: {'min': 0.025925925925925925, 'max': 1.8360655737704918, 'mean': 0.6487687942076353}\n",
      "\n",
      "âœ… Label build complete for all experiments (class order consistent).\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 9 â€” Build labels + sample weights (per experiment) =====================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from imgofup.config import paths\n",
    "from imgofup.config.constants import (\n",
    "    MAPS_ID_COL,\n",
    "    CLASS_WEIGHT_MODE_DEFAULT,\n",
    "    USE_MAP_WEIGHT_DEFAULT,\n",
    ")\n",
    "from imgofup.datasets.labels_and_weights import build_labels_and_sample_weights\n",
    "\n",
    "OP_COL = paths.PATHS.OPERATOR_COL  # usually \"operator\"\n",
    "MAP_ID_COL = MAPS_ID_COL           # usually \"map_id\"\n",
    "\n",
    "LABELS = {}  # exp_name -> labels, weights, class_names, etc.\n",
    "\n",
    "print(\"\\n=== Building labels and sample weights per experiment ===\")\n",
    "\n",
    "for exp_name, split in SPLITS.items():\n",
    "    df_train = split[\"df_train\"].copy()\n",
    "    df_val   = split[\"df_val\"].copy()\n",
    "    df_test  = split[\"df_test\"].copy()\n",
    "\n",
    "    # Fail early if operator missing (this should NOT happen if Cell 6 merge worked)\n",
    "    for part_name, dfi in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n",
    "        if OP_COL not in dfi.columns:\n",
    "            raise ValueError(f\"{exp_name}: df_{part_name} missing operator column '{OP_COL}'.\")\n",
    "        n_miss = int(dfi[OP_COL].isna().sum())\n",
    "        if n_miss:\n",
    "            raise ValueError(\n",
    "                f\"{exp_name}: df_{part_name} has {n_miss} missing operator labels. \"\n",
    "                \"Fix the label merge in the data-loading step before training.\"\n",
    "            )\n",
    "\n",
    "    lab = build_labels_and_sample_weights(\n",
    "        df_train=df_train,\n",
    "        df_val=df_val,\n",
    "        df_test=df_test,\n",
    "        op_col=OP_COL,\n",
    "        map_id_col=MAP_ID_COL,\n",
    "        fixed_classes=FIXED_CLASSES,\n",
    "        use_map_weight=bool(USE_MAP_WEIGHT_DEFAULT),\n",
    "        class_weight_mode=str(CLASS_WEIGHT_MODE_DEFAULT),\n",
    "    )\n",
    "\n",
    "    class_names = np.array(lab.class_names)\n",
    "\n",
    "    LABELS[exp_name] = {\n",
    "        \"class_names\": class_names,\n",
    "        \"y_train_cls\": lab.y_train,\n",
    "        \"y_val_cls\":   lab.y_val,\n",
    "        \"y_test_cls\":  lab.y_test,\n",
    "        \"sample_w\":    lab.sample_w,\n",
    "        \"class_weight_map\": lab.class_weight_map,\n",
    "    }\n",
    "\n",
    "    print(f\"\\nðŸ§ª {exp_name}\")\n",
    "    print(\"Classes (fixed order):\", list(class_names))\n",
    "    print(\"Class weights:\", lab.class_weight_map)\n",
    "    print(\"y_train/y_val/y_test shapes:\", lab.y_train.shape, lab.y_val.shape, lab.y_test.shape)\n",
    "    sw = lab.sample_w\n",
    "    print(\"Sample weight summary:\", {\"min\": float(sw.min()), \"max\": float(sw.max()), \"mean\": float(sw.mean())})\n",
    "\n",
    "# Sanity: class order must match across experiments\n",
    "first = next(iter(LABELS.keys()))\n",
    "base_classes = LABELS[first][\"class_names\"].tolist()\n",
    "for exp_name in LABELS.keys():\n",
    "    if LABELS[exp_name][\"class_names\"].tolist() != base_classes:\n",
    "        raise ValueError(f\"Class order differs in experiment {exp_name}. This would break fair comparison.\")\n",
    "\n",
    "print(\"\\nâœ… Label build complete for all experiments (class order consistent).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988f274",
   "metadata": {},
   "source": [
    "## Step 10 â€” Operator Classification Model (MLP, Experiment-Aware)\n",
    "\n",
    "This step trains the operator classifier that predicts one of the four map generalization\n",
    "operators:\n",
    "\n",
    "`{simplify, select, aggregate, displace}`\n",
    "\n",
    "The same training protocol is applied **independently for each experiment** (prompt-only,\n",
    "USE + map, OpenAI + map) using the **shared split**. This ensures that differences in\n",
    "performance across experiments are attributable to the feature representation rather than\n",
    "changes in training procedure.\n",
    "\n",
    "---\n",
    "\n",
    "### Model and training strategy\n",
    "\n",
    "- We use an **MLPClassifier** (multi-layer perceptron).\n",
    "- Hyperparameters are explored via a lightweight random search over:\n",
    "  - hidden layer sizes\n",
    "  - weight decay (`alpha`)\n",
    "  - learning rate schedule\n",
    "  - batch size / optimization settings (as implemented in the helper)\n",
    "\n",
    "---\n",
    "\n",
    "### Validation protocol (leakage-free)\n",
    "\n",
    "To prevent leakage, we perform **grouped cross-validation** using `map_id`:\n",
    "\n",
    "- prompts from the same map are never split across folds\n",
    "\n",
    "This is critical because multiple prompts may refer to the same map and would otherwise\n",
    "inflate performance due to memorization.\n",
    "\n",
    "---\n",
    "\n",
    "### Model selection and evaluation\n",
    "\n",
    "The best configuration is selected using validation performance (with grouped CV used for\n",
    "reliable hyperparameter tuning). The selected model is then retrained on the full training\n",
    "split and evaluated on validation and test splits.\n",
    "\n",
    "---\n",
    "\n",
    "### Outputs (per experiment)\n",
    "\n",
    "For each experiment, the trained classifier is saved into the experimentâ€™s model folder as:\n",
    "\n",
    "- `classifier.joblib`\n",
    "\n",
    "This classifier is later used to:\n",
    "1. predict the operator class\n",
    "2. route each sample to the correct operator-specific parameter regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95133825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training operator classifiers for all experiments ===\n",
      "\n",
      "ðŸ§ª Experiment: openai_prompt_only\n",
      "   Classes   : ['simplify', 'select', 'aggregate', 'displace']\n",
      "   Train X   : (448, 1536)\n",
      "   Val X     : (57, 1536)\n",
      "   Test X    : (57, 1536)\n",
      "   Model out : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_openai_prompt_only\n",
      "\n",
      "Searching 50 MLP configs...\n",
      "[01/50] cvF1=0.931Â±0.021 | VAL F1=0.923 acc=0.930 | (128, 64), Î±=2.02e-02, lr=1.2e-03, bs=16\n",
      "[02/50] cvF1=0.918Â±0.038 | VAL F1=0.920 acc=0.930 | (256, 128), Î±=3.49e-05, lr=1.7e-04, bs=64\n",
      "[03/50] cvF1=0.920Â±0.042 | VAL F1=0.944 acc=0.947 | (256,), Î±=1.03e-02, lr=7.7e-04, bs=128\n",
      "[04/50] cvF1=0.913Â±0.036 | VAL F1=0.923 acc=0.930 | (256,), Î±=1.18e-05, lr=2.7e-03, bs=128\n",
      "[05/50] cvF1=0.910Â±0.034 | VAL F1=0.939 acc=0.947 | (256, 128, 64), Î±=5.47e-05, lr=1.9e-04, bs=16\n",
      "[06/50] cvF1=0.913Â±0.038 | VAL F1=0.920 acc=0.930 | (64,), Î±=1.14e-04, lr=6.0e-04, bs=128\n",
      "[07/50] cvF1=0.914Â±0.038 | VAL F1=0.920 acc=0.930 | (64,), Î±=1.03e-04, lr=8.0e-04, bs=32\n",
      "[08/50] cvF1=0.919Â±0.040 | VAL F1=0.944 acc=0.947 | (128, 64), Î±=2.43e-02, lr=2.2e-04, bs=32\n",
      "[09/50] cvF1=0.899Â±0.046 | VAL F1=0.939 acc=0.947 | (256, 128, 64), Î±=4.95e-05, lr=5.7e-04, bs=128\n",
      "[10/50] cvF1=0.924Â±0.029 | VAL F1=0.920 acc=0.930 | (64,), Î±=1.45e-05, lr=7.9e-04, bs=16\n",
      "[11/50] cvF1=0.909Â±0.036 | VAL F1=0.920 acc=0.930 | (64,), Î±=1.68e-05, lr=2.5e-03, bs=128\n",
      "[12/50] cvF1=0.913Â±0.034 | VAL F1=0.929 acc=0.930 | (256, 128, 64), Î±=6.47e-03, lr=2.8e-04, bs=16\n",
      "[13/50] cvF1=0.914Â±0.043 | VAL F1=0.944 acc=0.947 | (128,), Î±=2.39e-03, lr=4.5e-04, bs=64\n",
      "[14/50] cvF1=0.915Â±0.032 | VAL F1=0.939 acc=0.947 | (128, 64), Î±=5.27e-04, lr=1.1e-04, bs=32\n",
      "[15/50] cvF1=0.914Â±0.034 | VAL F1=0.920 acc=0.930 | (64,), Î±=7.94e-05, lr=9.5e-04, bs=32\n",
      "[16/50] cvF1=0.909Â±0.029 | VAL F1=0.939 acc=0.947 | (256, 128, 64), Î±=6.43e-04, lr=6.4e-04, bs=32\n",
      "[17/50] cvF1=0.869Â±0.052 | VAL F1=0.885 acc=0.895 | (256, 128), Î±=2.35e-02, lr=1.4e-03, bs=32\n",
      "[18/50] cvF1=0.921Â±0.039 | VAL F1=0.944 acc=0.947 | (128,), Î±=1.29e-02, lr=7.6e-04, bs=128\n",
      "[19/50] cvF1=0.904Â±0.040 | VAL F1=0.939 acc=0.947 | (256, 128, 64), Î±=4.80e-05, lr=1.2e-04, bs=128\n",
      "[20/50] cvF1=0.902Â±0.046 | VAL F1=0.920 acc=0.930 | (256, 128), Î±=2.25e-04, lr=2.5e-04, bs=16\n",
      "[21/50] cvF1=0.890Â±0.067 | VAL F1=0.944 acc=0.947 | (128,), Î±=2.27e-02, lr=7.9e-04, bs=16\n",
      "[22/50] cvF1=0.908Â±0.038 | VAL F1=0.920 acc=0.930 | (64,), Î±=1.07e-04, lr=1.8e-04, bs=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/50] cvF1=0.913Â±0.037 | VAL F1=0.939 acc=0.947 | (64,), Î±=4.84e-03, lr=2.0e-04, bs=128\n",
      "[24/50] cvF1=0.920Â±0.037 | VAL F1=0.923 acc=0.930 | (256,), Î±=4.91e-05, lr=1.1e-03, bs=64\n",
      "[25/50] cvF1=0.905Â±0.033 | VAL F1=0.944 acc=0.947 | (64,), Î±=1.28e-03, lr=2.3e-03, bs=32\n",
      "[26/50] cvF1=0.920Â±0.049 | VAL F1=0.929 acc=0.930 | (64,), Î±=1.52e-02, lr=1.8e-03, bs=128\n",
      "[27/50] cvF1=0.908Â±0.044 | VAL F1=0.920 acc=0.930 | (128,), Î±=2.15e-05, lr=3.5e-04, bs=32\n",
      "[28/50] cvF1=0.918Â±0.036 | VAL F1=0.923 acc=0.930 | (256, 128), Î±=3.44e-03, lr=8.7e-04, bs=64\n",
      "[29/50] cvF1=0.915Â±0.037 | VAL F1=0.920 acc=0.930 | (256,), Î±=4.38e-04, lr=1.5e-04, bs=32\n",
      "[30/50] cvF1=0.911Â±0.043 | VAL F1=0.944 acc=0.947 | (256,), Î±=4.42e-03, lr=6.7e-04, bs=64\n",
      "[31/50] cvF1=0.916Â±0.035 | VAL F1=0.939 acc=0.947 | (256, 128, 64), Î±=5.21e-04, lr=5.9e-04, bs=64\n",
      "[32/50] cvF1=0.910Â±0.041 | VAL F1=0.920 acc=0.930 | (128,), Î±=1.23e-05, lr=1.4e-04, bs=64\n",
      "[33/50] cvF1=0.914Â±0.038 | VAL F1=0.920 acc=0.930 | (64,), Î±=1.24e-04, lr=5.6e-04, bs=32\n",
      "[34/50] cvF1=0.913Â±0.039 | VAL F1=0.920 acc=0.930 | (256, 128), Î±=7.36e-05, lr=4.0e-04, bs=64\n",
      "[35/50] cvF1=0.908Â±0.041 | VAL F1=0.920 acc=0.930 | (256, 128), Î±=6.25e-05, lr=1.3e-04, bs=64\n",
      "[36/50] cvF1=0.918Â±0.034 | VAL F1=0.939 acc=0.947 | (256,), Î±=3.64e-05, lr=2.4e-03, bs=16\n",
      "[37/50] cvF1=0.908Â±0.036 | VAL F1=0.939 acc=0.947 | (256, 128, 64), Î±=1.59e-03, lr=1.9e-03, bs=128\n",
      "[38/50] cvF1=0.910Â±0.045 | VAL F1=0.939 acc=0.947 | (128, 64), Î±=4.45e-05, lr=2.1e-03, bs=64\n",
      "[39/50] cvF1=0.908Â±0.044 | VAL F1=0.920 acc=0.930 | (128,), Î±=2.66e-05, lr=3.4e-04, bs=32\n",
      "[40/50] cvF1=0.912Â±0.035 | VAL F1=0.923 acc=0.930 | (64,), Î±=8.84e-05, lr=9.1e-04, bs=16\n",
      "[41/50] cvF1=0.900Â±0.037 | VAL F1=0.920 acc=0.930 | (128, 64), Î±=1.68e-04, lr=2.8e-04, bs=32\n",
      "[42/50] cvF1=0.917Â±0.040 | VAL F1=0.920 acc=0.930 | (256,), Î±=2.83e-04, lr=2.1e-04, bs=64\n",
      "[43/50] cvF1=0.919Â±0.041 | VAL F1=0.923 acc=0.930 | (128,), Î±=1.49e-04, lr=2.5e-03, bs=32\n",
      "[44/50] cvF1=0.909Â±0.035 | VAL F1=0.939 acc=0.947 | (64,), Î±=2.54e-04, lr=1.2e-04, bs=32\n",
      "[45/50] cvF1=0.901Â±0.048 | VAL F1=0.947 acc=0.947 | (128, 64), Î±=7.22e-05, lr=1.1e-03, bs=64\n",
      "[46/50] cvF1=0.908Â±0.035 | VAL F1=0.920 acc=0.930 | (64,), Î±=3.27e-05, lr=3.0e-03, bs=64\n",
      "[47/50] cvF1=0.908Â±0.042 | VAL F1=0.920 acc=0.930 | (64,), Î±=2.49e-02, lr=4.0e-04, bs=64\n",
      "[48/50] cvF1=0.917Â±0.024 | VAL F1=0.920 acc=0.930 | (256, 128), Î±=1.58e-04, lr=8.6e-04, bs=32\n",
      "[49/50] cvF1=0.912Â±0.039 | VAL F1=0.944 acc=0.947 | (128,), Î±=7.02e-04, lr=4.6e-04, bs=32\n",
      "[50/50] cvF1=0.908Â±0.037 | VAL F1=0.923 acc=0.930 | (128, 64), Î±=1.91e-05, lr=3.5e-04, bs=16\n",
      "\n",
      "ðŸ† Selected params: {'hidden_layer_sizes': (128, 64), 'alpha': 7.218018741702544e-05, 'learning_rate_init': 0.0010678883315833304, 'batch_size': 64, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "\n",
      "=== Top 5 candidates ===\n",
      "VAL F1=0.947 acc=0.947 | cvF1=0.901Â±0.048\n",
      "VAL F1=0.944 acc=0.947 | cvF1=0.890Â±0.067\n",
      "VAL F1=0.944 acc=0.947 | cvF1=0.921Â±0.039\n",
      "VAL F1=0.944 acc=0.947 | cvF1=0.920Â±0.042\n",
      "VAL F1=0.944 acc=0.947 | cvF1=0.919Â±0.040\n",
      "\n",
      "===== VAL =====\n",
      "VAL: acc=0.9474  f1_macro=0.9472\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    simplify       1.00      0.92      0.96        13\n",
      "      select       0.90      1.00      0.95        19\n",
      "   aggregate       0.94      0.94      0.94        16\n",
      "    displace       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.95        57\n",
      "   macro avg       0.96      0.94      0.95        57\n",
      "weighted avg       0.95      0.95      0.95        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[12  0  1  0]\n",
      " [ 0 19  0  0]\n",
      " [ 0  1 15  0]\n",
      " [ 0  1  0  8]]\n",
      "\n",
      "===== TEST =====\n",
      "TEST: acc=0.9298  f1_macro=0.9378\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    simplify       0.92      0.92      0.92        13\n",
      "      select       0.94      0.89      0.92        19\n",
      "   aggregate       0.88      0.94      0.91        16\n",
      "    displace       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           0.93        57\n",
      "   macro avg       0.94      0.94      0.94        57\n",
      "weighted avg       0.93      0.93      0.93        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[12  0  1  0]\n",
      " [ 1 17  1  0]\n",
      " [ 0  1 15  0]\n",
      " [ 0  0  0  9]]\n",
      "\n",
      "Saved classifier to: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_openai_prompt_only/classifier.joblib\n",
      "   âœ… Classifier training done.\n",
      "   Saved to: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_openai_prompt_only/classifier.joblib\n",
      "   Best VAL: {'macro_f1': 0.9471691176470589, 'acc': 0.9473684210526315}\n",
      "   TEST     : {'macro_f1': 0.9377716877716878, 'acc': 0.9298245614035088}\n",
      "   (debug) Result fields: ['model_path', 'best_params', 'class_names', 'val_acc', 'val_f1_macro', 'test_acc', 'test_f1_macro']\n",
      "\n",
      "ðŸ§ª Experiment: use_prompt_only\n",
      "   Classes   : ['simplify', 'select', 'aggregate', 'displace']\n",
      "   Train X   : (448, 512)\n",
      "   Val X     : (57, 512)\n",
      "   Test X    : (57, 512)\n",
      "   Model out : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_use_prompt_only\n",
      "\n",
      "Searching 50 MLP configs...\n",
      "[01/50] cvF1=0.809Â±0.048 | VAL F1=0.944 acc=0.947 | (128, 64), Î±=2.02e-02, lr=1.2e-03, bs=16\n",
      "[02/50] cvF1=0.813Â±0.059 | VAL F1=0.944 acc=0.947 | (256, 128), Î±=3.49e-05, lr=1.7e-04, bs=64\n",
      "[03/50] cvF1=0.808Â±0.064 | VAL F1=0.944 acc=0.947 | (256,), Î±=1.03e-02, lr=7.7e-04, bs=128\n",
      "[04/50] cvF1=0.804Â±0.066 | VAL F1=0.944 acc=0.947 | (256,), Î±=1.18e-05, lr=2.7e-03, bs=128\n",
      "[05/50] cvF1=0.806Â±0.058 | VAL F1=0.944 acc=0.947 | (256, 128, 64), Î±=5.47e-05, lr=1.9e-04, bs=16\n",
      "[06/50] cvF1=0.804Â±0.066 | VAL F1=0.944 acc=0.947 | (64,), Î±=1.14e-04, lr=6.0e-04, bs=128\n",
      "[07/50] cvF1=0.800Â±0.062 | VAL F1=0.944 acc=0.947 | (64,), Î±=1.03e-04, lr=8.0e-04, bs=32\n",
      "[08/50] cvF1=0.803Â±0.062 | VAL F1=0.944 acc=0.947 | (128, 64), Î±=2.43e-02, lr=2.2e-04, bs=32\n",
      "[09/50] cvF1=0.805Â±0.060 | VAL F1=0.944 acc=0.947 | (256, 128, 64), Î±=4.95e-05, lr=5.7e-04, bs=128\n",
      "[10/50] cvF1=0.802Â±0.065 | VAL F1=0.947 acc=0.947 | (64,), Î±=1.45e-05, lr=7.9e-04, bs=16\n",
      "[11/50] cvF1=0.802Â±0.060 | VAL F1=0.944 acc=0.947 | (64,), Î±=1.68e-05, lr=2.5e-03, bs=128\n",
      "[12/50] cvF1=0.809Â±0.072 | VAL F1=0.944 acc=0.947 | (256, 128, 64), Î±=6.47e-03, lr=2.8e-04, bs=16\n",
      "[13/50] cvF1=0.806Â±0.056 | VAL F1=0.944 acc=0.947 | (128,), Î±=2.39e-03, lr=4.5e-04, bs=64\n",
      "[14/50] cvF1=0.812Â±0.052 | VAL F1=0.944 acc=0.947 | (128, 64), Î±=5.27e-04, lr=1.1e-04, bs=32\n",
      "[15/50] cvF1=0.799Â±0.062 | VAL F1=0.944 acc=0.947 | (64,), Î±=7.94e-05, lr=9.5e-04, bs=32\n",
      "[16/50] cvF1=0.811Â±0.052 | VAL F1=0.944 acc=0.947 | (256, 128, 64), Î±=6.43e-04, lr=6.4e-04, bs=32\n",
      "[17/50] cvF1=0.790Â±0.053 | VAL F1=0.944 acc=0.947 | (256, 128), Î±=2.35e-02, lr=1.4e-03, bs=32\n",
      "[18/50] cvF1=0.807Â±0.058 | VAL F1=0.944 acc=0.947 | (128,), Î±=1.29e-02, lr=7.6e-04, bs=128\n",
      "[19/50] cvF1=0.819Â±0.062 | VAL F1=0.944 acc=0.947 | (256, 128, 64), Î±=4.80e-05, lr=1.2e-04, bs=128\n",
      "[20/50] cvF1=0.814Â±0.064 | VAL F1=0.947 acc=0.947 | (256, 128), Î±=2.25e-04, lr=2.5e-04, bs=16\n",
      "[21/50] cvF1=0.814Â±0.051 | VAL F1=0.944 acc=0.947 | (128,), Î±=2.27e-02, lr=7.9e-04, bs=16\n",
      "[22/50] cvF1=0.801Â±0.060 | VAL F1=0.944 acc=0.947 | (64,), Î±=1.07e-04, lr=1.8e-04, bs=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/50] cvF1=0.817Â±0.054 | VAL F1=0.944 acc=0.947 | (64,), Î±=4.84e-03, lr=2.0e-04, bs=128\n",
      "[24/50] cvF1=0.804Â±0.066 | VAL F1=0.944 acc=0.947 | (256,), Î±=4.91e-05, lr=1.1e-03, bs=64\n",
      "[25/50] cvF1=0.806Â±0.061 | VAL F1=0.944 acc=0.947 | (64,), Î±=1.28e-03, lr=2.3e-03, bs=32\n",
      "[26/50] cvF1=0.808Â±0.061 | VAL F1=0.944 acc=0.947 | (64,), Î±=1.52e-02, lr=1.8e-03, bs=128\n",
      "[27/50] cvF1=0.801Â±0.060 | VAL F1=0.944 acc=0.947 | (128,), Î±=2.15e-05, lr=3.5e-04, bs=32\n",
      "[28/50] cvF1=0.799Â±0.066 | VAL F1=0.944 acc=0.947 | (256, 128), Î±=3.44e-03, lr=8.7e-04, bs=64\n",
      "[29/50] cvF1=0.807Â±0.065 | VAL F1=0.944 acc=0.947 | (256,), Î±=4.38e-04, lr=1.5e-04, bs=32\n",
      "[30/50] cvF1=0.808Â±0.064 | VAL F1=0.944 acc=0.947 | (256,), Î±=4.42e-03, lr=6.7e-04, bs=64\n",
      "[31/50] cvF1=0.806Â±0.061 | VAL F1=0.944 acc=0.947 | (256, 128, 64), Î±=5.21e-04, lr=5.9e-04, bs=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32/50] cvF1=0.804Â±0.055 | VAL F1=0.944 acc=0.947 | (128,), Î±=1.23e-05, lr=1.4e-04, bs=64\n",
      "[33/50] cvF1=0.806Â±0.063 | VAL F1=0.944 acc=0.947 | (64,), Î±=1.24e-04, lr=5.6e-04, bs=32\n",
      "[34/50] cvF1=0.810Â±0.060 | VAL F1=0.947 acc=0.947 | (256, 128), Î±=7.36e-05, lr=4.0e-04, bs=64\n",
      "[35/50] cvF1=0.807Â±0.056 | VAL F1=0.944 acc=0.947 | (256, 128), Î±=6.25e-05, lr=1.3e-04, bs=64\n",
      "[36/50] cvF1=0.807Â±0.059 | VAL F1=0.944 acc=0.947 | (256,), Î±=3.64e-05, lr=2.4e-03, bs=16\n",
      "[37/50] cvF1=0.814Â±0.051 | VAL F1=0.944 acc=0.947 | (256, 128, 64), Î±=1.59e-03, lr=1.9e-03, bs=128\n",
      "[38/50] cvF1=0.807Â±0.052 | VAL F1=0.947 acc=0.947 | (128, 64), Î±=4.45e-05, lr=2.1e-03, bs=64\n",
      "[39/50] cvF1=0.801Â±0.060 | VAL F1=0.944 acc=0.947 | (128,), Î±=2.66e-05, lr=3.4e-04, bs=32\n",
      "[40/50] cvF1=0.804Â±0.069 | VAL F1=0.947 acc=0.947 | (64,), Î±=8.84e-05, lr=9.1e-04, bs=16\n",
      "[41/50] cvF1=0.809Â±0.049 | VAL F1=0.947 acc=0.947 | (128, 64), Î±=1.68e-04, lr=2.8e-04, bs=32\n",
      "[42/50] cvF1=0.810Â±0.060 | VAL F1=0.944 acc=0.947 | (256,), Î±=2.83e-04, lr=2.1e-04, bs=64\n",
      "[43/50] cvF1=0.805Â±0.058 | VAL F1=0.944 acc=0.947 | (128,), Î±=1.49e-04, lr=2.5e-03, bs=32\n",
      "[44/50] cvF1=0.816Â±0.051 | VAL F1=0.944 acc=0.947 | (64,), Î±=2.54e-04, lr=1.2e-04, bs=32\n",
      "[45/50] cvF1=0.803Â±0.059 | VAL F1=0.947 acc=0.947 | (128, 64), Î±=7.22e-05, lr=1.1e-03, bs=64\n",
      "[46/50] cvF1=0.804Â±0.051 | VAL F1=0.944 acc=0.947 | (64,), Î±=3.27e-05, lr=3.0e-03, bs=64\n",
      "[47/50] cvF1=0.805Â±0.059 | VAL F1=0.944 acc=0.947 | (64,), Î±=2.49e-02, lr=4.0e-04, bs=64\n",
      "[48/50] cvF1=0.805Â±0.066 | VAL F1=0.947 acc=0.947 | (256, 128), Î±=1.58e-04, lr=8.6e-04, bs=32\n",
      "[49/50] cvF1=0.801Â±0.059 | VAL F1=0.944 acc=0.947 | (128,), Î±=7.02e-04, lr=4.6e-04, bs=32\n",
      "[50/50] cvF1=0.802Â±0.064 | VAL F1=0.947 acc=0.947 | (128, 64), Î±=1.91e-05, lr=3.5e-04, bs=16\n",
      "\n",
      "ðŸ† Selected params: {'hidden_layer_sizes': (256, 128), 'alpha': 0.00022463533171675809, 'learning_rate_init': 0.0002516607127550297, 'batch_size': 16, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "\n",
      "=== Top 5 candidates ===\n",
      "VAL F1=0.947 acc=0.947 | cvF1=0.814Â±0.064\n",
      "VAL F1=0.947 acc=0.947 | cvF1=0.810Â±0.060\n",
      "VAL F1=0.947 acc=0.947 | cvF1=0.809Â±0.049\n",
      "VAL F1=0.947 acc=0.947 | cvF1=0.807Â±0.052\n",
      "VAL F1=0.947 acc=0.947 | cvF1=0.805Â±0.066\n",
      "\n",
      "===== VAL =====\n",
      "VAL: acc=0.9474  f1_macro=0.9472\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    simplify       1.00      0.92      0.96        13\n",
      "      select       0.90      1.00      0.95        19\n",
      "   aggregate       0.94      0.94      0.94        16\n",
      "    displace       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.95        57\n",
      "   macro avg       0.96      0.94      0.95        57\n",
      "weighted avg       0.95      0.95      0.95        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[12  0  1  0]\n",
      " [ 0 19  0  0]\n",
      " [ 0  1 15  0]\n",
      " [ 0  1  0  8]]\n",
      "\n",
      "===== TEST =====\n",
      "TEST: acc=0.8772  f1_macro=0.8762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    simplify       0.81      1.00      0.90        13\n",
      "      select       0.89      0.89      0.89        19\n",
      "   aggregate       0.87      0.81      0.84        16\n",
      "    displace       1.00      0.78      0.88         9\n",
      "\n",
      "    accuracy                           0.88        57\n",
      "   macro avg       0.89      0.87      0.88        57\n",
      "weighted avg       0.88      0.88      0.88        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[13  0  0  0]\n",
      " [ 0 17  2  0]\n",
      " [ 1  2 13  0]\n",
      " [ 2  0  0  7]]\n",
      "\n",
      "Saved classifier to: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_use_prompt_only/classifier.joblib\n",
      "   âœ… Classifier training done.\n",
      "   Saved to: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_use_prompt_only/classifier.joblib\n",
      "   Best VAL: {'macro_f1': 0.9471691176470589, 'acc': 0.9473684210526315}\n",
      "   TEST     : {'macro_f1': 0.8762495609156373, 'acc': 0.8771929824561403}\n",
      "\n",
      "ðŸ§ª Experiment: map_only\n",
      "   Classes   : ['simplify', 'select', 'aggregate', 'displace']\n",
      "   Train X   : (448, 165)\n",
      "   Val X     : (57, 165)\n",
      "   Test X    : (57, 165)\n",
      "   Model out : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_map_only\n",
      "\n",
      "Searching 50 MLP configs...\n",
      "[01/50] cvF1=0.281Â±0.098 | VAL F1=0.184 acc=0.193 | (128, 64), Î±=2.02e-02, lr=1.2e-03, bs=16\n",
      "[02/50] cvF1=0.172Â±0.069 | VAL F1=0.281 acc=0.281 | (256, 128), Î±=3.49e-05, lr=1.7e-04, bs=64\n",
      "[03/50] cvF1=0.272Â±0.099 | VAL F1=0.244 acc=0.246 | (256,), Î±=1.03e-02, lr=7.7e-04, bs=128\n",
      "[04/50] cvF1=0.268Â±0.101 | VAL F1=0.233 acc=0.246 | (256,), Î±=1.18e-05, lr=2.7e-03, bs=128\n",
      "[05/50] cvF1=0.306Â±0.108 | VAL F1=0.280 acc=0.281 | (256, 128, 64), Î±=5.47e-05, lr=1.9e-04, bs=16\n",
      "[06/50] cvF1=0.293Â±0.109 | VAL F1=0.333 acc=0.333 | (64,), Î±=1.14e-04, lr=6.0e-04, bs=128\n",
      "[07/50] cvF1=0.263Â±0.105 | VAL F1=0.342 acc=0.351 | (64,), Î±=1.03e-04, lr=8.0e-04, bs=32\n",
      "[08/50] cvF1=0.265Â±0.110 | VAL F1=0.296 acc=0.298 | (128, 64), Î±=2.43e-02, lr=2.2e-04, bs=32\n",
      "[09/50] cvF1=0.268Â±0.091 | VAL F1=0.280 acc=0.281 | (256, 128, 64), Î±=4.95e-05, lr=5.7e-04, bs=128\n",
      "[10/50] cvF1=0.267Â±0.114 | VAL F1=0.379 acc=0.386 | (64,), Î±=1.45e-05, lr=7.9e-04, bs=16\n",
      "[11/50] cvF1=0.296Â±0.112 | VAL F1=0.296 acc=0.316 | (64,), Î±=1.68e-05, lr=2.5e-03, bs=128\n",
      "[12/50] cvF1=0.267Â±0.134 | VAL F1=0.198 acc=0.193 | (256, 128, 64), Î±=6.47e-03, lr=2.8e-04, bs=16\n",
      "[13/50] cvF1=0.240Â±0.136 | VAL F1=0.209 acc=0.211 | (128,), Î±=2.39e-03, lr=4.5e-04, bs=64\n",
      "[14/50] cvF1=0.204Â±0.088 | VAL F1=0.254 acc=0.246 | (128, 64), Î±=5.27e-04, lr=1.1e-04, bs=32\n",
      "[15/50] cvF1=0.256Â±0.116 | VAL F1=0.328 acc=0.351 | (64,), Î±=7.94e-05, lr=9.5e-04, bs=32\n",
      "[16/50] cvF1=0.266Â±0.125 | VAL F1=0.267 acc=0.263 | (256, 128, 64), Î±=6.43e-04, lr=6.4e-04, bs=32\n",
      "[17/50] cvF1=0.254Â±0.108 | VAL F1=0.220 acc=0.211 | (256, 128), Î±=2.35e-02, lr=1.4e-03, bs=32\n",
      "[18/50] cvF1=0.206Â±0.090 | VAL F1=0.216 acc=0.211 | (128,), Î±=1.29e-02, lr=7.6e-04, bs=128\n",
      "[19/50] cvF1=0.278Â±0.068 | VAL F1=0.263 acc=0.263 | (256, 128, 64), Î±=4.80e-05, lr=1.2e-04, bs=128\n",
      "[20/50] cvF1=0.182Â±0.073 | VAL F1=0.254 acc=0.246 | (256, 128), Î±=2.25e-04, lr=2.5e-04, bs=16\n",
      "[21/50] cvF1=0.249Â±0.135 | VAL F1=0.180 acc=0.175 | (128,), Î±=2.27e-02, lr=7.9e-04, bs=16\n",
      "[22/50] cvF1=0.251Â±0.118 | VAL F1=0.326 acc=0.333 | (64,), Î±=1.07e-04, lr=1.8e-04, bs=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/50] cvF1=0.261Â±0.080 | VAL F1=0.323 acc=0.333 | (64,), Î±=4.84e-03, lr=2.0e-04, bs=128\n",
      "[24/50] cvF1=0.245Â±0.120 | VAL F1=0.258 acc=0.263 | (256,), Î±=4.91e-05, lr=1.1e-03, bs=64\n",
      "[25/50] cvF1=0.240Â±0.131 | VAL F1=0.297 acc=0.316 | (64,), Î±=1.28e-03, lr=2.3e-03, bs=32\n",
      "[26/50] cvF1=0.278Â±0.108 | VAL F1=0.377 acc=0.386 | (64,), Î±=1.52e-02, lr=1.8e-03, bs=128\n",
      "[27/50] cvF1=0.247Â±0.125 | VAL F1=0.223 acc=0.228 | (128,), Î±=2.15e-05, lr=3.5e-04, bs=32\n",
      "[28/50] cvF1=0.170Â±0.087 | VAL F1=0.265 acc=0.263 | (256, 128), Î±=3.44e-03, lr=8.7e-04, bs=64\n",
      "[29/50] cvF1=0.266Â±0.102 | VAL F1=0.258 acc=0.263 | (256,), Î±=4.38e-04, lr=1.5e-04, bs=32\n",
      "[30/50] cvF1=0.262Â±0.120 | VAL F1=0.256 acc=0.263 | (256,), Î±=4.42e-03, lr=6.7e-04, bs=64\n",
      "[31/50] cvF1=0.301Â±0.115 | VAL F1=0.279 acc=0.281 | (256, 128, 64), Î±=5.21e-04, lr=5.9e-04, bs=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32/50] cvF1=0.212Â±0.091 | VAL F1=0.247 acc=0.246 | (128,), Î±=1.23e-05, lr=1.4e-04, bs=64\n",
      "[33/50] cvF1=0.208Â±0.078 | VAL F1=0.340 acc=0.351 | (64,), Î±=1.24e-04, lr=5.6e-04, bs=32\n",
      "[34/50] cvF1=0.189Â±0.073 | VAL F1=0.255 acc=0.246 | (256, 128), Î±=7.36e-05, lr=4.0e-04, bs=64\n",
      "[35/50] cvF1=0.177Â±0.074 | VAL F1=0.273 acc=0.263 | (256, 128), Î±=6.25e-05, lr=1.3e-04, bs=64\n",
      "[36/50] cvF1=0.225Â±0.078 | VAL F1=0.196 acc=0.193 | (256,), Î±=3.64e-05, lr=2.4e-03, bs=16\n",
      "[37/50] cvF1=0.249Â±0.093 | VAL F1=0.261 acc=0.263 | (256, 128, 64), Î±=1.59e-03, lr=1.9e-03, bs=128\n",
      "[38/50] cvF1=0.213Â±0.105 | VAL F1=0.278 acc=0.298 | (128, 64), Î±=4.45e-05, lr=2.1e-03, bs=64\n",
      "[39/50] cvF1=0.249Â±0.129 | VAL F1=0.223 acc=0.228 | (128,), Î±=2.66e-05, lr=3.4e-04, bs=32\n",
      "[40/50] cvF1=0.277Â±0.118 | VAL F1=0.327 acc=0.351 | (64,), Î±=8.84e-05, lr=9.1e-04, bs=16\n",
      "[41/50] cvF1=0.199Â±0.097 | VAL F1=0.262 acc=0.263 | (128, 64), Î±=1.68e-04, lr=2.8e-04, bs=32\n",
      "[42/50] cvF1=0.280Â±0.111 | VAL F1=0.245 acc=0.246 | (256,), Î±=2.83e-04, lr=2.1e-04, bs=64\n",
      "[43/50] cvF1=0.274Â±0.146 | VAL F1=0.192 acc=0.193 | (128,), Î±=1.49e-04, lr=2.5e-03, bs=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44/50] cvF1=0.244Â±0.089 | VAL F1=0.345 acc=0.368 | (64,), Î±=2.54e-04, lr=1.2e-04, bs=32\n",
      "[45/50] cvF1=0.201Â±0.087 | VAL F1=0.248 acc=0.263 | (128, 64), Î±=7.22e-05, lr=1.1e-03, bs=64\n",
      "[46/50] cvF1=0.248Â±0.098 | VAL F1=0.319 acc=0.333 | (64,), Î±=3.27e-05, lr=3.0e-03, bs=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47/50] cvF1=0.276Â±0.123 | VAL F1=0.271 acc=0.281 | (64,), Î±=2.49e-02, lr=4.0e-04, bs=64\n",
      "[48/50] cvF1=0.186Â±0.074 | VAL F1=0.268 acc=0.263 | (256, 128), Î±=1.58e-04, lr=8.6e-04, bs=32\n",
      "[49/50] cvF1=0.254Â±0.137 | VAL F1=0.208 acc=0.211 | (128,), Î±=7.02e-04, lr=4.6e-04, bs=32\n",
      "[50/50] cvF1=0.217Â±0.096 | VAL F1=0.239 acc=0.246 | (128, 64), Î±=1.91e-05, lr=3.5e-04, bs=16\n",
      "\n",
      "ðŸ† Selected params: {'hidden_layer_sizes': (64,), 'alpha': 1.4504865877614242e-05, 'learning_rate_init': 0.0007896186801026691, 'batch_size': 16, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "\n",
      "=== Top 5 candidates ===\n",
      "VAL F1=0.379 acc=0.386 | cvF1=0.267Â±0.114\n",
      "VAL F1=0.377 acc=0.386 | cvF1=0.278Â±0.108\n",
      "VAL F1=0.345 acc=0.368 | cvF1=0.244Â±0.089\n",
      "VAL F1=0.342 acc=0.351 | cvF1=0.263Â±0.105\n",
      "VAL F1=0.340 acc=0.351 | cvF1=0.208Â±0.078\n",
      "\n",
      "===== VAL =====\n",
      "VAL: acc=0.3860  f1_macro=0.3785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    simplify       0.45      0.38      0.42        13\n",
      "      select       0.40      0.32      0.35        19\n",
      "   aggregate       0.40      0.50      0.44        16\n",
      "    displace       0.27      0.33      0.30         9\n",
      "\n",
      "    accuracy                           0.39        57\n",
      "   macro avg       0.38      0.38      0.38        57\n",
      "weighted avg       0.39      0.39      0.38        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[5 5 3 0]\n",
      " [2 6 6 5]\n",
      " [3 2 8 3]\n",
      " [1 2 3 3]]\n",
      "\n",
      "===== TEST =====\n",
      "TEST: acc=0.2632  f1_macro=0.2572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    simplify       0.21      0.23      0.22        13\n",
      "      select       0.38      0.32      0.34        19\n",
      "   aggregate       0.21      0.25      0.23        16\n",
      "    displace       0.25      0.22      0.24         9\n",
      "\n",
      "    accuracy                           0.26        57\n",
      "   macro avg       0.26      0.25      0.26        57\n",
      "weighted avg       0.27      0.26      0.27        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[3 2 5 3]\n",
      " [2 6 9 2]\n",
      " [6 5 4 1]\n",
      " [3 3 1 2]]\n",
      "\n",
      "Saved classifier to: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_map_only/classifier.joblib\n",
      "   âœ… Classifier training done.\n",
      "   Saved to: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_map_only/classifier.joblib\n",
      "   Best VAL: {'macro_f1': 0.3785130718954249, 'acc': 0.38596491228070173}\n",
      "   TEST     : {'macro_f1': 0.2572362278244631, 'acc': 0.2631578947368421}\n",
      "\n",
      "ðŸ§ª Experiment: use_map\n",
      "   Classes   : ['simplify', 'select', 'aggregate', 'displace']\n",
      "   Train X   : (448, 677)\n",
      "   Val X     : (57, 677)\n",
      "   Test X    : (57, 677)\n",
      "   Model out : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_use_map\n",
      "\n",
      "Searching 50 MLP configs...\n",
      "[01/50] cvF1=0.627Â±0.086 | VAL F1=0.814 acc=0.825 | (128, 64), Î±=2.02e-02, lr=1.2e-03, bs=16\n",
      "[02/50] cvF1=0.706Â±0.098 | VAL F1=0.873 acc=0.877 | (256, 128), Î±=3.49e-05, lr=1.7e-04, bs=64\n",
      "[03/50] cvF1=0.675Â±0.113 | VAL F1=0.788 acc=0.807 | (256,), Î±=1.03e-02, lr=7.7e-04, bs=128\n",
      "[04/50] cvF1=0.718Â±0.113 | VAL F1=0.814 acc=0.825 | (256,), Î±=1.18e-05, lr=2.7e-03, bs=128\n",
      "[05/50] cvF1=0.711Â±0.117 | VAL F1=0.884 acc=0.895 | (256, 128, 64), Î±=5.47e-05, lr=1.9e-04, bs=16\n",
      "[06/50] cvF1=0.722Â±0.103 | VAL F1=0.833 acc=0.842 | (64,), Î±=1.14e-04, lr=6.0e-04, bs=128\n",
      "[07/50] cvF1=0.727Â±0.093 | VAL F1=0.833 acc=0.842 | (64,), Î±=1.03e-04, lr=8.0e-04, bs=32\n",
      "[08/50] cvF1=0.644Â±0.122 | VAL F1=0.808 acc=0.825 | (128, 64), Î±=2.43e-02, lr=2.2e-04, bs=32\n",
      "[09/50] cvF1=0.709Â±0.126 | VAL F1=0.833 acc=0.842 | (256, 128, 64), Î±=4.95e-05, lr=5.7e-04, bs=128\n",
      "[10/50] cvF1=0.731Â±0.088 | VAL F1=0.851 acc=0.860 | (64,), Î±=1.45e-05, lr=7.9e-04, bs=16\n",
      "[11/50] cvF1=0.725Â±0.098 | VAL F1=0.833 acc=0.842 | (64,), Î±=1.68e-05, lr=2.5e-03, bs=128\n",
      "[12/50] cvF1=0.638Â±0.096 | VAL F1=0.827 acc=0.842 | (256, 128, 64), Î±=6.47e-03, lr=2.8e-04, bs=16\n",
      "[13/50] cvF1=0.719Â±0.107 | VAL F1=0.853 acc=0.860 | (128,), Î±=2.39e-03, lr=4.5e-04, bs=64\n",
      "[14/50] cvF1=0.741Â±0.081 | VAL F1=0.871 acc=0.877 | (128, 64), Î±=5.27e-04, lr=1.1e-04, bs=32\n",
      "[15/50] cvF1=0.731Â±0.088 | VAL F1=0.851 acc=0.860 | (64,), Î±=7.94e-05, lr=9.5e-04, bs=32\n",
      "[16/50] cvF1=0.692Â±0.121 | VAL F1=0.870 acc=0.877 | (256, 128, 64), Î±=6.43e-04, lr=6.4e-04, bs=32\n",
      "[17/50] cvF1=0.687Â±0.094 | VAL F1=0.826 acc=0.842 | (256, 128), Î±=2.35e-02, lr=1.4e-03, bs=32\n",
      "[18/50] cvF1=0.698Â±0.117 | VAL F1=0.827 acc=0.842 | (128,), Î±=1.29e-02, lr=7.6e-04, bs=128\n",
      "[19/50] cvF1=0.695Â±0.111 | VAL F1=0.853 acc=0.860 | (256, 128, 64), Î±=4.80e-05, lr=1.2e-04, bs=128\n",
      "[20/50] cvF1=0.710Â±0.101 | VAL F1=0.866 acc=0.877 | (256, 128), Î±=2.25e-04, lr=2.5e-04, bs=16\n",
      "[21/50] cvF1=0.630Â±0.107 | VAL F1=0.774 acc=0.789 | (128,), Î±=2.27e-02, lr=7.9e-04, bs=16\n",
      "[22/50] cvF1=0.729Â±0.087 | VAL F1=0.833 acc=0.842 | (64,), Î±=1.07e-04, lr=1.8e-04, bs=16\n",
      "[23/50] cvF1=0.703Â±0.113 | VAL F1=0.851 acc=0.860 | (64,), Î±=4.84e-03, lr=2.0e-04, bs=128\n",
      "[24/50] cvF1=0.724Â±0.118 | VAL F1=0.834 acc=0.842 | (256,), Î±=4.91e-05, lr=1.1e-03, bs=64\n",
      "[25/50] cvF1=0.672Â±0.107 | VAL F1=0.814 acc=0.825 | (64,), Î±=1.28e-03, lr=2.3e-03, bs=32\n",
      "[26/50] cvF1=0.660Â±0.108 | VAL F1=0.827 acc=0.842 | (64,), Î±=1.52e-02, lr=1.8e-03, bs=128\n",
      "[27/50] cvF1=0.744Â±0.087 | VAL F1=0.871 acc=0.877 | (128,), Î±=2.15e-05, lr=3.5e-04, bs=32\n",
      "[28/50] cvF1=0.642Â±0.104 | VAL F1=0.827 acc=0.842 | (256, 128), Î±=3.44e-03, lr=8.7e-04, bs=64\n",
      "[29/50] cvF1=0.720Â±0.113 | VAL F1=0.834 acc=0.842 | (256,), Î±=4.38e-04, lr=1.5e-04, bs=32\n",
      "[30/50] cvF1=0.676Â±0.119 | VAL F1=0.788 acc=0.807 | (256,), Î±=4.42e-03, lr=6.7e-04, bs=64\n",
      "[31/50] cvF1=0.706Â±0.128 | VAL F1=0.853 acc=0.860 | (256, 128, 64), Î±=5.21e-04, lr=5.9e-04, bs=64\n",
      "[32/50] cvF1=0.743Â±0.088 | VAL F1=0.871 acc=0.877 | (128,), Î±=1.23e-05, lr=1.4e-04, bs=64\n",
      "[33/50] cvF1=0.726Â±0.085 | VAL F1=0.833 acc=0.842 | (64,), Î±=1.24e-04, lr=5.6e-04, bs=32\n",
      "[34/50] cvF1=0.700Â±0.094 | VAL F1=0.853 acc=0.860 | (256, 128), Î±=7.36e-05, lr=4.0e-04, bs=64\n",
      "[35/50] cvF1=0.707Â±0.098 | VAL F1=0.873 acc=0.877 | (256, 128), Î±=6.25e-05, lr=1.3e-04, bs=64\n",
      "[36/50] cvF1=0.738Â±0.084 | VAL F1=0.873 acc=0.877 | (256,), Î±=3.64e-05, lr=2.4e-03, bs=16\n",
      "[37/50] cvF1=0.678Â±0.100 | VAL F1=0.833 acc=0.842 | (256, 128, 64), Î±=1.59e-03, lr=1.9e-03, bs=128\n",
      "[38/50] cvF1=0.733Â±0.080 | VAL F1=0.889 acc=0.895 | (128, 64), Î±=4.45e-05, lr=2.1e-03, bs=64\n",
      "[39/50] cvF1=0.741Â±0.088 | VAL F1=0.871 acc=0.877 | (128,), Î±=2.66e-05, lr=3.4e-04, bs=32\n",
      "[40/50] cvF1=0.735Â±0.088 | VAL F1=0.854 acc=0.860 | (64,), Î±=8.84e-05, lr=9.1e-04, bs=16\n",
      "[41/50] cvF1=0.747Â±0.084 | VAL F1=0.871 acc=0.877 | (128, 64), Î±=1.68e-04, lr=2.8e-04, bs=32\n",
      "[42/50] cvF1=0.729Â±0.110 | VAL F1=0.853 acc=0.860 | (256,), Î±=2.83e-04, lr=2.1e-04, bs=64\n",
      "[43/50] cvF1=0.726Â±0.108 | VAL F1=0.833 acc=0.842 | (128,), Î±=1.49e-04, lr=2.5e-03, bs=32\n",
      "[44/50] cvF1=0.729Â±0.086 | VAL F1=0.851 acc=0.860 | (64,), Î±=2.54e-04, lr=1.2e-04, bs=32\n",
      "[45/50] cvF1=0.741Â±0.084 | VAL F1=0.854 acc=0.860 | (128, 64), Î±=7.22e-05, lr=1.1e-03, bs=64\n",
      "[46/50] cvF1=0.730Â±0.093 | VAL F1=0.833 acc=0.842 | (64,), Î±=3.27e-05, lr=3.0e-03, bs=64\n",
      "[47/50] cvF1=0.654Â±0.133 | VAL F1=0.788 acc=0.807 | (64,), Î±=2.49e-02, lr=4.0e-04, bs=64\n",
      "[48/50] cvF1=0.718Â±0.085 | VAL F1=0.873 acc=0.877 | (256, 128), Î±=1.58e-04, lr=8.6e-04, bs=32\n",
      "[49/50] cvF1=0.736Â±0.087 | VAL F1=0.853 acc=0.860 | (128,), Î±=7.02e-04, lr=4.6e-04, bs=32\n",
      "[50/50] cvF1=0.752Â±0.090 | VAL F1=0.854 acc=0.860 | (128, 64), Î±=1.91e-05, lr=3.5e-04, bs=16\n",
      "\n",
      "ðŸ† Selected params: {'hidden_layer_sizes': (128, 64), 'alpha': 4.45375904389123e-05, 'learning_rate_init': 0.0020816986844858954, 'batch_size': 64, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "\n",
      "=== Top 5 candidates ===\n",
      "VAL F1=0.889 acc=0.895 | cvF1=0.733Â±0.080\n",
      "VAL F1=0.884 acc=0.895 | cvF1=0.711Â±0.117\n",
      "VAL F1=0.873 acc=0.877 | cvF1=0.738Â±0.084\n",
      "VAL F1=0.873 acc=0.877 | cvF1=0.718Â±0.085\n",
      "VAL F1=0.873 acc=0.877 | cvF1=0.707Â±0.098\n",
      "\n",
      "===== VAL =====\n",
      "VAL: acc=0.8947  f1_macro=0.8892\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    simplify       1.00      0.85      0.92        13\n",
      "      select       0.90      0.95      0.92        19\n",
      "   aggregate       0.88      0.88      0.88        16\n",
      "    displace       0.80      0.89      0.84         9\n",
      "\n",
      "    accuracy                           0.89        57\n",
      "   macro avg       0.89      0.89      0.89        57\n",
      "weighted avg       0.90      0.89      0.90        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[11  1  1  0]\n",
      " [ 0 18  1  0]\n",
      " [ 0  0 14  2]\n",
      " [ 0  1  0  8]]\n",
      "\n",
      "===== TEST =====\n",
      "TEST: acc=0.7544  f1_macro=0.7667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    simplify       0.83      0.77      0.80        13\n",
      "      select       0.82      0.74      0.78        19\n",
      "   aggregate       0.61      0.69      0.65        16\n",
      "    displace       0.80      0.89      0.84         9\n",
      "\n",
      "    accuracy                           0.75        57\n",
      "   macro avg       0.77      0.77      0.77        57\n",
      "weighted avg       0.76      0.75      0.76        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[10  1  2  0]\n",
      " [ 0 14  5  0]\n",
      " [ 1  2 11  2]\n",
      " [ 1  0  0  8]]\n",
      "\n",
      "Saved classifier to: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_use_map/classifier.joblib\n",
      "   âœ… Classifier training done.\n",
      "   Saved to: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_use_map/classifier.joblib\n",
      "   Best VAL: {'macro_f1': 0.8892122132253711, 'acc': 0.8947368421052632}\n",
      "   TEST     : {'macro_f1': 0.7667354661162711, 'acc': 0.7543859649122807}\n",
      "\n",
      "ðŸ§ª Experiment: openai_map\n",
      "   Classes   : ['simplify', 'select', 'aggregate', 'displace']\n",
      "   Train X   : (448, 1701)\n",
      "   Val X     : (57, 1701)\n",
      "   Test X    : (57, 1701)\n",
      "   Model out : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_openai_map\n",
      "\n",
      "Searching 50 MLP configs...\n",
      "[01/50] cvF1=0.668Â±0.109 | VAL F1=0.798 acc=0.807 | (128, 64), Î±=2.02e-02, lr=1.2e-03, bs=16\n",
      "[02/50] cvF1=0.793Â±0.096 | VAL F1=0.944 acc=0.947 | (256, 128), Î±=3.49e-05, lr=1.7e-04, bs=64\n",
      "[03/50] cvF1=0.710Â±0.113 | VAL F1=0.830 acc=0.842 | (256,), Î±=1.03e-02, lr=7.7e-04, bs=128\n",
      "[04/50] cvF1=0.795Â±0.076 | VAL F1=0.920 acc=0.930 | (256,), Î±=1.18e-05, lr=2.7e-03, bs=128\n",
      "[05/50] cvF1=0.794Â±0.082 | VAL F1=0.920 acc=0.930 | (256, 128, 64), Î±=5.47e-05, lr=1.9e-04, bs=16\n",
      "[06/50] cvF1=0.816Â±0.062 | VAL F1=0.944 acc=0.947 | (64,), Î±=1.14e-04, lr=6.0e-04, bs=128\n",
      "[07/50] cvF1=0.821Â±0.069 | VAL F1=0.920 acc=0.930 | (64,), Î±=1.03e-04, lr=8.0e-04, bs=32\n",
      "[08/50] cvF1=0.657Â±0.127 | VAL F1=0.790 acc=0.807 | (128, 64), Î±=2.43e-02, lr=2.2e-04, bs=32\n",
      "[09/50] cvF1=0.782Â±0.083 | VAL F1=0.940 acc=0.947 | (256, 128, 64), Î±=4.95e-05, lr=5.7e-04, bs=128\n",
      "[10/50] cvF1=0.832Â±0.070 | VAL F1=0.920 acc=0.930 | (64,), Î±=1.45e-05, lr=7.9e-04, bs=16\n",
      "[11/50] cvF1=0.821Â±0.063 | VAL F1=0.927 acc=0.930 | (64,), Î±=1.68e-05, lr=2.5e-03, bs=128\n",
      "[12/50] cvF1=0.728Â±0.068 | VAL F1=0.850 acc=0.860 | (256, 128, 64), Î±=6.47e-03, lr=2.8e-04, bs=16\n",
      "[13/50] cvF1=0.789Â±0.078 | VAL F1=0.903 acc=0.912 | (128,), Î±=2.39e-03, lr=4.5e-04, bs=64\n",
      "[14/50] cvF1=0.805Â±0.066 | VAL F1=0.927 acc=0.930 | (128, 64), Î±=5.27e-04, lr=1.1e-04, bs=32\n",
      "[15/50] cvF1=0.816Â±0.070 | VAL F1=0.944 acc=0.947 | (64,), Î±=7.94e-05, lr=9.5e-04, bs=32\n",
      "[16/50] cvF1=0.736Â±0.103 | VAL F1=0.920 acc=0.930 | (256, 128, 64), Î±=6.43e-04, lr=6.4e-04, bs=32\n",
      "[17/50] cvF1=0.679Â±0.114 | VAL F1=0.873 acc=0.877 | (256, 128), Î±=2.35e-02, lr=1.4e-03, bs=32\n",
      "[18/50] cvF1=0.716Â±0.121 | VAL F1=0.851 acc=0.860 | (128,), Î±=1.29e-02, lr=7.6e-04, bs=128\n",
      "[19/50] cvF1=0.790Â±0.084 | VAL F1=0.940 acc=0.947 | (256, 128, 64), Î±=4.80e-05, lr=1.2e-04, bs=128\n",
      "[20/50] cvF1=0.792Â±0.066 | VAL F1=0.920 acc=0.930 | (256, 128), Î±=2.25e-04, lr=2.5e-04, bs=16\n",
      "[21/50] cvF1=0.617Â±0.121 | VAL F1=0.737 acc=0.754 | (128,), Î±=2.27e-02, lr=7.9e-04, bs=16\n",
      "[22/50] cvF1=0.827Â±0.062 | VAL F1=0.920 acc=0.930 | (64,), Î±=1.07e-04, lr=1.8e-04, bs=16\n",
      "[23/50] cvF1=0.802Â±0.068 | VAL F1=0.944 acc=0.947 | (64,), Î±=4.84e-03, lr=2.0e-04, bs=128\n",
      "[24/50] cvF1=0.823Â±0.070 | VAL F1=0.920 acc=0.930 | (256,), Î±=4.91e-05, lr=1.1e-03, bs=64\n",
      "[25/50] cvF1=0.669Â±0.103 | VAL F1=0.846 acc=0.860 | (64,), Î±=1.28e-03, lr=2.3e-03, bs=32\n",
      "[26/50] cvF1=0.676Â±0.120 | VAL F1=0.793 acc=0.807 | (64,), Î±=1.52e-02, lr=1.8e-03, bs=128\n",
      "[27/50] cvF1=0.821Â±0.047 | VAL F1=0.944 acc=0.947 | (128,), Î±=2.15e-05, lr=3.5e-04, bs=32\n",
      "[28/50] cvF1=0.680Â±0.127 | VAL F1=0.863 acc=0.877 | (256, 128), Î±=3.44e-03, lr=8.7e-04, bs=64\n",
      "[29/50] cvF1=0.817Â±0.084 | VAL F1=0.920 acc=0.930 | (256,), Î±=4.38e-04, lr=1.5e-04, bs=32\n",
      "[30/50] cvF1=0.696Â±0.121 | VAL F1=0.812 acc=0.825 | (256,), Î±=4.42e-03, lr=6.7e-04, bs=64\n",
      "[31/50] cvF1=0.781Â±0.094 | VAL F1=0.940 acc=0.947 | (256, 128, 64), Î±=5.21e-04, lr=5.9e-04, bs=64\n",
      "[32/50] cvF1=0.822Â±0.045 | VAL F1=0.927 acc=0.930 | (128,), Î±=1.23e-05, lr=1.4e-04, bs=64\n",
      "[33/50] cvF1=0.828Â±0.060 | VAL F1=0.944 acc=0.947 | (64,), Î±=1.24e-04, lr=5.6e-04, bs=32\n",
      "[34/50] cvF1=0.782Â±0.092 | VAL F1=0.962 acc=0.965 | (256, 128), Î±=7.36e-05, lr=4.0e-04, bs=64\n",
      "[35/50] cvF1=0.793Â±0.095 | VAL F1=0.944 acc=0.947 | (256, 128), Î±=6.25e-05, lr=1.3e-04, bs=64\n",
      "[36/50] cvF1=0.779Â±0.088 | VAL F1=0.903 acc=0.912 | (256,), Î±=3.64e-05, lr=2.4e-03, bs=16\n",
      "[37/50] cvF1=0.753Â±0.099 | VAL F1=0.881 acc=0.895 | (256, 128, 64), Î±=1.59e-03, lr=1.9e-03, bs=128\n",
      "[38/50] cvF1=0.814Â±0.053 | VAL F1=0.927 acc=0.930 | (128, 64), Î±=4.45e-05, lr=2.1e-03, bs=64\n",
      "[39/50] cvF1=0.824Â±0.047 | VAL F1=0.944 acc=0.947 | (128,), Î±=2.66e-05, lr=3.4e-04, bs=32\n",
      "[40/50] cvF1=0.822Â±0.065 | VAL F1=0.920 acc=0.930 | (64,), Î±=8.84e-05, lr=9.1e-04, bs=16\n",
      "[41/50] cvF1=0.805Â±0.061 | VAL F1=0.927 acc=0.930 | (128, 64), Î±=1.68e-04, lr=2.8e-04, bs=32\n",
      "[42/50] cvF1=0.821Â±0.066 | VAL F1=0.920 acc=0.930 | (256,), Î±=2.83e-04, lr=2.1e-04, bs=64\n",
      "[43/50] cvF1=0.808Â±0.066 | VAL F1=0.920 acc=0.930 | (128,), Î±=1.49e-04, lr=2.5e-03, bs=32\n",
      "[44/50] cvF1=0.823Â±0.068 | VAL F1=0.944 acc=0.947 | (64,), Î±=2.54e-04, lr=1.2e-04, bs=32\n",
      "[45/50] cvF1=0.799Â±0.072 | VAL F1=0.927 acc=0.930 | (128, 64), Î±=7.22e-05, lr=1.1e-03, bs=64\n",
      "[46/50] cvF1=0.813Â±0.056 | VAL F1=0.944 acc=0.947 | (64,), Î±=3.27e-05, lr=3.0e-03, bs=64\n",
      "[47/50] cvF1=0.706Â±0.092 | VAL F1=0.812 acc=0.825 | (64,), Î±=2.49e-02, lr=4.0e-04, bs=64\n",
      "[48/50] cvF1=0.787Â±0.088 | VAL F1=0.920 acc=0.930 | (256, 128), Î±=1.58e-04, lr=8.6e-04, bs=32\n",
      "[49/50] cvF1=0.788Â±0.080 | VAL F1=0.920 acc=0.930 | (128,), Î±=7.02e-04, lr=4.6e-04, bs=32\n",
      "[50/50] cvF1=0.813Â±0.066 | VAL F1=0.944 acc=0.947 | (128, 64), Î±=1.91e-05, lr=3.5e-04, bs=16\n",
      "\n",
      "ðŸ† Selected params: {'hidden_layer_sizes': (256, 128), 'alpha': 7.35900856867979e-05, 'learning_rate_init': 0.0004038176882071837, 'batch_size': 64, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "\n",
      "=== Top 5 candidates ===\n",
      "VAL F1=0.962 acc=0.965 | cvF1=0.782Â±0.092\n",
      "VAL F1=0.944 acc=0.947 | cvF1=0.828Â±0.060\n",
      "VAL F1=0.944 acc=0.947 | cvF1=0.824Â±0.047\n",
      "VAL F1=0.944 acc=0.947 | cvF1=0.823Â±0.068\n",
      "VAL F1=0.944 acc=0.947 | cvF1=0.821Â±0.047\n",
      "\n",
      "===== VAL =====\n",
      "VAL: acc=0.9649  f1_macro=0.9616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    simplify       0.93      1.00      0.96        13\n",
      "      select       0.95      1.00      0.97        19\n",
      "   aggregate       1.00      0.94      0.97        16\n",
      "    displace       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.96        57\n",
      "   macro avg       0.97      0.96      0.96        57\n",
      "weighted avg       0.97      0.96      0.96        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[13  0  0  0]\n",
      " [ 0 19  0  0]\n",
      " [ 1  0 15  0]\n",
      " [ 0  1  0  8]]\n",
      "\n",
      "===== TEST =====\n",
      "TEST: acc=0.8421  f1_macro=0.8519\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    simplify       0.73      0.85      0.79        13\n",
      "      select       0.94      0.79      0.86        19\n",
      "   aggregate       0.78      0.88      0.82        16\n",
      "    displace       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.84        57\n",
      "   macro avg       0.86      0.85      0.85        57\n",
      "weighted avg       0.86      0.84      0.84        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[11  0  2  0]\n",
      " [ 2 15  2  0]\n",
      " [ 1  1 14  0]\n",
      " [ 1  0  0  8]]\n",
      "\n",
      "Saved classifier to: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_openai_map/classifier.joblib\n",
      "   âœ… Classifier training done.\n",
      "   Saved to: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_openai_map/classifier.joblib\n",
      "   Best VAL: {'macro_f1': 0.9615600858485109, 'acc': 0.9649122807017544}\n",
      "   TEST     : {'macro_f1': 0.8518907563025211, 'acc': 0.8421052631578947}\n",
      "\n",
      "âœ… All classifiers trained.\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 10 â€” Train classifier (per experiment, MLP search + final fit) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from dataclasses import asdict, is_dataclass\n",
    "\n",
    "from imgofup.config.paths import CFG  # âœ… needed (uses CFG.SEED below)\n",
    "from imgofup.config.constants import (\n",
    "    MAPS_ID_COL,\n",
    "    CLS_SEARCH_N_ITER_DEFAULT,\n",
    "    CLS_SEARCH_N_SPLITS_DEFAULT,\n",
    "    CLS_SEARCH_SEED_DEFAULT,\n",
    ")\n",
    "from imgofup.models.train_classifier import train_mlp_classifier_with_search\n",
    "\n",
    "CLF_RESULTS = {}\n",
    "\n",
    "def _safe_get(obj, *names, default=None):\n",
    "    for n in names:\n",
    "        if hasattr(obj, n):\n",
    "            return getattr(obj, n)\n",
    "    return default\n",
    "\n",
    "print(\"\\n=== Training operator classifiers for all experiments ===\")\n",
    "\n",
    "printed_debug_fields = False\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    split = SPLITS[exp_name]\n",
    "    pre   = PREPROC[exp_name]\n",
    "    lab   = LABELS[exp_name]\n",
    "\n",
    "    X_train_s = pre[\"X_train_s\"]\n",
    "    X_val_s   = pre[\"X_val_s\"]\n",
    "    X_test_s  = pre[\"X_test_s\"]\n",
    "\n",
    "    y_train  = lab[\"y_train_cls\"]\n",
    "    y_val    = lab[\"y_val_cls\"]\n",
    "    y_test   = lab[\"y_test_cls\"]\n",
    "    sample_w = lab[\"sample_w\"]\n",
    "\n",
    "    class_names = [str(x) for x in lab[\"class_names\"]]\n",
    "\n",
    "    # Sanity checks\n",
    "    if X_train_s.shape[0] != len(y_train):\n",
    "        raise ValueError(f\"{exp_name}: X_train rows {X_train_s.shape[0]} != y_train {len(y_train)}\")\n",
    "    if X_val_s.shape[0] != len(y_val):\n",
    "        raise ValueError(f\"{exp_name}: X_val rows {X_val_s.shape[0]} != y_val {len(y_val)}\")\n",
    "    if X_test_s.shape[0] != len(y_test):\n",
    "        raise ValueError(f\"{exp_name}: X_test rows {X_test_s.shape[0]} != y_test {len(y_test)}\")\n",
    "\n",
    "    # Grouped CV: group by map_id to avoid leakage across folds\n",
    "    if MAPS_ID_COL not in split[\"df_train\"].columns:\n",
    "        raise ValueError(f\"{exp_name}: df_train missing '{MAPS_ID_COL}' for grouped CV.\")\n",
    "    groups_tr = split[\"df_train\"][MAPS_ID_COL].astype(str).to_numpy()\n",
    "\n",
    "    model_out_dir = Path(cfg[\"model_out\"]).expanduser().resolve()\n",
    "    model_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   Classes   : {class_names}\")\n",
    "    print(f\"   Train X   : {X_train_s.shape}\")\n",
    "    print(f\"   Val X     : {X_val_s.shape}\")\n",
    "    print(f\"   Test X    : {X_test_s.shape}\")\n",
    "    print(f\"   Model out : {model_out_dir}\")\n",
    "\n",
    "    res_clf = train_mlp_classifier_with_search(\n",
    "        exp_name=exp_name,\n",
    "        X_train=X_train_s,\n",
    "        y_train=y_train,\n",
    "        groups_train=groups_tr,\n",
    "        sample_w=sample_w,\n",
    "        X_val=X_val_s,\n",
    "        y_val=y_val,\n",
    "        X_test=X_test_s,\n",
    "        y_test=y_test,\n",
    "        class_names=class_names,\n",
    "        out_dir=model_out_dir,\n",
    "        n_iter=int(CLS_SEARCH_N_ITER_DEFAULT),          \n",
    "        n_splits=int(CLS_SEARCH_N_SPLITS_DEFAULT),\n",
    "        seed=int(getattr(CFG, \"SEED\", CLS_SEARCH_SEED_DEFAULT)),\n",
    "        verbose=True,\n",
    "        save_name=\"classifier.joblib\",\n",
    "    )\n",
    "\n",
    "    CLF_RESULTS[exp_name] = res_clf\n",
    "\n",
    "    # ---- robust reporting (no assumptions about field names) ----\n",
    "    model_path    = _safe_get(res_clf, \"model_path\", \"path\", default=str(model_out_dir / \"classifier.joblib\"))\n",
    "    best_val_f1   = _safe_get(res_clf, \"val_f1_macro\", \"best_val_f1\", \"val_f1\", \"best_f1\", default=None)\n",
    "    best_val_acc  = _safe_get(res_clf, \"val_acc\", \"best_val_acc\", \"best_accuracy\", default=None)\n",
    "    test_f1       = _safe_get(res_clf, \"test_f1_macro\", \"test_f1\", default=None)\n",
    "    test_acc      = _safe_get(res_clf, \"test_acc\", \"accuracy_test\", default=None)\n",
    "\n",
    "    print(\"   âœ… Classifier training done.\")\n",
    "    print(\"   Saved to:\", model_path)\n",
    "    if best_val_f1 is not None or best_val_acc is not None:\n",
    "        print(\"   Best VAL:\", {\"macro_f1\": best_val_f1, \"acc\": best_val_acc})\n",
    "    if test_f1 is not None or test_acc is not None:\n",
    "        print(\"   TEST     :\", {\"macro_f1\": test_f1, \"acc\": test_acc})\n",
    "\n",
    "    # Save lightweight meta for evaluation / reporting\n",
    "    clf_meta = {\n",
    "        \"experiment\": exp_name,\n",
    "        \"feature_mode\": cfg[\"feature_mode\"],\n",
    "        \"class_names\": class_names,\n",
    "        \"best_val\": {\"macro_f1\": best_val_f1, \"acc\": best_val_acc},\n",
    "        \"test\": {\"macro_f1\": test_f1, \"acc\": test_acc},\n",
    "        \"model_path\": str(model_path),\n",
    "    }\n",
    "    (model_out_dir / \"classifier_meta.json\").write_text(json.dumps(clf_meta, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # Print available fields once for debugging\n",
    "    if not printed_debug_fields:\n",
    "        printed_debug_fields = True\n",
    "        if is_dataclass(res_clf):\n",
    "            print(\"   (debug) Result fields:\", list(asdict(res_clf).keys()))\n",
    "        else:\n",
    "            print(\"   (debug) Result attrs :\", [a for a in dir(res_clf) if not a.startswith(\"_\")])\n",
    "\n",
    "print(\"\\nâœ… All classifiers trained.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f878f54",
   "metadata": {},
   "source": [
    "## Step 11 â€” Parameter Regression (Per-Operator) and Final Model Bundle (Experiment-Aware)\n",
    "\n",
    "This step trains **operator-specific regressors** to predict the generalization parameter in a\n",
    "**scale-independent form**, and then packages all trained components into a single,\n",
    "experiment-scoped model bundle.\n",
    "\n",
    "The same procedure is applied **independently for each experiment** (prompt-only,\n",
    "USE + map, OpenAI + map), using the shared data split and preprocessing pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### Regression target\n",
    "\n",
    "Regressors are trained on the normalized target `param_norm`, defined as:\n",
    "\n",
    "- **Distance-based operators** (`simplify`, `aggregate`, `displace`):  \n",
    "  `param_norm = param_value / extent_diag_m`\n",
    "\n",
    "- **Area-based operators** (`select`):  \n",
    "  `param_norm = param_value / extent_area_m2`\n",
    "\n",
    "This normalization allows each regressor to generalize across maps of different spatial\n",
    "extent while preserving physical meaning. During inference, predictions are converted back\n",
    "to real-world units using the same per-map extent references.\n",
    "\n",
    "---\n",
    "\n",
    "### Training strategy\n",
    "\n",
    "- One **MLPRegressor per operator**\n",
    "- Training data restricted to samples of the corresponding operator\n",
    "- **Grouped cross-validation** (`GroupKFold`) by `map_id` to prevent spatial leakage\n",
    "- Hyperparameter optimization via `RandomizedSearchCV` for each operator independently\n",
    "\n",
    "---\n",
    "\n",
    "### Final model bundle\n",
    "\n",
    "For each experiment, the trained components are stored together in a single bundle:\n",
    "\n",
    "- `cls_plus_regressors.joblib`\n",
    "\n",
    "This bundle contains:\n",
    "- the trained operator classifier\n",
    "- the dictionary of operator-specific regressors\n",
    "- the fixed class order\n",
    "- normalization metadata (operator groups and extent columns)\n",
    "\n",
    "Along with the experimentâ€™s `preproc.joblib`, this bundle is sufficient for the evaluation\n",
    "notebook to compute:\n",
    "\n",
    "1. **Classifier-only metrics**  \n",
    "2. **Regressor-only metrics** (oracle operator routing)  \n",
    "3. **End-to-end pipeline metrics** (predicted operator routing)\n",
    "\n",
    "This design keeps evaluation simple, reproducible, and fully decoupled from the training\n",
    "notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea7905b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training per-operator regressors and saving final bundles ===\n",
      "\n",
      "ðŸ§ª Experiment: openai_prompt_only\n",
      "   Model out: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_openai_prompt_only\n",
      "   Train X  : (448, 1536) | df_train: (448, 16)\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'simplify' (predicting param_norm) ===\n",
      "samples=109, groups=66, cv_splits=5, used_sample_weight=True\n",
      "best CV RMSE (scaled): 0.9979444083319207\n",
      "best CV RMSE (param_norm units): 0.004330552695041153\n",
      "best params: {'alpha': np.float64(0.0041619125396912095), 'hidden_layer_sizes': (64,), 'learning_rate_init': np.float64(0.00010558059144381523)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'select' (predicting param_norm) ===\n",
      "samples=144, groups=90, cv_splits=5, used_sample_weight=True\n",
      "best CV RMSE (scaled): 1.0163241334729527\n",
      "best CV RMSE (param_norm units): 0.0003594583729757764\n",
      "best params: {'alpha': np.float64(0.003904209851777714), 'hidden_layer_sizes': (64,), 'learning_rate_init': np.float64(0.00010546221020664906)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'aggregate' (predicting param_norm) ===\n",
      "samples=134, groups=84, cv_splits=5, used_sample_weight=True\n",
      "best CV RMSE (scaled): 1.0314572287836163\n",
      "best CV RMSE (param_norm units): 0.0035611152321258434\n",
      "best params: {'alpha': np.float64(0.0041619125396912095), 'hidden_layer_sizes': (64,), 'learning_rate_init': np.float64(0.00010558059144381523)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'displace' (predicting param_norm) ===\n",
      "samples=61, groups=45, cv_splits=5, used_sample_weight=True\n",
      "best CV RMSE (scaled): 1.0451587384992396\n",
      "best CV RMSE (param_norm units): 0.003707552540400798\n",
      "best params: {'alpha': np.float64(0.0041619125396912095), 'hidden_layer_sizes': (64,), 'learning_rate_init': np.float64(0.00010558059144381523)}\n",
      "   âœ… Saved bundle: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_openai_prompt_only/cls_plus_regressors.joblib\n",
      "   âœ… Regressors trained for: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "ðŸ§ª Experiment: use_prompt_only\n",
      "   Model out: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_use_prompt_only\n",
      "   Train X  : (448, 512) | df_train: (448, 16)\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'simplify' (predicting param_norm) ===\n",
      "samples=109, groups=66, cv_splits=5, used_sample_weight=True\n",
      "best CV RMSE (scaled): 1.001749518487077\n",
      "best CV RMSE (param_norm units): 0.0043470648673623385\n",
      "best params: {'alpha': np.float64(0.003904209851777714), 'hidden_layer_sizes': (64,), 'learning_rate_init': np.float64(0.00010546221020664906)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'select' (predicting param_norm) ===\n",
      "samples=144, groups=90, cv_splits=5, used_sample_weight=True\n",
      "best CV RMSE (scaled): 1.024593862422861\n",
      "best CV RMSE (param_norm units): 0.00036238325020281493\n",
      "best params: {'alpha': np.float64(0.003904209851777714), 'hidden_layer_sizes': (64,), 'learning_rate_init': np.float64(0.00010546221020664906)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'aggregate' (predicting param_norm) ===\n",
      "samples=134, groups=84, cv_splits=5, used_sample_weight=True\n",
      "best CV RMSE (scaled): 1.0133264116251575\n",
      "best CV RMSE (param_norm units): 0.0034985184250531753\n",
      "best params: {'alpha': np.float64(0.003904209851777714), 'hidden_layer_sizes': (64,), 'learning_rate_init': np.float64(0.00010546221020664906)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'displace' (predicting param_norm) ===\n",
      "samples=61, groups=45, cv_splits=5, used_sample_weight=True\n",
      "best CV RMSE (scaled): 1.034262775822706\n",
      "best CV RMSE (param_norm units): 0.003668900656611833\n",
      "best params: {'alpha': np.float64(0.003904209851777714), 'hidden_layer_sizes': (64,), 'learning_rate_init': np.float64(0.00010546221020664906)}\n",
      "   âœ… Saved bundle: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_use_prompt_only/cls_plus_regressors.joblib\n",
      "   âœ… Regressors trained for: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "ðŸ§ª Experiment: map_only\n",
      "   Model out: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_map_only\n",
      "   Train X  : (448, 165) | df_train: (448, 16)\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'simplify' (predicting param_norm) ===\n",
      "samples=109, groups=66, cv_splits=5, used_sample_weight=True\n",
      "best CV RMSE (scaled): 0.9718750144599333\n",
      "best CV RMSE (param_norm units): 0.004217425267352941\n",
      "best params: {'alpha': np.float64(0.000522114714225509), 'hidden_layer_sizes': (256, 128), 'learning_rate_init': np.float64(0.00016149614799999194)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'select' (predicting param_norm) ===\n",
      "samples=144, groups=90, cv_splits=5, used_sample_weight=True\n",
      "best CV RMSE (scaled): 0.7144460753377586\n",
      "best CV RMSE (param_norm units): 0.0002526886997578851\n",
      "best params: {'alpha': np.float64(8.00057734834173e-06), 'hidden_layer_sizes': (64,), 'learning_rate_init': np.float64(0.0002913009501549591)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'aggregate' (predicting param_norm) ===\n",
      "samples=134, groups=84, cv_splits=5, used_sample_weight=True\n",
      "best CV RMSE (scaled): 1.0595340899191237\n",
      "best CV RMSE (param_norm units): 0.0036580508442576698\n",
      "best params: {'alpha': np.float64(0.003904209851777714), 'hidden_layer_sizes': (64,), 'learning_rate_init': np.float64(0.00010546221020664906)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'displace' (predicting param_norm) ===\n",
      "samples=61, groups=45, cv_splits=5, used_sample_weight=True\n",
      "best CV RMSE (scaled): 0.9184685029585167\n",
      "best CV RMSE (param_norm units): 0.003258136880060582\n",
      "best params: {'alpha': np.float64(3.5186816415472715e-06), 'hidden_layer_sizes': (256, 128), 'learning_rate_init': np.float64(0.0017011002697669686)}\n",
      "   âœ… Saved bundle: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_map_only/cls_plus_regressors.joblib\n",
      "   âœ… Regressors trained for: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "ðŸ§ª Experiment: use_map\n",
      "   Model out: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_use_map\n",
      "   Train X  : (448, 677) | df_train: (448, 16)\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'simplify' (predicting param_norm) ===\n",
      "samples=109, groups=66, cv_splits=5, used_sample_weight=True\n",
      "best CV RMSE (scaled): 1.0128916050725851\n",
      "best CV RMSE (param_norm units): 0.004395415649919362\n",
      "best params: {'alpha': np.float64(1.9255661420887873e-06), 'hidden_layer_sizes': (256, 128), 'learning_rate_init': np.float64(0.001195960383019184)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'select' (predicting param_norm) ===\n",
      "samples=144, groups=90, cv_splits=5, used_sample_weight=True\n",
      "best CV RMSE (scaled): 0.7029201830351159\n",
      "best CV RMSE (param_norm units): 0.0002486121671264653\n",
      "best params: {'alpha': np.float64(3.5186816415472715e-06), 'hidden_layer_sizes': (256, 128), 'learning_rate_init': np.float64(0.0017011002697669686)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'aggregate' (predicting param_norm) ===\n",
      "samples=134, groups=84, cv_splits=5, used_sample_weight=True\n",
      "best CV RMSE (scaled): 1.0745477097051608\n",
      "best CV RMSE (param_norm units): 0.00370988549975032\n",
      "best params: {'alpha': np.float64(3.11927680501103e-05), 'hidden_layer_sizes': (256,), 'learning_rate_init': np.float64(0.00010725209743172001)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'displace' (predicting param_norm) ===\n",
      "samples=61, groups=45, cv_splits=5, used_sample_weight=True\n",
      "best CV RMSE (scaled): 0.9332264160325883\n",
      "best CV RMSE (param_norm units): 0.0033104884857002724\n",
      "best params: {'alpha': np.float64(0.0041619125396912095), 'hidden_layer_sizes': (64,), 'learning_rate_init': np.float64(0.00010558059144381523)}\n",
      "   âœ… Saved bundle: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_use_map/cls_plus_regressors.joblib\n",
      "   âœ… Regressors trained for: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "ðŸ§ª Experiment: openai_map\n",
      "   Model out: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_openai_map\n",
      "   Train X  : (448, 1701) | df_train: (448, 16)\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'simplify' (predicting param_norm) ===\n",
      "samples=109, groups=66, cv_splits=5, used_sample_weight=True\n",
      "best CV RMSE (scaled): 1.0696929107073192\n",
      "best CV RMSE (param_norm units): 0.004641903375232153\n",
      "best params: {'alpha': np.float64(3.11927680501103e-05), 'hidden_layer_sizes': (256,), 'learning_rate_init': np.float64(0.00010725209743172001)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'select' (predicting param_norm) ===\n",
      "samples=144, groups=90, cv_splits=5, used_sample_weight=True\n",
      "best CV RMSE (scaled): 0.7025967910803684\n",
      "best CV RMSE (param_norm units): 0.0002484977883155541\n",
      "best params: {'alpha': np.float64(3.5186816415472715e-06), 'hidden_layer_sizes': (256, 128), 'learning_rate_init': np.float64(0.0017011002697669686)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'aggregate' (predicting param_norm) ===\n",
      "samples=134, groups=84, cv_splits=5, used_sample_weight=True\n",
      "best CV RMSE (scaled): 1.101133652951709\n",
      "best CV RMSE (param_norm units): 0.003801673704645026\n",
      "best params: {'alpha': np.float64(0.0041619125396912095), 'hidden_layer_sizes': (64,), 'learning_rate_init': np.float64(0.00010558059144381523)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'displace' (predicting param_norm) ===\n",
      "samples=61, groups=45, cv_splits=5, used_sample_weight=True\n",
      "best CV RMSE (scaled): 0.9547415388462334\n",
      "best CV RMSE (param_norm units): 0.003386810335488665\n",
      "best params: {'alpha': np.float64(2.1453931225439485e-06), 'hidden_layer_sizes': (128,), 'learning_rate_init': np.float64(0.0001483039268456802)}\n",
      "   âœ… Saved bundle: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_openai_map/cls_plus_regressors.joblib\n",
      "   âœ… Regressors trained for: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "âœ… All bundles saved.\n",
      " - openai_prompt_only: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_openai_prompt_only/cls_plus_regressors.joblib\n",
      " - use_prompt_only   : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_use_prompt_only/cls_plus_regressors.joblib\n",
      " - map_only          : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_map_only/cls_plus_regressors.joblib\n",
      " - use_map           : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_use_map/cls_plus_regressors.joblib\n",
      " - openai_map        : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/models/exp_openai_map/cls_plus_regressors.joblib\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 11 â€” Train per-operator regressors + save final bundle (per experiment) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from imgofup.config.paths import CFG\n",
    "from imgofup.config.constants import (\n",
    "    MAPS_ID_COL,\n",
    "    PARAM_TARGET_NAME,\n",
    "    EXTENT_DIAG_COL,\n",
    "    EXTENT_AREA_COL,\n",
    "    REG_USE_LOG1P_DEFAULT,\n",
    "    REG_N_SPLITS_DEFAULT,\n",
    "    REG_N_ITER_DEFAULT,\n",
    "    REG_RANDOM_STATE_DEFAULT,\n",
    "    REG_VERBOSE_DEFAULT,\n",
    ")\n",
    "from imgofup.models.train_regressors import train_regressors_per_operator\n",
    "from imgofup.models.save_bundle import save_cls_plus_regressors_bundle\n",
    "\n",
    "BUNDLES = {}     # exp_name -> bundle path\n",
    "REG_RESULTS = {} # exp_name -> regressor training result\n",
    "\n",
    "def _safe_get(obj, *names, default=None):\n",
    "    for n in names:\n",
    "        if hasattr(obj, n):\n",
    "            return getattr(obj, n)\n",
    "    return default\n",
    "\n",
    "print(\"\\n=== Training per-operator regressors and saving final bundles ===\")\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    split = SPLITS[exp_name]\n",
    "    pre   = PREPROC[exp_name]\n",
    "    lab   = LABELS[exp_name]\n",
    "    res_clf = CLF_RESULTS[exp_name]\n",
    "\n",
    "    X_train_s = pre[\"X_train_s\"]\n",
    "    df_train  = split[\"df_train\"]\n",
    "    y_train_cls = lab[\"y_train_cls\"]\n",
    "    sample_w = lab[\"sample_w\"]\n",
    "\n",
    "    # Make sure class_names is list[str] (stable ordering)\n",
    "    cn = [str(x) for x in lab[\"class_names\"]]\n",
    "\n",
    "    model_out_dir = Path(cfg[\"model_out\"]).expanduser().resolve()\n",
    "    model_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   Model out: {model_out_dir}\")\n",
    "    print(f\"   Train X  : {X_train_s.shape} | df_train: {df_train.shape}\")\n",
    "\n",
    "    # ---- (1) Train per-operator regressors on TRAIN only ----\n",
    "    reg_res = train_regressors_per_operator(\n",
    "        X_train_s=X_train_s,\n",
    "        df_train=df_train,\n",
    "        y_train_cls=y_train_cls,\n",
    "        class_names=cn,\n",
    "        sample_w=sample_w,\n",
    "        group_col=MAPS_ID_COL,\n",
    "        target_col=PARAM_TARGET_NAME,\n",
    "        use_log1p=bool(REG_USE_LOG1P_DEFAULT),\n",
    "        n_splits=int(REG_N_SPLITS_DEFAULT),\n",
    "        n_iter=int(REG_N_ITER_DEFAULT),\n",
    "        random_state=int(getattr(CFG, \"SEED\", REG_RANDOM_STATE_DEFAULT)),\n",
    "        verbose=int(REG_VERBOSE_DEFAULT),\n",
    "    )\n",
    "\n",
    "    REG_RESULTS[exp_name] = reg_res\n",
    "\n",
    "    # ---- (2) Load the trained classifier model (from Cell 10 output) ----\n",
    "    clf_model_path = _safe_get(res_clf, \"model_path\", \"path\", default=str(model_out_dir / \"classifier.joblib\"))\n",
    "    clf_pack = joblib.load(Path(clf_model_path))\n",
    "    final_clf = clf_pack[\"model\"] if isinstance(clf_pack, dict) and \"model\" in clf_pack else clf_pack\n",
    "\n",
    "    # ---- (3) Save combined bundle for evaluation notebook ----\n",
    "    bundle_res = save_cls_plus_regressors_bundle(\n",
    "        exp_name=exp_name,\n",
    "        out_dir=model_out_dir,\n",
    "        classifier=final_clf,\n",
    "        regressors_by_class=reg_res.regressors_by_class,\n",
    "        class_names=cn,\n",
    "        use_log1p=reg_res.use_log1p,\n",
    "        cv_summary=reg_res.cv_summary,\n",
    "        distance_ops=DISTANCE_OPS,\n",
    "        area_ops=AREA_OPS,\n",
    "        diag_col=EXTENT_DIAG_COL,\n",
    "        area_col=EXTENT_AREA_COL,\n",
    "        save_name=\"cls_plus_regressors.joblib\",\n",
    "    )\n",
    "\n",
    "    BUNDLES[exp_name] = bundle_res.bundle_path\n",
    "\n",
    "    print(\"   âœ… Saved bundle:\", bundle_res.bundle_path)\n",
    "    print(\"   âœ… Regressors trained for:\", sorted(list(reg_res.regressors_by_class.keys())))\n",
    "\n",
    "print(\"\\nâœ… All bundles saved.\")\n",
    "for k, v in BUNDLES.items():\n",
    "    print(f\" - {k:18s}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87eea1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Classifier comparison (sorted by TEST macro-F1) ===\n",
      "           experiment   val_acc  val_f1_macro  test_acc  test_f1_macro  \\\n",
      "0  openai_prompt_only  0.947368      0.947169  0.929825       0.937772   \n",
      "1     use_prompt_only  0.947368      0.947169  0.877193       0.876250   \n",
      "2          openai_map  0.964912      0.961560  0.842105       0.851891   \n",
      "3             use_map  0.894737      0.889212  0.754386       0.766735   \n",
      "4            map_only  0.385965      0.378513  0.263158       0.257236   \n",
      "\n",
      "                                          model_path  \n",
      "0  /Users/amirdonyadide/Documents/GitHub/IMGOFUP/...  \n",
      "1  /Users/amirdonyadide/Documents/GitHub/IMGOFUP/...  \n",
      "2  /Users/amirdonyadide/Documents/GitHub/IMGOFUP/...  \n",
      "3  /Users/amirdonyadide/Documents/GitHub/IMGOFUP/...  \n",
      "4  /Users/amirdonyadide/Documents/GitHub/IMGOFUP/...  \n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 12A â€” Classifier comparison table =====================\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "rows = []\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    meta_path = Path(cfg[\"model_out\"]).expanduser().resolve() / \"classifier_meta.json\"\n",
    "\n",
    "    if not meta_path.is_file():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing classifier_meta.json for experiment '{exp_name}' at:\\n  {meta_path}\\n\"\n",
    "            \"Make sure Cell 10 completed successfully.\"\n",
    "        )\n",
    "\n",
    "    meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    rows.append({\n",
    "        \"experiment\": exp_name,\n",
    "        \"val_acc\": meta.get(\"best_val\", {}).get(\"acc\"),\n",
    "        \"val_f1_macro\": meta.get(\"best_val\", {}).get(\"macro_f1\"),\n",
    "        \"test_acc\": meta.get(\"test\", {}).get(\"acc\"),\n",
    "        \"test_f1_macro\": meta.get(\"test\", {}).get(\"macro_f1\"),\n",
    "        \"model_path\": meta.get(\"model_path\"),\n",
    "    })\n",
    "\n",
    "df_clf = pd.DataFrame(rows)\n",
    "\n",
    "# Sort safely (None values go to bottom)\n",
    "df_clf = df_clf.sort_values(\n",
    "    by=\"test_f1_macro\",\n",
    "    ascending=False,\n",
    "    na_position=\"last\",\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== Classifier comparison (sorted by TEST macro-F1) ===\")\n",
    "print(df_clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f40e5d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RMSE (raw units, likely in [0,1]) ===\n",
      "                    simplify    select  aggregate  displace  mean_rmse\n",
      "experiment                                                            \n",
      "map_only            0.004217  0.000253   0.003658  0.003258   0.002847\n",
      "use_map             0.004395  0.000249   0.003710  0.003310   0.002916\n",
      "use_prompt_only     0.004347  0.000362   0.003499  0.003669   0.002969\n",
      "openai_prompt_only  0.004331  0.000359   0.003561  0.003708   0.002990\n",
      "openai_map          0.004642  0.000248   0.003802  0.003387   0.003020\n",
      "\n",
      "=== RMSE as percent of [0,1] range ===\n",
      "                    simplify RMSE (%)  select RMSE (%)  aggregate RMSE (%)  \\\n",
      "experiment                                                                   \n",
      "map_only                        0.422            0.025               0.366   \n",
      "use_map                         0.440            0.025               0.371   \n",
      "use_prompt_only                 0.435            0.036               0.350   \n",
      "openai_prompt_only              0.433            0.036               0.356   \n",
      "openai_map                      0.464            0.025               0.380   \n",
      "\n",
      "                    displace RMSE (%)  Mean RMSE (%)  \n",
      "experiment                                            \n",
      "map_only                        0.326          0.285  \n",
      "use_map                         0.331          0.292  \n",
      "use_prompt_only                 0.367          0.297  \n",
      "openai_prompt_only              0.371          0.299  \n",
      "openai_map                      0.339          0.302  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                    simplify    select  aggregate  displace  mean_rmse\n",
       " experiment                                                            \n",
       " map_only            0.004217  0.000253   0.003658  0.003258   0.002847\n",
       " use_map             0.004395  0.000249   0.003710  0.003310   0.002916\n",
       " use_prompt_only     0.004347  0.000362   0.003499  0.003669   0.002969\n",
       " openai_prompt_only  0.004331  0.000359   0.003561  0.003708   0.002990\n",
       " openai_map          0.004642  0.000248   0.003802  0.003387   0.003020,\n",
       "                     simplify RMSE (%)  select RMSE (%)  aggregate RMSE (%)  \\\n",
       " experiment                                                                   \n",
       " map_only                        0.422            0.025               0.366   \n",
       " use_map                         0.440            0.025               0.371   \n",
       " use_prompt_only                 0.435            0.036               0.350   \n",
       " openai_prompt_only              0.433            0.036               0.356   \n",
       " openai_map                      0.464            0.025               0.380   \n",
       " \n",
       "                     displace RMSE (%)  Mean RMSE (%)  \n",
       " experiment                                            \n",
       " map_only                        0.326          0.285  \n",
       " use_map                         0.331          0.292  \n",
       " use_prompt_only                 0.367          0.297  \n",
       " openai_prompt_only              0.371          0.299  \n",
       " openai_map                      0.339          0.302  )"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===================== CELL 12B â€” Regressor comparison table (RMSE by operator) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) Load cv_summary from bundles (robust)\n",
    "bund_cv = {}\n",
    "for exp_name, bundle_path in BUNDLES.items():\n",
    "    bundle_path = Path(bundle_path).expanduser().resolve()\n",
    "    pack = joblib.load(bundle_path)\n",
    "\n",
    "    # try a few likely locations\n",
    "    cv_summary = pack.get(\"cv_summary\", None)\n",
    "    if cv_summary is None and isinstance(pack.get(\"regressor_meta\", None), dict):\n",
    "        cv_summary = pack[\"regressor_meta\"].get(\"cv_summary\", None)\n",
    "    if cv_summary is None and isinstance(pack.get(\"meta\", None), dict):\n",
    "        cv_summary = pack[\"meta\"].get(\"cv_summary\", None)\n",
    "\n",
    "    if cv_summary is None:\n",
    "        raise ValueError(\n",
    "            f\"{exp_name}: bundle has no cv_summary at expected locations.\\n\"\n",
    "            f\"Checked keys: 'cv_summary', 'regressor_meta.cv_summary', 'meta.cv_summary'\\n\"\n",
    "            f\"Bundle keys: {list(pack.keys()) if isinstance(pack, dict) else type(pack)}\"\n",
    "        )\n",
    "\n",
    "    bund_cv[exp_name] = cv_summary\n",
    "\n",
    "# 2) Extract RMSE for each operator\n",
    "def get_rmse_param(cv_summary, op_name):\n",
    "    d = cv_summary.get(op_name, cv_summary.get(str(op_name), {}))\n",
    "    if not isinstance(d, dict):\n",
    "        return np.nan\n",
    "\n",
    "    # common key variants\n",
    "    for k in [\"best_rmse_param\", \"rmse_param\", \"rmse_param_units\", \"best_cv_rmse_param\"]:\n",
    "        if k in d and d[k] is not None:\n",
    "            return float(d[k])\n",
    "\n",
    "    # fallback: search any numeric rmse+param-like field\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, (int, float)) and (\"rmse\" in k.lower()) and (\"param\" in k.lower()):\n",
    "            return float(v)\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "# Use your global fixed class order (from Cell 7)\n",
    "ops = list(FIXED_CLASSES)\n",
    "\n",
    "rows = []\n",
    "for exp_name, cv_summary in bund_cv.items():\n",
    "    row = {\"experiment\": exp_name}\n",
    "    for op in ops:\n",
    "        row[op] = get_rmse_param(cv_summary, op)\n",
    "    rows.append(row)\n",
    "\n",
    "df_rmse = pd.DataFrame(rows).set_index(\"experiment\")\n",
    "\n",
    "# 3) Add summary stats\n",
    "df_rmse[\"mean_rmse\"] = df_rmse[ops].mean(axis=1)\n",
    "\n",
    "# 4) Sort by mean_rmse (lower is better)\n",
    "df_rmse_sorted = df_rmse.sort_values(\"mean_rmse\", ascending=True)\n",
    "\n",
    "# 5) Human-friendly percent view (only makes sense if RMSE is on [0,1] scale)\n",
    "df_pct = (df_rmse_sorted * 100).round(3)\n",
    "rename_ops = {op: f\"{op} RMSE (%)\" for op in ops}\n",
    "rename_ops[\"mean_rmse\"] = \"Mean RMSE (%)\"\n",
    "df_pct = df_pct.rename(columns=rename_ops)\n",
    "\n",
    "print(\"\\n=== RMSE (raw units, likely in [0,1]) ===\")\n",
    "print(df_rmse_sorted.round(6))\n",
    "\n",
    "print(\"\\n=== RMSE as percent of [0,1] range ===\")\n",
    "print(df_pct)\n",
    "\n",
    "df_rmse_sorted, df_pct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79040ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment</th>\n",
       "      <th>op_acc</th>\n",
       "      <th>param_rmse_if_op_correct</th>\n",
       "      <th>param_mae_if_op_correct</th>\n",
       "      <th>joint_success@0.05</th>\n",
       "      <th>rmse_penalized_all</th>\n",
       "      <th>n_test</th>\n",
       "      <th>n_op_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>openai_prompt_only</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>0.002063</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>0.264920</td>\n",
       "      <td>57</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>use_prompt_only</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.350450</td>\n",
       "      <td>57</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openai_map</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.397371</td>\n",
       "      <td>57</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>use_map</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.495603</td>\n",
       "      <td>57</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>map_only</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.001652</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.858396</td>\n",
       "      <td>57</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           experiment    op_acc  param_rmse_if_op_correct  \\\n",
       "0  openai_prompt_only  0.929825                  0.002816   \n",
       "1     use_prompt_only  0.877193                  0.002993   \n",
       "2          openai_map  0.842105                  0.003311   \n",
       "3             use_map  0.754386                  0.003401   \n",
       "4            map_only  0.263158                  0.002274   \n",
       "\n",
       "   param_mae_if_op_correct  joint_success@0.05  rmse_penalized_all  n_test  \\\n",
       "0                 0.002063            0.929825            0.264920      57   \n",
       "1                 0.002168            0.877193            0.350450      57   \n",
       "2                 0.002366            0.842105            0.397371      57   \n",
       "3                 0.002483            0.754386            0.495603      57   \n",
       "4                 0.001652            0.263158            0.858396      57   \n",
       "\n",
       "   n_op_correct  \n",
       "0            53  \n",
       "1            50  \n",
       "2            48  \n",
       "3            43  \n",
       "4            15  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===================== CELL 12C â€” End-to-end evaluation (TEST) =====================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from imgofup.config.constants import PARAM_TARGET_NAME\n",
    "\n",
    "TOL = 0.05  # tolerance in param_norm units (0..1)\n",
    "\n",
    "def _predict_param(reg_and_scaler, Xi):\n",
    "    \"\"\"\n",
    "    reg_and_scaler is either:\n",
    "      - regressor\n",
    "      - (regressor, y_scaler) where y_scaler was fit on target y (shape [n,1])\n",
    "    Xi is a 2D row: shape (1, n_features)\n",
    "    \"\"\"\n",
    "    if isinstance(reg_and_scaler, (tuple, list)):\n",
    "        reg = reg_and_scaler[0]\n",
    "        y_scaler = reg_and_scaler[1] if len(reg_and_scaler) > 1 else None\n",
    "    else:\n",
    "        reg = reg_and_scaler\n",
    "        y_scaler = None\n",
    "\n",
    "    y_hat = float(reg.predict(Xi)[0])\n",
    "\n",
    "    if y_scaler is not None:\n",
    "        try:\n",
    "            y_hat = float(y_scaler.inverse_transform(np.array([[y_hat]], dtype=float))[0, 0])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def _safe_pack_get(pack, key, default=None):\n",
    "    if isinstance(pack, dict):\n",
    "        return pack.get(key, default)\n",
    "    return getattr(pack, key, default)\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    bundle_path = BUNDLES[exp_name]\n",
    "    bundle = joblib.load(bundle_path)\n",
    "\n",
    "    clf = _safe_pack_get(bundle, \"classifier\")\n",
    "    regs = _safe_pack_get(bundle, \"regressors_by_class\")\n",
    "    class_names = [str(x) for x in _safe_pack_get(bundle, \"class_names\", [])]\n",
    "\n",
    "    if clf is None or regs is None or not class_names:\n",
    "        raise ValueError(\n",
    "            f\"{exp_name}: bundle missing required keys. \"\n",
    "            f\"Have classifier={clf is not None}, regressors_by_class={regs is not None}, class_names={len(class_names)}\"\n",
    "        )\n",
    "\n",
    "    pre   = PREPROC[exp_name]\n",
    "    split = SPLITS[exp_name]\n",
    "    lab   = LABELS[exp_name]\n",
    "\n",
    "    X_test = pre[\"X_test_s\"]\n",
    "    y_true_cls = lab[\"y_test_cls\"]\n",
    "    df_test = split[\"df_test\"]\n",
    "\n",
    "    if PARAM_TARGET_NAME not in df_test.columns:\n",
    "        raise KeyError(\n",
    "            f\"{exp_name}: df_test has no '{PARAM_TARGET_NAME}' column. Available: {list(df_test.columns)}\"\n",
    "        )\n",
    "    y_true_param = df_test[PARAM_TARGET_NAME].to_numpy(dtype=float)\n",
    "\n",
    "    # ---- Predict operator ----\n",
    "    y_pred_cls = clf.predict(X_test)\n",
    "    op_acc = float((y_pred_cls == y_true_cls).mean())\n",
    "\n",
    "    pred_names = [class_names[int(i)] for i in y_pred_cls]\n",
    "    true_names = [class_names[int(i)] for i in y_true_cls]\n",
    "\n",
    "    # ---- Predict parameter using regressor of the PREDICTED operator ----\n",
    "    y_pred_param = np.zeros_like(y_true_param, dtype=float)\n",
    "\n",
    "    # normalize reg keys once (robust to casing)\n",
    "    regs_norm = {str(k).strip().lower(): v for k, v in regs.items()}\n",
    "\n",
    "    for i, op in enumerate(pred_names):\n",
    "        key = str(op).strip().lower()\n",
    "        if key not in regs_norm:\n",
    "            raise KeyError(\n",
    "                f\"{exp_name}: no regressor for predicted class '{op}'. \"\n",
    "                f\"Available keys: {list(regs_norm.keys())}\"\n",
    "            )\n",
    "        Xi = X_test[i:i+1]\n",
    "        y_pred_param[i] = _predict_param(regs_norm[key], Xi)\n",
    "\n",
    "    # errors\n",
    "    abs_err = np.abs(y_pred_param - y_true_param)\n",
    "    correct_mask = (np.array(pred_names) == np.array(true_names))\n",
    "\n",
    "    # Param RMSE/MAE only when operator correct\n",
    "    if correct_mask.any():\n",
    "        rmse_cond = float(np.sqrt(np.mean((y_pred_param[correct_mask] - y_true_param[correct_mask]) ** 2)))\n",
    "        mae_cond  = float(np.mean(abs_err[correct_mask]))\n",
    "    else:\n",
    "        rmse_cond, mae_cond = np.nan, np.nan\n",
    "\n",
    "    # Joint metric: operator correct AND parameter close enough\n",
    "    joint_success = float(np.mean(correct_mask & (abs_err <= TOL)))\n",
    "\n",
    "    # Penalized RMSE over ALL: if operator wrong, set error = 1.0 (max on [0,1])\n",
    "    penalized_err = abs_err.copy()\n",
    "    penalized_err[~correct_mask] = 1.0\n",
    "    rmse_penalized = float(np.sqrt(np.mean(penalized_err ** 2)))\n",
    "\n",
    "    rows.append({\n",
    "        \"experiment\": exp_name,\n",
    "        \"op_acc\": op_acc,\n",
    "        \"param_rmse_if_op_correct\": rmse_cond,\n",
    "        \"param_mae_if_op_correct\": mae_cond,\n",
    "        f\"joint_success@{TOL}\": joint_success,\n",
    "        \"rmse_penalized_all\": rmse_penalized,\n",
    "        \"n_test\": int(len(y_true_cls)),\n",
    "        \"n_op_correct\": int(correct_mask.sum()),\n",
    "    })\n",
    "\n",
    "df_e2e = pd.DataFrame(rows).sort_values(\"rmse_penalized_all\", ascending=True).reset_index(drop=True)\n",
    "df_e2e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c75dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
