{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "289dc466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Repo root: /Users/amirdonyadide/Documents/GitHub/IMGOFUP\n",
      "ðŸ“¦ Using src from: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/src\n",
      "ðŸ”§ PROJ_ROOT env set to: /Users/amirdonyadide/Documents/GitHub/IMGOFUP\n"
     ]
    }
   ],
   "source": [
    "# ===================== 02_build_training_data â€” CELL 0: Bootstrap =====================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "p = Path.cwd().resolve()\n",
    "REPO_ROOT = None\n",
    "for candidate in [p, *p.parents]:\n",
    "    if (candidate / \"src\" / \"imgofup\").is_dir():\n",
    "        REPO_ROOT = candidate\n",
    "        break\n",
    "if REPO_ROOT is None:\n",
    "    raise RuntimeError(\"Could not find repo root (no 'src/imgofup' found).\")\n",
    "\n",
    "SRC_DIR = REPO_ROOT / \"src\"\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "os.environ[\"PROJ_ROOT\"] = str(REPO_ROOT)\n",
    "\n",
    "print(\"ðŸ“¦ Repo root:\", REPO_ROOT)\n",
    "print(\"ðŸ“¦ Using src from:\", SRC_DIR)\n",
    "print(\"ðŸ”§ PROJ_ROOT env set to:\", os.environ[\"PROJ_ROOT\"])\n",
    "\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09e7d833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Experiments:\n",
      " - openai_prompt_only | mode=prompt_only    | prompt=openai-small  \n",
      " - use_prompt_only    | mode=prompt_only    | prompt=dan           \n",
      " - map_only           | mode=map_only       | prompt=-             \n",
      " - use_map            | mode=prompt_plus_map | prompt=dan           \n",
      " - openai_map         | mode=prompt_plus_map | prompt=openai-small  \n"
     ]
    }
   ],
   "source": [
    "# ===================== 02_build_training_data â€” CELL 1: Experiment registry =====================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    \"openai_prompt_only\": {\n",
    "        \"train_out\": DATA_DIR / \"output\" / \"train_out_openai_prompt_only\",\n",
    "        \"model_out\": DATA_DIR / \"output\" / \"models\" / \"exp_openai_prompt_only\",\n",
    "        \"feature_mode\": \"prompt_only\",\n",
    "        \"prompt_encoder_kind\": \"openai-small\",\n",
    "    },\n",
    "    \"use_prompt_only\": {\n",
    "        \"train_out\": DATA_DIR / \"output\" / \"train_out_use_prompt_only\",\n",
    "        \"model_out\": DATA_DIR / \"output\" / \"models\" / \"exp_use_prompt_only\",\n",
    "        \"feature_mode\": \"prompt_only\",\n",
    "        \"prompt_encoder_kind\": \"dan\",\n",
    "    },\n",
    "    \"map_only\": {\n",
    "        \"train_out\": DATA_DIR / \"output\" / \"train_out_map_only\",\n",
    "        \"model_out\": DATA_DIR / \"output\" / \"models\" / \"exp_map_only\",\n",
    "        \"feature_mode\": \"map_only\",\n",
    "    },\n",
    "    \"use_map\": {\n",
    "        \"train_out\": DATA_DIR / \"output\" / \"train_out_use_map\",\n",
    "        \"model_out\": DATA_DIR / \"output\" / \"models\" / \"exp_use_map\",\n",
    "        \"feature_mode\": \"prompt_plus_map\",\n",
    "        \"prompt_encoder_kind\": \"dan\",\n",
    "    },\n",
    "    \"openai_map\": {\n",
    "        \"train_out\": DATA_DIR / \"output\" / \"train_out_openai_map\",\n",
    "        \"model_out\": DATA_DIR / \"output\" / \"models\" / \"exp_openai_map\",\n",
    "        \"feature_mode\": \"prompt_plus_map\",\n",
    "        \"prompt_encoder_kind\": \"openai-small\",\n",
    "    },\n",
    "}\n",
    "\n",
    "for exp_cfg in EXPERIMENTS.values():\n",
    "    exp_cfg[\"train_out\"] = Path(exp_cfg[\"train_out\"])\n",
    "    exp_cfg[\"model_out\"] = Path(exp_cfg[\"model_out\"])\n",
    "    exp_cfg[\"train_out\"].mkdir(parents=True, exist_ok=True)\n",
    "    exp_cfg[\"model_out\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ðŸ§ª Experiments:\")\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    pe = cfg.get(\"prompt_encoder_kind\", \"-\")\n",
    "    print(f\" - {exp_name:18s} | mode={cfg['feature_mode']:14s} | prompt={pe:14s}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4053d003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP_EMB_DIR: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/map_out/shared_extent\n",
      "maps_npz: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/map_out/shared_extent/maps_embeddings.npz | exists: True\n",
      "maps_pq : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/map_out/shared_extent/maps.parquet | exists: True\n",
      "\n",
      "openai_prompt_only: prompt_out=/Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_openai_prompt_only/prompt_out\n",
      "  npz: prompts_embeddings.npz | exists: True\n",
      "  pq : prompts.parquet | exists: True\n",
      "\n",
      "use_prompt_only: prompt_out=/Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_use_prompt_only/prompt_out\n",
      "  npz: prompts_embeddings.npz | exists: True\n",
      "  pq : prompts.parquet | exists: True\n",
      "\n",
      "use_map: prompt_out=/Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_use_map/prompt_out\n",
      "  npz: prompts_embeddings.npz | exists: True\n",
      "  pq : prompts.parquet | exists: True\n",
      "\n",
      "openai_map: prompt_out=/Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_openai_map/prompt_out\n",
      "  npz: prompts_embeddings.npz | exists: True\n",
      "  pq : prompts.parquet | exists: True\n"
     ]
    }
   ],
   "source": [
    "# ===================== 02_build_training_data â€” CELL 2: Locate required artifacts =====================\n",
    "\n",
    "from pathlib import Path\n",
    "from imgofup.config import paths as CONFIG\n",
    "from imgofup.config.constants import (\n",
    "    MAP_EMBEDDINGS_NPZ_NAME, MAPS_PARQUET_NAME,\n",
    "    PROMPT_EMBEDDINGS_NPZ_NAME, PROMPTS_PARQUET_NAME,\n",
    ")\n",
    "\n",
    "# Shared map embeddings location (matches 01 notebook convention)\n",
    "MAP_EMB_DIR = Path(CONFIG.PATHS.MAP_OUT) / \"shared_extent\"\n",
    "maps_npz = MAP_EMB_DIR / MAP_EMBEDDINGS_NPZ_NAME\n",
    "maps_pq  = MAP_EMB_DIR / MAPS_PARQUET_NAME\n",
    "\n",
    "print(\"MAP_EMB_DIR:\", MAP_EMB_DIR)\n",
    "print(\"maps_npz:\", maps_npz, \"| exists:\", maps_npz.is_file())\n",
    "print(\"maps_pq :\", maps_pq,  \"| exists:\", maps_pq.is_file())\n",
    "\n",
    "if not maps_npz.is_file() or not maps_pq.is_file():\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing shared map embedding artifacts.\\n\"\n",
    "        f\"Expected:\\n  {maps_npz}\\n  {maps_pq}\\n\"\n",
    "        \"Run notebooks/01_generate_embeddings.ipynb first.\"\n",
    "    )\n",
    "\n",
    "# Per-experiment prompt artifacts expected under <train_out>/prompt_out/\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    if cfg[\"feature_mode\"] == \"map_only\":\n",
    "        continue\n",
    "    prompt_out = Path(cfg[\"train_out\"]) / \"prompt_out\"\n",
    "    prm_npz = prompt_out / PROMPT_EMBEDDINGS_NPZ_NAME\n",
    "    prm_pq  = prompt_out / PROMPTS_PARQUET_NAME\n",
    "    print(f\"\\n{exp_name}: prompt_out={prompt_out}\")\n",
    "    print(\"  npz:\", prm_npz.name, \"| exists:\", prm_npz.is_file())\n",
    "    print(\"  pq :\", prm_pq.name,  \"| exists:\", prm_pq.is_file())\n",
    "    if not prm_npz.is_file() or not prm_pq.is_file():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing prompt artifacts for {exp_name}.\\n\"\n",
    "            f\"Expected:\\n  {prm_npz}\\n  {prm_pq}\\n\"\n",
    "            \"Run notebooks/01_generate_embeddings.ipynb first.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc7e1828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading training data for all experiments (unified loader) ===\n",
      "\n",
      "ðŸ§ª Experiment: openai_prompt_only\n",
      "   train_out : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_openai_prompt_only\n",
      "   mode      : prompt_only\n",
      "   âœ… Loaded: X=(562, 1536) | df=(562, 15)\n",
      "   Operators: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "ðŸ§ª Experiment: use_prompt_only\n",
      "   train_out : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_use_prompt_only\n",
      "   mode      : prompt_only\n",
      "   âœ… Loaded: X=(562, 512) | df=(562, 15)\n",
      "   Operators: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "ðŸ§ª Experiment: map_only\n",
      "   train_out : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_map_only\n",
      "   mode      : map_only\n",
      "   âœ… Loaded: X=(562, 165) | df=(562, 15)\n",
      "   Operators: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "ðŸ§ª Experiment: use_map\n",
      "   train_out : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_use_map\n",
      "   mode      : prompt_plus_map\n",
      "   âœ… Loaded: X=(562, 677) | df=(562, 15)\n",
      "   Operators: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "ðŸ§ª Experiment: openai_map\n",
      "   train_out : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out_openai_map\n",
      "   mode      : prompt_plus_map\n",
      "   âœ… Loaded: X=(562, 1701) | df=(562, 15)\n",
      "   Operators: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "First loaded experiment: openai_prompt_only\n"
     ]
    }
   ],
   "source": [
    "# ===================== 02_build_training_data â€” CELL 3: Load training data + compute param_norm =====================\n",
    "\n",
    "from dataclasses import replace\n",
    "from pathlib import Path\n",
    "\n",
    "from imgofup.config import paths\n",
    "from imgofup.datasets.load_training_data import load_training_data_with_dynamic_param_norm\n",
    "\n",
    "TRAIN_DATA = {}  # exp_name -> {\"X\":..., \"df\":..., \"paths\":...}\n",
    "\n",
    "print(\"\\n=== Loading training data for all experiments (unified loader) ===\")\n",
    "\n",
    "for exp_name, exp_cfg in EXPERIMENTS.items():\n",
    "    train_out_dir = Path(exp_cfg[\"train_out\"]).expanduser().resolve()\n",
    "    if not train_out_dir.is_dir():\n",
    "        raise FileNotFoundError(f\"Missing train_out directory for {exp_name}: {train_out_dir}\")\n",
    "\n",
    "    feature_mode = str(exp_cfg[\"feature_mode\"]).strip().lower()\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   train_out : {train_out_dir}\")\n",
    "    print(f\"   mode      : {feature_mode}\")\n",
    "\n",
    "    # Important: override TRAIN_OUT per experiment\n",
    "    PATHS_EXP = replace(paths.PATHS, TRAIN_OUT=train_out_dir)\n",
    "\n",
    "    # Only require text when prompts are part of the feature space\n",
    "    require_text = feature_mode in {\"prompt_only\", \"prompt_plus_map\"}\n",
    "\n",
    "    data = load_training_data_with_dynamic_param_norm(\n",
    "        exp_name=exp_name,\n",
    "        feature_mode=feature_mode,\n",
    "        paths=PATHS_EXP,\n",
    "        cfg=paths.CFG,\n",
    "        distance_ops=paths.DISTANCE_OPS,\n",
    "        area_ops=paths.AREA_OPS,\n",
    "        require_text=require_text,\n",
    "    )\n",
    "\n",
    "    X = data.X\n",
    "    df = data.df\n",
    "\n",
    "    print(f\"   âœ… Loaded: X={X.shape} | df={df.shape}\")\n",
    "    if PATHS_EXP.OPERATOR_COL in df.columns:\n",
    "        print(\"   Operators:\", sorted(df[PATHS_EXP.OPERATOR_COL].dropna().unique().tolist()))\n",
    "\n",
    "    TRAIN_DATA[exp_name] = {\"X\": X, \"df\": df, \"paths\": PATHS_EXP}\n",
    "\n",
    "first_key = next(iter(TRAIN_DATA.keys()))\n",
    "print(\"\\nFirst loaded experiment:\", first_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6da952ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Computing shared split using reference experiment: use_prompt_only ===\n",
      "ref_df: (562, 16) | ref_X: (562, 512)\n",
      "Saving split to: /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out/splits/splits_shared.json\n",
      "=== DATASET SUMMARY ===\n",
      "Total rows (prompts): 562\n",
      "Unique maps: 399\n",
      "Multi-prompt maps (>1 prompt): 22\n",
      "Single-prompt maps (=1 prompt): 377\n",
      "\n",
      "Top 10 maps by prompt count:\n",
      "map_id\n",
      "1646    30\n",
      "1304    29\n",
      "1755    26\n",
      "1532    13\n",
      "0127    10\n",
      "0168     8\n",
      "0142     7\n",
      "0078     6\n",
      "0080     6\n",
      "0001     6\n",
      "dtype: int64\n",
      "\n",
      "âœ… Saved splits to /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out/splits/splits_shared.json\n",
      "\n",
      "âœ… Shared split created:\n",
      "   Train keys: 448 | Val keys: 57 | Test keys: 57\n",
      "   Saved to  : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/train_out/splits/splits_shared.json\n",
      "\n",
      "ðŸ§ª openai_prompt_only\n",
      "Rows -> Train: (448, 1536) Val: (57, 1536) Test: (57, 1536)\n",
      "\n",
      "ðŸ§ª use_prompt_only\n",
      "Rows -> Train: (448, 512) Val: (57, 512) Test: (57, 512)\n",
      "\n",
      "ðŸ§ª map_only\n",
      "Rows -> Train: (448, 165) Val: (57, 165) Test: (57, 165)\n",
      "\n",
      "ðŸ§ª use_map\n",
      "Rows -> Train: (448, 677) Val: (57, 677) Test: (57, 677)\n",
      "\n",
      "ðŸ§ª openai_map\n",
      "Rows -> Train: (448, 1701) Val: (57, 1701) Test: (57, 1701)\n"
     ]
    }
   ],
   "source": [
    "# ===================== 02_build_training_data â€” CELL 4: Shared Train/Val/Test Split =====================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from imgofup.config import paths\n",
    "from imgofup.config.constants import MAPS_ID_COL, PROMPTS_PROMPT_ID_COL\n",
    "from imgofup.datasets.splitting import make_splits_multi_prompt_to_train\n",
    "\n",
    "FIXED_CLASSES = [\"simplify\", \"select\", \"aggregate\", \"displace\"]\n",
    "USE_INTENSITY_FOR_STRAT = True\n",
    "\n",
    "OP_COL  = paths.PATHS.OPERATOR_COL\n",
    "INT_COL = paths.PATHS.INTENSITY_COL\n",
    "\n",
    "MAP_ID_COL = MAPS_ID_COL\n",
    "PROMPT_ID_COL = PROMPTS_PROMPT_ID_COL\n",
    "\n",
    "# Save ONE shared split for all experiments\n",
    "SPLITS_DIR = Path(paths.PATHS.SPLIT_OUT).expanduser().resolve()\n",
    "SPLITS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "split_path = SPLITS_DIR / \"splits_shared.json\"\n",
    "\n",
    "# Choose reference experiment (prefer prompt-based)\n",
    "preferred_order = [\"use_prompt_only\", \"use_map\", \"openai_map\", \"map_only\"]\n",
    "ref_exp = next((name for name in preferred_order if name in TRAIN_DATA), None)\n",
    "if ref_exp is None:\n",
    "    ref_exp = next(iter(TRAIN_DATA.keys()))\n",
    "\n",
    "ref_df = TRAIN_DATA[ref_exp][\"df\"].copy().reset_index(drop=True)\n",
    "ref_X  = TRAIN_DATA[ref_exp][\"X\"]\n",
    "\n",
    "if not {MAP_ID_COL, PROMPT_ID_COL}.issubset(ref_df.columns):\n",
    "    raise ValueError(f\"Expected columns {{{MAP_ID_COL!r},{PROMPT_ID_COL!r}}} in df for split mapping.\")\n",
    "if OP_COL not in ref_df.columns:\n",
    "    raise ValueError(f\"Reference df missing operator column '{OP_COL}'.\")\n",
    "\n",
    "ref_df[\"row_key\"] = ref_df[MAP_ID_COL].astype(str).str.zfill(4) + \"::\" + ref_df[PROMPT_ID_COL].astype(str)\n",
    "\n",
    "print(f\"\\n=== Computing shared split using reference experiment: {ref_exp} ===\")\n",
    "print(\"ref_df:\", ref_df.shape, \"| ref_X:\", ref_X.shape)\n",
    "print(\"Saving split to:\", split_path)\n",
    "\n",
    "split = make_splits_multi_prompt_to_train(\n",
    "    df=ref_df,\n",
    "    X=ref_X,\n",
    "    op_col=OP_COL,\n",
    "    intensity_col=INT_COL if (USE_INTENSITY_FOR_STRAT and INT_COL in ref_df.columns) else None,\n",
    "    map_id_col=MAP_ID_COL,\n",
    "    fixed_classes=FIXED_CLASSES,\n",
    "    use_intensity_for_strat=USE_INTENSITY_FOR_STRAT,\n",
    "    seed=int(paths.CFG.SEED),\n",
    "    val_ratio=float(paths.CFG.VAL_RATIO),\n",
    "    test_ratio=float(paths.CFG.TEST_RATIO),\n",
    "    max_attempts=500,\n",
    "    save_splits_json=split_path,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "train_idx_ref, val_idx_ref, test_idx_ref = split.train_idx, split.val_idx, split.test_idx\n",
    "\n",
    "train_keys = set(ref_df.loc[train_idx_ref, \"row_key\"].tolist())\n",
    "val_keys   = set(ref_df.loc[val_idx_ref,   \"row_key\"].tolist()) if len(val_idx_ref) else set()\n",
    "test_keys  = set(ref_df.loc[test_idx_ref,  \"row_key\"].tolist()) if len(test_idx_ref) else set()\n",
    "\n",
    "assert train_keys.isdisjoint(val_keys)\n",
    "assert train_keys.isdisjoint(test_keys)\n",
    "assert val_keys.isdisjoint(test_keys)\n",
    "\n",
    "print(\"\\nâœ… Shared split created:\")\n",
    "print(f\"   Train keys: {len(train_keys)} | Val keys: {len(val_keys)} | Test keys: {len(test_keys)}\")\n",
    "print(f\"   Saved to  : {split_path}\")\n",
    "\n",
    "# Apply split to each experiment\n",
    "SPLITS = {}\n",
    "needed_keys = train_keys | val_keys | test_keys\n",
    "\n",
    "for exp_name, pack in TRAIN_DATA.items():\n",
    "    df = pack[\"df\"].copy().reset_index(drop=True)\n",
    "    X  = pack[\"X\"]\n",
    "\n",
    "    if not {MAP_ID_COL, PROMPT_ID_COL}.issubset(df.columns):\n",
    "        raise ValueError(f\"Experiment '{exp_name}' df missing {MAP_ID_COL}/{PROMPT_ID_COL} needed for split mapping.\")\n",
    "\n",
    "    df[\"row_key\"] = df[MAP_ID_COL].astype(str).str.zfill(4) + \"::\" + df[PROMPT_ID_COL].astype(str)\n",
    "\n",
    "    missing = needed_keys - set(df[\"row_key\"].tolist())\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"Experiment '{exp_name}' is missing {len(missing)} rows from the shared split \"\n",
    "            f\"(first few: {list(sorted(missing))[:5]}).\\n\"\n",
    "            \"This usually means the pairs universe differs between experiments.\\n\"\n",
    "            \"Fix: ensure map_only uses the same prompts.parquet universe and concat is consistent.\"\n",
    "        )\n",
    "\n",
    "    train_idx = df.index[df[\"row_key\"].isin(train_keys)].to_numpy()\n",
    "    val_idx   = df.index[df[\"row_key\"].isin(val_keys)].to_numpy() if val_keys else df.index[:0].to_numpy()\n",
    "    test_idx  = df.index[df[\"row_key\"].isin(test_keys)].to_numpy() if test_keys else df.index[:0].to_numpy()\n",
    "\n",
    "    X_train, X_val, X_test = X[train_idx], X[val_idx], X[test_idx]\n",
    "    df_train = df.loc[train_idx].reset_index(drop=True)\n",
    "    df_val   = df.loc[val_idx].reset_index(drop=True)\n",
    "    df_test  = df.loc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    SPLITS[exp_name] = {\n",
    "        \"train_idx\": train_idx,\n",
    "        \"val_idx\": val_idx,\n",
    "        \"test_idx\": test_idx,\n",
    "        \"X_train\": X_train, \"X_val\": X_val, \"X_test\": X_test,\n",
    "        \"df_train\": df_train, \"df_val\": df_val, \"df_test\": df_test,\n",
    "    }\n",
    "\n",
    "    print(f\"\\nðŸ§ª {exp_name}\")\n",
    "    print(\"Rows -> Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e7e49cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fitting modality-aware preprocessing per experiment ===\n",
      "\n",
      "ðŸ§ª Experiment: openai_prompt_only\n",
      "   Feature mode : prompt_only -> preproc_mode=prompt_only\n",
      "   map_dim      : 0\n",
      "   prompt_dim   : 1536\n",
      "   Save preproc : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/models/exp_openai_prompt_only/preproc.joblib\n",
      "   âœ… Preprocessing complete.\n",
      "   Shapes: (448, 1536) (57, 1536) (57, 1536)\n",
      "\n",
      "ðŸ§ª Experiment: use_prompt_only\n",
      "   Feature mode : prompt_only -> preproc_mode=prompt_only\n",
      "   map_dim      : 0\n",
      "   prompt_dim   : 512\n",
      "   Save preproc : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/models/exp_use_prompt_only/preproc.joblib\n",
      "   âœ… Preprocessing complete.\n",
      "   Shapes: (448, 512) (57, 512) (57, 512)\n",
      "\n",
      "ðŸ§ª Experiment: map_only\n",
      "   Feature mode : map_only -> preproc_mode=prompt_plus_map\n",
      "   map_dim      : 165\n",
      "   prompt_dim   : 0\n",
      "   Save preproc : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/models/exp_map_only/preproc.joblib\n",
      "   âœ… Preprocessing complete.\n",
      "   Shapes: (448, 165) (57, 165) (57, 165)\n",
      "\n",
      "ðŸ§ª Experiment: use_map\n",
      "   Feature mode : prompt_plus_map -> preproc_mode=prompt_plus_map\n",
      "   map_dim      : 165\n",
      "   prompt_dim   : 512\n",
      "   Save preproc : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/models/exp_use_map/preproc.joblib\n",
      "   âœ… Preprocessing complete.\n",
      "   Shapes: (448, 677) (57, 677) (57, 677)\n",
      "\n",
      "ðŸ§ª Experiment: openai_map\n",
      "   Feature mode : prompt_plus_map -> preproc_mode=prompt_plus_map\n",
      "   map_dim      : 165\n",
      "   prompt_dim   : 1536\n",
      "   Save preproc : /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/models/exp_openai_map/preproc.joblib\n",
      "   âœ… Preprocessing complete.\n",
      "   Shapes: (448, 1701) (57, 1701) (57, 1701)\n",
      "\n",
      "âœ… All preprocessing finished.\n"
     ]
    }
   ],
   "source": [
    "# ===================== 02_build_training_data â€” CELL 5: Preprocessing (per experiment) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from imgofup.preprocessing.preprocessing import fit_transform_modality_preproc\n",
    "from imgofup.config.constants import (\n",
    "    MAP_CLIP_Q_DEFAULT,\n",
    "    MAP_IMPUTE_STRATEGY_DEFAULT,\n",
    "    MAP_ROBUST_QRANGE_DEFAULT,\n",
    "    MAP_VAR_EPS_DEFAULT,\n",
    "    MAP_EMBEDDINGS_NPZ_NAME,\n",
    "    PROMPT_EMBEDDINGS_NPZ_NAME,\n",
    ")\n",
    "\n",
    "def _infer_dim_from_npz(npz_path: Path) -> int:\n",
    "    with np.load(npz_path, allow_pickle=True) as z:\n",
    "        E = z[\"E\"]\n",
    "    return int(E.shape[1])\n",
    "\n",
    "# Shared MAP_DIM from shared embeddings\n",
    "MAP_DIM_INF = _infer_dim_from_npz(Path(MAP_EMB_DIR) / MAP_EMBEDDINGS_NPZ_NAME)\n",
    "\n",
    "# Fill dims into EXPERIMENTS (like 01 did)\n",
    "for exp_name, exp_cfg in EXPERIMENTS.items():\n",
    "    fm = str(exp_cfg[\"feature_mode\"]).strip().lower()\n",
    "    prompt_dim = 0\n",
    "    if fm in {\"prompt_only\", \"prompt_plus_map\"}:\n",
    "        prm_npz = Path(exp_cfg[\"train_out\"]) / \"prompt_out\" / PROMPT_EMBEDDINGS_NPZ_NAME\n",
    "        prompt_dim = _infer_dim_from_npz(prm_npz)\n",
    "\n",
    "    if fm == \"prompt_only\":\n",
    "        exp_cfg[\"map_dim\"], exp_cfg[\"prompt_dim\"] = 0, prompt_dim\n",
    "    elif fm == \"map_only\":\n",
    "        exp_cfg[\"map_dim\"], exp_cfg[\"prompt_dim\"] = MAP_DIM_INF, 0\n",
    "    elif fm == \"prompt_plus_map\":\n",
    "        exp_cfg[\"map_dim\"], exp_cfg[\"prompt_dim\"] = MAP_DIM_INF, prompt_dim\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown feature_mode: {fm}\")\n",
    "\n",
    "PREPROC = {}\n",
    "print(\"\\n=== Fitting modality-aware preprocessing per experiment ===\")\n",
    "\n",
    "def _to_preproc_mode(feature_mode: str) -> str:\n",
    "    fm = str(feature_mode).strip().lower()\n",
    "    if fm == \"prompt_only\":\n",
    "        return \"prompt_only\"\n",
    "    if fm in {\"prompt_plus_map\", \"map_only\"}:\n",
    "        return \"prompt_plus_map\"\n",
    "    raise ValueError(f\"Unsupported feature_mode for preprocessing: {feature_mode}\")\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    split = SPLITS[exp_name]\n",
    "    feature_mode = cfg[\"feature_mode\"]\n",
    "    preproc_mode = _to_preproc_mode(feature_mode)\n",
    "\n",
    "    map_dim    = int(cfg[\"map_dim\"])\n",
    "    prompt_dim = int(cfg[\"prompt_dim\"])\n",
    "\n",
    "    model_out_dir = Path(cfg[\"model_out\"]).expanduser().resolve()\n",
    "    model_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    preproc_path = model_out_dir / \"preproc.joblib\"\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   Feature mode : {feature_mode} -> preproc_mode={preproc_mode}\")\n",
    "    print(f\"   map_dim      : {map_dim}\")\n",
    "    print(f\"   prompt_dim   : {prompt_dim}\")\n",
    "    print(f\"   Save preproc : {preproc_path}\")\n",
    "\n",
    "    Xtr = split[\"X_train\"]\n",
    "    if Xtr.shape[1] != (map_dim + prompt_dim):\n",
    "        raise ValueError(\n",
    "            f\"Dim mismatch in {exp_name}: X_train has {Xtr.shape[1]} cols, \"\n",
    "            f\"but map_dim+prompt_dim={map_dim + prompt_dim}.\"\n",
    "        )\n",
    "\n",
    "    res = fit_transform_modality_preproc(\n",
    "        X_train=split[\"X_train\"],\n",
    "        X_val=split[\"X_val\"],\n",
    "        X_test=split[\"X_test\"],\n",
    "        feature_mode=preproc_mode,\n",
    "        map_dim=map_dim,\n",
    "        prompt_dim=prompt_dim,\n",
    "        eps=float(MAP_VAR_EPS_DEFAULT),\n",
    "        clip_q=tuple(MAP_CLIP_Q_DEFAULT),\n",
    "        impute_strategy=str(MAP_IMPUTE_STRATEGY_DEFAULT),\n",
    "        robust_qrange=tuple(MAP_ROBUST_QRANGE_DEFAULT),\n",
    "        save_path=preproc_path,\n",
    "    )\n",
    "\n",
    "    PREPROC[exp_name] = {\n",
    "        \"X_train_s\": res.X_train_s,\n",
    "        \"X_val_s\":   res.X_val_s,\n",
    "        \"X_test_s\":  res.X_test_s,\n",
    "        \"bundle_path\": res.bundle_path,\n",
    "    }\n",
    "\n",
    "    print(\"   âœ… Preprocessing complete.\")\n",
    "    print(\"   Shapes:\", res.X_train_s.shape, res.X_val_s.shape, res.X_test_s.shape)\n",
    "\n",
    "print(\"\\nâœ… All preprocessing finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dcbccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Building labels and sample weights per experiment ===\n",
      "\n",
      "ðŸ§ª openai_prompt_only\n",
      "Classes (fixed order): [np.str_('simplify'), np.str_('select'), np.str_('aggregate'), np.str_('displace')]\n",
      "Class weights: {'simplify': 1.0275229357798166, 'select': 0.7777777777777778, 'aggregate': 0.835820895522388, 'displace': 1.8360655737704918}\n",
      "y_train/y_val/y_test shapes: (448,) (57,) (57,)\n",
      "Sample weight summary: {'min': 0.025925925925925925, 'max': 1.8360655737704918, 'mean': 0.6487687942076353}\n",
      "\n",
      "ðŸ§ª use_prompt_only\n",
      "Classes (fixed order): [np.str_('simplify'), np.str_('select'), np.str_('aggregate'), np.str_('displace')]\n",
      "Class weights: {'simplify': 1.0275229357798166, 'select': 0.7777777777777778, 'aggregate': 0.835820895522388, 'displace': 1.8360655737704918}\n",
      "y_train/y_val/y_test shapes: (448,) (57,) (57,)\n",
      "Sample weight summary: {'min': 0.025925925925925925, 'max': 1.8360655737704918, 'mean': 0.6487687942076353}\n",
      "\n",
      "ðŸ§ª map_only\n",
      "Classes (fixed order): [np.str_('simplify'), np.str_('select'), np.str_('aggregate'), np.str_('displace')]\n",
      "Class weights: {'simplify': 1.0275229357798166, 'select': 0.7777777777777778, 'aggregate': 0.835820895522388, 'displace': 1.8360655737704918}\n",
      "y_train/y_val/y_test shapes: (448,) (57,) (57,)\n",
      "Sample weight summary: {'min': 0.025925925925925925, 'max': 1.8360655737704918, 'mean': 0.6487687942076353}\n",
      "\n",
      "ðŸ§ª use_map\n",
      "Classes (fixed order): [np.str_('simplify'), np.str_('select'), np.str_('aggregate'), np.str_('displace')]\n",
      "Class weights: {'simplify': 1.0275229357798166, 'select': 0.7777777777777778, 'aggregate': 0.835820895522388, 'displace': 1.8360655737704918}\n",
      "y_train/y_val/y_test shapes: (448,) (57,) (57,)\n",
      "Sample weight summary: {'min': 0.025925925925925925, 'max': 1.8360655737704918, 'mean': 0.6487687942076353}\n",
      "\n",
      "ðŸ§ª openai_map\n",
      "Classes (fixed order): [np.str_('simplify'), np.str_('select'), np.str_('aggregate'), np.str_('displace')]\n",
      "Class weights: {'simplify': 1.0275229357798166, 'select': 0.7777777777777778, 'aggregate': 0.835820895522388, 'displace': 1.8360655737704918}\n",
      "y_train/y_val/y_test shapes: (448,) (57,) (57,)\n",
      "Sample weight summary: {'min': 0.025925925925925925, 'max': 1.8360655737704918, 'mean': 0.6487687942076353}\n",
      "\n",
      "âœ… Label build complete for all experiments (class order consistent).\n"
     ]
    }
   ],
   "source": [
    "# ===================== 02_build_training_data â€” CELL 6: Labels + sample weights =====================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from imgofup.config import paths\n",
    "from imgofup.config.constants import (\n",
    "    MAPS_ID_COL,\n",
    "    CLASS_WEIGHT_MODE_DEFAULT,\n",
    "    USE_MAP_WEIGHT_DEFAULT,\n",
    ")\n",
    "from imgofup.datasets.labels_and_weights import build_labels_and_sample_weights\n",
    "\n",
    "OP_COL = paths.PATHS.OPERATOR_COL\n",
    "MAP_ID_COL = MAPS_ID_COL\n",
    "\n",
    "LABELS = {}\n",
    "\n",
    "print(\"\\n=== Building labels and sample weights per experiment ===\")\n",
    "\n",
    "for exp_name, split in SPLITS.items():\n",
    "    df_train = split[\"df_train\"].copy()\n",
    "    df_val   = split[\"df_val\"].copy()\n",
    "    df_test  = split[\"df_test\"].copy()\n",
    "\n",
    "    for part_name, dfi in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n",
    "        if OP_COL not in dfi.columns:\n",
    "            raise ValueError(f\"{exp_name}: df_{part_name} missing operator column '{OP_COL}'.\")\n",
    "        n_miss = int(dfi[OP_COL].isna().sum())\n",
    "        if n_miss:\n",
    "            raise ValueError(\n",
    "                f\"{exp_name}: df_{part_name} has {n_miss} missing operator labels. \"\n",
    "                \"Fix the label merge before training.\"\n",
    "            )\n",
    "\n",
    "    lab = build_labels_and_sample_weights(\n",
    "        df_train=df_train,\n",
    "        df_val=df_val,\n",
    "        df_test=df_test,\n",
    "        op_col=OP_COL,\n",
    "        map_id_col=MAP_ID_COL,\n",
    "        fixed_classes=FIXED_CLASSES,\n",
    "        use_map_weight=bool(USE_MAP_WEIGHT_DEFAULT),\n",
    "        class_weight_mode=str(CLASS_WEIGHT_MODE_DEFAULT),\n",
    "    )\n",
    "\n",
    "    class_names = np.array(lab.class_names)\n",
    "\n",
    "    LABELS[exp_name] = {\n",
    "        \"class_names\": class_names,\n",
    "        \"y_train_cls\": lab.y_train,\n",
    "        \"y_val_cls\":   lab.y_val,\n",
    "        \"y_test_cls\":  lab.y_test,\n",
    "        \"sample_w\":    lab.sample_w,\n",
    "        \"class_weight_map\": lab.class_weight_map,\n",
    "    }\n",
    "\n",
    "    print(f\"\\nðŸ§ª {exp_name}\")\n",
    "    print(\"Classes (fixed order):\", list(class_names))\n",
    "    print(\"Class weights:\", lab.class_weight_map)\n",
    "    print(\"y_train/y_val/y_test shapes:\", lab.y_train.shape, lab.y_val.shape, lab.y_test.shape)\n",
    "    sw = lab.sample_w\n",
    "    print(\"Sample weight summary:\", {\"min\": float(sw.min()), \"max\": float(sw.max()), \"mean\": float(sw.mean())})\n",
    "\n",
    "first = next(iter(LABELS.keys()))\n",
    "base_classes = LABELS[first][\"class_names\"].tolist()\n",
    "for exp_name in LABELS.keys():\n",
    "    if LABELS[exp_name][\"class_names\"].tolist() != base_classes:\n",
    "        raise ValueError(f\"Class order differs in experiment {exp_name}.\")\n",
    "\n",
    "print(\"\\nâœ… Label build complete for all experiments (class order consistent).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46856fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Saving Stage-2 cache for notebook 03 (disk persistence) ===\n",
      "âœ… openai_prompt_only: wrote cache to /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/models/exp_openai_prompt_only/cache_stage2\n",
      "âœ… use_prompt_only: wrote cache to /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/models/exp_use_prompt_only/cache_stage2\n",
      "âœ… map_only: wrote cache to /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/models/exp_map_only/cache_stage2\n",
      "âœ… use_map: wrote cache to /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/models/exp_use_map/cache_stage2\n",
      "âœ… openai_map: wrote cache to /Users/amirdonyadide/Documents/GitHub/IMGOFUP/data/output/models/exp_openai_map/cache_stage2\n",
      "\n",
      "âœ… Stage-2 cache saved. Notebook 03 can now run standalone.\n"
     ]
    }
   ],
   "source": [
    "# ===================== 02_build_training_data â€” FINAL CELL: Save Stage-2 cache =====================\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n=== Saving Stage-2 cache for notebook 03 (disk persistence) ===\")\n",
    "\n",
    "STAGE2_DIRNAME = \"cache_stage2\"  # folder created inside each experiment's model_out\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    out_dir = Path(cfg[\"model_out\"]).expanduser().resolve() / STAGE2_DIRNAME\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    split = SPLITS[exp_name]\n",
    "    pre   = PREPROC[exp_name]\n",
    "    lab   = LABELS[exp_name]\n",
    "\n",
    "    # ---- 1) Save split indices (so we can reconstruct in 03 if needed)\n",
    "    np.savez_compressed(\n",
    "        out_dir / \"splits.npz\",\n",
    "        train_idx=np.asarray(split[\"train_idx\"], dtype=int),\n",
    "        val_idx=np.asarray(split[\"val_idx\"], dtype=int),\n",
    "        test_idx=np.asarray(split[\"test_idx\"], dtype=int),\n",
    "    )\n",
    "\n",
    "    # ---- 2) Save scaled arrays (what classifier/regressor actually trains on)\n",
    "    np.savez_compressed(\n",
    "        out_dir / \"X_scaled.npz\",\n",
    "        X_train_s=np.asarray(pre[\"X_train_s\"], dtype=np.float64),\n",
    "        X_val_s=np.asarray(pre[\"X_val_s\"], dtype=np.float64),\n",
    "        X_test_s=np.asarray(pre[\"X_test_s\"], dtype=np.float64),\n",
    "    )\n",
    "\n",
    "    # ---- 3) Save labels + weights\n",
    "    np.savez_compressed(\n",
    "        out_dir / \"labels.npz\",\n",
    "        y_train_cls=np.asarray(lab[\"y_train_cls\"], dtype=int),\n",
    "        y_val_cls=np.asarray(lab[\"y_val_cls\"], dtype=int),\n",
    "        y_test_cls=np.asarray(lab[\"y_test_cls\"], dtype=int),\n",
    "        sample_w=np.asarray(lab[\"sample_w\"], dtype=np.float64),\n",
    "    )\n",
    "\n",
    "    # ---- 4) Save class names\n",
    "    class_names = [str(x) for x in lab[\"class_names\"]]\n",
    "    (out_dir / \"class_names.json\").write_text(\n",
    "        json.dumps(class_names, indent=2), encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    # ---- 5) Save dataframes (needed for grouped CV + regressors target)\n",
    "    split[\"df_train\"].to_parquet(out_dir / \"df_train.parquet\", index=False)\n",
    "    split[\"df_val\"].to_parquet(out_dir / \"df_val.parquet\", index=False)\n",
    "    split[\"df_test\"].to_parquet(out_dir / \"df_test.parquet\", index=False)\n",
    "\n",
    "    # ---- 6) Save small meta (for sanity/debug)\n",
    "    meta = {\n",
    "        \"exp_name\": exp_name,\n",
    "        \"feature_mode\": cfg[\"feature_mode\"],\n",
    "        \"map_dim\": int(cfg.get(\"map_dim\", -1)),\n",
    "        \"prompt_dim\": int(cfg.get(\"prompt_dim\", -1)),\n",
    "        \"fused_dim\": int(cfg.get(\"fused_dim\", -1)),\n",
    "        \"paths\": {\n",
    "            \"model_out\": str(Path(cfg[\"model_out\"]).expanduser().resolve()),\n",
    "            \"cache_dir\": str(out_dir),\n",
    "        },\n",
    "        \"shapes\": {\n",
    "            \"X_train_s\": list(pre[\"X_train_s\"].shape),\n",
    "            \"X_val_s\": list(pre[\"X_val_s\"].shape),\n",
    "            \"X_test_s\": list(pre[\"X_test_s\"].shape),\n",
    "            \"df_train\": [int(split[\"df_train\"].shape[0]), int(split[\"df_train\"].shape[1])],\n",
    "        },\n",
    "    }\n",
    "    (out_dir / \"meta.json\").write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"âœ… {exp_name}: wrote cache to {out_dir}\")\n",
    "\n",
    "print(\"\\nâœ… Stage-2 cache saved. Notebook 03 can now run standalone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3558c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
