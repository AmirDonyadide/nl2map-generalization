{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a4e97c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /Users/amirdonyadide/Documents/GitHub/Thesis\n",
      "sys.path[0]: /Users/amirdonyadide/Documents/GitHub/Thesis\n"
     ]
    }
   ],
   "source": [
    "# --- Bootstrap ---\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "ROOT = None\n",
    "try:\n",
    "    import src.utils.notebook_bootstrap as nb\n",
    "    # Try common names\n",
    "    for fn in [\"find_repo_root\", \"find_root\", \"repo_root\", \"get_repo_root\"]:\n",
    "        if hasattr(nb, fn):\n",
    "            ROOT = getattr(nb, fn)()\n",
    "            break\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if ROOT is None:\n",
    "    # fallback: assume notebook is inside Thesis/notebooks/\n",
    "    ROOT = Path.cwd()\n",
    "    while ROOT.name != \"Thesis\" and ROOT != ROOT.parent:\n",
    "        ROOT = ROOT.parent\n",
    "\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"sys.path[0]:\", sys.path[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86c3bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "from src import config as cfg\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fab364b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits: True /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/splits/splits_shared.json\n",
      "UserStudy: True /Users/amirdonyadide/Documents/GitHub/Thesis/data/userstudy/UserStudy.xlsx\n"
     ]
    }
   ],
   "source": [
    "DATA = ROOT / \"data\" / \"output\"\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    \"exp_prompt_only\": {\n",
    "        \"X\": DATA / \"train_out_prompt_only\" / \"X_prompt_only.npy\",\n",
    "        \"pairs\": DATA / \"train_out_prompt_only\" / \"train_pairs_prompt_only.parquet\",\n",
    "        \"bundle\": DATA / \"models\" / \"exp_prompt_only\" / \"cls_plus_regressors.joblib\",\n",
    "        \"preproc\": DATA / \"models\" / \"exp_prompt_only\" / \"preproc.joblib\",\n",
    "    },\n",
    "    \"exp_use_map\": {\n",
    "        \"X\": DATA / \"train_out_use\" / \"X_use_map.npy\",\n",
    "        \"pairs\": DATA / \"train_out_use\" / \"train_pairs_use_map.parquet\",\n",
    "        \"bundle\": DATA / \"models\" / \"exp_use_map\" / \"cls_plus_regressors.joblib\",\n",
    "        \"preproc\": DATA / \"models\" / \"exp_use_map\" / \"preproc.joblib\",\n",
    "    },\n",
    "    \"exp_map_only\": {\n",
    "        \"X\": DATA / \"train_out_map_only\" / \"X_map_only.npy\",\n",
    "        \"pairs\": DATA / \"train_out_map_only\" / \"train_pairs_map_only.parquet\",\n",
    "        \"bundle\": DATA / \"models\" / \"exp_map_only\" / \"cls_plus_regressors.joblib\",\n",
    "        \"preproc\": DATA / \"models\" / \"exp_map_only\" / \"preproc.joblib\",\n",
    "    },\n",
    "    \"exp_openai_map\": {\n",
    "        \"X\": DATA / \"train_out_openai\" / \"X_openai_map.npy\",\n",
    "        \"pairs\": DATA / \"train_out_openai\" / \"train_pairs_openai_map.parquet\",\n",
    "        \"bundle\": DATA / \"models\" / \"exp_openai_map\" / \"cls_plus_regressors.joblib\",\n",
    "        \"preproc\": DATA / \"models\" / \"exp_openai_map\" / \"preproc.joblib\",\n",
    "    },\n",
    "}\n",
    "\n",
    "SPLITS_PATH = DATA / \"train_out\" / \"splits\" / \"splits_shared.json\"\n",
    "USERSTUDY_XLSX = ROOT / \"data\" / \"userstudy\" / \"UserStudy.xlsx\"\n",
    "\n",
    "print(\"Splits:\", SPLITS_PATH.exists(), SPLITS_PATH)\n",
    "print(\"UserStudy:\", USERSTUDY_XLSX.exists(), USERSTUDY_XLSX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "152ac10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _load_bundle(path):\n",
    "    obj = joblib.load(path)\n",
    "    # Common patterns: dict, namespace-like, dataclass-like\n",
    "    return obj\n",
    "\n",
    "def _get_from_bundle(bundle, *keys):\n",
    "    \"\"\"\n",
    "    Tries keys in order in dict-like or attribute-like bundle.\n",
    "    \"\"\"\n",
    "    for k in keys:\n",
    "        if isinstance(bundle, dict) and k in bundle:\n",
    "            return bundle[k]\n",
    "        if hasattr(bundle, k):\n",
    "            return getattr(bundle, k)\n",
    "    raise KeyError(f\"None of keys {keys} found in bundle. Type={type(bundle)}\")\n",
    "\n",
    "def _apply_preproc(preproc, X):\n",
    "    \"\"\"\n",
    "    Works for sklearn transformers or your saved custom preproc bundle.\n",
    "    \"\"\"\n",
    "    if hasattr(preproc, \"transform\"):\n",
    "        return preproc.transform(X)\n",
    "    # fallback: if it's a dict-like bundle from src/train/preprocessing.py\n",
    "    if isinstance(preproc, dict):\n",
    "        # try to use your internal helper if it exists\n",
    "        try:\n",
    "            from src.train.preprocessing import apply_preproc_bundle  # if you have it\n",
    "            return apply_preproc_bundle(preproc, X)\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise TypeError(\"Don't know how to apply this preproc. \"\n",
    "                    \"Expected sklearn transformer or supported bundle.\")\n",
    "\n",
    "def _read_splits(path: Path):\n",
    "    with open(path, \"r\") as f:\n",
    "        s = json.load(f)\n",
    "    # Accept a few possible schemas\n",
    "    # Expect keys like: train/val/test each containing list of map_ids\n",
    "    for k in [\"train\", \"val\", \"test\"]:\n",
    "        if k not in s:\n",
    "            raise KeyError(f\"Split file missing '{k}'. Keys={list(s.keys())}\")\n",
    "    return s\n",
    "\n",
    "def _rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def _call_by_signature(func, **kwargs):\n",
    "    \"\"\"\n",
    "    Calls func with only the kwargs it accepts (based on signature).\n",
    "    Also checks for required keyword-only args and raises a helpful error.\n",
    "    \"\"\"\n",
    "    sig = inspect.signature(func)\n",
    "\n",
    "    accepted = {k: v for k, v in kwargs.items() if k in sig.parameters}\n",
    "\n",
    "    # Detect missing required keyword-only parameters\n",
    "    missing_required_kwonly = []\n",
    "    for name, p in sig.parameters.items():\n",
    "        if p.kind == inspect.Parameter.KEYWORD_ONLY and p.default is inspect._empty:\n",
    "            if name not in accepted:\n",
    "                missing_required_kwonly.append(name)\n",
    "\n",
    "    if missing_required_kwonly:\n",
    "        raise TypeError(\n",
    "            f\"{func.__name__} is missing required keyword-only args: {missing_required_kwonly}. \"\n",
    "            f\"Provided: {sorted(list(accepted.keys()))}\"\n",
    "        )\n",
    "\n",
    "    return func(**accepted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0cf1149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*, exp_name: 'str', feature_mode: 'FeatureMode', paths: 'Any', cfg: 'Any', distance_ops: 'Sequence[str]', area_ops: 'Sequence[str]', require_text: 'bool' = True) -> 'LoadedTrainingData'\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from src.train.load_training_data import load_training_data_with_dynamic_param_norm\n",
    "\n",
    "print(inspect.signature(load_training_data_with_dynamic_param_norm))\n",
    "\n",
    "# Required inputs (from your config)\n",
    "distance_ops = cfg.DISTANCE_OPS\n",
    "area_ops = cfg.AREA_OPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55e2e5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PATHS object: <class 'src.config.ProjectPaths'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, Any, cast\n",
    "\n",
    "from src.train.load_training_data import load_training_data_with_dynamic_param_norm\n",
    "\n",
    "# Map exp folder name -> feature_mode expected by loader\n",
    "FEATURE_MODE_BY_EXP = {\n",
    "    \"exp_prompt_only\": \"prompt_only\",\n",
    "    \"exp_use_map\": \"use_map\",\n",
    "    \"exp_map_only\": \"map_only\",\n",
    "    \"exp_openai_map\": \"openai_map\",\n",
    "}\n",
    "\n",
    "def _get_paths_obj():\n",
    "    # Try common config patterns\n",
    "    for k in [\"PATHS\", \"paths\"]:\n",
    "        if hasattr(cfg, k):\n",
    "            return getattr(cfg, k)\n",
    "    for fn in [\"get_paths\", \"make_paths\", \"build_paths\"]:\n",
    "        if hasattr(cfg, fn):\n",
    "            return getattr(cfg, fn)()\n",
    "    raise AttributeError(\n",
    "        \"Couldn't find a PATHS object in cfg. \"\n",
    "        \"Check src/config.py for PATHS/paths or a get_paths() function.\"\n",
    "    )\n",
    "\n",
    "PATHS_OBJ = _get_paths_obj()\n",
    "print(\"Using PATHS object:\", type(PATHS_OBJ))\n",
    "\n",
    "def load_labeled_exp(exp_name: str) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "    # Load YOUR correct artifacts\n",
    "    X = np.load(EXPERIMENTS[exp_name][\"X\"])\n",
    "    pairs = pd.read_parquet(EXPERIMENTS[exp_name][\"pairs\"])\n",
    "\n",
    "    feature_mode = cast(Any, FEATURE_MODE_BY_EXP[exp_name])\n",
    "\n",
    "    # Call loader, but force it to use provided X+pairs (not PATHS internal filenames)\n",
    "    out = load_training_data_with_dynamic_param_norm(\n",
    "        exp_name=exp_name,\n",
    "        cfg=cfg,\n",
    "        feature_mode=feature_mode,\n",
    "        paths=PATHS_OBJ,              # still needed for excel/userstudy settings\n",
    "        distance_ops=cfg.DISTANCE_OPS,\n",
    "        area_ops=cfg.AREA_OPS,\n",
    "        X=X,                          # ✅ override missing train_out/X_exp_*.npy\n",
    "        pairs=pairs,                  # ✅ override missing pairs in train_out/\n",
    "        excel_path=str(USERSTUDY_XLSX),  # if your function accepts it, great; if not, it will be ignored\n",
    "        sheet_name=\"UserStudy\",          # same\n",
    "    )\n",
    "\n",
    "    # normalize returns\n",
    "    if isinstance(out, tuple):\n",
    "        df = out[0]\n",
    "        X_out = out[1] if len(out) > 1 else X\n",
    "    else:\n",
    "        df = out\n",
    "        X_out = X\n",
    "\n",
    "    assert isinstance(df, pd.DataFrame), f\"Expected DataFrame, got {type(df)}\"\n",
    "    assert isinstance(X_out, np.ndarray), f\"Expected ndarray, got {type(X_out)}\"\n",
    "\n",
    "    return df.reset_index(drop=True), X_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60dbc069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize_param(op_series: pd.Series, param_norm: np.ndarray, df: pd.DataFrame) -> np.ndarray:\n",
    "    op = op_series.values\n",
    "    out = np.full_like(param_norm, np.nan, dtype=float)\n",
    "\n",
    "    is_dist = np.isin(op, list(cfg.DISTANCE_OPS))\n",
    "    is_area = np.isin(op, list(cfg.AREA_OPS))\n",
    "\n",
    "    out[is_dist] = param_norm[is_dist] * df.loc[is_dist, cfg.EXTENT_DIAG_COL].to_numpy(dtype=float)\n",
    "    out[is_area] = param_norm[is_area] * df.loc[is_area, cfg.EXTENT_AREA_COL].to_numpy(dtype=float)\n",
    "    return out\n",
    "\n",
    "def predict_param_norm_by_op(bundle, Xp: np.ndarray, ops: np.ndarray) -> np.ndarray:\n",
    "    regressors = _get_from_bundle(bundle, \"regressors\", \"op_to_regressor\", \"regressor_by_op\", \"op_regressors\", \"reg_models\")\n",
    "\n",
    "    target_scalers = None\n",
    "    for k in [\"target_scalers\", \"y_scalers\", \"scalers_by_op\"]:\n",
    "        if isinstance(bundle, dict) and k in bundle:\n",
    "            target_scalers = bundle[k]\n",
    "            break\n",
    "        if hasattr(bundle, k):\n",
    "            target_scalers = getattr(bundle, k)\n",
    "            break\n",
    "\n",
    "    log1p = False\n",
    "    for k in [\"use_log1p\", \"log1p\", \"log_target\"]:\n",
    "        if isinstance(bundle, dict) and k in bundle:\n",
    "            log1p = bool(bundle[k])\n",
    "            break\n",
    "        if hasattr(bundle, k):\n",
    "            log1p = bool(getattr(bundle, k))\n",
    "            break\n",
    "\n",
    "    yhat = np.full(len(ops), np.nan, dtype=float)\n",
    "\n",
    "    for op in pd.unique(ops):\n",
    "        mask = (ops == op)\n",
    "        if op not in regressors:\n",
    "            continue\n",
    "\n",
    "        reg = regressors[op]\n",
    "        pred = reg.predict(Xp[mask]).astype(float)\n",
    "\n",
    "        if target_scalers is not None and op in target_scalers:\n",
    "            sc = target_scalers[op]\n",
    "            if hasattr(sc, \"inverse_transform\"):\n",
    "                pred = sc.inverse_transform(pred.reshape(-1, 1)).ravel()\n",
    "\n",
    "        if log1p:\n",
    "            pred = np.expm1(pred)\n",
    "\n",
    "        yhat[mask] = pred\n",
    "\n",
    "    return yhat\n",
    "\n",
    "def _safe_reg_metrics(y_true, y_pred):\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan, np.nan\n",
    "    return mean_absolute_error(y_true[mask], y_pred[mask]), _rmse(y_true[mask], y_pred[mask])\n",
    "\n",
    "def evaluate_experiment(exp_name: str):\n",
    "    df, X = load_labeled_exp(exp_name)\n",
    "\n",
    "    bundle = _load_bundle(EXPERIMENTS[exp_name][\"bundle\"])\n",
    "    preproc = joblib.load(EXPERIMENTS[exp_name][\"preproc\"])\n",
    "    Xp = _apply_preproc(preproc, X)\n",
    "\n",
    "    classifier = _get_from_bundle(bundle, \"classifier\", \"clf\", \"cls\", \"model\")\n",
    "\n",
    "    splits = _read_splits(SPLITS_PATH)\n",
    "    test_maps = set(splits[\"test\"])\n",
    "    test_mask = df[\"map_id\"].isin(test_maps).to_numpy()\n",
    "\n",
    "    idx = np.where(test_mask)[0]\n",
    "    dfT = df.iloc[idx].reset_index(drop=True)\n",
    "    XpT = Xp[idx]\n",
    "\n",
    "    y_true = dfT[\"operator\"].to_numpy()\n",
    "    y_pred = classifier.predict(XpT)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    y_norm_true = dfT[\"param_norm\"].to_numpy(dtype=float)\n",
    "    y_real_true = dfT[\"param_value\"].to_numpy(dtype=float)\n",
    "\n",
    "    y_norm_oracle = predict_param_norm_by_op(bundle, XpT, y_true)\n",
    "    y_real_oracle = unnormalize_param(pd.Series(y_true), y_norm_oracle, dfT)\n",
    "\n",
    "    y_norm_pipe = predict_param_norm_by_op(bundle, XpT, y_pred)\n",
    "    y_real_pipe = unnormalize_param(pd.Series(y_pred), y_norm_pipe, dfT)\n",
    "\n",
    "    mae_norm_oracle, rmse_norm_oracle = _safe_reg_metrics(y_norm_true, y_norm_oracle)\n",
    "    mae_norm_pipe, rmse_norm_pipe = _safe_reg_metrics(y_norm_true, y_norm_pipe)\n",
    "\n",
    "    mae_real_oracle, rmse_real_oracle = _safe_reg_metrics(y_real_true, y_real_oracle)\n",
    "    mae_real_pipe, rmse_real_pipe = _safe_reg_metrics(y_real_true, y_real_pipe)\n",
    "\n",
    "    return {\n",
    "        \"exp\": exp_name,\n",
    "        \"n_test\": len(dfT),\n",
    "        \"cls_acc\": acc,\n",
    "        \"cls_macro_f1\": macro_f1,\n",
    "        \"oracle_mae_norm\": mae_norm_oracle,\n",
    "        \"oracle_rmse_norm\": rmse_norm_oracle,\n",
    "        \"pipe_mae_norm\": mae_norm_pipe,\n",
    "        \"pipe_rmse_norm\": rmse_norm_pipe,\n",
    "        \"oracle_mae_real\": mae_real_oracle,\n",
    "        \"oracle_rmse_real\": rmse_real_oracle,\n",
    "        \"pipe_mae_real\": mae_real_pipe,\n",
    "        \"pipe_rmse_real\": rmse_real_pipe,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "304fbfa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>exp_prompt_only</td>\n",
       "      <td>load_training_data_with_dynamic_param_norm() got an unexpected keyword argument 'X'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exp_use_map</td>\n",
       "      <td>load_training_data_with_dynamic_param_norm() got an unexpected keyword argument 'X'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exp_map_only</td>\n",
       "      <td>load_training_data_with_dynamic_param_norm() got an unexpected keyword argument 'X'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>exp_openai_map</td>\n",
       "      <td>load_training_data_with_dynamic_param_norm() got an unexpected keyword argument 'X'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               exp  \\\n",
       "0  exp_prompt_only   \n",
       "1      exp_use_map   \n",
       "2     exp_map_only   \n",
       "3   exp_openai_map   \n",
       "\n",
       "                                                                                 error  \n",
       "0  load_training_data_with_dynamic_param_norm() got an unexpected keyword argument 'X'  \n",
       "1  load_training_data_with_dynamic_param_norm() got an unexpected keyword argument 'X'  \n",
       "2  load_training_data_with_dynamic_param_norm() got an unexpected keyword argument 'X'  \n",
       "3  load_training_data_with_dynamic_param_norm() got an unexpected keyword argument 'X'  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'cls_macro_f1'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/var/folders/6r/lw1cp6x117zfdkxgl862fymc0000gn/T/ipykernel_26836/2696287223.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     16\u001b[39m     df_ok = df_results[df_results[\u001b[33m\"error\"\u001b[39m].isna()].copy()\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     18\u001b[39m     df_ok = df_results.copy()\n\u001b[32m     19\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m df_ok = df_ok.sort_values([\u001b[33m\"cls_macro_f1\"\u001b[39m, \u001b[33m\"pipe_rmse_real\"\u001b[39m], ascending=[\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m])\n\u001b[32m     21\u001b[39m df_ok\n\u001b[32m     22\u001b[39m \n",
      "\u001b[32m/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   7175\u001b[39m                 \u001b[33mf\"Length of ascending ({len(ascending)})\"\u001b[39m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   7176\u001b[39m                 \u001b[33mf\" != length of by ({len(by)})\"\u001b[39m\n\u001b[32m   7177\u001b[39m             )\n\u001b[32m   7178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m len(by) > \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7179\u001b[39m             keys = [self._get_label_or_level_values(x, axis=axis) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;28;01min\u001b[39;00m by]\n\u001b[32m   7180\u001b[39m \n\u001b[32m   7181\u001b[39m             \u001b[38;5;66;03m# need to rewrap columns in Series to apply key function\u001b[39;00m\n\u001b[32m   7182\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m7179\u001b[39m         ...     key=\u001b[38;5;28;01mlambda\u001b[39;00m x: np.argsort(index_natsorted(df[\u001b[33m\"time\"\u001b[39m]))\n",
      "\u001b[32m/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'cls_macro_f1'"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for exp in EXPERIMENTS:\n",
    "    try:\n",
    "        res = evaluate_experiment(exp)\n",
    "        rows.append(res)\n",
    "    except Exception as e:\n",
    "        rows.append({\"exp\": exp, \"error\": str(e)})\n",
    "\n",
    "df_results = pd.DataFrame(rows)\n",
    "\n",
    "# Put errors on top if any\n",
    "if \"error\" in df_results.columns:\n",
    "    display(df_results[df_results[\"error\"].notna()])\n",
    "\n",
    "if \"error\" in df_results.columns:\n",
    "    df_ok = df_results[df_results[\"error\"].isna()].copy()\n",
    "else:\n",
    "    df_ok = df_results.copy()\n",
    "\n",
    "df_ok = df_ok.sort_values([\"cls_macro_f1\", \"pipe_rmse_real\"], ascending=[False, True])\n",
    "df_ok\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "537902c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>exp_prompt_only</td>\n",
       "      <td>load_training_data_with_dynamic_param_norm() got an unexpected keyword argument 'X'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exp_use_map</td>\n",
       "      <td>load_training_data_with_dynamic_param_norm() got an unexpected keyword argument 'X'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exp_map_only</td>\n",
       "      <td>load_training_data_with_dynamic_param_norm() got an unexpected keyword argument 'X'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>exp_openai_map</td>\n",
       "      <td>load_training_data_with_dynamic_param_norm() got an unexpected keyword argument 'X'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               exp  \\\n",
       "0  exp_prompt_only   \n",
       "1      exp_use_map   \n",
       "2     exp_map_only   \n",
       "3   exp_openai_map   \n",
       "\n",
       "                                                                                 error  \n",
       "0  load_training_data_with_dynamic_param_norm() got an unexpected keyword argument 'X'  \n",
       "1  load_training_data_with_dynamic_param_norm() got an unexpected keyword argument 'X'  \n",
       "2  load_training_data_with_dynamic_param_norm() got an unexpected keyword argument 'X'  \n",
       "3  load_training_data_with_dynamic_param_norm() got an unexpected keyword argument 'X'  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- exp_prompt_only ---\n",
      "load_training_data_with_dynamic_param_norm() got an unexpected keyword argument 'X'\n",
      "\n",
      "--- exp_use_map ---\n",
      "load_training_data_with_dynamic_param_norm() got an unexpected keyword argument 'X'\n",
      "\n",
      "--- exp_map_only ---\n",
      "load_training_data_with_dynamic_param_norm() got an unexpected keyword argument 'X'\n",
      "\n",
      "--- exp_openai_map ---\n",
      "load_training_data_with_dynamic_param_norm() got an unexpected keyword argument 'X'\n"
     ]
    }
   ],
   "source": [
    "display(df_results)\n",
    "\n",
    "for _, r in df_results.iterrows():\n",
    "    if \"error\" in r and pd.notna(r[\"error\"]):\n",
    "        print(\"\\n---\", r[\"exp\"], \"---\")\n",
    "        print(r[\"error\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65aab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_results(df):\n",
    "    show = df.copy()\n",
    "    num_cols = [c for c in show.columns if c not in [\"exp\"] and pd.api.types.is_numeric_dtype(show[c])]\n",
    "    show[num_cols] = show[num_cols].astype(float)\n",
    "\n",
    "    # Round\n",
    "    show[num_cols] = show[num_cols].round(4)\n",
    "\n",
    "    # Highlight: max for scores, min for errors\n",
    "    score_cols = [\"cls_acc\", \"cls_macro_f1\"]\n",
    "    err_cols = [c for c in show.columns if \"mae\" in c or \"rmse\" in c]\n",
    "\n",
    "    sty = show.style\n",
    "    for c in score_cols:\n",
    "        if c in show.columns:\n",
    "            sty = sty.highlight_max(subset=[c])\n",
    "    for c in err_cols:\n",
    "        if c in show.columns:\n",
    "            sty = sty.highlight_min(subset=[c])\n",
    "\n",
    "    return sty\n",
    "\n",
    "style_results(df_ok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7700895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def per_class_report(exp_name: str):\n",
    "    df, X = load_labeled_exp(exp_name)\n",
    "    bundle = _load_bundle(EXPERIMENTS[exp_name][\"bundle\"])\n",
    "    preproc = joblib.load(EXPERIMENTS[exp_name][\"preproc\"])\n",
    "    Xp = _apply_preproc(preproc, X)\n",
    "    classifier = _get_from_bundle(bundle, \"classifier\", \"cls\", \"model\")\n",
    "\n",
    "    splits = _read_splits(SPLITS_PATH)\n",
    "    test_maps = set(splits[\"test\"])\n",
    "    test_mask = df[\"map_id\"].isin(test_maps).to_numpy()\n",
    "\n",
    "    dfT = df.loc[test_mask].reset_index(drop=True)\n",
    "    XpT = Xp[test_mask]\n",
    "    y_true = dfT[\"operator\"].to_numpy()\n",
    "    y_pred = classifier.predict(XpT)\n",
    "\n",
    "    rep = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    # keep only operator rows\n",
    "    ops = sorted(set(y_true) | set(y_pred))\n",
    "    rep_ops = pd.DataFrame({op: rep.get(op, {}) for op in ops}).T\n",
    "    rep_ops = rep_ops[[\"precision\", \"recall\", \"f1-score\", \"support\"]].sort_index()\n",
    "    rep_ops.insert(0, \"exp\", exp_name)\n",
    "    return rep_ops\n",
    "\n",
    "all_reports = []\n",
    "for exp in EXPERIMENTS:\n",
    "    try:\n",
    "        all_reports.append(per_class_report(exp))\n",
    "    except Exception as e:\n",
    "        print(exp, \"failed:\", e)\n",
    "\n",
    "df_per_class = pd.concat(all_reports, axis=0).reset_index(names=\"operator\")\n",
    "df_per_class\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
