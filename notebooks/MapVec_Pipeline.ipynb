{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f56a2c",
   "metadata": {},
   "source": [
    "## üß© 0) Setup & Imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "367f89f6ac45439b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:21:53.302406Z",
     "start_time": "2025-10-27T11:21:53.298709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFIG SUMMARY ===\n",
      "PROJ_ROOT  : /Users/amirdonyadide/Documents/GitHub/Thesis\n",
      "DATA_DIR   : /Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
      "INPUT_DIR  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input\n",
      "OUTPUT_DIR : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output\n",
      "MAPS_ROOT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/samples/pairs\n",
      "INPUT PAT. : *_input.geojson\n",
      "--- User Study ---\n",
      "USER_STUDY_XLSX : /Users/amirdonyadide/Documents/GitHub/Thesis/data/userstudy/UserStudy.xlsx\n",
      "RESPONSES_SHEET : Responses\n",
      "TILE_ID_COL     : tile_id\n",
      "COMPLETE_COL    : complete\n",
      "REMOVE_COL      : remove\n",
      "TEXT_COL        : cleaned_text\n",
      "PARAM_VALUE_COL : param_value\n",
      "OPERATOR_COL    : operator\n",
      "INTENSITY_COL   : intensity\n",
      "--- Filters / IDs / Split ---\n",
      "ONLY_COMPLETE   : True\n",
      "EXCLUDE_REMOVED : True\n",
      "PROMPT_ID       : r{i:08d}\n",
      "SPLIT_BY        : tile\n",
      "--- Outputs ---\n",
      "PROMPT_OUT : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "MAP_OUT    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n",
      "TRAIN_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out\n",
      "MODEL_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models\n",
      "SPLIT_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/splits\n",
      "PRM_NPZ    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/prompts_embeddings.npz\n",
      "--- Model ---\n",
      "PROMPT_ENCODER: openai-small\n",
      "MAP_DIM       : 165\n",
      "PROMPT_DIM    : 1536\n",
      "FUSED_DIM     : 1701\n",
      "BATCH_SIZE    : 512\n",
      "VAL/TEST      : 0.15 0.15\n",
      "SEED          : 42\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models\n",
      "‚úÖ All output folders cleaned and recreated fresh.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================== PARAMETERS / IMPORTS =====================\n",
    "from pathlib import Path\n",
    "import sys, subprocess, numpy as np, pandas as pd, joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, make_scorer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from scipy.stats import loguniform, randint\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Project config\n",
    "PROJ_ROOT = Path(\"../\").resolve()\n",
    "SRC_DIR   = PROJ_ROOT / \"src\"\n",
    "if str(PROJ_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJ_ROOT))\n",
    "\n",
    "from src.config import PATHS, CFG, print_summary\n",
    "print_summary()\n",
    "\n",
    "# Dims (fallbacks if CFG unset)\n",
    "MAP_DIM = PROMPT_DIM = FUSED_DIM = None\n",
    "\n",
    "BATCH_SIZE  = CFG.BATCH_SIZE\n",
    "\n",
    "# Clean outputs for a fresh run\n",
    "PATHS.clean_outputs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a2350e",
   "metadata": {},
   "source": [
    "## üìö 1) Build Prompt Embeddings (USE) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0df45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:21:55.572701Z",
     "start_time": "2025-10-27T11:21:55.570071Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 21:24:23 | INFO | Reading Excel: /Users/amirdonyadide/Documents/GitHub/Thesis/data/userstudy/UserStudy.xlsx (sheet=Responses)\n",
      "2026-01-21 21:24:23 | INFO | Filtered Excel rows: 786 ‚Üí 564 (only_complete=True, exclude_removed=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 564 prompts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 21:24:23 | INFO | Embedding 564 prompts with OpenAI model=text-embedding-3-small (batch_size=512, l2=True)‚Ä¶\n",
      "2026-01-21 21:24:25 | INFO | HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-21 21:24:26 | INFO | HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-21 21:24:26 | INFO | Done OpenAI embedding in 2.31s (dim=1536).\n",
      "2026-01-21 21:24:26 | INFO | Writing outputs to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "2026-01-21 21:24:26 | INFO |   saved prompts_embeddings.npz (shape=(564, 1536))\n",
      "2026-01-21 21:24:26 | INFO |   saved prompts.parquet (rows=564)\n",
      "2026-01-21 21:24:26 | INFO |   saved meta.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prompt embeddings completed.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Import the module\n",
    "from src.mapvec.prompts import prompt_embeddings as pe\n",
    "\n",
    "# Choose paths\n",
    "data_dir = PATHS.DATA_DIR  # or Path(\"../data\").resolve()\n",
    "in_path  = PATHS.USER_STUDY_XLSX\n",
    "out_dir  = PATHS.PROMPT_OUT\n",
    "\n",
    "# Logging (match what CLI does)\n",
    "pe.setup_logging(verbosity=1)\n",
    "\n",
    "# Load prompts (will filter complete==True & remove==False because you updated the function)\n",
    "ids, texts, id_colname = pe.load_prompts_from_source(\n",
    "    input_path=Path(in_path),\n",
    "    sheet_name=PATHS.RESPONSES_SHEET,\n",
    "    tile_id_col=PATHS.TILE_ID_COL,\n",
    "    complete_col=PATHS.COMPLETE_COL,\n",
    "    remove_col=PATHS.REMOVE_COL,\n",
    "    text_col=PATHS.TEXT_COL,\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(texts)} prompts.\")\n",
    "\n",
    "# Get embedder based on CFG.PROMPT_ENCODER (dan/transformer/openai-small/openai-large)\n",
    "embed_fn, model_label = pe.get_embedder(\n",
    "    kind=CFG.PROMPT_ENCODER,\n",
    "    data_dir=Path(data_dir),\n",
    "    l2_normalize=True,\n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    ")\n",
    "\n",
    "# Embed\n",
    "E = embed_fn(texts)\n",
    "\n",
    "# Save outputs in the same format as before\n",
    "pe.save_outputs(\n",
    "    out_dir=Path(out_dir),\n",
    "    ids=ids,\n",
    "    texts=texts,\n",
    "    E=E,\n",
    "    model_name=model_label,\n",
    "    l2_normalized=True,\n",
    "    id_colname=\"prompt_id\",\n",
    "    also_save_embeddings_csv=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Prompt embeddings completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c0b51",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è 2) Build Map Embeddings (geometric) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ca0c3d8b71fc70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:26:59.687350Z",
     "start_time": "2025-10-27T11:26:19.901557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed tiles from Excel: 401\n",
      "Maps to embed after filtering: 401\n",
      "‚úÖ Map embeddings completed.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.mapvec.maps import map_embeddings as me\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Load allowed tile_ids from Excel\n",
    "# ------------------------------------------------------------\n",
    "dfu = pd.read_excel(PATHS.USER_STUDY_XLSX, sheet_name=PATHS.RESPONSES_SHEET)\n",
    "\n",
    "dfu[PATHS.COMPLETE_COL] = dfu[PATHS.COMPLETE_COL].astype(bool)\n",
    "dfu[PATHS.REMOVE_COL]   = dfu[PATHS.REMOVE_COL].astype(bool)\n",
    "\n",
    "mask = pd.Series(True, index=dfu.index)\n",
    "if PATHS.ONLY_COMPLETE:\n",
    "    mask &= (dfu[PATHS.COMPLETE_COL] == True)\n",
    "if PATHS.EXCLUDE_REMOVED:\n",
    "    mask &= (dfu[PATHS.REMOVE_COL] == False)\n",
    "dfu = dfu[mask].copy()\n",
    "\n",
    "\n",
    "tile_raw = dfu[PATHS.TILE_ID_COL]\n",
    "tile_num = pd.to_numeric(tile_raw, errors=\"coerce\")\n",
    "if tile_num.notna().all():\n",
    "    allowed_tile_ids = set(tile_num.astype(int).astype(str).str.zfill(4).tolist())\n",
    "else:\n",
    "    allowed_tile_ids = set(tile_raw.astype(str).str.strip().str.zfill(4).tolist())\n",
    "\n",
    "print(f\"Allowed tiles from Excel: {len(allowed_tile_ids)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Discover GeoJSONs and filter by tile_id\n",
    "# ------------------------------------------------------------\n",
    "me.setup_logging(verbosity=1)\n",
    "\n",
    "pairs = list(me.find_geojsons(PATHS.MAPS_ROOT, PATHS.INPUT_MAPS_PATTERN))\n",
    "\n",
    "pairs = [\n",
    "    (map_id, path)\n",
    "    for (map_id, path) in pairs\n",
    "    if str(map_id) in allowed_tile_ids\n",
    "]\n",
    "\n",
    "if not pairs:\n",
    "    raise RuntimeError(\"No maps left after Excel filtering.\")\n",
    "\n",
    "print(f\"Maps to embed after filtering: {len(pairs)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) First pass: polygon counting\n",
    "# ------------------------------------------------------------\n",
    "counts = {}\n",
    "for map_id, path in pairs:\n",
    "    try:\n",
    "        counts[map_id] = me._count_valid_polygons(path)\n",
    "    except Exception:\n",
    "        counts[map_id] = 0\n",
    "\n",
    "max_polygons = max(max(counts.values()), 1)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Second pass: embed maps\n",
    "# ------------------------------------------------------------\n",
    "ids, vecs, rows = [], [], []\n",
    "feat_names = None\n",
    "first_dim = None\n",
    "\n",
    "for map_id, path in pairs:\n",
    "    vec, names = me.embed_one_map(\n",
    "        path,\n",
    "        max_polygons=max_polygons,\n",
    "        norm=\"fixed\",\n",
    "        norm_wh=\"400x400\",\n",
    "    )\n",
    "\n",
    "    if first_dim is None:\n",
    "        first_dim = vec.shape[0]\n",
    "        feat_names = names\n",
    "    elif vec.shape[0] != first_dim:\n",
    "        print(f\"Skipping {map_id}: dim mismatch\")\n",
    "        continue\n",
    "\n",
    "    ids.append(map_id)\n",
    "    vecs.append(vec)\n",
    "\n",
    "    rows.append({\n",
    "        \"map_id\": map_id,\n",
    "        \"geojson\": str(path),\n",
    "        \"n_polygons\": counts.get(map_id, 0),\n",
    "    })\n",
    "\n",
    "E = np.vstack(vecs).astype(np.float32)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Save outputs (same format as script)\n",
    "# ------------------------------------------------------------\n",
    "me.save_outputs(\n",
    "    out_dir=PATHS.MAP_OUT,\n",
    "    rows=rows,\n",
    "    E=E,\n",
    "    ids=ids,\n",
    "    feat_names=feat_names or [],\n",
    "    save_csv=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Map embeddings completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e8cdf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inferred dims: {'MAP_DIM': 165, 'PROMPT_DIM': 1536, 'FUSED_DIM': 1701}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def infer_dims(paths):\n",
    "    prm_npz = paths.PROMPT_OUT / \"prompts_embeddings.npz\"\n",
    "    map_npz = paths.MAP_OUT / \"maps_embeddings.npz\"\n",
    "\n",
    "    if not prm_npz.exists():\n",
    "        raise FileNotFoundError(f\"Missing {prm_npz} (run prompt embeddings first)\")\n",
    "    z = np.load(prm_npz, allow_pickle=True)\n",
    "    PROMPT_DIM = int(z[\"E\"].shape[1])\n",
    "\n",
    "    if not map_npz.exists():\n",
    "        raise FileNotFoundError(f\"Missing {map_npz} (run map embeddings first)\")\n",
    "    z2 = np.load(map_npz, allow_pickle=True)\n",
    "    MAP_DIM = int(z2[\"E\"].shape[1])\n",
    "\n",
    "    FUSED_DIM = MAP_DIM + PROMPT_DIM\n",
    "    return MAP_DIM, PROMPT_DIM, FUSED_DIM\n",
    "\n",
    "MAP_DIM, PROMPT_DIM, FUSED_DIM = infer_dims(PATHS)\n",
    "print(\"‚úÖ Inferred dims:\", {\"MAP_DIM\": MAP_DIM, \"PROMPT_DIM\": PROMPT_DIM, \"FUSED_DIM\": FUSED_DIM})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd186319e89f445",
   "metadata": {},
   "source": [
    "## üîó 3) Concatenate (pairs ‚Üí fused rows) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa2b07a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Concatenation completed.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from src.mapvec.concat import concat_embeddings as ce\n",
    "ce.setup_logging(verbosity=1)\n",
    "\n",
    "map_npz_path = Path(PATHS.MAP_OUT / \"maps_embeddings.npz\")\n",
    "prm_npz_path = Path(PATHS.PROMPT_OUT / \"prompts_embeddings.npz\")\n",
    "prompts_pq   = Path(PATHS.PROMPT_OUT / \"prompts.parquet\")\n",
    "out_dir      = Path(PATHS.TRAIN_OUT)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- build pairs from prompts.parquet (authoritative) ----\n",
    "pairs = pd.read_parquet(prompts_pq)\n",
    "\n",
    "if \"prompt_id\" not in pairs.columns or \"tile_id\" not in pairs.columns:\n",
    "    raise RuntimeError(\"prompts.parquet must contain columns: prompt_id, tile_id\")\n",
    "\n",
    "pairs = pairs.rename(columns={\"tile_id\": \"map_id\"})[[\"map_id\", \"prompt_id\"]].copy()\n",
    "pairs[\"map_id\"] = pairs[\"map_id\"].astype(str).str.strip()\n",
    "pairs[\"prompt_id\"] = pairs[\"prompt_id\"].astype(str).str.strip()\n",
    "pairs = pairs.dropna(subset=[\"map_id\", \"prompt_id\"])\n",
    "pairs = pairs[(pairs[\"map_id\"] != \"\") & (pairs[\"prompt_id\"] != \"\")]\n",
    "pairs = pairs.drop_duplicates(subset=[\"map_id\", \"prompt_id\"])\n",
    "\n",
    "# ---- load embeddings ----\n",
    "E_map, map_ids = ce.load_npz(map_npz_path)\n",
    "E_prm, prm_ids = ce.load_npz(prm_npz_path)\n",
    "\n",
    "idx_map = {k: i for i, k in enumerate(map_ids)}\n",
    "idx_prm = {k: i for i, k in enumerate(prm_ids)}\n",
    "\n",
    "# ---- match & build X ----\n",
    "chosen_rows, im_list, ip_list = [], [], []\n",
    "missing = 0\n",
    "\n",
    "for i, row in enumerate(pairs.itertuples(index=False), start=0):\n",
    "    im = idx_map.get(row.map_id)\n",
    "    ip = idx_prm.get(row.prompt_id)\n",
    "    if im is None or ip is None:\n",
    "        missing += 1\n",
    "        continue\n",
    "    chosen_rows.append(i)\n",
    "    im_list.append(im)\n",
    "    ip_list.append(ip)\n",
    "\n",
    "if not im_list:\n",
    "    raise RuntimeError(\"No valid pairs after ID matching.\")\n",
    "\n",
    "if missing:\n",
    "    print(f\"‚ö†Ô∏è Skipped {missing} rows with missing IDs\")\n",
    "\n",
    "X_map = E_map[np.asarray(im_list, dtype=int)].astype(np.float32, copy=False)\n",
    "X_prm = E_prm[np.asarray(ip_list, dtype=int)].astype(np.float32, copy=False)\n",
    "X = np.hstack([X_map, X_prm]).astype(np.float32, copy=False)\n",
    "\n",
    "np.save(out_dir / \"X_concat.npy\", X)\n",
    "join_df = pairs.iloc[chosen_rows].reset_index(drop=True)\n",
    "join_df.to_parquet(out_dir / \"train_pairs.parquet\", index=False)\n",
    "\n",
    "meta = {\n",
    "    \"shape\": [int(X.shape[0]), int(X.shape[1])],\n",
    "    \"map_dim\": int(E_map.shape[1]),\n",
    "    \"prompt_dim\": int(E_prm.shape[1]),\n",
    "    \"rows\": int(X.shape[0]),\n",
    "    \"skipped_pairs\": int(missing),\n",
    "    \"sources\": {\n",
    "        \"prompts_parquet\": str(prompts_pq),\n",
    "        \"map_npz\": str(map_npz_path),\n",
    "        \"prompt_npz\": str(prm_npz_path),\n",
    "    },\n",
    "}\n",
    "(out_dir / \"meta.json\").write_text(json.dumps(meta, indent=2))\n",
    "\n",
    "print(\"‚úÖ Concatenation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5142d4b68c273d37",
   "metadata": {},
   "source": [
    "## üì• 4) Load & Basic Cleaning ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a494fd27dfe7681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded X: (564, 1701), pairs: (564, 2)\n",
      "After cleaning: X=(564, 1701), df=(564, 5), ops=['aggregate', 'displace', 'select', 'simplify']\n",
      "Example rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>map_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>operator</th>\n",
       "      <th>param_value</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1304</td>\n",
       "      <td>r00000000</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1690</td>\n",
       "      <td>r00000001</td>\n",
       "      <td>select</td>\n",
       "      <td>47.584</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1646</td>\n",
       "      <td>r00000002</td>\n",
       "      <td>select</td>\n",
       "      <td>129.722</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1663</td>\n",
       "      <td>r00000005</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0856</td>\n",
       "      <td>r00000006</td>\n",
       "      <td>simplify</td>\n",
       "      <td>16.917</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>1697</td>\n",
       "      <td>r00000781</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>0078</td>\n",
       "      <td>r00000782</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>4.000</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>1525</td>\n",
       "      <td>r00000783</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>7.000</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>1288</td>\n",
       "      <td>r00000784</td>\n",
       "      <td>select</td>\n",
       "      <td>80.000</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0363</td>\n",
       "      <td>r00000785</td>\n",
       "      <td>displace</td>\n",
       "      <td>6.285</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>564 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    map_id  prompt_id   operator  param_value intensity\n",
       "0     1304  r00000000  aggregate        0.000    medium\n",
       "1     1690  r00000001     select       47.584       low\n",
       "2     1646  r00000002     select      129.722    medium\n",
       "3     1663  r00000005  aggregate        0.000    medium\n",
       "4     0856  r00000006   simplify       16.917      high\n",
       "..     ...        ...        ...          ...       ...\n",
       "559   1697  r00000781  aggregate        0.000       low\n",
       "560   0078  r00000782  aggregate        4.000      high\n",
       "561   1525  r00000783  aggregate        7.000      high\n",
       "562   1288  r00000784     select       80.000    medium\n",
       "563   0363  r00000785   displace        6.285      high\n",
       "\n",
       "[564 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === LOAD FUSED DATA (operator + param_value only) ===\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = np.load(PATHS.TRAIN_OUT / \"X_concat.npy\")\n",
    "pairs_df = pd.read_parquet(PATHS.TRAIN_OUT / \"train_pairs.parquet\")\n",
    "print(f\"Loaded X: {X.shape}, pairs: {pairs_df.shape}\")\n",
    "\n",
    "# --- Rebuild labels from Excel and merge (since train_pairs.parquet has only ids) ---\n",
    "dfu = pd.read_excel(PATHS.USER_STUDY_XLSX, sheet_name=PATHS.RESPONSES_SHEET)\n",
    "\n",
    "dfu[PATHS.COMPLETE_COL] = dfu[PATHS.COMPLETE_COL].astype(bool)\n",
    "dfu[PATHS.REMOVE_COL]   = dfu[PATHS.REMOVE_COL].astype(bool)\n",
    "\n",
    "mask_excel = pd.Series(True, index=dfu.index)\n",
    "if PATHS.ONLY_COMPLETE:\n",
    "    mask_excel &= (dfu[PATHS.COMPLETE_COL] == True)\n",
    "if PATHS.EXCLUDE_REMOVED:\n",
    "    mask_excel &= (dfu[PATHS.REMOVE_COL] == False)\n",
    "dfu = dfu[mask_excel].copy()\n",
    "\n",
    "# match prompt_embeddings.py: prompt_id uses original Excel row index\n",
    "dfu = dfu.reset_index(drop=False).rename(columns={\"index\": \"_row\"})\n",
    "prefix = PATHS.PROMPT_ID_PREFIX\n",
    "width  = PATHS.PROMPT_ID_WIDTH\n",
    "dfu[\"prompt_id\"] = dfu[\"_row\"].apply(lambda r: f\"{prefix}{int(r):0{width}d}\")\n",
    "\n",
    "# normalize tile_id -> map_id (4-digit folder ids like 0001, 0345)\n",
    "tile_raw = dfu[PATHS.TILE_ID_COL]\n",
    "tile_num = pd.to_numeric(tile_raw, errors=\"coerce\")\n",
    "if tile_num.notna().all():\n",
    "    dfu[\"map_id\"] = tile_num.astype(int).astype(str).str.zfill(4)\n",
    "else:\n",
    "    dfu[\"map_id\"] = tile_raw.astype(str).str.strip().str.zfill(4)\n",
    "\n",
    "labels = dfu[[\n",
    "    \"map_id\",\n",
    "    \"prompt_id\",\n",
    "    PATHS.OPERATOR_COL,\n",
    "    PATHS.PARAM_VALUE_COL,\n",
    "    PATHS.INTENSITY_COL,   # <-- ADD THIS\n",
    "]].copy()\n",
    "\n",
    "\n",
    "df = pairs_df.merge(labels, on=[\"map_id\", \"prompt_id\"], how=\"left\")\n",
    "\n",
    "OP_COL = PATHS.OPERATOR_COL               # \"operator\"\n",
    "PARAM_COL = PATHS.PARAM_VALUE_COL         # \"param_value\"\n",
    "\n",
    "# clean target columns\n",
    "df[OP_COL] = df[OP_COL].astype(str).str.strip().str.lower()\n",
    "df[PARAM_COL] = pd.to_numeric(df[PARAM_COL], errors=\"coerce\")\n",
    "\n",
    "# keep only rows that have both targets\n",
    "mask = df[OP_COL].notna() & df[PARAM_COL].notna()\n",
    "\n",
    "X  = X[mask.values].astype(\"float64\", copy=False)\n",
    "df = df.loc[mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"After cleaning: X={X.shape}, df={df.shape}, ops={sorted(df[OP_COL].unique())}\")\n",
    "print(\"Example rows:\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997cab7",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è 5) Split & Targets ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc0897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET SUMMARY ===\n",
      "Total rows (prompts): 564\n",
      "Unique maps: 401\n",
      "Multi-prompt maps (>1 prompt): 22\n",
      "Single-prompt maps (=1 prompt): 379\n",
      "\n",
      "Top 10 maps by prompt count:\n",
      "map_id\n",
      "1646    30\n",
      "1304    29\n",
      "1755    26\n",
      "1532    13\n",
      "0127    10\n",
      "0168     8\n",
      "0142     7\n",
      "0078     6\n",
      "0080     6\n",
      "0001     6\n",
      "dtype: int64\n",
      "\n",
      "=== SPLIT SUMMARY ===\n",
      "‚úÖ Split found (seed=42)\n",
      "Train maps: 287  (includes multi-prompt maps: 22)\n",
      "Val maps:   57\n",
      "Test maps:  57\n",
      "Rows -> Train: (450, 1701), Val: (57, 1701), Test: (57, 1701)\n",
      "‚úÖ Verified: no map_id leakage across splits.\n",
      "‚úÖ Verified: all multi-prompt maps are in TRAIN.\n",
      "\n",
      "TRAIN ‚Äî Operator counts\n",
      "operator\n",
      "select       146\n",
      "aggregate    134\n",
      "simplify     109\n",
      "displace      61\n",
      "Name: count, dtype: int64\n",
      "\n",
      "VAL ‚Äî Operator counts\n",
      "operator\n",
      "select       19\n",
      "aggregate    16\n",
      "simplify     13\n",
      "displace      9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "TEST ‚Äî Operator counts\n",
      "operator\n",
      "select       19\n",
      "aggregate    16\n",
      "simplify     13\n",
      "displace      9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "TRAIN ‚Äî Operator √ó Intensity table (counts)\n",
      "intensity  high  low  medium\n",
      "operator                    \n",
      "aggregate    37   38      59\n",
      "displace     13   20      28\n",
      "select       35   35      76\n",
      "simplify     25   26      58\n",
      "\n",
      "TRAIN ‚Äî Operator totals (row sums)\n",
      "operator\n",
      "aggregate    134\n",
      "displace      61\n",
      "select       146\n",
      "simplify     109\n",
      "dtype: int64\n",
      "\n",
      "TRAIN ‚Äî Intensity totals (col sums)\n",
      "intensity\n",
      "high      110\n",
      "low       119\n",
      "medium    221\n",
      "dtype: int64\n",
      "\n",
      "VAL ‚Äî Operator √ó Intensity table (counts)\n",
      "intensity  high  low  medium\n",
      "operator                    \n",
      "aggregate     5    6       5\n",
      "displace      3    3       3\n",
      "select        6    6       7\n",
      "simplify      4    4       5\n",
      "\n",
      "VAL ‚Äî Operator totals (row sums)\n",
      "operator\n",
      "aggregate    16\n",
      "displace      9\n",
      "select       19\n",
      "simplify     13\n",
      "dtype: int64\n",
      "\n",
      "VAL ‚Äî Intensity totals (col sums)\n",
      "intensity\n",
      "high      18\n",
      "low       19\n",
      "medium    20\n",
      "dtype: int64\n",
      "\n",
      "TEST ‚Äî Operator √ó Intensity table (counts)\n",
      "intensity  high  low  medium\n",
      "operator                    \n",
      "aggregate     5    6       5\n",
      "displace      3    3       3\n",
      "select        6    6       7\n",
      "simplify      4    4       5\n",
      "\n",
      "TEST ‚Äî Operator totals (row sums)\n",
      "operator\n",
      "aggregate    16\n",
      "displace      9\n",
      "select       19\n",
      "simplify     13\n",
      "dtype: int64\n",
      "\n",
      "TEST ‚Äî Intensity totals (col sums)\n",
      "intensity\n",
      "high      18\n",
      "low       19\n",
      "medium    20\n",
      "dtype: int64\n",
      "\n",
      "‚úÖ TRAIN: All operator√óintensity combos present.\n",
      "\n",
      "‚úÖ VAL: All operator√óintensity combos present.\n",
      "\n",
      "‚úÖ TEST: All operator√óintensity combos present.\n",
      "\n",
      "TRAIN ‚Äî prompts per map statistics\n",
      "TRAIN ‚Äî #maps with >1 prompt: 22\n",
      "\n",
      "VAL ‚Äî prompts per map statistics\n",
      "VAL ‚Äî #maps with >1 prompt: 0\n",
      "\n",
      "TEST ‚Äî prompts per map statistics\n",
      "TEST ‚Äî #maps with >1 prompt: 0\n",
      "\n",
      "TRAIN ‚Äî Top multi-prompt maps (forced to train):\n",
      "map_id\n",
      "1646    30\n",
      "1304    29\n",
      "1755    26\n",
      "1532    13\n",
      "0127    10\n",
      "0168     8\n",
      "0142     7\n",
      "0080     6\n",
      "0001     6\n",
      "0078     6\n",
      "0073     6\n",
      "0159     5\n",
      "0074     5\n",
      "0079     4\n",
      "0081     4\n",
      "0077     4\n",
      "0025     3\n",
      "0120     3\n",
      "0240     3\n",
      "0118     3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "FIXED_CLASSES = [\"simplify\", \"select\", \"aggregate\", \"displace\"]\n",
    "USE_INTENSITY_FOR_STRAT = True  # will fall back to operator-only if needed\n",
    "\n",
    "OP_COL   = PATHS.OPERATOR_COL        # \"operator\"\n",
    "PARAM_COL = PATHS.PARAM_VALUE_COL    # \"param_value\"\n",
    "INT_COL  = PATHS.INTENSITY_COL       # \"intensity\"\n",
    "\n",
    "df = df.copy()\n",
    "df[OP_COL] = df[OP_COL].astype(str).str.strip().str.lower()\n",
    "if INT_COL in df.columns:\n",
    "    df[INT_COL] = df[INT_COL].astype(str).str.strip().str.lower()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Group stats: prompts per map_id\n",
    "# ------------------------------------------------------------\n",
    "prompt_counts = df.groupby(\"map_id\").size()\n",
    "multi_map_ids = prompt_counts[prompt_counts > 1].index.tolist()\n",
    "single_map_ids = prompt_counts[prompt_counts == 1].index.tolist()\n",
    "\n",
    "print(\"=== DATASET SUMMARY ===\")\n",
    "print(f\"Total rows (prompts): {len(df)}\")\n",
    "print(f\"Unique maps: {prompt_counts.shape[0]}\")\n",
    "print(f\"Multi-prompt maps (>1 prompt): {len(multi_map_ids)}\")\n",
    "print(f\"Single-prompt maps (=1 prompt): {len(single_map_ids)}\")\n",
    "print(\"\\nTop 10 maps by prompt count:\")\n",
    "print(prompt_counts.sort_values(ascending=False).head(10))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Map-level table for single maps (one row per map_id)\n",
    "# ------------------------------------------------------------\n",
    "df_single = df[df[\"map_id\"].isin(single_map_ids)].copy()\n",
    "map_level = df_single.groupby(\"map_id\").first().reset_index()\n",
    "\n",
    "# Build strat label: operator√óintensity if feasible, else operator only\n",
    "if USE_INTENSITY_FOR_STRAT and INT_COL in map_level.columns:\n",
    "    map_level[\"_strat\"] = map_level[OP_COL] + \"__\" + map_level[INT_COL]\n",
    "    vc = map_level[\"_strat\"].value_counts()\n",
    "    if (vc < 2).any():\n",
    "        print(\"\\n‚ö†Ô∏è Some operator√óintensity groups too rare (<2 single-maps). Falling back to operator-only stratification.\")\n",
    "        map_level[\"_strat\"] = map_level[OP_COL]\n",
    "else:\n",
    "    map_level[\"_strat\"] = map_level[OP_COL]\n",
    "\n",
    "def has_all_ops(dfx: pd.DataFrame) -> bool:\n",
    "    return set(dfx[OP_COL].unique()) >= set(FIXED_CLASSES)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Split single maps into train/val/test with retries\n",
    "# ------------------------------------------------------------\n",
    "test_ratio = CFG.TEST_RATIO\n",
    "val_ratio = CFG.VAL_RATIO\n",
    "val_rel = val_ratio / (1.0 - test_ratio)\n",
    "\n",
    "X_idx = np.arange(len(map_level))\n",
    "y_strat = map_level[\"_strat\"].to_numpy()\n",
    "map_ids_arr = map_level[\"map_id\"].to_numpy()\n",
    "\n",
    "best = None\n",
    "for attempt in range(500):\n",
    "    rs = CFG.SEED + attempt\n",
    "\n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=test_ratio, random_state=rs)\n",
    "    trainval_i, test_i = next(sss1.split(X_idx, y_strat))\n",
    "\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_rel, random_state=rs + 999)\n",
    "    train_i, val_i = next(sss2.split(trainval_i, y_strat[trainval_i]))\n",
    "\n",
    "    single_train_maps = set(map_ids_arr[trainval_i[train_i]])\n",
    "    single_val_maps   = set(map_ids_arr[trainval_i[val_i]])\n",
    "    single_test_maps  = set(map_ids_arr[test_i])\n",
    "\n",
    "    train_maps = set(multi_map_ids) | single_train_maps\n",
    "    val_maps   = single_val_maps\n",
    "    test_maps  = single_test_maps\n",
    "\n",
    "    # leakage check\n",
    "    if (train_maps & val_maps) or (train_maps & test_maps) or (val_maps & test_maps):\n",
    "        continue\n",
    "\n",
    "    df_train_tmp = df[df[\"map_id\"].isin(train_maps)]\n",
    "    df_val_tmp   = df[df[\"map_id\"].isin(val_maps)]\n",
    "    df_test_tmp  = df[df[\"map_id\"].isin(test_maps)]\n",
    "\n",
    "    # must contain all operators in each split\n",
    "    if not (has_all_ops(df_train_tmp) and has_all_ops(df_val_tmp) and has_all_ops(df_test_tmp)):\n",
    "        continue\n",
    "\n",
    "    best = (train_maps, val_maps, test_maps, rs)\n",
    "    break\n",
    "\n",
    "if best is None:\n",
    "    raise RuntimeError(\n",
    "        \"Could not find a leakage-safe split with operator coverage in all splits \"\n",
    "        \"and multi-prompt maps forced to TRAIN. \"\n",
    "        \"Try: USE_INTENSITY_FOR_STRAT=False, or adjust VAL/TEST ratios.\"\n",
    "    )\n",
    "\n",
    "train_maps, val_maps, test_maps, used_seed = best\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Build row-level splits (no leakage)\n",
    "# ------------------------------------------------------------\n",
    "train_idx = df.index[df[\"map_id\"].isin(train_maps)].to_numpy()\n",
    "val_idx   = df.index[df[\"map_id\"].isin(val_maps)].to_numpy()\n",
    "test_idx  = df.index[df[\"map_id\"].isin(test_maps)].to_numpy()\n",
    "\n",
    "X_train, X_val, X_test = X[train_idx], X[val_idx], X[test_idx]\n",
    "df_train = df.loc[train_idx].reset_index(drop=True)\n",
    "df_val   = df.loc[val_idx].reset_index(drop=True)\n",
    "df_test  = df.loc[test_idx].reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== SPLIT SUMMARY ===\")\n",
    "print(f\"‚úÖ Split found (seed={used_seed})\")\n",
    "print(f\"Train maps: {len(train_maps)}  (includes multi-prompt maps: {len(set(multi_map_ids))})\")\n",
    "print(f\"Val maps:   {len(val_maps)}\")\n",
    "print(f\"Test maps:  {len(test_maps)}\")\n",
    "print(f\"Rows -> Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Hard guarantees\n",
    "assert set(df_train[\"map_id\"]).isdisjoint(df_val[\"map_id\"])\n",
    "assert set(df_train[\"map_id\"]).isdisjoint(df_test[\"map_id\"])\n",
    "assert set(df_val[\"map_id\"]).isdisjoint(df_test[\"map_id\"])\n",
    "assert set(multi_map_ids).issubset(train_maps)\n",
    "print(\"‚úÖ Verified: no map_id leakage across splits.\")\n",
    "print(\"‚úÖ Verified: all multi-prompt maps are in TRAIN.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Detailed diagnostics: operator + intensity coverage\n",
    "# ------------------------------------------------------------\n",
    "def print_operator_counts(dfx, name):\n",
    "    print(f\"\\n{name} ‚Äî Operator counts\")\n",
    "    print(dfx[OP_COL].value_counts())\n",
    "\n",
    "def print_op_intensity_table(dfx, name):\n",
    "    if INT_COL not in dfx.columns:\n",
    "        print(f\"\\n{name} ‚Äî intensity column missing; skipping op√óintensity table.\")\n",
    "        return\n",
    "    print(f\"\\n{name} ‚Äî Operator √ó Intensity table (counts)\")\n",
    "    tab = (\n",
    "        dfx.groupby([OP_COL, INT_COL]).size()\n",
    "        .unstack(fill_value=0)\n",
    "        .sort_index()\n",
    "    )\n",
    "    print(tab)\n",
    "    print(f\"\\n{name} ‚Äî Operator totals (row sums)\")\n",
    "    print(tab.sum(axis=1))\n",
    "    print(f\"\\n{name} ‚Äî Intensity totals (col sums)\")\n",
    "    print(tab.sum(axis=0))\n",
    "\n",
    "def report_missing_combos(dfx, name, all_ops, all_ints):\n",
    "    if INT_COL not in dfx.columns:\n",
    "        return\n",
    "    present = set(zip(dfx[OP_COL], dfx[INT_COL]))\n",
    "    missing = [(op, it) for op in all_ops for it in all_ints if (op, it) not in present]\n",
    "    if missing:\n",
    "        print(f\"\\n‚ö†Ô∏è {name}: Missing operator√óintensity combos ({len(missing)}):\")\n",
    "        print(missing[:40], \"...\" if len(missing) > 40 else \"\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ {name}: All operator√óintensity combos present.\")\n",
    "\n",
    "ALL_OPS = FIXED_CLASSES[:]  # enforce fixed order\n",
    "ALL_INTS = sorted(df[INT_COL].unique()) if INT_COL in df.columns else []\n",
    "\n",
    "print_operator_counts(df_train, \"TRAIN\")\n",
    "print_operator_counts(df_val,   \"VAL\")\n",
    "print_operator_counts(df_test,  \"TEST\")\n",
    "\n",
    "print_op_intensity_table(df_train, \"TRAIN\")\n",
    "print_op_intensity_table(df_val,   \"VAL\")\n",
    "print_op_intensity_table(df_test,  \"TEST\")\n",
    "\n",
    "report_missing_combos(df_train, \"TRAIN\", ALL_OPS, ALL_INTS)\n",
    "report_missing_combos(df_val,   \"VAL\",   ALL_OPS, ALL_INTS)\n",
    "report_missing_combos(df_test,  \"TEST\",  ALL_OPS, ALL_INTS)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Map-level prompt multiplicity info per split\n",
    "# ------------------------------------------------------------\n",
    "def map_prompt_stats(map_set, name):\n",
    "    sub_counts = prompt_counts.loc[list(map_set)]\n",
    "    print(f\"\\n{name} ‚Äî prompts per map statistics\")\n",
    "    print(f\"{name} ‚Äî #maps with >1 prompt:\", int((sub_counts > 1).sum()))\n",
    "\n",
    "map_prompt_stats(train_maps, \"TRAIN\")\n",
    "map_prompt_stats(val_maps,   \"VAL\")\n",
    "map_prompt_stats(test_maps,  \"TEST\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) Optional: show top multi-prompt maps in TRAIN\n",
    "# ------------------------------------------------------------\n",
    "top_multi = prompt_counts.loc[multi_map_ids].sort_values(ascending=False).head(20)\n",
    "print(\"\\nTRAIN ‚Äî Top multi-prompt maps (forced to train):\")\n",
    "print(top_multi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187473d",
   "metadata": {},
   "source": [
    "## üßº 6) Modality-Aware Preprocessing (map only) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41942b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modality-aware preprocessing complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/preproc.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === MODALITY-AWARE PREPROCESSING ===\n",
    "def split_blocks(X):\n",
    "    X_map    = X[:, :MAP_DIM].astype(np.float64, copy=True)\n",
    "    X_prompt = X[:, MAP_DIM:MAP_DIM+PROMPT_DIM].astype(np.float64, copy=True)\n",
    "    return X_map, X_prompt\n",
    "\n",
    "def l2_normalize_rows(A, eps=1e-12):\n",
    "    nrm = np.sqrt((A * A).sum(axis=1, keepdims=True))\n",
    "    return A / np.maximum(nrm, eps)\n",
    "\n",
    "# split\n",
    "Xm_tr, Xp_tr = split_blocks(X_train)\n",
    "Xm_va, Xp_va = split_blocks(X_val)\n",
    "Xm_te, Xp_te = split_blocks(X_test)\n",
    "\n",
    "# prompts: L2 only\n",
    "Xp_tr = l2_normalize_rows(Xp_tr)\n",
    "Xp_va = l2_normalize_rows(Xp_va)\n",
    "Xp_te = l2_normalize_rows(Xp_te)\n",
    "\n",
    "# maps: inf‚ÜíNaN\n",
    "for A in (Xm_tr, Xm_va, Xm_te):\n",
    "    A[~np.isfinite(A)] = np.nan\n",
    "\n",
    "# impute (train)\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "Xm_tr_imp = imp.fit_transform(Xm_tr)\n",
    "Xm_va_imp = imp.transform(Xm_va)\n",
    "Xm_te_imp = imp.transform(Xm_te)\n",
    "\n",
    "# clip (5‚Äì95%) train thresholds\n",
    "q_lo = np.nanpercentile(Xm_tr_imp, 5, axis=0)\n",
    "q_hi = np.nanpercentile(Xm_tr_imp, 95, axis=0)\n",
    "def clip_to_q(A, lo, hi): return np.clip(A, lo, hi)\n",
    "\n",
    "Xm_tr_imp = clip_to_q(Xm_tr_imp, q_lo, q_hi)\n",
    "Xm_va_imp = clip_to_q(Xm_va_imp, q_lo, q_hi)\n",
    "Xm_te_imp = clip_to_q(Xm_te_imp, q_lo, q_hi)\n",
    "\n",
    "# drop zero-variance cols on train\n",
    "stds = np.nanstd(Xm_tr_imp, axis=0)\n",
    "keep_mask = stds > 1e-12\n",
    "\n",
    "# scale kept columns (train fit)\n",
    "scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5, 95))\n",
    "Xm_tr_kept = scaler.fit_transform(Xm_tr_imp[:, keep_mask])\n",
    "Xm_va_kept = scaler.transform(Xm_va_imp[:, keep_mask])\n",
    "Xm_te_kept = scaler.transform(Xm_te_imp[:, keep_mask])\n",
    "\n",
    "# rebuild full map dim (dropped cols = 0)\n",
    "Xm_tr_s = np.zeros_like(Xm_tr_imp, dtype=np.float64)\n",
    "Xm_va_s = np.zeros_like(Xm_va_imp, dtype=np.float64)\n",
    "Xm_te_s = np.zeros_like(Xm_te_imp, dtype=np.float64)\n",
    "Xm_tr_s[:, keep_mask] = Xm_tr_kept.astype(np.float64)\n",
    "Xm_va_s[:, keep_mask] = Xm_va_kept.astype(np.float64)\n",
    "Xm_te_s[:, keep_mask] = Xm_te_kept.astype(np.float64)\n",
    "\n",
    "# fuse back\n",
    "X_train_s = np.concatenate([Xm_tr_s, Xp_tr], axis=1).astype(np.float64)\n",
    "X_val_s   = np.concatenate([Xm_va_s, Xp_va], axis=1).astype(np.float64)\n",
    "X_test_s  = np.concatenate([Xm_te_s, Xp_te], axis=1).astype(np.float64)\n",
    "\n",
    "assert np.isfinite(X_train_s).all() and np.isfinite(X_val_s).all() and np.isfinite(X_test_s).all(), \"Non-finite after preprocessing.\"\n",
    "print(\"‚úÖ Modality-aware preprocessing complete.\")\n",
    "\n",
    "# save preprocessing bundle\n",
    "joblib.dump({\n",
    "    \"imp\": imp, \"q_lo\": q_lo, \"q_hi\": q_hi,\n",
    "    \"keep_mask\": keep_mask, \"scaler\": scaler,\n",
    "    \"map_dim\": MAP_DIM, \"prompt_dim\": PROMPT_DIM\n",
    "}, PATHS.TRAIN_OUT / \"preproc.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c32c2",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è 7) Class Weights ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0df2b2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator classes: ['aggregate', 'displace', 'select', 'simplify']\n",
      "Class weights: {'aggregate': np.float64(0.8395522388059702), 'displace': np.float64(1.8442622950819672), 'select': np.float64(0.7705479452054794), 'simplify': np.float64(1.0321100917431192)}\n",
      "Sample weight summary: {'min': 0.025684931506849314, 'max': 1.8442622950819672, 'mean': 0.6500530407829777}\n"
     ]
    }
   ],
   "source": [
    "# === BUILD y_train_cls + CLASS + MAP-AWARE SAMPLE WEIGHTS ===\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "OP_COL = PATHS.OPERATOR_COL  # \"operator\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Build y_train_cls directly from df_train\n",
    "# ------------------------------------------------------------\n",
    "# factorize gives stable integer labels starting at 0\n",
    "y_train_cls, class_names = pd.factorize(df_train[OP_COL], sort=True)\n",
    "n_classes = len(class_names)\n",
    "\n",
    "print(\"Operator classes:\", list(class_names))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Class weights (operator imbalance)\n",
    "# ------------------------------------------------------------\n",
    "classes = np.arange(n_classes)\n",
    "\n",
    "cls_w = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=y_train_cls\n",
    ")\n",
    "cls_w = np.asarray(cls_w, dtype=\"float64\")\n",
    "\n",
    "class_weight_map = dict(zip(class_names, cls_w))\n",
    "print(\"Class weights:\", class_weight_map)\n",
    "\n",
    "# per-sample class weight\n",
    "w_class = np.array([cls_w[c] for c in y_train_cls], dtype=\"float64\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Map-level weighting (prompt multiplicity correction)\n",
    "# ------------------------------------------------------------\n",
    "map_counts = df_train[\"map_id\"].value_counts()\n",
    "\n",
    "# each map contributes ~1 total weight\n",
    "w_map = df_train[\"map_id\"].map(lambda m: 1.0 / map_counts[m]).to_numpy(dtype=\"float64\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Final sample weights\n",
    "# ------------------------------------------------------------\n",
    "sample_w = w_class * w_map\n",
    "\n",
    "print(\n",
    "    \"Sample weight summary:\",\n",
    "    {\n",
    "        \"min\": float(sample_w.min()),\n",
    "        \"max\": float(sample_w.max()),\n",
    "        \"mean\": float(sample_w.mean()),\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf3c9cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['aggregate', 'displace', 'select', 'simplify']\n",
      "y_train_cls shape: (450,)\n",
      "y_val_cls shape: (57,)\n",
      "y_test_cls shape: (57,)\n"
     ]
    }
   ],
   "source": [
    "# === BUILD CLASS LABELS (train / val / test) ===\n",
    "\n",
    "OP_COL = PATHS.OPERATOR_COL  # \"operator\"\n",
    "\n",
    "# Train labels + class names\n",
    "y_train_cls, class_names = pd.factorize(df_train[OP_COL], sort=True)\n",
    "\n",
    "# Val/Test labels must use SAME class order\n",
    "y_val_cls = pd.Categorical(df_val[OP_COL], categories=class_names).codes\n",
    "y_test_cls = pd.Categorical(df_test[OP_COL], categories=class_names).codes\n",
    "\n",
    "# Safety checks\n",
    "assert (y_val_cls >= 0).all(), \"VAL contains unseen operator labels\"\n",
    "assert (y_test_cls >= 0).all(), \"TEST contains unseen operator labels\"\n",
    "\n",
    "print(\"Classes:\", list(class_names))\n",
    "print(\"y_train_cls shape:\", y_train_cls.shape)\n",
    "print(\"y_val_cls shape:\", y_val_cls.shape)\n",
    "print(\"y_test_cls shape:\", y_test_cls.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988f274",
   "metadata": {},
   "source": [
    "## üß† 8) Train MLP ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95133825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching 50 MLP configs...\n",
      "[01/50] cvF1=0.694¬±0.068 | VAL F1=0.772 acc=0.772 | (128, 64), Œ±=2.02e-02, lr=1.2e-03, bs=16\n",
      "[02/50] cvF1=0.868¬±0.058 | VAL F1=0.896 acc=0.895 | (256, 128), Œ±=3.49e-05, lr=1.7e-04, bs=64\n",
      "[03/50] cvF1=0.791¬±0.044 | VAL F1=0.801 acc=0.807 | (256,), Œ±=1.03e-02, lr=7.7e-04, bs=128\n",
      "[04/50] cvF1=0.835¬±0.043 | VAL F1=0.889 acc=0.895 | (256,), Œ±=1.18e-05, lr=2.7e-03, bs=128\n",
      "[05/50] cvF1=0.856¬±0.052 | VAL F1=0.853 acc=0.860 | (256, 128, 64), Œ±=5.47e-05, lr=1.9e-04, bs=16\n",
      "[06/50] cvF1=0.870¬±0.067 | VAL F1=0.844 acc=0.842 | (64,), Œ±=1.14e-04, lr=6.0e-04, bs=128\n",
      "[07/50] cvF1=0.870¬±0.060 | VAL F1=0.881 acc=0.877 | (64,), Œ±=1.03e-04, lr=8.0e-04, bs=32\n",
      "[08/50] cvF1=0.753¬±0.075 | VAL F1=0.842 acc=0.842 | (128, 64), Œ±=2.43e-02, lr=2.2e-04, bs=32\n",
      "[09/50] cvF1=0.833¬±0.030 | VAL F1=0.860 acc=0.860 | (256, 128, 64), Œ±=4.95e-05, lr=5.7e-04, bs=128\n",
      "[10/50] cvF1=0.864¬±0.064 | VAL F1=0.881 acc=0.877 | (64,), Œ±=1.45e-05, lr=7.9e-04, bs=16\n",
      "[11/50] cvF1=0.858¬±0.054 | VAL F1=0.879 acc=0.877 | (64,), Œ±=1.68e-05, lr=2.5e-03, bs=128\n",
      "[12/50] cvF1=0.771¬±0.038 | VAL F1=0.820 acc=0.825 | (256, 128, 64), Œ±=6.47e-03, lr=2.8e-04, bs=16\n",
      "[13/50] cvF1=0.836¬±0.064 | VAL F1=0.839 acc=0.842 | (128,), Œ±=2.39e-03, lr=4.5e-04, bs=64\n",
      "[14/50] cvF1=0.869¬±0.064 | VAL F1=0.895 acc=0.895 | (128, 64), Œ±=5.27e-04, lr=1.1e-04, bs=32\n",
      "[15/50] cvF1=0.868¬±0.062 | VAL F1=0.910 acc=0.912 | (64,), Œ±=7.94e-05, lr=9.5e-04, bs=32\n",
      "[16/50] cvF1=0.822¬±0.028 | VAL F1=0.804 acc=0.807 | (256, 128, 64), Œ±=6.43e-04, lr=6.4e-04, bs=32\n",
      "[17/50] cvF1=0.738¬±0.061 | VAL F1=0.563 acc=0.561 | (256, 128), Œ±=2.35e-02, lr=1.4e-03, bs=32\n",
      "[18/50] cvF1=0.778¬±0.048 | VAL F1=0.801 acc=0.807 | (128,), Œ±=1.29e-02, lr=7.6e-04, bs=128\n",
      "[19/50] cvF1=0.854¬±0.043 | VAL F1=0.882 acc=0.877 | (256, 128, 64), Œ±=4.80e-05, lr=1.2e-04, bs=128\n",
      "[20/50] cvF1=0.838¬±0.056 | VAL F1=0.845 acc=0.842 | (256, 128), Œ±=2.25e-04, lr=2.5e-04, bs=16\n",
      "[21/50] cvF1=0.716¬±0.023 | VAL F1=0.742 acc=0.754 | (128,), Œ±=2.27e-02, lr=7.9e-04, bs=16\n",
      "[22/50] cvF1=0.869¬±0.061 | VAL F1=0.881 acc=0.877 | (64,), Œ±=1.07e-04, lr=1.8e-04, bs=16\n",
      "[23/50] cvF1=0.870¬±0.044 | VAL F1=0.862 acc=0.860 | (64,), Œ±=4.84e-03, lr=2.0e-04, bs=128\n",
      "[24/50] cvF1=0.857¬±0.064 | VAL F1=0.878 acc=0.877 | (256,), Œ±=4.91e-05, lr=1.1e-03, bs=64\n",
      "[25/50] cvF1=0.748¬±0.042 | VAL F1=0.736 acc=0.754 | (64,), Œ±=1.28e-03, lr=2.3e-03, bs=32\n",
      "[26/50] cvF1=0.760¬±0.020 | VAL F1=0.804 acc=0.807 | (64,), Œ±=1.52e-02, lr=1.8e-03, bs=128\n",
      "[27/50] cvF1=0.863¬±0.069 | VAL F1=0.910 acc=0.912 | (128,), Œ±=2.15e-05, lr=3.5e-04, bs=32\n",
      "[28/50] cvF1=0.777¬±0.034 | VAL F1=0.822 acc=0.825 | (256, 128), Œ±=3.44e-03, lr=8.7e-04, bs=64\n",
      "[29/50] cvF1=0.862¬±0.063 | VAL F1=0.858 acc=0.860 | (256,), Œ±=4.38e-04, lr=1.5e-04, bs=32\n",
      "[30/50] cvF1=0.806¬±0.068 | VAL F1=0.822 acc=0.825 | (256,), Œ±=4.42e-03, lr=6.7e-04, bs=64\n",
      "[31/50] cvF1=0.851¬±0.037 | VAL F1=0.854 acc=0.860 | (256, 128, 64), Œ±=5.21e-04, lr=5.9e-04, bs=64\n",
      "[32/50] cvF1=0.873¬±0.061 | VAL F1=0.889 acc=0.895 | (128,), Œ±=1.23e-05, lr=1.4e-04, bs=64\n",
      "[33/50] cvF1=0.874¬±0.063 | VAL F1=0.910 acc=0.912 | (64,), Œ±=1.24e-04, lr=5.6e-04, bs=32\n",
      "[34/50] cvF1=0.866¬±0.057 | VAL F1=0.909 acc=0.912 | (256, 128), Œ±=7.36e-05, lr=4.0e-04, bs=64\n",
      "[35/50] cvF1=0.868¬±0.058 | VAL F1=0.882 acc=0.877 | (256, 128), Œ±=6.25e-05, lr=1.3e-04, bs=64\n",
      "[36/50] cvF1=0.827¬±0.030 | VAL F1=0.846 acc=0.842 | (256,), Œ±=3.64e-05, lr=2.4e-03, bs=16\n",
      "[37/50] cvF1=0.801¬±0.038 | VAL F1=0.858 acc=0.860 | (256, 128, 64), Œ±=1.59e-03, lr=1.9e-03, bs=128\n",
      "[38/50] cvF1=0.853¬±0.046 | VAL F1=0.878 acc=0.877 | (128, 64), Œ±=4.45e-05, lr=2.1e-03, bs=64\n",
      "[39/50] cvF1=0.863¬±0.067 | VAL F1=0.895 acc=0.895 | (128,), Œ±=2.66e-05, lr=3.4e-04, bs=32\n",
      "[40/50] cvF1=0.856¬±0.062 | VAL F1=0.861 acc=0.860 | (64,), Œ±=8.84e-05, lr=9.1e-04, bs=16\n",
      "[41/50] cvF1=0.865¬±0.072 | VAL F1=0.895 acc=0.895 | (128, 64), Œ±=1.68e-04, lr=2.8e-04, bs=32\n",
      "[42/50] cvF1=0.865¬±0.062 | VAL F1=0.875 acc=0.877 | (256,), Œ±=2.83e-04, lr=2.1e-04, bs=64\n",
      "[43/50] cvF1=0.840¬±0.037 | VAL F1=0.864 acc=0.860 | (128,), Œ±=1.49e-04, lr=2.5e-03, bs=32\n",
      "[44/50] cvF1=0.876¬±0.060 | VAL F1=0.861 acc=0.860 | (64,), Œ±=2.54e-04, lr=1.2e-04, bs=32\n",
      "[45/50] cvF1=0.861¬±0.061 | VAL F1=0.878 acc=0.877 | (128, 64), Œ±=7.22e-05, lr=1.1e-03, bs=64\n",
      "[46/50] cvF1=0.868¬±0.069 | VAL F1=0.808 acc=0.807 | (64,), Œ±=3.27e-05, lr=3.0e-03, bs=64\n",
      "[47/50] cvF1=0.774¬±0.052 | VAL F1=0.757 acc=0.754 | (64,), Œ±=2.49e-02, lr=4.0e-04, bs=64\n",
      "[48/50] cvF1=0.860¬±0.050 | VAL F1=0.859 acc=0.860 | (256, 128), Œ±=1.58e-04, lr=8.6e-04, bs=32\n",
      "[49/50] cvF1=0.841¬±0.067 | VAL F1=0.858 acc=0.860 | (128,), Œ±=7.02e-04, lr=4.6e-04, bs=32\n",
      "[50/50] cvF1=0.868¬±0.056 | VAL F1=0.895 acc=0.895 | (128, 64), Œ±=1.91e-05, lr=3.5e-04, bs=16\n",
      "\n",
      "=== Top candidates (by VAL macro-F1) ===\n",
      "VAL F1=0.910 (acc=0.912) | cvF1=0.874¬±0.063 | params={'hidden_layer_sizes': (64,), 'alpha': 0.00012389502377355922, 'learning_rate_init': 0.0005639239991667559, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.910 (acc=0.912) | cvF1=0.868¬±0.062 | params={'hidden_layer_sizes': (64,), 'alpha': 7.939796552655359e-05, 'learning_rate_init': 0.0009519754482692684, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.910 (acc=0.912) | cvF1=0.863¬±0.069 | params={'hidden_layer_sizes': (128,), 'alpha': 2.1466070134458155e-05, 'learning_rate_init': 0.00035297465462888, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.909 (acc=0.912) | cvF1=0.866¬±0.057 | params={'hidden_layer_sizes': (256, 128), 'alpha': 7.35900856867979e-05, 'learning_rate_init': 0.0004038176882071837, 'batch_size': 64, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.896 (acc=0.895) | cvF1=0.868¬±0.058 | params={'hidden_layer_sizes': (256, 128), 'alpha': 3.487351559952691e-05, 'learning_rate_init': 0.00016998978382700761, 'batch_size': 64, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "\n",
      "üèÜ Selected params:\n",
      "{'activation': 'relu',\n",
      " 'alpha': 0.00012389502377355922,\n",
      " 'batch_size': 32,\n",
      " 'early_stopping': False,\n",
      " 'hidden_layer_sizes': (64,),\n",
      " 'learning_rate_init': 0.0005639239991667559,\n",
      " 'max_iter': 800,\n",
      " 'random_state': 42,\n",
      " 'solver': 'adam',\n",
      " 'tol': 0.0001,\n",
      " 'verbose': False}\n",
      "\n",
      "===== VAL =====\n",
      "VAL: acc=0.9123  f1_macro=0.9096\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   aggregate       0.93      0.88      0.90        16\n",
      "    displace       0.89      0.89      0.89         9\n",
      "      select       0.90      0.95      0.92        19\n",
      "    simplify       0.92      0.92      0.92        13\n",
      "\n",
      "    accuracy                           0.91        57\n",
      "   macro avg       0.91      0.91      0.91        57\n",
      "weighted avg       0.91      0.91      0.91        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[14  1  0  1]\n",
      " [ 0  8  1  0]\n",
      " [ 1  0 18  0]\n",
      " [ 0  0  1 12]]\n",
      "\n",
      "===== TEST =====\n",
      "TEST: acc=0.8947  f1_macro=0.9039\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   aggregate       0.81      0.81      0.81        16\n",
      "    displace       1.00      1.00      1.00         9\n",
      "      select       0.90      0.95      0.92        19\n",
      "    simplify       0.92      0.85      0.88        13\n",
      "\n",
      "    accuracy                           0.89        57\n",
      "   macro avg       0.91      0.90      0.90        57\n",
      "weighted avg       0.90      0.89      0.89        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[13  0  2  1]\n",
      " [ 0  9  0  0]\n",
      " [ 1  0 18  0]\n",
      " [ 2  0  0 11]]\n",
      "\n",
      "‚úÖ Saved final MLP (trained on ALL TRAIN) to: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/best_mlp_fulltrain.joblib\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# MLP search where each model trains on ALL training data\n",
    "# =========================\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# ---- numerics: keep float64 everywhere ----\n",
    "X_train_s = X_train_s.astype(np.float64, copy=False)\n",
    "X_val_s   = X_val_s.astype(np.float64, copy=False)\n",
    "X_test_s  = X_test_s.astype(np.float64, copy=False)\n",
    "sample_w  = sample_w.astype(np.float64, copy=False)\n",
    "\n",
    "# ---- group by map_id (maps can repeat; prompts don't) ----\n",
    "assert \"map_id\" in df_train.columns, \"df_train must contain 'map_id' for grouped CV.\"\n",
    "groups_tr = df_train[\"map_id\"].astype(str).values\n",
    "\n",
    "# ---- CV splitter (for scoring only) ----\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ---- search space helpers ----\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "def draw_params(n):\n",
    "    sizes = [(64,), (128,), (256,), (128, 64), (256, 128), (256, 128, 64)]\n",
    "    batches = [16, 32, 64, 128]\n",
    "    for _ in range(n):\n",
    "        yield {\n",
    "            \"hidden_layer_sizes\": sizes[rng.randint(len(sizes))],\n",
    "            \"alpha\": 10**rng.uniform(-5, np.log10(3e-2)),          # loguniform(1e-5, 3e-2)\n",
    "            \"learning_rate_init\": 10**rng.uniform(-4, np.log10(3e-3)),  # loguniform(1e-4, 3e-3)\n",
    "            \"batch_size\": batches[rng.randint(len(batches))],\n",
    "            \"activation\": \"relu\",\n",
    "            \"solver\": \"adam\",\n",
    "            \"max_iter\": 800,            # allow convergence w/o early stopping\n",
    "            \"early_stopping\": False,    # <‚Äî IMPORTANT: use ALL training samples\n",
    "            \"random_state\": 42,\n",
    "            \"verbose\": False,\n",
    "            \"tol\": 1e-4\n",
    "        }\n",
    "\n",
    "# ---- CV scorer using grouped folds; model sees only its fold-train here (for the score only) ----\n",
    "def cv_macro_f1(params):\n",
    "    scores = []\n",
    "    for tr_idx, va_idx in cv.split(X_train_s, y_train_cls, groups_tr):\n",
    "        clf = MLPClassifier(**params)\n",
    "        clf.fit(X_train_s[tr_idx], y_train_cls[tr_idx], sample_weight=sample_w[tr_idx])\n",
    "        pred = clf.predict(X_train_s[va_idx])\n",
    "        scores.append(f1_score(y_train_cls[va_idx], pred, average=\"macro\"))\n",
    "    return float(np.mean(scores)), float(np.std(scores))\n",
    "\n",
    "@dataclass\n",
    "class Candidate:\n",
    "    params: dict\n",
    "    cv_mean: float\n",
    "    cv_std: float\n",
    "    val_f1: float\n",
    "    val_acc: float\n",
    "\n",
    "# ---- run search ----\n",
    "N_ITER = 50   # tune this for time/quality tradeoff\n",
    "candidates = []\n",
    "\n",
    "print(f\"\\nSearching {N_ITER} MLP configs...\")\n",
    "for i, params in enumerate(draw_params(N_ITER), 1):\n",
    "    cv_mean, cv_std = cv_macro_f1(params)\n",
    "\n",
    "    # IMPORTANT PART: refit SAME PARAMS on FULL TRAIN (no early_stopping) so the model sees ALL training data\n",
    "    clf_full = MLPClassifier(**params)\n",
    "    clf_full.fit(X_train_s, y_train_cls, sample_weight=sample_w)\n",
    "\n",
    "    # evaluate on external VAL (never used for training)\n",
    "    val_pred = clf_full.predict(X_val_s)\n",
    "    val_f1 = f1_score(y_val_cls, val_pred, average=\"macro\")\n",
    "    val_acc = accuracy_score(y_val_cls, val_pred)\n",
    "\n",
    "    candidates.append(Candidate(params, cv_mean, cv_std, val_f1, val_acc))\n",
    "    print(f\"[{i:02d}/{N_ITER}] cvF1={cv_mean:.3f}¬±{cv_std:.3f} | VAL F1={val_f1:.3f} acc={val_acc:.3f} | {params['hidden_layer_sizes']}, Œ±={params['alpha']:.2e}, lr={params['learning_rate_init']:.1e}, bs={params['batch_size']}\")\n",
    "\n",
    "# ---- pick winner by external VAL macro-F1 (tie-breaker: VAL acc, then CV mean) ----\n",
    "candidates.sort(key=lambda c: (c.val_f1, c.val_acc, c.cv_mean), reverse=True)\n",
    "best = candidates[0]\n",
    "print(\"\\n=== Top candidates (by VAL macro-F1) ===\")\n",
    "for c in candidates[:5]:\n",
    "    print(f\"VAL F1={c.val_f1:.3f} (acc={c.val_acc:.3f}) | cvF1={c.cv_mean:.3f}¬±{c.cv_std:.3f} | params={c.params}\")\n",
    "\n",
    "print(\"\\nüèÜ Selected params:\")\n",
    "pprint(best.params)\n",
    "\n",
    "# ---- train final model on FULL TRAIN (no early_stopping so it uses 100% of train) ----\n",
    "final_mlp = MLPClassifier(**best.params)\n",
    "final_mlp.fit(X_train_s, y_train_cls, sample_weight=sample_w)\n",
    "\n",
    "# ---- evaluate on VAL & TEST ----\n",
    "for name, Xs, ys in [(\"VAL\", X_val_s, y_val_cls), (\"TEST\", X_test_s, y_test_cls)]:\n",
    "    yhat = final_mlp.predict(Xs)\n",
    "    acc  = accuracy_score(ys, yhat)\n",
    "    f1m  = f1_score(ys, yhat, average=\"macro\")\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    print(f\"{name}: acc={acc:.4f}  f1_macro={f1m:.4f}\")\n",
    "    print(classification_report(ys, yhat, target_names=list(class_names)))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(ys, yhat))\n",
    "\n",
    "# ---- save final model ----\n",
    "out_dir = Path(PATHS.TRAIN_OUT); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "import joblib\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"model\": final_mlp,\n",
    "        \"class_names\": list(class_names),\n",
    "        \"best_params\": best.params,\n",
    "    },\n",
    "    out_dir / \"best_mlp_fulltrain.joblib\"\n",
    ")\n",
    "print(f\"\\n‚úÖ Saved final MLP (trained on ALL TRAIN) to: {out_dir / 'best_mlp_fulltrain.joblib'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea7905b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'aggregate' ===\n",
      "best CV RMSE: 1.0973357636023973\n",
      "best params:\n",
      "{'alpha': np.float64(0.0041619125396912095),\n",
      " 'hidden_layer_sizes': (64,),\n",
      " 'learning_rate_init': np.float64(0.00010558059144381523)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'displace' ===\n",
      "best CV RMSE: 1.0909824522953682\n",
      "best params:\n",
      "{'alpha': np.float64(2.1453931225439485e-06),\n",
      " 'hidden_layer_sizes': (128,),\n",
      " 'learning_rate_init': np.float64(0.0001483039268456802)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'select' ===\n",
      "best CV RMSE: 0.775812459723739\n",
      "best params:\n",
      "{'alpha': np.float64(3.11927680501103e-05),\n",
      " 'hidden_layer_sizes': (256,),\n",
      " 'learning_rate_init': np.float64(0.00010725209743172001)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'simplify' ===\n",
      "best CV RMSE: 1.0010446621807334\n",
      "best params:\n",
      "{'alpha': np.float64(2.1453931225439485e-06),\n",
      " 'hidden_layer_sizes': (128,),\n",
      " 'learning_rate_init': np.float64(0.0001483039268456802)}\n",
      "\n",
      "--- Regression with predicted classes (realistic) ---\n",
      "VAL: MAE=9.5896  RMSE=16.0857\n",
      "TEST: MAE=21.9662  RMSE=89.8767\n",
      "\n",
      "--- Regression with TRUE classes (oracle routing) ---\n",
      "VAL-oracle: MAE=7.9412  RMSE=13.0106\n",
      "TEST-oracle: MAE=18.5420  RMSE=88.0600\n",
      "\n",
      "‚úÖ Saved classification+regression bundle to: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/cls_plus_regressors.joblib\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Regression branch (one MLPRegressor per operator)\n",
    "# =========================\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import joblib\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import StratifiedGroupKFold, GroupKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "# ---- 1) Prepare numeric regression targets\n",
    "def _coerce_param_to_float(s):\n",
    "    try:\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "PARAM_COL = PATHS.PARAM_VALUE_COL  # \"param_value\"\n",
    "\n",
    "y_train_reg = df_train[PARAM_COL].apply(_coerce_param_to_float).to_numpy()\n",
    "y_val_reg   = df_val[PARAM_COL].apply(_coerce_param_to_float).to_numpy()\n",
    "y_test_reg  = df_test[PARAM_COL].apply(_coerce_param_to_float).to_numpy()\n",
    "\n",
    "assert np.isfinite(y_train_reg).all() and np.isfinite(y_val_reg).all() and np.isfinite(y_test_reg).all(), \\\n",
    "    f\"Non-finite values found in regression target '{PARAM_COL}'. Clean/parse them first.\"\n",
    "\n",
    "# Optional: log1p transform if param is positive and skewed\n",
    "USE_LOG1P = False\n",
    "if USE_LOG1P:\n",
    "    assert (y_train_reg >= 0).all() and (y_val_reg >= 0).all() and (y_test_reg >= 0).all(), \\\n",
    "        \"log1p selected but param has negatives.\"\n",
    "    ytr_reg_t = np.log1p(y_train_reg)\n",
    "    yva_reg_t = np.log1p(y_val_reg)\n",
    "    yte_reg_t = np.log1p(y_test_reg)\n",
    "    def inv_t(x): return np.expm1(x)\n",
    "else:\n",
    "    ytr_reg_t = y_train_reg.copy()\n",
    "    yva_reg_t = y_val_reg.copy()\n",
    "    yte_reg_t = y_test_reg.copy()\n",
    "    def inv_t(x): return x\n",
    "\n",
    "# ---- 2) Grouped CV by map_id for *regression* (no stratification needed on a numeric target)\n",
    "assert \"map_id\" in df_train.columns\n",
    "gk = GroupKFold(n_splits=5)\n",
    "groups_tr = df_train[\"map_id\"].astype(str).values\n",
    "\n",
    "# ---- 3) Search space for MLPRegressor (kept modest; widen n_iter to search more)\n",
    "base_reg = MLPRegressor(\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    learning_rate=\"adaptive\",   # <‚Äî helps convergence on tough subsets\n",
    "    early_stopping=False,       # keep OFF during search so it uses all class data\n",
    "    max_iter=2000,              # <‚Äî more runway\n",
    "    tol=1e-3,                   # <‚Äî slightly easier convergence threshold\n",
    "    random_state=42,\n",
    "    verbose=False,\n",
    "    batch_size=\"auto\"           # <‚Äî avoids clipping warnings\n",
    ")\n",
    "param_dist_reg = {\n",
    "    \"hidden_layer_sizes\": [(64,), (128,), (256,), (128, 64), (256, 128)],\n",
    "    \"alpha\": loguniform(1e-6, 3e-2),        # widen upper range for stronger regularization\n",
    "    \"learning_rate_init\": loguniform(1e-4, 3e-3),\n",
    "    # \"batch_size\": [\"auto\"]  # not tuning batch size anymore\n",
    "}\n",
    "\n",
    "# ---- 4) Fit one regressor per class\n",
    "n_classes = len(class_names)\n",
    "regressors = {}\n",
    "search_summaries = {}\n",
    "\n",
    "for cls_idx, cls_name in enumerate(class_names):\n",
    "    m_tr = (y_train_cls == cls_idx)\n",
    "\n",
    "    Xk = X_train_s[m_tr]\n",
    "    yk = ytr_reg_t[m_tr]\n",
    "    gk_tr = groups_tr[m_tr]\n",
    "    wk = sample_w[m_tr]  # <-- ADD THIS\n",
    "\n",
    "    if Xk.shape[0] < 10:\n",
    "        print(f\"‚ö†Ô∏è Skipping class '{cls_name}' (too few samples: {Xk.shape[0]}).\")\n",
    "        continue\n",
    "\n",
    "    t_scaler = StandardScaler()\n",
    "    yk_s = t_scaler.fit_transform(yk.reshape(-1, 1)).ravel()\n",
    "\n",
    "    splits = list(gk.split(Xk, yk_s, groups=gk_tr))\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base_reg,\n",
    "        param_distributions=param_dist_reg,\n",
    "        n_iter=40,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        cv=splits,\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # ‚úÖ weighted fitting\n",
    "    search.fit(Xk, yk_s, sample_weight=wk)\n",
    "\n",
    "    print(f\"\\n=== Regressor for class '{cls_name}' ===\")\n",
    "    print(\"best CV RMSE:\", -search.best_score_)\n",
    "    print(\"best params:\"); pprint(search.best_params_)\n",
    "    search_summaries[cls_name] = {\"neg_rmse_cv\": search.best_score_, \"params\": search.best_params_}\n",
    "\n",
    "    reg_full = MLPRegressor(\n",
    "        **{**search.best_estimator_.get_params(), \"early_stopping\": False, \"max_iter\": 2000, \"random_state\": 42}\n",
    "    )\n",
    "\n",
    "    # ‚úÖ weighted refit on full subset\n",
    "    reg_full.fit(Xk, yk_s, sample_weight=wk)\n",
    "\n",
    "    regressors[cls_name] = (reg_full, t_scaler)\n",
    "\n",
    "\n",
    "# ---- 5) Evaluate on VAL & TEST using your classifier's prediction to route to regressors\n",
    "def route_and_predict(Xs, pred_cls_idx):\n",
    "    yhat_reg = np.zeros(len(pred_cls_idx), dtype=float)\n",
    "    for i, cidx in enumerate(pred_cls_idx):\n",
    "        cname = class_names[cidx]\n",
    "        pack = regressors.get(cname)\n",
    "        if pack is None:\n",
    "            yhat_reg[i] = np.nan\n",
    "            continue\n",
    "        reg, t_scaler = pack\n",
    "        y_pred_scaled = reg.predict(Xs[i:i+1])[0]\n",
    "        # inverse target scaling\n",
    "        y_pred = t_scaler.inverse_transform([[y_pred_scaled]])[0,0]\n",
    "        yhat_reg[i] = y_pred\n",
    "    return yhat_reg\n",
    "\n",
    "\n",
    "# helper to print metrics (older sklearn: no squared=False)\n",
    "def print_reg_metrics(name, y_true, y_pred_transformed):\n",
    "    # inverse-transform predictions if you used log1p\n",
    "    y_pred = inv_t(y_pred_transformed)\n",
    "\n",
    "    # guard against NaNs (e.g., missing regressor for a class)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    if mask.sum() == 0:\n",
    "        print(f\"{name}: no finite pairs to evaluate.\")\n",
    "        return np.nan, np.nan\n",
    "    if mask.sum() < len(y_true):\n",
    "        print(f\"{name}: dropped {len(y_true) - mask.sum()} samples with NaNs.\")\n",
    "\n",
    "    y_true_m = y_true[mask]\n",
    "    y_pred_m = y_pred[mask]\n",
    "\n",
    "    mae = mean_absolute_error(y_true_m, y_pred_m)\n",
    "    mse = mean_squared_error(y_true_m, y_pred_m)   # older sklearn doesn't support squared=False\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"{name}: MAE={mae:.4f}  RMSE={rmse:.4f}\")\n",
    "    return mae, rmse\n",
    "\n",
    "\n",
    "# Classification predictions (already trained classifier)\n",
    "clf_cls = final_mlp \n",
    "val_pred_cls = clf_cls.predict(X_val_s)\n",
    "test_pred_cls = clf_cls.predict(X_test_s)\n",
    "\n",
    "# route to per-class regressors\n",
    "yhat_val_reg_t  = route_and_predict(X_val_s,  val_pred_cls)\n",
    "yhat_test_reg_t = route_and_predict(X_test_s, test_pred_cls)\n",
    "\n",
    "print(\"\\n--- Regression with predicted classes (realistic) ---\")\n",
    "print_reg_metrics(\"VAL\",  y_val_reg,  yhat_val_reg_t)\n",
    "print_reg_metrics(\"TEST\", y_test_reg, yhat_test_reg_t)\n",
    "\n",
    "# ---- 6) Optional: 'oracle' evaluation to isolate regressor quality (use TRUE class for routing)\n",
    "yhat_val_oracle_t  = route_and_predict(X_val_s,  y_val_cls)\n",
    "yhat_test_oracle_t = route_and_predict(X_test_s, y_test_cls)\n",
    "\n",
    "print(\"\\n--- Regression with TRUE classes (oracle routing) ---\")\n",
    "print_reg_metrics(\"VAL-oracle\",  y_val_reg,  yhat_val_oracle_t)\n",
    "print_reg_metrics(\"TEST-oracle\", y_test_reg, yhat_test_oracle_t)\n",
    "\n",
    "# ---- 7) Save bundle\n",
    "bundle = {\n",
    "    \"classifier\": clf_cls,\n",
    "    \"regressors_by_class\": regressors,\n",
    "    \"class_names\": class_names,\n",
    "    \"use_log1p\": USE_LOG1P\n",
    "}\n",
    "\n",
    "\n",
    "out_dir = Path(PATHS.TRAIN_OUT)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(bundle, out_dir / \"cls_plus_regressors.joblib\")\n",
    "print(f\"\\n‚úÖ Saved classification+regression bundle to: {out_dir / 'cls_plus_regressors.joblib'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e382fcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN param_value summary\n",
      "count    450.000000\n",
      "mean      30.689262\n",
      "std       44.916061\n",
      "min        0.000000\n",
      "50%        5.631000\n",
      "90%      121.822000\n",
      "95%      129.722000\n",
      "99%      142.458090\n",
      "max      167.000000\n",
      "Name: param_value, dtype: float64\n",
      "max: 167.0\n",
      "min: 0.0\n",
      "\n",
      "VAL param_value summary\n",
      "count     57.000000\n",
      "mean      29.399544\n",
      "std       41.171192\n",
      "min        0.000000\n",
      "50%        5.172000\n",
      "90%       93.567200\n",
      "95%      113.033800\n",
      "99%      120.280000\n",
      "max      127.000000\n",
      "Name: param_value, dtype: float64\n",
      "max: 127.0\n",
      "min: 0.0\n",
      "\n",
      "TEST param_value summary\n",
      "count     57.00000\n",
      "mean      38.30614\n",
      "std      102.82649\n",
      "min        0.00000\n",
      "50%        5.57300\n",
      "90%       88.90940\n",
      "95%      101.47200\n",
      "99%      393.43448\n",
      "max      753.84600\n",
      "Name: param_value, dtype: float64\n",
      "max: 753.846\n",
      "min: 0.0\n",
      "\n",
      "Per-operator TEST ranges:\n",
      "operator        \n",
      "aggregate  count     16.000000\n",
      "           mean       1.082500\n",
      "           std        2.141785\n",
      "           min        0.000000\n",
      "           25%        0.000000\n",
      "           50%        0.000000\n",
      "           75%        0.519250\n",
      "           max        7.000000\n",
      "displace   count      9.000000\n",
      "           mean       4.867111\n",
      "           std        2.017124\n",
      "           min        2.882000\n",
      "           25%        3.245000\n",
      "           50%        3.733000\n",
      "           75%        6.257000\n",
      "           max        8.023000\n",
      "select     count     19.000000\n",
      "           mean     108.579842\n",
      "           std      158.304905\n",
      "           min       27.790000\n",
      "           25%       60.466000\n",
      "           50%       80.000000\n",
      "           75%       94.500000\n",
      "           max      753.846000\n",
      "simplify   count     13.000000\n",
      "           mean       4.562231\n",
      "           std        2.035045\n",
      "           min        1.052000\n",
      "           25%        3.645000\n",
      "           50%        4.219000\n",
      "           75%        5.629000\n",
      "           max        7.323000\n",
      "Name: param_value, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "PARAM_COL = PATHS.PARAM_VALUE_COL\n",
    "\n",
    "def summarize(name, dfx):\n",
    "    s = pd.to_numeric(dfx[PARAM_COL], errors=\"coerce\")\n",
    "    print(f\"\\n{name} param_value summary\")\n",
    "    print(s.describe(percentiles=[.5,.9,.95,.99]))\n",
    "    print(\"max:\", s.max())\n",
    "    print(\"min:\", s.min())\n",
    "\n",
    "summarize(\"TRAIN\", df_train)\n",
    "summarize(\"VAL\", df_val)\n",
    "summarize(\"TEST\", df_test)\n",
    "\n",
    "print(\"\\nPer-operator TEST ranges:\")\n",
    "print(df_test.groupby(PATHS.OPERATOR_COL)[PARAM_COL].apply(lambda x: pd.to_numeric(x, errors=\"coerce\").describe()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e54be0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
