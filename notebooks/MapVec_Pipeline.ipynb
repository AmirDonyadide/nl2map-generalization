{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c1f56a2c",
      "metadata": {},
      "source": [
        "# MapVec end-to-end pipeline ðŸ“’\n",
        "\n",
        "This notebook runs the **entire pipeline**:\n",
        "1. Prompt embeddings (Universal Sentence Encoder)\n",
        "2. Map embeddings (handcrafted polygon features)\n",
        "3. Concatenation into a training matrix\n",
        "4. Helper cells to inspect vectors by `prompt_id` or `map_id`\n",
        "\n",
        "**Edit the Parameters** in the next cell to match your project layout.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(PosixPath('..'), PosixPath('../data'))"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ===================== PARAMETERS (EDIT ME) =====================\n",
        "from pathlib import Path\n",
        "\n",
        "# Project root that contains `src/` and `data/`\n",
        "PROJ_ROOT = Path('../')  # e.g., Path('/Users/you/Documents/Semester_5/Thesis/CODES')\n",
        "\n",
        "# Data locations\n",
        "DATA_DIR    = PROJ_ROOT / 'data'\n",
        "PROMPTS_CSV = DATA_DIR / 'input' / 'prompts.csv'           # CSV with columns prompt_id,text (or id,text)\n",
        "PAIRS_CSV   = DATA_DIR / 'input' / 'pairs.csv'             # CSV with map_id,prompt_id\n",
        "MAPS_ROOT   = DATA_DIR / 'input' / 'samples' / 'pairs'     # Folder with *_input.geojson files\n",
        "INPUT_MAPS_PATTERN = '*_input.geojson'\n",
        "OUTPUT_MAPS_PATTERN = '*_generalized.geojson'\n",
        "\n",
        "# Output directories\n",
        "PROMPT_OUT = DATA_DIR / 'output' / 'prompt_out'\n",
        "MAP_OUT    = DATA_DIR / 'output' / 'map_out'\n",
        "TRAIN_OUT  = DATA_DIR / 'output' / 'train_out'\n",
        "PAIR_MAP_OUT    = DATA_DIR / 'output' / 'pair_map_out'\n",
        "SPLIT_OUT   = DATA_DIR / \"train_out\" / \"splits\"\n",
        "MODEL_OUT   = DATA_DIR / \"models\"\n",
        "MODEL_OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "# USE model: 'dan' or 'transformer'\n",
        "USE_MODEL = 'dan'\n",
        "\n",
        "# Expected dims (do not change unless you know what you're doing)\n",
        "MAP_DIM = 249\n",
        "PROMPT_DIM = 512\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "PROJ_ROOT, DATA_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "1e8f95c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ§¹ Removing old directory: ../data/output/prompt_out\n",
            "ðŸ§¹ Removing old directory: ../data/output/map_out\n",
            "ðŸ§¹ Removing old directory: ../data/output/train_out\n",
            "âœ… All output folders cleaned and recreated fresh.\n"
          ]
        }
      ],
      "source": [
        "# ===================== CLEAN PREVIOUS OUTPUTS =====================\n",
        "import shutil\n",
        "\n",
        "for d in [PROMPT_OUT, MAP_OUT, TRAIN_OUT]:\n",
        "    if d.exists():\n",
        "        print(f\"ðŸ§¹ Removing old directory: {d}\")\n",
        "        shutil.rmtree(d)\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"âœ… All output folders cleaned and recreated fresh.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PYTHONPATH updated with: ..\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(PosixPath('../data/output/prompt_out'),\n",
              " PosixPath('../data/output/map_out'),\n",
              " PosixPath('../data/output/train_out'))"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Make sure Python can import your local modules (src/)\n",
        "import sys\n",
        "sys.path.insert(0, str(PROJ_ROOT))\n",
        "print('PYTHONPATH updated with:', PROJ_ROOT)\n",
        "\n",
        "# Create output folders\n",
        "PROMPT_OUT.mkdir(parents=True, exist_ok=True)\n",
        "MAP_OUT.mkdir(parents=True, exist_ok=True)\n",
        "TRAIN_OUT.mkdir(parents=True, exist_ok=True)\n",
        "PROMPT_OUT, MAP_OUT, TRAIN_OUT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Dependency check (Parquet engine)\n",
        "We ensure `pyarrow` or `fastparquet` is available for `pandas.to_parquet`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parquet engine: OK\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "ok = importlib.util.find_spec('pyarrow') or importlib.util.find_spec('fastparquet')\n",
        "if not ok:\n",
        "    raise SystemExit('Missing parquet engine (pyarrow/fastparquet). Install with: conda install pyarrow -y')\n",
        "print('Parquet engine: OK')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Prompt embeddings\n",
        "Runs `src/mapvec/prompts/prompt_embeddings.py` using your chosen USE model and saves artifacts to `PROMPT_OUT`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python ../src/mapvec/prompts/prompt_embeddings.py --input ../data/input/prompts.csv --model dan --l2 --out_dir ../data/output/prompt_out -v\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "19:34:28 | DEBUG | FILE_DIR=/Users/amirdonyadide/Documents/Semester_5/Thesis/CODES/src/mapvec/prompts\n",
            "19:34:28 | DEBUG | PROJECT_ROOT=/Users/amirdonyadide/Documents/Semester_5/Thesis/CODES\n",
            "19:34:28 | DEBUG | DEFAULT_DATA_DIR=/Users/amirdonyadide/Documents/Semester_5/Thesis/CODES/data\n",
            "19:34:28 | INFO | DATA_DIR=/Users/amirdonyadide/Documents/Semester_5/Thesis/CODES/data\n",
            "19:34:28 | INFO | INPUT=/Users/amirdonyadide/Documents/Semester_5/Thesis/CODES/data/input/prompts.csv\n",
            "19:34:28 | INFO | OUT_DIR=/Users/amirdonyadide/Documents/Semester_5/Thesis/CODES/data/output/prompt_out\n",
            "19:34:28 | INFO | Reading CSV: /Users/amirdonyadide/Documents/Semester_5/Thesis/CODES/data/input/prompts.csv\n",
            "19:34:28 | INFO | Loaded 500 prompts (id_col=prompt_id). Sample IDs: p001, p002, p003â€¦\n",
            "19:34:28 | INFO | Using local USE-dan at /Users/amirdonyadide/Documents/Semester_5/Thesis/CODES/data/input/model_dan\n",
            "19:34:28 | INFO | Loading USE-dan from local path: /Users/amirdonyadide/Documents/Semester_5/Thesis/CODES/data/input/model_dan â€¦\n",
            "19:34:32 | INFO | Fingerprint not found. Saved model loading will continue.\n",
            "19:34:32 | INFO | path_and_singleprint metric could not be logged. Saved model loading will continue.\n",
            "19:34:32 | INFO | Model loaded in 3.51s\n",
            "19:34:32 | INFO | Embedding 500 prompts (batch_size=512, l2=True)â€¦\n",
            "19:34:32 | DEBUG |   embedded rows [1:500)\n",
            "19:34:32 | INFO | Done embedding in 0.13s (dim=512).\n",
            "19:34:32 | INFO | Writing outputs to /Users/amirdonyadide/Documents/Semester_5/Thesis/CODES/data/output/prompt_out\n",
            "19:34:32 | INFO |   saved embeddings.npz (shape=(500, 512))\n",
            "19:34:32 | INFO |   saved prompts.parquet (rows=500)\n",
            "19:34:32 | INFO |   saved meta.json\n",
            "19:34:32 | INFO | All done âœ…\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt embeddings completed.\n"
          ]
        }
      ],
      "source": [
        "import subprocess, shlex\n",
        "cmd = (\n",
        "    f\"python {shlex.quote(str(PROJ_ROOT / 'src' / 'mapvec' / 'prompts' / 'prompt_embeddings.py'))} \"\n",
        "    f\"--input {shlex.quote(str(PROMPTS_CSV))} --model {shlex.quote(str(USE_MODEL))} --l2 --out_dir {shlex.quote(str(PROMPT_OUT))} -v\"\n",
        ")\n",
        "print(cmd)\n",
        "res = subprocess.run(cmd, shell=True)\n",
        "if res.returncode != 0:\n",
        "    raise SystemExit('Prompt embedding step failed.')\n",
        "print('Prompt embeddings completed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Map embeddings\n",
        "Runs the map embedding module on the GeoJSON inputs. Skips problematic features, logs warnings, and writes `embeddings.npz` to `PAIR_MAP_OUT`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "fa2b07a9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CMD: /opt/anaconda3/envs/thesis/bin/python -m src.mapvec.maps.pair_map_embeddings --root data/input/samples/pairs --input_pattern *_input.geojson --gen_pattern *_generalized.geojson --out_dir data/output/pair_map_out -v\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "19:34:37 | DEBUG | PROJECT_ROOT=/Users/amirdonyadide/Documents/Semester_5/Thesis/CODES\n",
            "19:34:37 | DEBUG | DATA_DIR=/Users/amirdonyadide/Documents/Semester_5/Thesis/CODES/data\n",
            "19:34:37 | INFO | Scanning /Users/amirdonyadide/Documents/Semester_5/Thesis/CODES/data/input/samples/pairs (in=*_input.geojson, gen=*_generalized.geojson)â€¦\n",
            "19:34:41 | INFO | OK  map_id=0073  -> pair_vec[996] (per_map_dim=249)\n",
            "19:34:42 | INFO | OK  map_id=0080  -> pair_vec[996] (per_map_dim=249)\n",
            "19:34:44 | INFO | OK  map_id=0093  -> pair_vec[996] (per_map_dim=249)\n",
            "19:34:48 | INFO | OK  map_id=0122  -> pair_vec[996] (per_map_dim=249)\n",
            "19:34:50 | INFO | OK  map_id=0123  -> pair_vec[996] (per_map_dim=249)\n",
            "19:34:51 | INFO | OK  map_id=0127  -> pair_vec[996] (per_map_dim=249)\n",
            "19:34:52 | INFO | OK  map_id=0158  -> pair_vec[996] (per_map_dim=249)\n",
            "19:34:57 | INFO | OK  map_id=0159  -> pair_vec[996] (per_map_dim=249)\n",
            "19:34:58 | INFO | OK  map_id=0160  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:01 | INFO | OK  map_id=0165  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:03 | INFO | OK  map_id=0167  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:05 | INFO | OK  map_id=0168  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:06 | INFO | OK  map_id=0171  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:11 | INFO | OK  map_id=0208  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:18 | INFO | OK  map_id=0209  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:19 | INFO | OK  map_id=0215  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:21 | INFO | OK  map_id=0240  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:22 | INFO | OK  map_id=0256  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:23 | INFO | OK  map_id=0257  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:24 | INFO | OK  map_id=0262  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:27 | INFO | OK  map_id=0285  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:30 | INFO | OK  map_id=0286  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:34 | INFO | OK  map_id=0288  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:39 | INFO | OK  map_id=0289  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:40 | INFO | OK  map_id=0313  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:42 | INFO | OK  map_id=0341  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:43 | INFO | OK  map_id=0362  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:48 | INFO | OK  map_id=0363  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:51 | INFO | OK  map_id=0364  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:53 | INFO | OK  map_id=0379  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:56 | INFO | OK  map_id=0389  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:00 | INFO | OK  map_id=0390  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:02 | INFO | OK  map_id=0409  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:05 | INFO | OK  map_id=0410  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:07 | INFO | OK  map_id=0412  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:09 | INFO | OK  map_id=0413  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:12 | INFO | OK  map_id=0414  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:13 | INFO | OK  map_id=0417  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:14 | INFO | OK  map_id=0421  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:17 | INFO | OK  map_id=0426  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:18 | INFO | OK  map_id=0427  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:19 | INFO | OK  map_id=0432  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:20 | INFO | OK  map_id=0433  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:24 | INFO | OK  map_id=0437  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:26 | INFO | OK  map_id=0438  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:29 | INFO | OK  map_id=0439  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:31 | INFO | OK  map_id=0454  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:32 | INFO | OK  map_id=0458  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:34 | INFO | OK  map_id=0459  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:36 | INFO | OK  map_id=0460  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:39 | INFO | OK  map_id=0466  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:41 | INFO | OK  map_id=0469  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:44 | INFO | OK  map_id=0471  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:47 | INFO | OK  map_id=0472  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:53 | INFO | OK  map_id=0474  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:55 | INFO | OK  map_id=0475  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:58 | INFO | OK  map_id=0479  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:59 | INFO | OK  map_id=0480  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:01 | INFO | OK  map_id=0481  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:02 | INFO | OK  map_id=0482  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:04 | INFO | OK  map_id=0508  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:07 | INFO | OK  map_id=0509  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:09 | INFO | OK  map_id=0518  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:18 | INFO | OK  map_id=0520  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:21 | INFO | OK  map_id=0521  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:23 | INFO | OK  map_id=0523  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:26 | INFO | OK  map_id=0527  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:30 | INFO | OK  map_id=0528  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:31 | INFO | OK  map_id=0529  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:34 | INFO | OK  map_id=0530  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:36 | INFO | OK  map_id=0553  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:38 | INFO | OK  map_id=0557  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:40 | INFO | OK  map_id=0575  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:41 | INFO | OK  map_id=0576  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:43 | INFO | OK  map_id=0594  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:46 | INFO | OK  map_id=0595  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:48 | INFO | OK  map_id=0600  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:52 | INFO | OK  map_id=0605  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:55 | INFO | OK  map_id=0606  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:56 | INFO | OK  map_id=0608  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:59 | INFO | OK  map_id=0609  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:01 | INFO | OK  map_id=0611  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:05 | INFO | OK  map_id=0614  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:07 | INFO | OK  map_id=0615  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:09 | INFO | OK  map_id=0618  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:12 | INFO | OK  map_id=0623  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:13 | INFO | OK  map_id=0624  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:15 | INFO | OK  map_id=0645  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:17 | INFO | OK  map_id=0646  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:18 | INFO | OK  map_id=0655  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:19 | INFO | OK  map_id=0656  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:22 | INFO | OK  map_id=0657  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:26 | INFO | OK  map_id=0658  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:27 | INFO | OK  map_id=0659  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:29 | INFO | OK  map_id=0667  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:31 | INFO | OK  map_id=0672  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:33 | INFO | OK  map_id=0699  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:36 | INFO | OK  map_id=0700  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:39 | INFO | OK  map_id=0701  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:42 | INFO | OK  map_id=0706  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:43 | INFO | OK  map_id=0707  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:44 | INFO | OK  map_id=0715  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:47 | INFO | OK  map_id=0721  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:49 | INFO | OK  map_id=0747  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:52 | INFO | OK  map_id=0748  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:53 | INFO | OK  map_id=0749  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:55 | INFO | OK  map_id=0755  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:56 | INFO | OK  map_id=0758  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:58 | INFO | OK  map_id=0759  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:00 | INFO | OK  map_id=0762  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:01 | INFO | OK  map_id=0770  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:03 | INFO | OK  map_id=0804  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:05 | INFO | OK  map_id=0807  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:06 | INFO | OK  map_id=0808  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:09 | INFO | OK  map_id=0809  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:12 | INFO | OK  map_id=0819  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:13 | INFO | OK  map_id=0848  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:18 | INFO | OK  map_id=0853  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:21 | INFO | OK  map_id=0854  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:23 | INFO | OK  map_id=0856  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:24 | INFO | OK  map_id=0857  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:37 | INFO | OK  map_id=0858  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:40 | INFO | OK  map_id=0859  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:41 | INFO | OK  map_id=0867  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:48 | INFO | OK  map_id=0868  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:49 | INFO | OK  map_id=0869  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:51 | INFO | OK  map_id=0901  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:55 | INFO | OK  map_id=0903  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:58 | INFO | OK  map_id=0904  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:59 | INFO | OK  map_id=0905  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:12 | INFO | OK  map_id=0906  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:13 | INFO | OK  map_id=0907  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:15 | INFO | OK  map_id=0908  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:18 | INFO | OK  map_id=0917  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:21 | INFO | OK  map_id=0918  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:25 | INFO | OK  map_id=0926  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:27 | INFO | OK  map_id=0947  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:33 | INFO | OK  map_id=0948  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:38 | INFO | OK  map_id=0949  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:39 | INFO | OK  map_id=0950  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:40 | INFO | OK  map_id=0951  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:43 | INFO | OK  map_id=0952  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:46 | INFO | OK  map_id=0966  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:52 | INFO | OK  map_id=0967  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:53 | INFO | OK  map_id=0970  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:56 | INFO | OK  map_id=0971  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:59 | INFO | OK  map_id=0974  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:01 | INFO | OK  map_id=0975  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:03 | INFO | OK  map_id=0976  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:04 | INFO | OK  map_id=0994  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:07 | INFO | OK  map_id=0995  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:09 | INFO | OK  map_id=0997  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:14 | INFO | OK  map_id=0998  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:16 | INFO | OK  map_id=1019  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:23 | INFO | OK  map_id=1020  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:27 | INFO | OK  map_id=1052  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:31 | INFO | OK  map_id=1053  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:35 | INFO | OK  map_id=1054  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:36 | INFO | OK  map_id=1055  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:38 | INFO | OK  map_id=1056  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:40 | INFO | OK  map_id=1057  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:45 | INFO | OK  map_id=1069  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:46 | INFO | OK  map_id=1070  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:49 | INFO | OK  map_id=1090  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:52 | INFO | OK  map_id=1091  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:53 | INFO | OK  map_id=1092  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:54 | INFO | OK  map_id=1100  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:57 | INFO | OK  map_id=1103  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:00 | INFO | OK  map_id=1105  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:03 | INFO | OK  map_id=1106  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:05 | INFO | OK  map_id=1118  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:07 | INFO | OK  map_id=1119  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:10 | INFO | OK  map_id=1120  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:12 | INFO | OK  map_id=1139  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:14 | INFO | OK  map_id=1140  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:16 | INFO | OK  map_id=1148  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:17 | INFO | OK  map_id=1155  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:21 | INFO | OK  map_id=1157  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:26 | INFO | OK  map_id=1168  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:28 | INFO | OK  map_id=1169  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:30 | INFO | OK  map_id=1170  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:33 | INFO | OK  map_id=1197  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:36 | INFO | OK  map_id=1198  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:38 | INFO | OK  map_id=1202  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:44 | INFO | OK  map_id=1203  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:46 | INFO | OK  map_id=1204  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:47 | INFO | OK  map_id=1217  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:52 | INFO | OK  map_id=1218  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:56 | INFO | OK  map_id=1219  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:57 | INFO | OK  map_id=1221  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:58 | INFO | OK  map_id=1222  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:00 | INFO | OK  map_id=1231  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:02 | INFO | OK  map_id=1233  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:04 | INFO | OK  map_id=1234  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:06 | INFO | OK  map_id=1261  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:08 | INFO | OK  map_id=1269  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:17 | INFO | OK  map_id=1270  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:18 | INFO | OK  map_id=1271  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:20 | INFO | OK  map_id=1276  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:23 | INFO | OK  map_id=1277  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:25 | INFO | OK  map_id=1283  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:28 | INFO | OK  map_id=1284  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:30 | INFO | OK  map_id=1285  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:35 | INFO | OK  map_id=1295  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:40 | INFO | OK  map_id=1296  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:41 | INFO | OK  map_id=1297  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:43 | INFO | OK  map_id=1303  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:45 | INFO | OK  map_id=1304  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:46 | INFO | OK  map_id=1310  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:52 | INFO | OK  map_id=1319  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:55 | INFO | OK  map_id=1333  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:56 | INFO | OK  map_id=1334  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:02 | INFO | OK  map_id=1344  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:03 | INFO | OK  map_id=1349  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:06 | INFO | OK  map_id=1364  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:12 | INFO | OK  map_id=1365  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:16 | INFO | OK  map_id=1366  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:20 | INFO | OK  map_id=1367  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:22 | INFO | OK  map_id=1368  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:24 | INFO | OK  map_id=1369  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:25 | INFO | OK  map_id=1377  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:27 | INFO | OK  map_id=1378  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:29 | INFO | OK  map_id=1385  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:32 | INFO | OK  map_id=1386  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:34 | INFO | OK  map_id=1399  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:35 | INFO | OK  map_id=1401  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:38 | INFO | OK  map_id=1408  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:42 | INFO | OK  map_id=1409  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:44 | INFO | OK  map_id=1410  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:46 | INFO | OK  map_id=1413  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:51 | INFO | OK  map_id=1414  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:53 | INFO | OK  map_id=1415  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:55 | INFO | OK  map_id=1416  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:56 | INFO | OK  map_id=1417  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:01 | INFO | OK  map_id=1418  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:03 | INFO | OK  map_id=1434  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:04 | INFO | OK  map_id=1438  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:09 | INFO | OK  map_id=1439  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:11 | INFO | OK  map_id=1450  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:13 | INFO | OK  map_id=1451  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:19 | INFO | OK  map_id=1458  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:22 | INFO | OK  map_id=1459  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:23 | INFO | OK  map_id=1460  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:25 | INFO | OK  map_id=1465  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:29 | INFO | OK  map_id=1466  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:31 | INFO | OK  map_id=1467  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:34 | INFO | OK  map_id=1473  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:36 | INFO | OK  map_id=1474  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:38 | INFO | OK  map_id=1476  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:41 | INFO | OK  map_id=1479  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:42 | INFO | OK  map_id=1486  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:44 | INFO | OK  map_id=1487  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:46 | INFO | OK  map_id=1496  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:48 | INFO | OK  map_id=1500  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:50 | INFO | OK  map_id=1501  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:53 | INFO | OK  map_id=1507  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:55 | INFO | OK  map_id=1508  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:59 | INFO | OK  map_id=1509  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:01 | INFO | OK  map_id=1514  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:04 | INFO | OK  map_id=1515  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:06 | INFO | OK  map_id=1557  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:08 | INFO | OK  map_id=1563  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:13 | INFO | OK  map_id=1564  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:17 | INFO | OK  map_id=1565  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:19 | INFO | OK  map_id=1570  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:20 | INFO | OK  map_id=1579  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:22 | INFO | OK  map_id=1580  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:24 | INFO | OK  map_id=1583  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:25 | INFO | OK  map_id=1584  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:27 | INFO | OK  map_id=1598  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:29 | INFO | OK  map_id=1613  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:30 | INFO | OK  map_id=1614  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:33 | INFO | OK  map_id=1618  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:34 | INFO | OK  map_id=1619  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:36 | INFO | OK  map_id=1629  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:42 | INFO | OK  map_id=1630  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:43 | INFO | OK  map_id=1631  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:45 | INFO | OK  map_id=1647  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:47 | INFO | OK  map_id=1649  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:48 | INFO | OK  map_id=1650  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:50 | INFO | OK  map_id=1653  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:52 | INFO | OK  map_id=1666  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:55 | INFO | OK  map_id=1667  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:57 | INFO | OK  map_id=1672  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:59 | INFO | OK  map_id=1673  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:01 | INFO | OK  map_id=1679  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:03 | INFO | OK  map_id=1691  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:05 | INFO | OK  map_id=1696  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:11 | INFO | OK  map_id=1700  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:16 | INFO | OK  map_id=1702  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:20 | INFO | OK  map_id=1703  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:22 | INFO | OK  map_id=1709  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:23 | INFO | OK  map_id=1710  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:25 | INFO | OK  map_id=1748  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:27 | INFO | OK  map_id=1749  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:29 | INFO | OK  map_id=1750  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:32 | INFO | OK  map_id=1751  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:33 | INFO | OK  map_id=1752  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:35 | INFO | OK  map_id=1755  -> pair_vec[996] (per_map_dim=249)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pair map embeddings completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "19:47:37 | INFO | OK  map_id=1757  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:37 | INFO | Saved 300 pair vectors (failed=0) to /Users/amirdonyadide/Documents/Semester_5/Thesis/CODES/data/output/pair_map_out\n"
          ]
        }
      ],
      "source": [
        "# notebook snippet\n",
        "import sys, subprocess, pathlib\n",
        "cmd = [\n",
        "    sys.executable, \"-m\", \"src.mapvec.maps.pair_map_embeddings\",\n",
        "    \"--root\", str(MAPS_ROOT),\n",
        "    \"--input_pattern\", str(INPUT_MAPS_PATTERN),\n",
        "    \"--gen_pattern\", str(OUTPUT_MAPS_PATTERN),\n",
        "    \"--out_dir\", str(PAIR_MAP_OUT),\n",
        "    \"-v\"\n",
        "]\n",
        "print(\"CMD:\", \" \".join(cmd))\n",
        "res = subprocess.run(cmd, cwd=str(PROJ_ROOT))\n",
        "if res.returncode != 0:\n",
        "    raise SystemExit(\"Pair map embedding step failed.\")\n",
        "print(\"Pair map embeddings completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Concatenate\n",
        "Joins map & prompt vectors using `pairs.csv` and writes `X_concat.npy` and `train_pairs.parquet` to `TRAIN_OUT`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CMD: /opt/anaconda3/envs/thesis/bin/python -m src.mapvec.concat.concat_embeddings --pairs ../data/input/pairs.csv --map_npz ../data/output/pair_map_out/embeddings.npz --prompt_npz ../data/output/prompt_out/embeddings.npz --out_dir ../data/output/train_out --drop_dupes\n",
            "Concatenation completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20:14:52 | INFO | Map  embeddings: (300, 996) from /Users/amirdonyadide/Documents/Semester_5/Thesis/CODES/data/output/pair_map_out/embeddings.npz\n",
            "20:14:52 | INFO | Prompt embeddings: (500, 512) from /Users/amirdonyadide/Documents/Semester_5/Thesis/CODES/data/output/prompt_out/embeddings.npz\n",
            "20:14:52 | INFO | X shape = (450, 1508)  (map_dim=996, prompt_dim=512)\n",
            "20:14:52 | INFO | Saved to /Users/amirdonyadide/Documents/Semester_5/Thesis/CODES/data/output/train_out in 0.03s\n"
          ]
        }
      ],
      "source": [
        "import sys, subprocess\n",
        "\n",
        "cmd = [\n",
        "    sys.executable, \"-m\", \"src.mapvec.concat.concat_embeddings\",\n",
        "    \"--pairs\",      str(PAIRS_CSV),\n",
        "    \"--map_npz\",    str(PAIR_MAP_OUT / \"embeddings.npz\"),   # from pair_map_out\n",
        "    \"--prompt_npz\", str(PROMPT_OUT / \"embeddings.npz\"),\n",
        "    \"--out_dir\",    str(TRAIN_OUT),\n",
        "    \"--drop_dupes\",                                   # optional: drop duplicate (map_id,prompt_id)\n",
        "    # \"--fail_on_missing\",                            # optional: stop instead of skipping missing IDs\n",
        "]\n",
        "print(\"CMD:\", \" \".join(cmd))\n",
        "\n",
        "# Run from the project root so src/ is importable\n",
        "res = subprocess.run(cmd, cwd=str(PROJ_ROOT))\n",
        "if res.returncode != 0:\n",
        "    raise SystemExit(\"Concatenation step failed.\")\n",
        "print(\"Concatenation completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b997cab7",
      "metadata": {},
      "source": [
        "## 4) Split dataset  \n",
        "Splits the concatenated feature matrix `X_concat.npy` and its metadata `train_pairs.parquet` into separate **training**, **validation**, and **test** subsets.  \n",
        "Each split preserves row alignment between features and metadata, and the resulting files are saved under `TRAIN_OUT/splits/` as:  \n",
        "\n",
        "- `X_train.npy`, `pairs_train.parquet`  \n",
        "- `X_val.npy`, `pairs_val.parquet`  \n",
        "- `X_test.npy`, `pairs_test.parquet`  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "3bc0897d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded X: (450, 1508), pairs: (450, 2)\n",
            "Train: (315, 1508), Val: (67, 1508), Test: (68, 1508)\n",
            "Saved splits to ../data/output/train_out/splits\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ==== CONFIG ====\n",
        "TRAIN_OUT = Path(TRAIN_OUT)\n",
        "VAL_RATIO  = 0.15     # 15% validation\n",
        "TEST_RATIO = 0.15     # 15% test (remaining 70% train)\n",
        "SEED       = 42       # reproducibility\n",
        "# =================\n",
        "\n",
        "# Load data\n",
        "X = np.load(TRAIN_OUT / \"X_concat.npy\")\n",
        "pairs_df = pd.read_parquet(TRAIN_OUT / \"train_pairs.parquet\")\n",
        "\n",
        "print(f\"Loaded X: {X.shape}, pairs: {pairs_df.shape}\")\n",
        "\n",
        "# --- Step 1: Train/Test split\n",
        "X_train, X_temp, df_train, df_temp = train_test_split(\n",
        "    X, pairs_df, test_size=VAL_RATIO + TEST_RATIO, random_state=SEED, shuffle=True\n",
        ")\n",
        "\n",
        "# --- Step 2: Split temp into Val/Test\n",
        "relative_test_ratio = TEST_RATIO / (VAL_RATIO + TEST_RATIO)\n",
        "X_val, X_test, df_val, df_test = train_test_split(\n",
        "    X_temp, df_temp, test_size=relative_test_ratio, random_state=SEED, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "# --- Save splits\n",
        "split_dir = TRAIN_OUT / \"splits\"\n",
        "split_dir.mkdir(exist_ok=True)\n",
        "\n",
        "np.save(split_dir / \"X_train.npy\", X_train)\n",
        "np.save(split_dir / \"X_val.npy\",   X_val)\n",
        "np.save(split_dir / \"X_test.npy\",  X_test)\n",
        "\n",
        "df_train.to_parquet(split_dir / \"pairs_train.parquet\", index=False)\n",
        "df_val.to_parquet(split_dir / \"pairs_val.parquet\", index=False)\n",
        "df_test.to_parquet(split_dir / \"pairs_test.parquet\", index=False)\n",
        "\n",
        "print(f\"Saved splits to {split_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "9ace9893",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Balanced sizes -> train: (315, 1508) val: (67, 1508) test: (68, 1508)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(1)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# 4) Train logistic regression baseline (compatibility classifier)\u001b[39;00m\n\u001b[32m    109\u001b[39m clf = LogisticRegression(max_iter=\u001b[32m2000\u001b[39m, class_weight=\u001b[33m\"\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m\"\u001b[39m, n_jobs=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34meval_split\u001b[39m(Xs, ys):\n\u001b[32m    113\u001b[39m     p  = clf.predict(Xs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1335\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1333\u001b[39m classes_ = \u001b[38;5;28mself\u001b[39m.classes_\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_classes < \u001b[32m2\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1335\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1336\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis solver needs samples of at least 2 classes\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1337\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m in the data, but the data contains only one\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1338\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % classes_[\u001b[32m0\u001b[39m]\n\u001b[32m   1339\u001b[39m     )\n\u001b[32m   1341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.classes_) == \u001b[32m2\u001b[39m:\n\u001b[32m   1342\u001b[39m     n_classes = \u001b[32m1\u001b[39m\n",
            "\u001b[31mValueError\u001b[39m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(1)"
          ]
        }
      ],
      "source": [
        "# 5) Self-supervised baseline (no labels.csv needed)\n",
        "# - Positive = observed pairs\n",
        "# - Negatives = shuffled/mismatched pairs\n",
        "# - Train a classifier to predict compatibility\n",
        "\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
        "\n",
        "NEG_PER_POS = 3  # number of negatives per positive; tune as needed\n",
        "RNG_SEED    = 42\n",
        "\n",
        "# 1) Load splits\n",
        "X_tr = np.load(split_dir / \"X_train.npy\")\n",
        "X_va = np.load(split_dir / \"X_val.npy\")\n",
        "X_te = np.load(split_dir / \"X_test.npy\")\n",
        "\n",
        "J_tr = pd.read_parquet(split_dir / \"pairs_train.parquet\")\n",
        "J_va = pd.read_parquet(split_dir / \"pairs_val.parquet\")\n",
        "J_te = pd.read_parquet(split_dir / \"pairs_test.parquet\")\n",
        "\n",
        "rng = np.random.default_rng(RNG_SEED)\n",
        "\n",
        "def make_negatives(join_df: pd.DataFrame, neg_per_pos: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create negatives by shuffling prompt_id within the split.\n",
        "    Ensures (map_id, prompt_id) is not an observed positive.\n",
        "    Returns a table with columns: map_id, prompt_id, y (1 for pos, 0 for neg).\n",
        "    \"\"\"\n",
        "    pos = join_df[[\"map_id\", \"prompt_id\"]].astype(str).copy()\n",
        "    pos[\"y\"] = 1\n",
        "\n",
        "    negl = []\n",
        "    prompts = join_df[\"prompt_id\"].astype(str).to_numpy()\n",
        "    maps    = join_df[\"map_id\"].astype(str).to_numpy()\n",
        "\n",
        "    for _ in range(neg_per_pos):\n",
        "        shuffled = prompts.copy()\n",
        "        rng.shuffle(shuffled)\n",
        "        neg = pd.DataFrame({\"map_id\": maps, \"prompt_id\": shuffled})\n",
        "        # drop collisions with positives\n",
        "        neg = neg.merge(pos[[\"map_id\",\"prompt_id\"]], on=[\"map_id\",\"prompt_id\"],\n",
        "                        how=\"left\", indicator=True)\n",
        "        neg = neg[neg[\"_merge\"] == \"left_only\"].drop(columns=\"_merge\")\n",
        "        negl.append(neg)\n",
        "\n",
        "    neg_all = pd.concat(negl, ignore_index=True)\n",
        "    neg_all[\"y\"] = 0\n",
        "\n",
        "    out = pd.concat([pos, neg_all], ignore_index=True)\n",
        "    out = out.sample(frac=1.0, random_state=RNG_SEED).reset_index(drop=True)\n",
        "    return out\n",
        "\n",
        "# Build train/val/test label tables (self-supervised)\n",
        "Y_tr_tbl = make_negatives(J_tr, NEG_PER_POS)\n",
        "Y_va_tbl = make_negatives(J_va, NEG_PER_POS)\n",
        "Y_te_tbl = make_negatives(J_te, NEG_PER_POS)\n",
        "\n",
        "# 2) Align X rows to these tables\n",
        "\n",
        "def make_index(df: pd.DataFrame) -> dict[tuple[str, str], int]:\n",
        "    # Map (map_id, prompt_id) -> row index within the split\n",
        "    return {\n",
        "        (m, p): i\n",
        "        for i, (m, p) in enumerate(\n",
        "            df[[\"map_id\", \"prompt_id\"]].astype(str).itertuples(index=False, name=None)\n",
        "        )\n",
        "    }\n",
        "\n",
        "idx_tr = make_index(J_tr)\n",
        "idx_va = make_index(J_va)\n",
        "idx_te = make_index(J_te)\n",
        "\n",
        "def rows_from_table(X_split: np.ndarray,\n",
        "                    index_lookup: dict[tuple[str, str], int],\n",
        "                    tbl: pd.DataFrame) -> tuple[np.ndarray, np.ndarray]:\n",
        "    inds = []\n",
        "    ys   = []\n",
        "    for m, p, y in tbl[[\"map_id\", \"prompt_id\", \"y\"]].astype(str).itertuples(index=False, name=None):\n",
        "        key = (m, p)\n",
        "        i = index_lookup.get(key)\n",
        "        if i is None:\n",
        "            # This can happen if a shuffled negative coincides with some other split's positive.\n",
        "            # We simply skip such cases.\n",
        "            continue\n",
        "        inds.append(i)\n",
        "        ys.append(int(y))\n",
        "    inds = np.asarray(inds, dtype=int)\n",
        "    ys   = np.asarray(ys, dtype=int)\n",
        "    return X_split[inds], ys\n",
        "\n",
        "X_tr_b, y_tr = rows_from_table(X_tr, idx_tr, Y_tr_tbl)\n",
        "X_va_b, y_va = rows_from_table(X_va, idx_va, Y_va_tbl)\n",
        "X_te_b, y_te = rows_from_table(X_te, idx_te, Y_te_tbl)\n",
        "\n",
        "print(\"Balanced sizes ->\",\n",
        "      \"train:\", X_tr_b.shape, \"val:\", X_va_b.shape, \"test:\", X_te_b.shape)\n",
        "\n",
        "# 3) Scale (fit on train only)\n",
        "scaler = StandardScaler()\n",
        "X_tr_s = scaler.fit_transform(X_tr_b)\n",
        "X_va_s = scaler.transform(X_va_b)\n",
        "X_te_s = scaler.transform(X_te_b)\n",
        "\n",
        "# 4) Train logistic regression baseline (compatibility classifier)\n",
        "clf = LogisticRegression(max_iter=2000, class_weight=\"balanced\", n_jobs=None)\n",
        "clf.fit(X_tr_s, y_tr)\n",
        "\n",
        "def eval_split(Xs, ys):\n",
        "    p  = clf.predict(Xs)\n",
        "    pr = clf.predict_proba(Xs)[:, 1]\n",
        "    return {\n",
        "        \"accuracy\": float(accuracy_score(ys, p)),\n",
        "        \"f1\": float(f1_score(ys, p)),\n",
        "        \"auroc\": float(roc_auc_score(ys, pr)),\n",
        "    }\n",
        "\n",
        "metrics = {\n",
        "    \"val\":  eval_split(X_va_s, y_va),\n",
        "    \"test\": eval_split(X_te_s, y_te),\n",
        "}\n",
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6e8991e",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "thesis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
