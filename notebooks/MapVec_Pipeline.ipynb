{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ccd509",
   "metadata": {},
   "source": [
    "### Cell 0 â€” Repository Bootstrap & Experiment Registry (Required)\n",
    "\n",
    "This cell ensures that the project repository is discoverable by Python **and**\n",
    "defines the registry of experiments that will be executed in this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why this is needed\n",
    "\n",
    "- The notebook lives inside the `notebooks/` directory  \n",
    "- Python does not automatically know where the project root is  \n",
    "- All project code lives under the `src/` directory  \n",
    "- Multiple experimental configurations (prompt-only, USE + map, OpenAI + map)\n",
    "  must be executed in a single, reproducible workflow  \n",
    "\n",
    "---\n",
    "\n",
    "#### What this cell does\n",
    "\n",
    "- Walks up the directory tree starting from the current notebook location  \n",
    "- Finds the repository root (identified by the presence of a `src/` folder)  \n",
    "- Adds that directory to `sys.path` so imports such as  \n",
    "  `from src.config import ...` work correctly  \n",
    "- Defines a central **experiment registry** describing:\n",
    "  - which feature representation is used\n",
    "  - where training data is read from\n",
    "  - where trained models and metadata are saved  \n",
    "\n",
    "---\n",
    "\n",
    "#### Design principles\n",
    "\n",
    "- Executed once at the very top of the notebook  \n",
    "- Contains no learning or model-specific logic  \n",
    "- Provides a single source of truth for all experiment configurations  \n",
    "- Enables fair and controlled comparison between different feature setups  \n",
    "\n",
    "---\n",
    "\n",
    "This cell must be executed **before any imports from `src.*`**.  \n",
    "All subsequent cells rely on the repository path and experiment definitions\n",
    "established here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2152ca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: /Users/amirdonyadide/Documents/GitHub/Thesis\n",
      "ðŸ§ª Will run experiments: prompt_only, map_only, use_map, openai_map\n",
      " - prompt_only  | mode=prompt_only    | prompt=dan          | train_out=train_out_prompt_only | model_out=exp_prompt_only\n",
      " - map_only     | mode=map_only       | prompt=-            | train_out=train_out_map_only | model_out=exp_map_only\n",
      " - use_map      | mode=prompt_plus_map | prompt=dan          | train_out=train_out_use | model_out=exp_use_map\n",
      " - openai_map   | mode=prompt_plus_map | prompt=openai-small | train_out=train_out_openai | model_out=exp_openai_map\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 0: bootstrap so \"src\" is importable + run-all-experiments config ---\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Make repo root importable (find folder containing \"src\")\n",
    "# -------------------------------------------------\n",
    "p = Path.cwd().resolve()\n",
    "for candidate in [p, *p.parents]:\n",
    "    if (candidate / \"src\").is_dir():\n",
    "        if str(candidate) not in sys.path:\n",
    "            sys.path.insert(0, str(candidate))\n",
    "        REPO_ROOT = candidate\n",
    "        print(\"Repo root:\", candidate)\n",
    "        break\n",
    "else:\n",
    "    raise RuntimeError(\"Could not find repo root (no 'src' folder found in parents).\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Experiment registry (ALL experiments will run)\n",
    "# -------------------------------------------------\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "\n",
    "# NOTE:\n",
    "# - feature_mode strings are aligned with helpers:\n",
    "#     \"prompt_only\", \"map_only\", \"prompt_plus_map\"\n",
    "# - prompt_encoder_kind must be one of:\n",
    "#     \"dan\", \"transformer\", \"openai-small\", \"openai-large\"\n",
    "EXPERIMENTS = {\n",
    "    \"prompt_only\": {\n",
    "        \"train_out\": DATA_DIR / \"output\" / \"train_out_prompt_only\",\n",
    "        \"model_out\": DATA_DIR / \"output\" / \"models\" / \"exp_prompt_only\",\n",
    "        \"feature_mode\": \"prompt_only\",\n",
    "        \"prompt_encoder_kind\": \"dan\",\n",
    "    },\n",
    "    \"map_only\": {\n",
    "        \"train_out\": DATA_DIR / \"output\" / \"train_out_map_only\",\n",
    "        \"model_out\": DATA_DIR / \"output\" / \"models\" / \"exp_map_only\",\n",
    "        \"feature_mode\": \"map_only\",\n",
    "        # no prompt encoder needed\n",
    "    },\n",
    "    \"use_map\": {\n",
    "        \"train_out\": DATA_DIR / \"output\" / \"train_out_use\",\n",
    "        \"model_out\": DATA_DIR / \"output\" / \"models\" / \"exp_use_map\",\n",
    "        \"feature_mode\": \"prompt_plus_map\",\n",
    "        \"prompt_encoder_kind\": \"dan\",\n",
    "    },\n",
    "    \"openai_map\": {\n",
    "        \"train_out\": DATA_DIR / \"output\" / \"train_out_openai\",\n",
    "        \"model_out\": DATA_DIR / \"output\" / \"models\" / \"exp_openai_map\",\n",
    "        \"feature_mode\": \"prompt_plus_map\",\n",
    "        \"prompt_encoder_kind\": \"openai-small\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create output folders\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    cfg[\"train_out\"] = Path(cfg[\"train_out\"])\n",
    "    cfg[\"model_out\"] = Path(cfg[\"model_out\"])\n",
    "    cfg[\"train_out\"].mkdir(parents=True, exist_ok=True)\n",
    "    cfg[\"model_out\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ðŸ§ª Will run experiments:\", \", \".join(EXPERIMENTS.keys()))\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    pe = cfg.get(\"prompt_encoder_kind\", \"-\")\n",
    "    print(\n",
    "        f\" - {exp_name:12s} | \"\n",
    "        f\"mode={cfg['feature_mode']:14s} | \"\n",
    "        f\"prompt={pe:12s} | \"\n",
    "        f\"train_out={cfg['train_out'].name} | \"\n",
    "        f\"model_out={cfg['model_out'].name}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f56a2c",
   "metadata": {},
   "source": [
    "### Cell 1 â€” Experiment Setup & Global Configuration\n",
    "\n",
    "This cell initializes the experiment environment and loads the global configuration required\n",
    "for training and evaluation. It is designed so the notebook can run **all experiment\n",
    "configurations in one pass** (prompt-only, USE + map, OpenAI + map) without manual edits.\n",
    "\n",
    "**What this cell does:**\n",
    "\n",
    "- **Loads global project configuration**\n",
    "  - Paths (`PATHS`)\n",
    "  - Runtime settings (`CFG`)\n",
    "  - Operator groups (`DISTANCE_OPS`, `AREA_OPS`)\n",
    "  - Dynamic extent configuration flags (`USE_DYNAMIC_EXTENT_REFS`, `ALLOW_FALLBACK_EXTENT`)\n",
    "  - Extent reference column names (`EXTENT_DIAG_COL`, `EXTENT_AREA_COL`)\n",
    "\n",
    "- **Defines a central experiment registry**\n",
    "  - `EXPERIMENTS` â€” a dictionary where each experiment specifies:\n",
    "    - `feature_mode`:\n",
    "      - `prompt_only` (uses prompt embeddings only)\n",
    "      - `fused` (uses concatenated map + prompt embeddings)\n",
    "    - `train_out` â€” where the prepared matrices (`X_*`) and `train_pairs.parquet` are read from\n",
    "    - `model_out` â€” where trained artifacts are saved\n",
    "\n",
    "- **Sets embedding dimensions**\n",
    "  - `MAP_DIM`, `PROMPT_DIM`\n",
    "  - `FUSED_DIM = MAP_DIM + PROMPT_DIM`\n",
    "  - The effective input dimension is derived per experiment:\n",
    "    - `prompt_only` â†’ `PROMPT_DIM`\n",
    "    - `fused` â†’ `FUSED_DIM`\n",
    "\n",
    "- **Validates experiment folders**\n",
    "  - Ensures each experimentâ€™s `train_out` and `model_out` directories exist\n",
    "  - Performs basic schema checks (required keys, valid feature modes)\n",
    "\n",
    "**Design principles**\n",
    "\n",
    "- Executed once near the top of the notebook (before data loading/training)\n",
    "- Contains no model training logic\n",
    "- Provides a single source of truth for experiment configuration\n",
    "- Prevents accidental overwrites by saving each experiment into its own output folder\n",
    "\n",
    "This cell must be executed **before any training or evaluation cells**. All experiment\n",
    "comparisons depend on the consistent configuration established here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "367f89f6ac45439b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:21:53.302406Z",
     "start_time": "2025-10-27T11:21:53.298709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFIG SUMMARY ===\n",
      "PROJ_ROOT  : /Users/amirdonyadide/Documents/GitHub/Thesis\n",
      "DATA_DIR   : /Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
      "INPUT_DIR  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input\n",
      "OUTPUT_DIR : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output\n",
      "MAPS_ROOT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/samples/pairs\n",
      "INPUT PAT. : *_input.geojson\n",
      "--- User Study ---\n",
      "USER_STUDY_XLSX : /Users/amirdonyadide/Documents/GitHub/Thesis/data/userstudy/UserStudy.xlsx\n",
      "RESPONSES_SHEET : Responses\n",
      "TILE_ID_COL     : tile_id\n",
      "COMPLETE_COL    : complete\n",
      "REMOVE_COL      : remove\n",
      "TEXT_COL        : cleaned_text\n",
      "PARAM_VALUE_COL : param_value\n",
      "OPERATOR_COL    : operator\n",
      "INTENSITY_COL   : intensity\n",
      "--- Filters / IDs / Split ---\n",
      "ONLY_COMPLETE   : True\n",
      "EXCLUDE_REMOVED : True\n",
      "PROMPT_ID       : r{i:08d}\n",
      "SPLIT_BY        : tile\n",
      "--- Outputs ---\n",
      "PROMPT_OUT : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "MAP_OUT    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n",
      "TRAIN_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out\n",
      "MODEL_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models\n",
      "SPLIT_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/splits\n",
      "PRM_NPZ    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/prompts_embeddings.npz\n",
      "MAPS_PQ    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/maps.parquet\n",
      "PAIRS_PQ   : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/train_pairs.parquet\n",
      "--- Model ---\n",
      "PROMPT_ENCODER: openai-small\n",
      "MAP_DIM       : 165\n",
      "PROMPT_DIM    : 512\n",
      "FUSED_DIM     : 677\n",
      "BATCH_SIZE    : 512\n",
      "VAL/TEST      : 0.15 0.15\n",
      "SEED          : 42\n",
      "--- Normalization behavior ---\n",
      "USE_DYNAMIC_EXTENT_REFS : True\n",
      "ALLOW_FALLBACK_EXTENT   : True\n",
      "--- Fallback tile scale (ONLY if dynamic refs missing) ---\n",
      "DEFAULT_TILE_W/H (m) : 400.0 400.0\n",
      "DEFAULT_TILE_DIAG_M  : 565.685424949238\n",
      "DEFAULT_TILE_AREA_M2 : 160000.0\n",
      "--- Extent columns ---\n",
      "EXTENT_DIAG_COL : extent_diag_m\n",
      "EXTENT_AREA_COL : extent_area_m2\n",
      "--- Operator groups ---\n",
      "DISTANCE_OPS : ('aggregate', 'displace', 'simplify')\n",
      "AREA_OPS     : ('select',)\n",
      "--- Operator groups ---\n",
      "DISTANCE_OPS : ('aggregate', 'displace', 'simplify')\n",
      "AREA_OPS     : ('select',)\n",
      "--- Param estimation ---\n",
      "PARAM_STRATEGY : mlp\n",
      "QUAL_TO_QUANTILE: {'very_small': 0.1, 'small': 0.25, 'medium': 0.5, 'large': 0.75, 'very_large': 0.9}\n",
      "DEFAULT_PARAM_BY_OPERATOR: {'aggregate': 5.0, 'displace': 5.0, 'simplify': 5.0, 'select': 50.0}\n",
      "USE_DYNAMIC_EXTENT_REFS: True\n",
      "ALLOW_FALLBACK_EXTENT  : True\n",
      "EXTENT_DIAG_COL: extent_diag_m  EXTENT_AREA_COL: extent_area_m2\n",
      "CFG dims -> MAP_DIM: 165 | PROMPT_DIM: 512 | FUSED_DIM: 677\n",
      "BATCH_SIZE: 512\n",
      "\n",
      "ðŸ§ª Experiments to be executed:\n",
      " - prompt_only  | mode=prompt_only    | prompt=dan          | train_out=train_out_prompt_only | model_out=exp_prompt_only\n",
      " - map_only     | mode=map_only       | prompt=-            | train_out=train_out_map_only | model_out=exp_map_only\n",
      " - use_map      | mode=prompt_plus_map | prompt=dan          | train_out=train_out_use | model_out=exp_use_map\n",
      " - openai_map   | mode=prompt_plus_map | prompt=openai-small | train_out=train_out_openai | model_out=exp_openai_map\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 1 â€” PARAMETERS =====================\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from src.config import (\n",
    "    PATHS, CFG, print_summary,\n",
    "    DISTANCE_OPS, AREA_OPS,\n",
    "    USE_DYNAMIC_EXTENT_REFS, ALLOW_FALLBACK_EXTENT,\n",
    "    EXTENT_DIAG_COL, EXTENT_AREA_COL,\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Global config overview (same for all experiments)\n",
    "# -------------------------------------------------\n",
    "print_summary()\n",
    "print(\"USE_DYNAMIC_EXTENT_REFS:\", USE_DYNAMIC_EXTENT_REFS)\n",
    "print(\"ALLOW_FALLBACK_EXTENT  :\", ALLOW_FALLBACK_EXTENT)\n",
    "print(\"EXTENT_DIAG_COL:\", EXTENT_DIAG_COL, \" EXTENT_AREA_COL:\", EXTENT_AREA_COL)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Dimensions from config (may be overridden later by inferred dims)\n",
    "# -------------------------------------------------\n",
    "MAP_DIM_CFG = int(CFG.MAP_DIM)\n",
    "PROMPT_DIM_CFG = int(CFG.PROMPT_DIM)\n",
    "FUSED_DIM_CFG = MAP_DIM_CFG + PROMPT_DIM_CFG\n",
    "BATCH_SIZE = int(CFG.BATCH_SIZE)\n",
    "\n",
    "print(\"CFG dims -> MAP_DIM:\", MAP_DIM_CFG, \"| PROMPT_DIM:\", PROMPT_DIM_CFG, \"| FUSED_DIM:\", FUSED_DIM_CFG)\n",
    "print(\"BATCH_SIZE:\", BATCH_SIZE)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Validate experiment registry (from Cell 0)\n",
    "# -------------------------------------------------\n",
    "required_keys = {\"train_out\", \"model_out\", \"feature_mode\"}\n",
    "allowed_modes = {\"prompt_only\", \"map_only\", \"prompt_plus_map\"}\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    missing = required_keys - set(cfg.keys())\n",
    "    if missing:\n",
    "        raise ValueError(f\"Experiment '{exp_name}' is missing keys: {missing}\")\n",
    "\n",
    "    mode = str(cfg[\"feature_mode\"]).strip().lower()\n",
    "    if mode not in allowed_modes:\n",
    "        raise ValueError(\n",
    "            f\"Experiment '{exp_name}' has invalid feature_mode='{cfg['feature_mode']}'. \"\n",
    "            f\"Allowed: {sorted(allowed_modes)}\"\n",
    "        )\n",
    "    cfg[\"feature_mode\"] = mode  # normalize\n",
    "\n",
    "    cfg[\"train_out\"] = Path(cfg[\"train_out\"])\n",
    "    cfg[\"model_out\"] = Path(cfg[\"model_out\"])\n",
    "\n",
    "    cfg[\"train_out\"].mkdir(parents=True, exist_ok=True)\n",
    "    cfg[\"model_out\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Enforce prompt encoder presence/absence consistently\n",
    "    if mode in {\"prompt_only\", \"prompt_plus_map\"}:\n",
    "        if \"prompt_encoder_kind\" not in cfg:\n",
    "            raise ValueError(\n",
    "                f\"Experiment '{exp_name}' needs 'prompt_encoder_kind' because feature_mode='{mode}'.\"\n",
    "            )\n",
    "    if mode == \"map_only\":\n",
    "        # ignore if user accidentally set it; keep registry clean\n",
    "        cfg.pop(\"prompt_encoder_kind\", None)\n",
    "\n",
    "print(\"\\nðŸ§ª Experiments to be executed:\")\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    pe = cfg.get(\"prompt_encoder_kind\", \"-\")\n",
    "    print(\n",
    "        f\" - {exp_name:12s} | \"\n",
    "        f\"mode={cfg['feature_mode']:14s} | \"\n",
    "        f\"prompt={pe:12s} | \"\n",
    "        f\"train_out={cfg['train_out'].name} | \"\n",
    "        f\"model_out={cfg['model_out'].name}\"\n",
    "    )\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Helper: expected input dimensionality per feature mode\n",
    "# (final dims will be set later by embedding-file inference)\n",
    "# -------------------------------------------------\n",
    "def get_feature_dims_from_cfg(feature_mode: str):\n",
    "    \"\"\"\n",
    "    Returns (map_dim, prompt_dim, fused_dim) using CFG defaults.\n",
    "    NOTE: Later cells infer actual dims from NPZ files and should override these.\n",
    "    \"\"\"\n",
    "    fm = str(feature_mode).strip().lower()\n",
    "    if fm == \"prompt_only\":\n",
    "        return 0, PROMPT_DIM_CFG, PROMPT_DIM_CFG\n",
    "    if fm == \"map_only\":\n",
    "        return MAP_DIM_CFG, 0, MAP_DIM_CFG\n",
    "    if fm == \"prompt_plus_map\":\n",
    "        return MAP_DIM_CFG, PROMPT_DIM_CFG, FUSED_DIM_CFG\n",
    "    raise ValueError(f\"Unknown feature_mode: {feature_mode}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a2350e",
   "metadata": {},
   "source": [
    "## Step 2 â€” Prompt Embedding Generation (All Experiments)\n",
    "\n",
    "In this step, we generate vector embeddings for all user prompts for **each experiment\n",
    "configuration** (e.g., prompt-only, USE + map, OpenAI + map). The embedding backend is\n",
    "selected per experiment via `PROMPT_ENCODER` (e.g., **USE-DAN/Transformer** or **OpenAI\n",
    "text-embedding-3-* models**).\n",
    "\n",
    "### What happens here?\n",
    "\n",
    "For each entry in the experiment registry:\n",
    "\n",
    "- Prompts are loaded from the user study source file.\n",
    "- Only valid prompts are kept (`complete == True`, `remove == False`).\n",
    "- The prompt embedding model is selected from the experiment configuration\n",
    "  (mapped to `CFG.PROMPT_ENCODER`).\n",
    "- All prompts are embedded in batches (with optional L2 normalization).\n",
    "- The resulting embeddings and metadata are saved to an **experiment-specific folder**\n",
    "  so no configuration overwrites another.\n",
    "\n",
    "**Outputs per experiment** (written under `PATHS.PROMPT_OUT/<experiment_name>/`):\n",
    "\n",
    "- `prompts_embeddings.npz` â€” matrix `E` and `ids`\n",
    "- `prompts.parquet` â€” prompt_id, text, tile_id\n",
    "- `meta.json` â€” model label, dimensionality, and export info\n",
    "\n",
    "### Why this is encapsulated in a helper\n",
    "\n",
    "To keep the notebook clean and reproducible, all logic related to:\n",
    "\n",
    "- loading and filtering prompts,\n",
    "- selecting the embedding backend,\n",
    "- batching and normalization,\n",
    "- saving outputs in a consistent format,\n",
    "\n",
    "is encapsulated in `src/train/run_prompt_embeddings.py`.\n",
    "\n",
    "The notebook only *orchestrates* experiments by calling this helper with an\n",
    "experiment-specific configuration.\n",
    "\n",
    "This design ensures:\n",
    "\n",
    "- consistent prompt embeddings across training and evaluation,\n",
    "- easy comparison between USE and OpenAI backends,\n",
    "- clean separation between experiment orchestration (notebook) and implementation (src),\n",
    "- safe parallel storage of artifacts for multiple experiment runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed0df45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:21:55.572701Z",
     "start_time": "2025-10-27T11:21:55.570071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running prompt embeddings for experiments that require prompts ===\n",
      "\n",
      "ðŸ§ª Experiment: prompt_only\n",
      "   feature_mode   : prompt_only\n",
      "   PROMPT_ENCODER : dan\n",
      "   Output dir     : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/prompt_only\n",
      "   âœ… Prompt embeddings already exist â€” skipping recomputation.\n",
      "\n",
      "ðŸ§ª Experiment: map_only\n",
      "   (skip) feature_mode=map_only â†’ no prompt embeddings required.\n",
      "\n",
      "ðŸ§ª Experiment: use_map\n",
      "   feature_mode   : prompt_plus_map\n",
      "   PROMPT_ENCODER : dan\n",
      "   Output dir     : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/use_map\n",
      "   âœ… Prompt embeddings already exist â€” skipping recomputation.\n",
      "\n",
      "ðŸ§ª Experiment: openai_map\n",
      "   feature_mode   : prompt_plus_map\n",
      "   PROMPT_ENCODER : openai-small\n",
      "   Output dir     : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/openai_map\n",
      "   âœ… Prompt embeddings already exist â€” skipping recomputation.\n",
      "\n",
      "âœ… Prompt embedding step finished.\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 2 â€” Prompt embeddings (experiment-scoped) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "from dataclasses import replace\n",
    "\n",
    "from src.train.run_prompt_embeddings import run_prompt_embeddings_from_config\n",
    "\n",
    "print(\"\\n=== Running prompt embeddings for experiments that require prompts ===\")\n",
    "\n",
    "prompt_meta_by_experiment = {}\n",
    "\n",
    "for exp_name, cfg_exp in EXPERIMENTS.items():\n",
    "\n",
    "    feature_mode = cfg_exp[\"feature_mode\"]\n",
    "\n",
    "    # Map-only experiments do not need prompt embeddings\n",
    "    if feature_mode == \"map_only\":\n",
    "        print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "        print(\"   (skip) feature_mode=map_only â†’ no prompt embeddings required.\")\n",
    "        continue\n",
    "\n",
    "    # IMPORTANT: prompt_encoder_kind must be one of:\n",
    "    #   \"dan\", \"transformer\", \"openai-small\", \"openai-large\"\n",
    "    prompt_encoder_kind = cfg_exp.get(\"prompt_encoder_kind\", CFG.PROMPT_ENCODER)\n",
    "\n",
    "    # Create an experiment-specific cfg (CFG is frozen â†’ use replace)\n",
    "    CFG_EXP = replace(CFG, PROMPT_ENCODER=prompt_encoder_kind)\n",
    "\n",
    "    prompt_out_dir = Path(PATHS.PROMPT_OUT) / exp_name\n",
    "    prompt_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Skip recomputation if outputs already exist\n",
    "    emb_npz = prompt_out_dir / \"prompts_embeddings.npz\"\n",
    "    prm_pq  = prompt_out_dir / \"prompts.parquet\"\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   feature_mode   : {feature_mode}\")\n",
    "    print(f\"   PROMPT_ENCODER : {CFG_EXP.PROMPT_ENCODER}\")\n",
    "    print(f\"   Output dir     : {prompt_out_dir}\")\n",
    "\n",
    "    if emb_npz.exists() and prm_pq.exists():\n",
    "        print(\"   âœ… Prompt embeddings already exist â€” skipping recomputation.\")\n",
    "        meta = {\n",
    "            \"out_dir\": str(prompt_out_dir),\n",
    "            \"embeddings_path\": str(emb_npz),\n",
    "            \"prompts_parquet_path\": str(prm_pq),\n",
    "            \"skipped\": True,\n",
    "        }\n",
    "    else:\n",
    "        meta = run_prompt_embeddings_from_config(\n",
    "            input_path=Path(PATHS.USER_STUDY_XLSX),\n",
    "            out_dir=prompt_out_dir,\n",
    "            cfg=CFG_EXP,\n",
    "            paths=PATHS,\n",
    "            verbosity=1,\n",
    "            l2_normalize=True,\n",
    "            also_save_embeddings_csv=False,\n",
    "        )\n",
    "        print(\"   âœ… Prompt embeddings completed.\")\n",
    "\n",
    "    prompt_meta_by_experiment[exp_name] = meta\n",
    "\n",
    "print(\"\\nâœ… Prompt embedding step finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c0b51",
   "metadata": {},
   "source": [
    "## Step 3 â€” Map Embeddings (Dynamic, Shared Across Experiments)\n",
    "\n",
    "In this step, we compute **map embeddings** for all GeoJSON tiles that are eligible for the user study.  \n",
    "Because map embeddings depend only on the input map data (not on the prompt encoder), they are computed **once** and stored in a **shared output folder**, then reused across all experiments.\n",
    "\n",
    "### What this step does\n",
    "\n",
    "1. **Filters tiles using the user study Excel**\n",
    "   - Keeps only rows marked as *complete*\n",
    "   - Excludes rows marked as *removed*\n",
    "   - Extracts the set of allowed `tile_id`s (used to select which map folders to embed)\n",
    "\n",
    "2. **Discovers and embeds GeoJSON maps**\n",
    "   - Finds all GeoJSON files under `PATHS.MAPS_ROOT`\n",
    "   - Keeps only those whose `map_id` is in the allowed set\n",
    "   - Counts valid polygons per map to determine a dataset-wide `max_polygons`\n",
    "     (used to normalize the `poly_count` feature safely)\n",
    "\n",
    "3. **Computes map embeddings**\n",
    "   - Uses `norm=\"extent\"` for **dynamic per-map normalization**\n",
    "   - Ensures all vectors have consistent dimensionality\n",
    "   - Skips maps with invalid geometries or degenerate extents\n",
    "\n",
    "4. **Stores dynamic extent references (required for parameter scaling)**\n",
    "   - `extent_diag_m`\n",
    "   - `extent_area_m2`\n",
    "\n",
    "   These are saved alongside embeddings and are later used to convert\n",
    "   normalized parameters (`param_norm`) into real-world units:\n",
    "   - distance operators â†’ meters via `extent_diag_m`\n",
    "   - area operators â†’ mÂ² via `extent_area_m2`\n",
    "\n",
    "5. **Writes outputs once to a shared directory**\n",
    "   - Prevents redundant computation across experiments\n",
    "   - Guarantees every experiment uses the **same map representation**\n",
    "   - Avoids accidental overwrites while keeping artifacts reusable\n",
    "\n",
    "### Why this is important\n",
    "\n",
    "- Ensures **consistent normalization** between training and evaluation  \n",
    "- Provides the necessary per-map reference scales for parameter un-normalization  \n",
    "- Improves reproducibility and efficiency by reusing identical map embeddings  \n",
    "- Supports fair comparison between:\n",
    "  - prompt-only baselines (which ignore map embeddings)\n",
    "  - fused prompt + map hybrids\n",
    "  - different prompt backends (USE vs OpenAI)\n",
    "\n",
    "At the end of this step, the repository contains a self-contained set of map embeddings\n",
    "ready to be concatenated with prompt embeddings in the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ca0c3d8b71fc70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:26:59.687350Z",
     "start_time": "2025-10-27T11:26:19.901557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Map embeddings (shared across all experiments) ===\n",
      "Target dir: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/shared_extent\n",
      "âœ… Map embeddings completed.\n",
      "âœ… Map embedding artifacts ready:\n",
      " - /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/shared_extent/maps_embeddings.npz\n",
      " - /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/shared_extent/maps.parquet\n",
      "MAP_EMB_DIR: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/shared_extent\n",
      "MapEmbeddingRunMeta(n_tiles_allowed=399, n_maps_found=824, n_maps_used=399, max_polygons=653, out_dir='/Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/shared_extent', embeddings_path='/Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/shared_extent/maps_embeddings.npz', maps_parquet_path='/Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/shared_extent/maps.parquet')\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 3 â€” Map embeddings (shared) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "from src.train.run_map_embeddings import run_map_embeddings_from_config\n",
    "\n",
    "# Map embeddings do NOT depend on the prompt backend, so we compute them once and reuse.\n",
    "# Keep a stable subfolder name so all experiments can point to it.\n",
    "MAP_EMB_DIR = Path(PATHS.MAP_OUT) / \"shared_extent\"\n",
    "MAP_EMB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Expected outputs\n",
    "maps_npz = MAP_EMB_DIR / \"maps_embeddings.npz\"\n",
    "maps_pq  = MAP_EMB_DIR / \"maps.parquet\"\n",
    "\n",
    "print(\"\\n=== Map embeddings (shared across all experiments) ===\")\n",
    "print(\"Target dir:\", MAP_EMB_DIR)\n",
    "\n",
    "# Skip if already computed\n",
    "if maps_npz.exists() and maps_pq.exists():\n",
    "    print(\"âœ… Map embeddings already exist â€” skipping recomputation.\")\n",
    "    map_meta = {\"out_dir\": str(MAP_EMB_DIR), \"skipped\": True}\n",
    "else:\n",
    "    map_meta = run_map_embeddings_from_config(\n",
    "        maps_root=Path(PATHS.MAPS_ROOT),\n",
    "        input_pattern=PATHS.INPUT_MAPS_PATTERN,\n",
    "        user_study_xlsx=Path(PATHS.USER_STUDY_XLSX),\n",
    "        responses_sheet=PATHS.RESPONSES_SHEET,\n",
    "        tile_id_col=PATHS.TILE_ID_COL,\n",
    "        complete_col=PATHS.COMPLETE_COL,\n",
    "        remove_col=PATHS.REMOVE_COL,\n",
    "        only_complete=bool(PATHS.ONLY_COMPLETE),\n",
    "        exclude_removed=bool(PATHS.EXCLUDE_REMOVED),\n",
    "        out_dir=MAP_EMB_DIR,\n",
    "        verbosity=1,\n",
    "        norm=\"extent\",  # dynamic per-map normalization\n",
    "    )\n",
    "    print(\"âœ… Map embeddings completed.\")\n",
    "\n",
    "# Hard checks (fail early if something went wrong)\n",
    "if not maps_npz.exists():\n",
    "    raise FileNotFoundError(f\"Missing maps_embeddings.npz at: {maps_npz}\")\n",
    "if not maps_pq.exists():\n",
    "    raise FileNotFoundError(f\"Missing maps.parquet at: {maps_pq}\")\n",
    "\n",
    "print(\"âœ… Map embedding artifacts ready:\")\n",
    "print(\" -\", maps_npz)\n",
    "print(\" -\", maps_pq)\n",
    "print(\"MAP_EMB_DIR:\", MAP_EMB_DIR)\n",
    "print(map_meta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaf32c5",
   "metadata": {},
   "source": [
    "### ðŸ”¢ Inferring Embedding Dimensions (Experiment-Aware)\n",
    "\n",
    "In this step, we **infer embedding dimensionalities directly from the saved embedding files**\n",
    "rather than relying on configuration defaults. Dimensions are inferred **per experiment** to\n",
    "account for different prompt embedding backends (e.g., USE vs OpenAI), while map embeddings\n",
    "are shared across experiments.\n",
    "\n",
    "This ensures:\n",
    "- A **single source of truth** for feature dimensions\n",
    "- **Consistency** between training and evaluation pipelines\n",
    "- Robustness to changes in embedding models or backend configurations\n",
    "- Correct handling of mixed feature modes (prompt-only vs. fused prompt + map)\n",
    "\n",
    "Specifically, we:\n",
    "\n",
    "- Load **prompt embeddings** from  \n",
    "  `PATHS.PROMPT_OUT/<experiment_name>/prompts_embeddings.npz`\n",
    "- Load **map embeddings** from the shared map embedding directory  \n",
    "  `PATHS.MAP_OUT/<shared_folder>/maps_embeddings.npz`\n",
    "- Infer dimensions as follows:\n",
    "  - `PROMPT_DIM` â€” from prompt embeddings (per experiment)\n",
    "  - `MAP_DIM` â€” from map embeddings (shared)\n",
    "  - `FUSED_DIM` â€” computed per experiment:\n",
    "    - `prompt_only` â†’ `PROMPT_DIM`\n",
    "    - `fused` â†’ `MAP_DIM + PROMPT_DIM`\n",
    "\n",
    "If inferred dimensions differ from those defined in the global configuration (`CFG`),\n",
    "the inferred values take precedence for all downstream processing.\n",
    "\n",
    "The inferred dimensions are stored in the experiment registry and used consistently by:\n",
    "- feature preprocessing\n",
    "- operator classification\n",
    "- parameter regression\n",
    "- evaluation and inference\n",
    "\n",
    "This guarantees that all downstream models operate on **correctly shaped feature vectors**\n",
    "and that comparisons between experiments remain valid and reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e8cdf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Inferred MAP_DIM from shared maps: 165\n",
      "\n",
      "âœ… Inferred dims per experiment:\n",
      " - prompt_only  | mode=prompt_only    | MAP_DIM=   0 | PROMPT_DIM= 512 | FUSED_DIM= 512\n",
      " - map_only     | mode=map_only       | MAP_DIM= 165 | PROMPT_DIM=   0 | FUSED_DIM= 165\n",
      " - use_map      | mode=prompt_plus_map | MAP_DIM= 165 | PROMPT_DIM= 512 | FUSED_DIM= 677\n",
      " - openai_map   | mode=prompt_plus_map | MAP_DIM= 165 | PROMPT_DIM=1536 | FUSED_DIM=1701\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 4 â€” Infer embedding dimensions (multi-experiment, incl. map-only) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def _infer_dim_from_npz(npz_path: Path) -> int:\n",
    "    if not npz_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing embeddings file: {npz_path}\")\n",
    "    with np.load(npz_path, allow_pickle=True) as z:\n",
    "        if \"E\" not in z:\n",
    "            raise ValueError(f\"{npz_path} missing array 'E'\")\n",
    "        E = z[\"E\"]\n",
    "    if E.ndim != 2 or E.shape[1] <= 0:\n",
    "        raise ValueError(f\"Invalid embedding matrix in {npz_path}: shape={E.shape}\")\n",
    "    return int(E.shape[1])\n",
    "\n",
    "# -------------------------------\n",
    "# Map dim (shared across all experiments)\n",
    "# -------------------------------\n",
    "maps_npz = Path(MAP_EMB_DIR) / \"maps_embeddings.npz\"\n",
    "MAP_DIM_INF = _infer_dim_from_npz(maps_npz)\n",
    "print(\"âœ… Inferred MAP_DIM from shared maps:\", MAP_DIM_INF)\n",
    "\n",
    "# -------------------------------\n",
    "# Prompt dim per experiment + final input dim per experiment\n",
    "# -------------------------------\n",
    "dims_by_experiment = {}\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    feature_mode = str(cfg[\"feature_mode\"]).strip().lower()\n",
    "\n",
    "    # Prompt dim is only relevant when prompts are used\n",
    "    PROMPT_DIM_INF = 0\n",
    "    prm_npz = Path(PATHS.PROMPT_OUT) / exp_name / \"prompts_embeddings.npz\"\n",
    "    if feature_mode in {\"prompt_only\", \"prompt_plus_map\"}:\n",
    "        PROMPT_DIM_INF = _infer_dim_from_npz(prm_npz)\n",
    "\n",
    "    if feature_mode == \"prompt_only\":\n",
    "        map_dim = 0\n",
    "        prompt_dim = PROMPT_DIM_INF\n",
    "        fused_dim = PROMPT_DIM_INF\n",
    "\n",
    "    elif feature_mode == \"map_only\":\n",
    "        map_dim = MAP_DIM_INF\n",
    "        prompt_dim = 0\n",
    "        fused_dim = MAP_DIM_INF\n",
    "\n",
    "    elif feature_mode == \"prompt_plus_map\":\n",
    "        map_dim = MAP_DIM_INF\n",
    "        prompt_dim = PROMPT_DIM_INF\n",
    "        fused_dim = MAP_DIM_INF + PROMPT_DIM_INF\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown feature_mode for {exp_name}: {feature_mode}\")\n",
    "\n",
    "    # Store into cfg for later cells\n",
    "    cfg[\"map_dim\"] = int(map_dim)\n",
    "    cfg[\"prompt_dim\"] = int(prompt_dim)\n",
    "    cfg[\"fused_dim\"] = int(fused_dim)\n",
    "\n",
    "    dims_by_experiment[exp_name] = {\n",
    "        \"feature_mode\": feature_mode,\n",
    "        \"MAP_DIM\": int(map_dim),\n",
    "        \"PROMPT_DIM\": int(prompt_dim),\n",
    "        \"FUSED_DIM\": int(fused_dim),\n",
    "    }\n",
    "\n",
    "print(\"\\nâœ… Inferred dims per experiment:\")\n",
    "for exp_name, d in dims_by_experiment.items():\n",
    "    print(\n",
    "        f\" - {exp_name:12s} | mode={d['feature_mode']:14s} | \"\n",
    "        f\"MAP_DIM={d['MAP_DIM']:4d} | PROMPT_DIM={d['PROMPT_DIM']:4d} | FUSED_DIM={d['FUSED_DIM']:4d}\"\n",
    "    )\n",
    "\n",
    "# Optional: warn if global CFG is stale (informational only)\n",
    "if MAP_DIM_INF != int(CFG.MAP_DIM):\n",
    "    print(\"\\nâš ï¸ CFG.MAP_DIM differs from inferred MAP_DIM (using inferred for experiments).\")\n",
    "    print(f\"   inferred MAP_DIM={MAP_DIM_INF} vs CFG.MAP_DIM={CFG.MAP_DIM}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd186319e89f445",
   "metadata": {},
   "source": [
    "### ðŸ”— Feature Construction (Prompt-Only vs. Fused Map + Prompt Embeddings)\n",
    "\n",
    "In this step, we construct the final feature matrices used for training and evaluation by\n",
    "aligning prompt embeddings with map embeddings and exporting **experiment-scoped** artifacts.\n",
    "\n",
    "#### What this step does (per experiment)\n",
    "\n",
    "- Loads **prompt embeddings** from  \n",
    "  `PATHS.PROMPT_OUT/<experiment_name>/prompts_embeddings.npz`\n",
    "- Loads **map embeddings** from the shared map embedding directory  \n",
    "  `PATHS.MAP_OUT/<shared_folder>/maps_embeddings.npz`\n",
    "- Aligns samples via the authoritative pairing table (`prompts.parquet` â†’ `map_id/tile_id` + `prompt_id`)\n",
    "- Merges **dynamic map extent metadata** (e.g., `extent_diag_m`, `extent_area_m2`) from `maps.parquet`\n",
    "  so downstream regression can convert normalized parameters into real-world units.\n",
    "\n",
    "#### Feature modes supported\n",
    "\n",
    "- **`prompt_only`**  \n",
    "  Uses only prompt vectors:  \n",
    "  `X = prompt_embedding`\n",
    "\n",
    "- **`fused`**  \n",
    "  Concatenates map and prompt vectors:  \n",
    "  `X = [map_embedding | prompt_embedding]`\n",
    "\n",
    "#### Outputs written per experiment\n",
    "\n",
    "All artifacts are saved into the experimentâ€™s `train_out` directory (to prevent overwrites):\n",
    "\n",
    "- `X_prompt.npy` or `X_concat.npy` â€” final feature matrix (depending on feature mode)\n",
    "- `train_pairs.parquet` â€” aligned metadata (including `operator`, `param_value`, and extent references)\n",
    "- `meta.json` â€” provenance (sources, options) and shape information\n",
    "\n",
    "#### Why this design\n",
    "\n",
    "- Keeps **training and evaluation perfectly aligned** by exporting a single, consistent pairing table\n",
    "- Avoids hard-coded dimensions by relying on the saved embedding files\n",
    "- Supports **multiple experiments side-by-side** without overwriting artifacts\n",
    "- Enables **dynamic extent-aware** parameter regression (meters / mÂ² scaling) downstream\n",
    "- Ensures fair comparison: prompt-only baselines vs. fused map+prompt models\n",
    "\n",
    "After this step, each experiment has a complete, self-contained dataset ready for:\n",
    "1. Operator classification  \n",
    "2. Per-operator parameter regression  \n",
    "3. End-to-end evaluation in the evaluation notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa2b07a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Building feature matrices for all experiments ===\n",
      "\n",
      "ðŸ§ª Experiment: prompt_only\n",
      "   Feature mode (cfg)   : prompt_only\n",
      "   Feature mode (concat): prompt_only\n",
      "   Prompt out dir       : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/prompt_only\n",
      "   Map out dir          : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/shared_extent\n",
      "   Train out dir        : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out_prompt_only\n",
      "   âœ… Feature construction completed.\n",
      "\n",
      "ðŸ§ª Experiment: map_only\n",
      "   Feature mode (cfg)   : map_only\n",
      "   Feature mode (concat): prompt_plus_map\n",
      "   Prompt out dir       : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/prompt_only [reused for pairing only]\n",
      "   Map out dir          : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/shared_extent\n",
      "   Train out dir        : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out_map_only\n",
      "   Using pairs parquet  : train_pairs_map_only.parquet\n",
      "   âœ… Built X_map.npy for map_only: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out_map_only/X_map.npy\n",
      "   âœ… Feature construction completed.\n",
      "\n",
      "ðŸ§ª Experiment: use_map\n",
      "   Feature mode (cfg)   : prompt_plus_map\n",
      "   Feature mode (concat): prompt_plus_map\n",
      "   Prompt out dir       : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/use_map\n",
      "   Map out dir          : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/shared_extent\n",
      "   Train out dir        : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out_use\n",
      "   âœ… Feature construction completed.\n",
      "\n",
      "ðŸ§ª Experiment: openai_map\n",
      "   Feature mode (cfg)   : prompt_plus_map\n",
      "   Feature mode (concat): prompt_plus_map\n",
      "   Prompt out dir       : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/openai_map\n",
      "   Map out dir          : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/shared_extent\n",
      "   Train out dir        : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out_openai\n",
      "   âœ… Feature construction completed.\n",
      "\n",
      "âœ… All feature construction finished.\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 5 â€” Feature construction (multi-experiment, incl. map-only) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.train.run_concat_features import run_concat_features_from_dirs\n",
    "\n",
    "print(\"\\n=== Building feature matrices for all experiments ===\")\n",
    "\n",
    "concat_meta_by_experiment = {}\n",
    "\n",
    "def _to_concat_mode(feature_mode: str) -> str:\n",
    "    \"\"\"\n",
    "    run_concat_features_from_dirs supports:\n",
    "      - \"prompt_only\"\n",
    "      - \"prompt_plus_map\"\n",
    "    We'll call:\n",
    "      - prompt_only      -> \"prompt_only\"\n",
    "      - prompt_plus_map  -> \"prompt_plus_map\"\n",
    "      - map_only         -> \"prompt_plus_map\" (pairing only), then build X_map.npy ourselves\n",
    "    \"\"\"\n",
    "    fm = str(feature_mode).strip().lower()\n",
    "    if fm == \"prompt_only\":\n",
    "        return \"prompt_only\"\n",
    "    if fm in {\"prompt_plus_map\", \"map_only\"}:\n",
    "        return \"prompt_plus_map\"\n",
    "    raise ValueError(f\"Unsupported feature_mode in EXPERIMENTS: {feature_mode}\")\n",
    "\n",
    "def _find_pairs_parquet(train_out_dir: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Find the parquet file produced by run_concat_features_from_dirs that contains the aligned pairs.\n",
    "    Different implementations may name this file differently; we locate it robustly.\n",
    "    \"\"\"\n",
    "    # Common candidates\n",
    "    candidates = [\n",
    "        train_out_dir / \"train_pairs.parquet\",\n",
    "        *sorted(train_out_dir.glob(\"*train_pairs*.parquet\")),\n",
    "        *sorted(train_out_dir.glob(\"*pairs*.parquet\")),\n",
    "        *sorted(train_out_dir.glob(\"*.parquet\")),\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            return c\n",
    "    raise FileNotFoundError(f\"No parquet found in {train_out_dir}. Expected train_pairs*.parquet\")\n",
    "\n",
    "def _build_x_map_from_pairs(*, pairs_parquet: Path, maps_npz: Path, out_x_map: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Create X_map.npy aligned to the rows of pairs_parquet.\n",
    "    \"\"\"\n",
    "    pairs_parquet = Path(pairs_parquet)\n",
    "    maps_npz = Path(maps_npz)\n",
    "    out_x_map = Path(out_x_map)\n",
    "\n",
    "    if not pairs_parquet.exists():\n",
    "        raise FileNotFoundError(f\"Missing pairs parquet: {pairs_parquet}\")\n",
    "    if not maps_npz.exists():\n",
    "        raise FileNotFoundError(f\"Missing maps embeddings npz: {maps_npz}\")\n",
    "\n",
    "    pairs = pd.read_parquet(pairs_parquet)\n",
    "    if \"map_id\" not in pairs.columns:\n",
    "        raise ValueError(f\"{pairs_parquet} missing required column 'map_id'.\")\n",
    "\n",
    "    with np.load(maps_npz, allow_pickle=True) as z:\n",
    "        if \"E\" not in z or \"ids\" not in z:\n",
    "            raise ValueError(f\"{maps_npz} must contain arrays 'E' and 'ids'\")\n",
    "        E_map = z[\"E\"]\n",
    "        ids = [str(x) for x in z[\"ids\"].tolist()]\n",
    "\n",
    "    idx_map = {str(mid).strip().zfill(4): i for i, mid in enumerate(ids)}\n",
    "\n",
    "    map_ids = pairs[\"map_id\"].astype(str).str.strip().str.zfill(4).tolist()\n",
    "    sel = []\n",
    "    missing_ids = []\n",
    "    for mid in map_ids:\n",
    "        j = idx_map.get(mid)\n",
    "        if j is None:\n",
    "            sel.append(-1)\n",
    "            missing_ids.append(mid)\n",
    "        else:\n",
    "            sel.append(j)\n",
    "\n",
    "    if missing_ids:\n",
    "        missing_ids = sorted(set(missing_ids))\n",
    "        raise ValueError(\n",
    "            f\"{len(missing_ids)} unique map_id(s) in {pairs_parquet.name} not found in {maps_npz.name}. \"\n",
    "            f\"Examples: {missing_ids[:10]}\"\n",
    "        )\n",
    "\n",
    "    X_map = E_map[np.array(sel, dtype=int)].astype(np.float32, copy=False)\n",
    "    np.save(out_x_map, X_map)\n",
    "\n",
    "    return {\n",
    "        \"pairs_parquet\": str(pairs_parquet),\n",
    "        \"maps_npz\": str(maps_npz),\n",
    "        \"rows\": int(X_map.shape[0]),\n",
    "        \"map_dim\": int(X_map.shape[1]),\n",
    "        \"saved\": str(out_x_map),\n",
    "    }\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "\n",
    "    feature_mode = cfg[\"feature_mode\"]\n",
    "    mode_for_concat = _to_concat_mode(feature_mode)\n",
    "\n",
    "    train_out_dir = Path(cfg[\"train_out\"])\n",
    "    train_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Shared map embeddings\n",
    "    map_out_dir = MAP_EMB_DIR\n",
    "\n",
    "    # Prompt embeddings:\n",
    "    # - prompt_only + prompt_plus_map: use exp prompt folder\n",
    "    # - map_only: reuse prompt_only prompt folder ONLY to let concat helper build aligned pairs\n",
    "    if feature_mode == \"map_only\":\n",
    "        prompt_out_dir = Path(PATHS.PROMPT_OUT) / \"prompt_only\"\n",
    "        pairing_note = \" [reused for pairing only]\"\n",
    "    else:\n",
    "        prompt_out_dir = Path(PATHS.PROMPT_OUT) / exp_name\n",
    "        pairing_note = \"\"\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   Feature mode (cfg)   : {feature_mode}\")\n",
    "    print(f\"   Feature mode (concat): {mode_for_concat}\")\n",
    "    print(f\"   Prompt out dir       : {prompt_out_dir}{pairing_note}\")\n",
    "    print(f\"   Map out dir          : {map_out_dir}\")\n",
    "    print(f\"   Train out dir        : {train_out_dir}\")\n",
    "\n",
    "    meta = run_concat_features_from_dirs(\n",
    "        prompt_out_dir=prompt_out_dir,\n",
    "        map_out_dir=map_out_dir,\n",
    "        out_dir=train_out_dir,\n",
    "        exp_name=exp_name,\n",
    "        feature_mode=mode_for_concat,\n",
    "        verbosity=1,\n",
    "    )\n",
    "\n",
    "    # ---- NEW: create X_map.npy for map_only (aligned to the produced pairs parquet) ----\n",
    "    if feature_mode == \"map_only\":\n",
    "        pairs_parquet = _find_pairs_parquet(train_out_dir)\n",
    "        maps_npz = Path(map_out_dir) / \"maps_embeddings.npz\"\n",
    "        out_x_map = train_out_dir / \"X_map.npy\"\n",
    "\n",
    "        print(\"   Using pairs parquet  :\", pairs_parquet.name)\n",
    "\n",
    "        build_meta = _build_x_map_from_pairs(\n",
    "            pairs_parquet=pairs_parquet,\n",
    "            maps_npz=maps_npz,\n",
    "            out_x_map=out_x_map,\n",
    "        )\n",
    "\n",
    "        extra = {\"map_only_X_map\": build_meta, \"pairs_parquet_used\": str(pairs_parquet)}\n",
    "        (train_out_dir / \"map_only_meta.json\").write_text(json.dumps(extra, indent=2), encoding=\"utf-8\")\n",
    "        print(\"   âœ… Built X_map.npy for map_only:\", out_x_map)\n",
    "\n",
    "    concat_meta_by_experiment[exp_name] = meta\n",
    "    print(\"   âœ… Feature construction completed.\")\n",
    "\n",
    "print(\"\\nâœ… All feature construction finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5142d4b68c273d37",
   "metadata": {},
   "source": [
    "## Step 6 â€” Load Training Data and Build Normalized Regression Target (Experiment-Aware)\n",
    "\n",
    "In this step, we load the **experiment-specific training data** produced by the feature\n",
    "construction stage and build the final learning targets for both classification and regression.\n",
    "\n",
    "Because this notebook runs **multiple experiments**, the same procedure is applied\n",
    "**independently for each experiment**, using its own `train_out` directory.\n",
    "\n",
    "### What happens here (per experiment)\n",
    "\n",
    "1. **Load the feature matrix**\n",
    "   - `prompt_only` experiments load prompt-only features (e.g., `X_prompt.npy`)\n",
    "   - fused experiments load concatenated features (e.g., `X_concat.npy`)\n",
    "\n",
    "2. **Load the paired metadata table**\n",
    "   - `train_pairs.parquet` containing aligned `(map_id, prompt_id)` rows and dynamic extent references\n",
    "\n",
    "3. **Attach labels and apply consistent filtering**\n",
    "   - Ensures only valid user study rows are included (e.g., `complete == True`, `remove == False`)\n",
    "   - Ensures `operator` and `param_value` are present (and prompt text if required)\n",
    "\n",
    "4. **Validate dynamic extent references**\n",
    "   - Confirms the presence of per-map reference scales required for normalization:\n",
    "     - `extent_diag_m`\n",
    "     - `extent_area_m2`\n",
    "\n",
    "5. **Compute the normalized regression target `param_norm`**\n",
    "   Normalization depends on the operator group:\n",
    "\n",
    "   - **Distance-based operators** (`aggregate`, `displace`, `simplify`):  \n",
    "     `param_norm = param_value / extent_diag_m`\n",
    "\n",
    "   - **Area-based operators** (`select`):  \n",
    "     `param_norm = param_value / extent_area_m2`\n",
    "\n",
    "### Why this normalization is used\n",
    "\n",
    "This step converts parameters from heterogeneous, map-scale-dependent units into a\n",
    "scale-aware normalized target. It allows per-operator regressors to generalize across\n",
    "maps of different extents while preserving physical meaning during inference\n",
    "(when `param_norm` is converted back to meters or mÂ² using the same extent references).\n",
    "\n",
    "### Outputs\n",
    "\n",
    "For each experiment, we obtain:\n",
    "\n",
    "- `X` â€” feature matrix aligned with labels  \n",
    "- `df` â€” cleaned metadata table including `operator`, `param_value`, `param_norm`, and extent references  \n",
    "\n",
    "These outputs feed directly into the subsequent training stages:\n",
    "1. Operator classification  \n",
    "2. Per-operator parameter regression  \n",
    "3. End-to-end evaluation across experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a494fd27dfe7681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading training data for all experiments ===\n",
      "\n",
      "ðŸ§ª Experiment: prompt_only\n",
      "   train_out : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out_prompt_only\n",
      "   mode      : prompt_only\n",
      "   âœ… Loaded: X=(562, 512) | df=(562, 15)\n",
      "   Operators: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "ðŸ§ª Experiment: map_only\n",
      "   train_out : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out_map_only\n",
      "   mode      : map_only\n",
      "   âœ… Loaded: X=(562, 165) | df=(562, 15)\n",
      "   Operators: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "ðŸ§ª Experiment: use_map\n",
      "   train_out : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out_use\n",
      "   mode      : prompt_plus_map\n",
      "   âœ… Loaded: X=(562, 677) | df=(562, 15)\n",
      "   Operators: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "ðŸ§ª Experiment: openai_map\n",
      "   train_out : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out_openai\n",
      "   mode      : prompt_plus_map\n",
      "   âœ… Loaded: X=(562, 1701) | df=(562, 15)\n",
      "   Operators: ['aggregate', 'displace', 'select', 'simplify']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>map_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>extent_diag_m</th>\n",
       "      <th>extent_area_m2</th>\n",
       "      <th>extent_width_m</th>\n",
       "      <th>extent_height_m</th>\n",
       "      <th>extent_minx</th>\n",
       "      <th>extent_miny</th>\n",
       "      <th>extent_maxx</th>\n",
       "      <th>extent_maxy</th>\n",
       "      <th>operator</th>\n",
       "      <th>param_value</th>\n",
       "      <th>intensity</th>\n",
       "      <th>param_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1304</td>\n",
       "      <td>r00000000</td>\n",
       "      <td>Union few of the buildings.</td>\n",
       "      <td>582.418016</td>\n",
       "      <td>169604.830164</td>\n",
       "      <td>412.352091</td>\n",
       "      <td>411.310707</td>\n",
       "      <td>369009.126498</td>\n",
       "      <td>5.624443e+06</td>\n",
       "      <td>369421.478589</td>\n",
       "      <td>5.624855e+06</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1457</td>\n",
       "      <td>r00000002</td>\n",
       "      <td>Remove small buildings and eliminate narrow an...</td>\n",
       "      <td>496.278103</td>\n",
       "      <td>116190.319796</td>\n",
       "      <td>404.903965</td>\n",
       "      <td>286.957723</td>\n",
       "      <td>370209.074685</td>\n",
       "      <td>5.626969e+06</td>\n",
       "      <td>370613.978650</td>\n",
       "      <td>5.627256e+06</td>\n",
       "      <td>select</td>\n",
       "      <td>17.805</td>\n",
       "      <td>low</td>\n",
       "      <td>0.000153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1663</td>\n",
       "      <td>r00000005</td>\n",
       "      <td>Bundle nearby buildings into larger blocks.</td>\n",
       "      <td>533.109561</td>\n",
       "      <td>138157.356917</td>\n",
       "      <td>329.923688</td>\n",
       "      <td>418.755494</td>\n",
       "      <td>371811.357509</td>\n",
       "      <td>5.630840e+06</td>\n",
       "      <td>372141.281197</td>\n",
       "      <td>5.631259e+06</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1122</td>\n",
       "      <td>r00000006</td>\n",
       "      <td>Simplify small geometric details below a speci...</td>\n",
       "      <td>597.176996</td>\n",
       "      <td>178021.909966</td>\n",
       "      <td>434.102877</td>\n",
       "      <td>410.091523</td>\n",
       "      <td>367409.757832</td>\n",
       "      <td>5.630048e+06</td>\n",
       "      <td>367843.860710</td>\n",
       "      <td>5.630458e+06</td>\n",
       "      <td>simplify</td>\n",
       "      <td>1.000</td>\n",
       "      <td>low</td>\n",
       "      <td>0.001675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1706</td>\n",
       "      <td>r00000009</td>\n",
       "      <td>Eliminate repeated blocks.</td>\n",
       "      <td>465.952069</td>\n",
       "      <td>108172.503069</td>\n",
       "      <td>315.345720</td>\n",
       "      <td>343.028290</td>\n",
       "      <td>372305.411445</td>\n",
       "      <td>5.628541e+06</td>\n",
       "      <td>372620.757165</td>\n",
       "      <td>5.628884e+06</td>\n",
       "      <td>select</td>\n",
       "      <td>18.226</td>\n",
       "      <td>low</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1174</td>\n",
       "      <td>r00000010</td>\n",
       "      <td>Make a space between the polygons.</td>\n",
       "      <td>485.597704</td>\n",
       "      <td>105023.121090</td>\n",
       "      <td>414.108392</td>\n",
       "      <td>253.612636</td>\n",
       "      <td>367806.423992</td>\n",
       "      <td>5.631230e+06</td>\n",
       "      <td>368220.532385</td>\n",
       "      <td>5.631484e+06</td>\n",
       "      <td>displace</td>\n",
       "      <td>1.975</td>\n",
       "      <td>low</td>\n",
       "      <td>0.004067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0116</td>\n",
       "      <td>r00000011</td>\n",
       "      <td>Aggregate some of the buildings.</td>\n",
       "      <td>577.475387</td>\n",
       "      <td>166688.512862</td>\n",
       "      <td>403.286009</td>\n",
       "      <td>413.325801</td>\n",
       "      <td>359417.860505</td>\n",
       "      <td>5.619642e+06</td>\n",
       "      <td>359821.146514</td>\n",
       "      <td>5.620056e+06</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0417</td>\n",
       "      <td>r00000013</td>\n",
       "      <td>Exclude shapes with an area less than 20 squar...</td>\n",
       "      <td>571.706715</td>\n",
       "      <td>163423.957488</td>\n",
       "      <td>404.661369</td>\n",
       "      <td>403.853617</td>\n",
       "      <td>361813.128457</td>\n",
       "      <td>5.622451e+06</td>\n",
       "      <td>362217.789827</td>\n",
       "      <td>5.622855e+06</td>\n",
       "      <td>select</td>\n",
       "      <td>20.000</td>\n",
       "      <td>low</td>\n",
       "      <td>0.000122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1488</td>\n",
       "      <td>r00000014</td>\n",
       "      <td>Aggregate the blocks.</td>\n",
       "      <td>500.054694</td>\n",
       "      <td>115279.716486</td>\n",
       "      <td>276.818974</td>\n",
       "      <td>416.444418</td>\n",
       "      <td>370607.033823</td>\n",
       "      <td>5.619641e+06</td>\n",
       "      <td>370883.852797</td>\n",
       "      <td>5.620058e+06</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0175</td>\n",
       "      <td>r00000015</td>\n",
       "      <td>Simplify the shapes of the small buildings by ...</td>\n",
       "      <td>503.061104</td>\n",
       "      <td>119618.905293</td>\n",
       "      <td>409.629591</td>\n",
       "      <td>292.017247</td>\n",
       "      <td>359816.877837</td>\n",
       "      <td>5.623767e+06</td>\n",
       "      <td>360226.507427</td>\n",
       "      <td>5.624059e+06</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  map_id  prompt_id                                               text  \\\n",
       "0   1304  r00000000                        Union few of the buildings.   \n",
       "1   1457  r00000002  Remove small buildings and eliminate narrow an...   \n",
       "2   1663  r00000005        Bundle nearby buildings into larger blocks.   \n",
       "3   1122  r00000006  Simplify small geometric details below a speci...   \n",
       "4   1706  r00000009                         Eliminate repeated blocks.   \n",
       "5   1174  r00000010                 Make a space between the polygons.   \n",
       "6   0116  r00000011                   Aggregate some of the buildings.   \n",
       "7   0417  r00000013  Exclude shapes with an area less than 20 squar...   \n",
       "8   1488  r00000014                              Aggregate the blocks.   \n",
       "9   0175  r00000015  Simplify the shapes of the small buildings by ...   \n",
       "\n",
       "   extent_diag_m  extent_area_m2  extent_width_m  extent_height_m  \\\n",
       "0     582.418016   169604.830164      412.352091       411.310707   \n",
       "1     496.278103   116190.319796      404.903965       286.957723   \n",
       "2     533.109561   138157.356917      329.923688       418.755494   \n",
       "3     597.176996   178021.909966      434.102877       410.091523   \n",
       "4     465.952069   108172.503069      315.345720       343.028290   \n",
       "5     485.597704   105023.121090      414.108392       253.612636   \n",
       "6     577.475387   166688.512862      403.286009       413.325801   \n",
       "7     571.706715   163423.957488      404.661369       403.853617   \n",
       "8     500.054694   115279.716486      276.818974       416.444418   \n",
       "9     503.061104   119618.905293      409.629591       292.017247   \n",
       "\n",
       "     extent_minx   extent_miny    extent_maxx   extent_maxy   operator  \\\n",
       "0  369009.126498  5.624443e+06  369421.478589  5.624855e+06  aggregate   \n",
       "1  370209.074685  5.626969e+06  370613.978650  5.627256e+06     select   \n",
       "2  371811.357509  5.630840e+06  372141.281197  5.631259e+06  aggregate   \n",
       "3  367409.757832  5.630048e+06  367843.860710  5.630458e+06   simplify   \n",
       "4  372305.411445  5.628541e+06  372620.757165  5.628884e+06     select   \n",
       "5  367806.423992  5.631230e+06  368220.532385  5.631484e+06   displace   \n",
       "6  359417.860505  5.619642e+06  359821.146514  5.620056e+06  aggregate   \n",
       "7  361813.128457  5.622451e+06  362217.789827  5.622855e+06     select   \n",
       "8  370607.033823  5.619641e+06  370883.852797  5.620058e+06  aggregate   \n",
       "9  359816.877837  5.623767e+06  360226.507427  5.624059e+06  aggregate   \n",
       "\n",
       "   param_value intensity  param_norm  \n",
       "0        0.000    medium    0.000000  \n",
       "1       17.805       low    0.000153  \n",
       "2        0.000    medium    0.000000  \n",
       "3        1.000       low    0.001675  \n",
       "4       18.226       low    0.000168  \n",
       "5        1.975       low    0.004067  \n",
       "6        0.000    medium    0.000000  \n",
       "7       20.000       low    0.000122  \n",
       "8        0.000    medium    0.000000  \n",
       "9        0.000    medium    0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===================== CELL 6 â€” Load training data + compute param_norm (multi-experiment, incl. map-only) =====================\n",
    "\n",
    "from dataclasses import replace\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.train.load_training_data import load_training_data_with_dynamic_param_norm\n",
    "\n",
    "TRAIN_DATA = {}\n",
    "\n",
    "print(\"\\n=== Loading training data for all experiments ===\")\n",
    "\n",
    "def _find_pairs_parquet(train_out_dir: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Locate the aligned pairs parquet in a train_out directory (robust to naming variations).\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        train_out_dir / \"train_pairs.parquet\",\n",
    "        *sorted(train_out_dir.glob(\"*train_pairs*.parquet\")),\n",
    "        *sorted(train_out_dir.glob(\"*pairs*.parquet\")),\n",
    "        *sorted(train_out_dir.glob(\"*.parquet\")),\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            return c\n",
    "    raise FileNotFoundError(f\"No parquet found in {train_out_dir}. Expected train_pairs*.parquet\")\n",
    "\n",
    "def _compute_param_norm(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute param_norm using dynamic extent refs.\n",
    "    Assumes df contains: operator, param_value, extent_diag_m, extent_area_m2.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if PATHS.OPERATOR_COL not in df.columns:\n",
    "        raise ValueError(f\"df missing operator column: {PATHS.OPERATOR_COL}\")\n",
    "    if PATHS.PARAM_VALUE_COL not in df.columns:\n",
    "        raise ValueError(f\"df missing param_value column: {PATHS.PARAM_VALUE_COL}\")\n",
    "    if \"extent_diag_m\" not in df.columns or \"extent_area_m2\" not in df.columns:\n",
    "        raise ValueError(\"df missing extent_diag_m / extent_area_m2 (needed for normalization).\")\n",
    "\n",
    "    op = df[PATHS.OPERATOR_COL].astype(str).str.strip().str.lower()\n",
    "    pv = pd.to_numeric(df[PATHS.PARAM_VALUE_COL], errors=\"coerce\").astype(float)\n",
    "\n",
    "    diag = pd.to_numeric(df[\"extent_diag_m\"], errors=\"coerce\").astype(float)\n",
    "    area = pd.to_numeric(df[\"extent_area_m2\"], errors=\"coerce\").astype(float)\n",
    "\n",
    "    param_norm = np.full(len(df), np.nan, dtype=float)\n",
    "\n",
    "    # distance ops\n",
    "    mask_d = op.isin([str(x).lower() for x in DISTANCE_OPS])\n",
    "    denom_d = diag.to_numpy()\n",
    "    num_d = pv.to_numpy()\n",
    "    param_norm[mask_d.to_numpy()] = num_d[mask_d.to_numpy()] / denom_d[mask_d.to_numpy()]\n",
    "\n",
    "    # area ops\n",
    "    mask_a = op.isin([str(x).lower() for x in AREA_OPS])\n",
    "    denom_a = area.to_numpy()\n",
    "    num_a = pv.to_numpy()\n",
    "    param_norm[mask_a.to_numpy()] = num_a[mask_a.to_numpy()] / denom_a[mask_a.to_numpy()]\n",
    "\n",
    "    df[\"param_norm\"] = param_norm\n",
    "    return df\n",
    "\n",
    "def _load_map_only_dataset(train_out_dir: Path) -> tuple[np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    For map_only:\n",
    "      - load X_map.npy (aligned to pairs)\n",
    "      - load pairs parquet from train_out_dir\n",
    "      - attach labels (operator, param_value, intensity) by joining through prompts.parquet + Excel\n",
    "      - compute param_norm\n",
    "    \"\"\"\n",
    "    x_map_path = train_out_dir / \"X_map.npy\"\n",
    "    if not x_map_path.exists():\n",
    "        raise FileNotFoundError(f\"map_only requires {x_map_path} (built in Cell 5).\")\n",
    "\n",
    "    pairs_pq = _find_pairs_parquet(train_out_dir)\n",
    "    df_pairs = pd.read_parquet(pairs_pq)\n",
    "\n",
    "    # Must have at least map_id and prompt_id and extent refs\n",
    "    need_pairs = {\"map_id\", \"prompt_id\", \"extent_diag_m\", \"extent_area_m2\"}\n",
    "    miss_pairs = [c for c in need_pairs if c not in df_pairs.columns]\n",
    "    if miss_pairs:\n",
    "        raise ValueError(f\"{pairs_pq.name} missing required columns: {miss_pairs}\")\n",
    "\n",
    "    # --- Load prompts table (authoritative prompt_id<->tile_id mapping) ---\n",
    "    # We reuse prompt_only prompts.parquet because map_only doesn't have its own prompt embedding run.\n",
    "    prompts_pq = Path(PATHS.PROMPT_OUT) / \"prompt_only\" / \"prompts.parquet\"\n",
    "    if not prompts_pq.exists():\n",
    "        raise FileNotFoundError(f\"Missing prompts.parquet needed for label merge: {prompts_pq}\")\n",
    "    df_prompts = pd.read_parquet(prompts_pq)\n",
    "\n",
    "    if \"prompt_id\" not in df_prompts.columns:\n",
    "        raise ValueError(f\"{prompts_pq} missing 'prompt_id'\")\n",
    "    if \"tile_id\" not in df_prompts.columns:\n",
    "        raise ValueError(f\"{prompts_pq} missing 'tile_id' (needed to align to maps)\")\n",
    "\n",
    "    # --- Load labels from Excel ---\n",
    "    df_x = pd.read_excel(Path(PATHS.USER_STUDY_XLSX), sheet_name=PATHS.RESPONSES_SHEET)\n",
    "\n",
    "    # Apply same filters as embedding\n",
    "    df_x[PATHS.COMPLETE_COL] = df_x[PATHS.COMPLETE_COL].astype(bool)\n",
    "    df_x[PATHS.REMOVE_COL]   = df_x[PATHS.REMOVE_COL].astype(bool)\n",
    "\n",
    "    mask = pd.Series(True, index=df_x.index)\n",
    "    if bool(PATHS.ONLY_COMPLETE):\n",
    "        mask &= (df_x[PATHS.COMPLETE_COL] == True)\n",
    "    if bool(PATHS.EXCLUDE_REMOVED):\n",
    "        mask &= (df_x[PATHS.REMOVE_COL] == False)\n",
    "    df_x = df_x[mask].copy()\n",
    "\n",
    "    # Build prompt_id the same way prompt_embeddings.py does: based on original row index\n",
    "    # prompt_embeddings.py resets index(drop=False) then uses that as \"_row\".\n",
    "    # Here we reconstruct that by using the original Excel row positions.\n",
    "    df_x = df_x.reset_index(drop=False).rename(columns={\"index\": \"_row\"})\n",
    "    prefix = getattr(PATHS, \"PROMPT_ID_PREFIX\", \"r\")\n",
    "    width  = int(getattr(PATHS, \"PROMPT_ID_WIDTH\", 8))\n",
    "    df_x[\"prompt_id\"] = [f\"{prefix}{int(r):0{width}d}\" for r in df_x[\"_row\"].tolist()]\n",
    "\n",
    "    # Normalize tile_id to map_id style\n",
    "    tile_raw = df_x[PATHS.TILE_ID_COL]\n",
    "    tile_num = pd.to_numeric(tile_raw, errors=\"coerce\")\n",
    "    if tile_num.notna().all():\n",
    "        df_x[\"tile_id_norm\"] = tile_num.astype(int).astype(str).str.zfill(4)\n",
    "    else:\n",
    "        df_x[\"tile_id_norm\"] = tile_raw.astype(str).str.strip().str.zfill(4)\n",
    "\n",
    "    # Keep label columns if present\n",
    "    keep_label_cols = [\"prompt_id\", \"tile_id_norm\"]\n",
    "    for c in [PATHS.OPERATOR_COL, PATHS.PARAM_VALUE_COL, PATHS.INTENSITY_COL, PATHS.TEXT_COL]:\n",
    "        if c in df_x.columns:\n",
    "            keep_label_cols.append(c)\n",
    "\n",
    "    df_labels = df_x[keep_label_cols].copy()\n",
    "    df_labels = df_labels.rename(columns={\"tile_id_norm\": \"map_id\"})\n",
    "\n",
    "    # Join labels onto pairs by prompt_id (most stable key)\n",
    "    df = df_pairs.merge(\n",
    "        df_labels[[c for c in df_labels.columns if c in {\"prompt_id\", \"map_id\", PATHS.OPERATOR_COL, PATHS.PARAM_VALUE_COL, PATHS.INTENSITY_COL}]],\n",
    "        on=[\"prompt_id\", \"map_id\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Check labels exist now\n",
    "    miss_after = []\n",
    "    for c in [PATHS.OPERATOR_COL, PATHS.PARAM_VALUE_COL]:\n",
    "        if c not in df.columns:\n",
    "            miss_after.append(c)\n",
    "    if miss_after:\n",
    "        raise ValueError(f\"After label merge, df is missing columns: {miss_after}\")\n",
    "\n",
    "    n_missing_labels = df[PATHS.OPERATOR_COL].isna().sum() + df[PATHS.PARAM_VALUE_COL].isna().sum()\n",
    "    if n_missing_labels:\n",
    "        print(f\"âš ï¸ map_only: {n_missing_labels} missing label fields after merge (operator/param_value).\")\n",
    "\n",
    "    df = _compute_param_norm(df)\n",
    "\n",
    "    X = np.load(x_map_path)\n",
    "    if X.shape[0] != len(df):\n",
    "        raise ValueError(f\"Row mismatch: X_map rows={X.shape[0]} vs df rows={len(df)} in {train_out_dir}\")\n",
    "\n",
    "    return X, df\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    train_out_dir = Path(cfg[\"train_out\"])\n",
    "    if not train_out_dir.exists():\n",
    "        raise FileNotFoundError(f\"Missing train_out for {exp_name}: {train_out_dir}\")\n",
    "\n",
    "    feature_mode = str(cfg[\"feature_mode\"]).strip().lower()\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   train_out : {train_out_dir}\")\n",
    "    print(f\"   mode      : {feature_mode}\")\n",
    "\n",
    "    # Many helpers read from PATHS.TRAIN_OUT -> create exp-scoped PATHS\n",
    "    PATHS_EXP = replace(PATHS, TRAIN_OUT=train_out_dir)\n",
    "\n",
    "    if feature_mode == \"map_only\":\n",
    "        # Custom loader path (because the existing loader doesn't know about map_only)\n",
    "        X, df = _load_map_only_dataset(train_out_dir)\n",
    "\n",
    "    else:\n",
    "        # Use existing loader for prompt_only / prompt_plus_map\n",
    "        # It will read the right X and pairs from PATHS_EXP.TRAIN_OUT\n",
    "        data = load_training_data_with_dynamic_param_norm(\n",
    "            exp_name=exp_name,\n",
    "            feature_mode=feature_mode,   # expects \"prompt_only\" or \"prompt_plus_map\"\n",
    "            paths=PATHS_EXP,\n",
    "            cfg=CFG,\n",
    "            distance_ops=DISTANCE_OPS,\n",
    "            area_ops=AREA_OPS,\n",
    "            require_text=True,\n",
    "        )\n",
    "        X = data.X\n",
    "        df = data.df\n",
    "\n",
    "    print(f\"   âœ… Loaded: X={X.shape} | df={df.shape}\")\n",
    "    print(\"   Operators:\", sorted(df[PATHS.OPERATOR_COL].dropna().unique().tolist()))\n",
    "\n",
    "    TRAIN_DATA[exp_name] = {\"X\": X, \"df\": df, \"paths\": PATHS_EXP}\n",
    "\n",
    "# Quick preview (optional)\n",
    "first_key = next(iter(TRAIN_DATA.keys()))\n",
    "display(TRAIN_DATA[first_key][\"df\"].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997cab7",
   "metadata": {},
   "source": [
    "## Step 7 â€” Train/Validation/Test Split (Shared, Leakage-Free by Map)\n",
    "\n",
    "In this step, we construct a **reproducible and fair** train/validation/test split that is\n",
    "**shared across all experiments** (prompt-only, USE + map, OpenAI + map).  \n",
    "The split is computed **once** and then applied to every experiment to ensure that\n",
    "performance differences are attributable solely to the feature representation.\n",
    "\n",
    "---\n",
    "\n",
    "### Constraints enforced\n",
    "\n",
    "- **No leakage by map**  \n",
    "  The same `map_id` never appears in more than one split (train / validation / test).\n",
    "\n",
    "- **Multi-prompt maps are forced into TRAIN**  \n",
    "  If a map has multiple prompts, *all* corresponding samples are assigned to the\n",
    "  training set.  \n",
    "  As a result, validation and test sets contain **single-prompt maps only**.\n",
    "\n",
    "- **Consistency across experiments**  \n",
    "  The exact same samples (identified by `(map_id, prompt_id)`) are used for\n",
    "  train/validation/test in every experiment.\n",
    "\n",
    "---\n",
    "\n",
    "### Stratification strategy\n",
    "\n",
    "To obtain balanced splits while respecting the above constraints, stratification is applied as:\n",
    "\n",
    "- Primary: `operator Ã— intensity` (if sufficient samples exist), otherwise\n",
    "- Fallback: `operator` only (automatically selected if finer stratification is infeasible)\n",
    "\n",
    "---\n",
    "\n",
    "### Coverage requirement\n",
    "\n",
    "Each split is required to contain **all operators** in the fixed class set:\n",
    "\n",
    "- `simplify`\n",
    "- `select`\n",
    "- `aggregate`\n",
    "- `displace`\n",
    "\n",
    "This guarantees that classification and regression models can be trained and evaluated\n",
    "for every operator.\n",
    "\n",
    "---\n",
    "\n",
    "### Outputs\n",
    "\n",
    "- A single shared split definition is saved to disk as:  \n",
    "  `splits_shared.json`\n",
    "- For each experiment, the split is applied to slice:\n",
    "  - `X` â€” the feature matrix\n",
    "  - `df` â€” the aligned metadata table\n",
    "\n",
    "The resulting subsets (`train`, `val`, `test`) are then used in all downstream\n",
    "training and evaluation steps.\n",
    "\n",
    "---\n",
    "\n",
    "This design ensures:\n",
    "- **Leakage-free evaluation**\n",
    "- **Fair, apples-to-apples comparison** between experiments\n",
    "- **Reproducibility**, since the split is deterministic and persisted to disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bc0897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Computing shared split using reference experiment: map_only ===\n",
      "ref_df: (562, 16) | ref_X: (562, 165)\n",
      "=== DATASET SUMMARY ===\n",
      "Total rows (prompts): 562\n",
      "Unique maps: 399\n",
      "Multi-prompt maps (>1 prompt): 22\n",
      "Single-prompt maps (=1 prompt): 377\n",
      "\n",
      "Top 10 maps by prompt count:\n",
      "map_id\n",
      "1646    30\n",
      "1304    29\n",
      "1755    26\n",
      "1532    13\n",
      "0127    10\n",
      "0168     8\n",
      "0142     7\n",
      "0078     6\n",
      "0080     6\n",
      "0001     6\n",
      "dtype: int64\n",
      "\n",
      "=== SPLIT SUMMARY ===\n",
      "âœ… Split found (seed=42)\n",
      "Train maps: 285  (includes multi-prompt maps: 22)\n",
      "Val maps:   57\n",
      "Test maps:  57\n",
      "âœ… Verified: no map_id leakage across splits.\n",
      "âœ… Verified: all multi-prompt maps are in TRAIN.\n",
      "\n",
      "TRAIN â€” Operator counts\n",
      "operator\n",
      "select       144\n",
      "aggregate    134\n",
      "simplify     109\n",
      "displace      61\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "VAL â€” Operator counts\n",
      "operator\n",
      "select       19\n",
      "aggregate    16\n",
      "simplify     13\n",
      "displace      9\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "TEST â€” Operator counts\n",
      "operator\n",
      "select       19\n",
      "aggregate    16\n",
      "simplify     13\n",
      "displace      9\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "TRAIN â€” Operator Ã— Intensity table (counts)\n",
      "intensity  high  low  medium\n",
      "operator                    \n",
      "aggregate    37   38      59\n",
      "displace     13   20      28\n",
      "select       34   34      76\n",
      "simplify     25   26      58\n",
      "\n",
      "VAL â€” Operator Ã— Intensity table (counts)\n",
      "intensity  high  low  medium\n",
      "operator                    \n",
      "aggregate     5    6       5\n",
      "displace      3    3       3\n",
      "select        5    7       7\n",
      "simplify      4    4       5\n",
      "\n",
      "TEST â€” Operator Ã— Intensity table (counts)\n",
      "intensity  high  low  medium\n",
      "operator                    \n",
      "aggregate     5    6       5\n",
      "displace      3    3       3\n",
      "select        6    6       7\n",
      "simplify      4    4       5\n",
      "\n",
      "âœ… Saved splits to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/splits/splits_shared.json\n",
      "\n",
      "âœ… Shared split created:\n",
      "   Train keys: 448 | Val keys: 57 | Test keys: 57\n",
      "   Saved to  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/splits/splits_shared.json\n",
      "\n",
      "ðŸ§ª prompt_only\n",
      "Rows -> Train: (448, 512) Val: (57, 512) Test: (57, 512)\n",
      "\n",
      "ðŸ§ª map_only\n",
      "Rows -> Train: (448, 165) Val: (57, 165) Test: (57, 165)\n",
      "\n",
      "ðŸ§ª use_map\n",
      "Rows -> Train: (448, 677) Val: (57, 677) Test: (57, 677)\n",
      "\n",
      "ðŸ§ª openai_map\n",
      "Rows -> Train: (448, 1701) Val: (57, 1701) Test: (57, 1701)\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 7 â€” Shared Train/Val/Test Split (fair across experiments, incl. map-only) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from src.train.splitting import make_splits_multi_prompt_to_train\n",
    "\n",
    "FIXED_CLASSES = [\"simplify\", \"select\", \"aggregate\", \"displace\"]\n",
    "USE_INTENSITY_FOR_STRAT = True\n",
    "\n",
    "OP_COL  = PATHS.OPERATOR_COL\n",
    "INT_COL = PATHS.INTENSITY_COL\n",
    "\n",
    "# Where to save ONE shared split (used by all experiments)\n",
    "SPLITS_DIR = Path(PATHS.SPLIT_OUT)\n",
    "SPLITS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "split_path = SPLITS_DIR / \"splits_shared.json\"\n",
    "\n",
    "# -------------------------------\n",
    "# Choose a *stable* reference experiment to compute the split once\n",
    "# Prefer a prompt-based dataset; avoid map_only as a ref if possible.\n",
    "# -------------------------------\n",
    "preferred_order = [\"map_only\", \"use_map\", \"openai_map\", \"prompt_only\"]\n",
    "ref_exp = None\n",
    "for name in preferred_order:\n",
    "    if name in TRAIN_DATA:\n",
    "        ref_exp = name\n",
    "        break\n",
    "if ref_exp is None:\n",
    "    ref_exp = next(iter(TRAIN_DATA.keys()))\n",
    "\n",
    "ref_df = TRAIN_DATA[ref_exp][\"df\"].copy()\n",
    "ref_X  = TRAIN_DATA[ref_exp][\"X\"]\n",
    "\n",
    "# Must have keys for stable matching across experiments\n",
    "if not {\"map_id\", \"prompt_id\"}.issubset(ref_df.columns):\n",
    "    raise ValueError(\"Expected columns {'map_id','prompt_id'} in df for split mapping.\")\n",
    "\n",
    "# Must have operator for stratification constraints\n",
    "if OP_COL not in ref_df.columns:\n",
    "    raise ValueError(f\"Reference df missing operator column '{OP_COL}'. Cannot build stratified split.\")\n",
    "\n",
    "# Build a stable key per row for mapping splits across experiments\n",
    "ref_df[\"row_key\"] = (\n",
    "    ref_df[\"map_id\"].astype(str).str.zfill(4) + \"::\" + ref_df[\"prompt_id\"].astype(str)\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Computing shared split using reference experiment: {ref_exp} ===\")\n",
    "print(\"ref_df:\", ref_df.shape, \"| ref_X:\", ref_X.shape)\n",
    "\n",
    "# Compute split ONCE\n",
    "split = make_splits_multi_prompt_to_train(\n",
    "    df=ref_df,\n",
    "    X=ref_X,\n",
    "    op_col=OP_COL,\n",
    "    intensity_col=INT_COL if (USE_INTENSITY_FOR_STRAT and INT_COL in ref_df.columns) else None,\n",
    "    map_id_col=\"map_id\",\n",
    "    fixed_classes=FIXED_CLASSES,\n",
    "    use_intensity_for_strat=USE_INTENSITY_FOR_STRAT,\n",
    "    seed=int(CFG.SEED),\n",
    "    val_ratio=float(CFG.VAL_RATIO),\n",
    "    test_ratio=float(CFG.TEST_RATIO),\n",
    "    max_attempts=500,\n",
    "    save_splits_json=split_path,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "train_idx_ref, val_idx_ref, test_idx_ref = split.train_idx, split.val_idx, split.test_idx\n",
    "\n",
    "# Convert indices -> row_key sets (transfer across experiments)\n",
    "train_keys = set(ref_df.loc[train_idx_ref, \"row_key\"].tolist())\n",
    "val_keys   = set(ref_df.loc[val_idx_ref,   \"row_key\"].tolist())\n",
    "test_keys  = set(ref_df.loc[test_idx_ref,  \"row_key\"].tolist())\n",
    "\n",
    "# Sanity: no overlap\n",
    "assert train_keys.isdisjoint(val_keys)\n",
    "assert train_keys.isdisjoint(test_keys)\n",
    "assert val_keys.isdisjoint(test_keys)\n",
    "\n",
    "print(\"\\nâœ… Shared split created:\")\n",
    "print(f\"   Train keys: {len(train_keys)} | Val keys: {len(val_keys)} | Test keys: {len(test_keys)}\")\n",
    "print(f\"   Saved to  : {split_path}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Apply the SAME split to every experiment by mapping row_key -> indices\n",
    "# -------------------------------\n",
    "SPLITS = {}  # exp_name -> dict with X_train/X_val/X_test and df_train/df_val/df_test\n",
    "\n",
    "for exp_name, pack in TRAIN_DATA.items():\n",
    "    df = pack[\"df\"].copy()\n",
    "    X  = pack[\"X\"]\n",
    "\n",
    "    if not {\"map_id\", \"prompt_id\"}.issubset(df.columns):\n",
    "        raise ValueError(f\"Experiment '{exp_name}' df missing map_id/prompt_id needed for split mapping.\")\n",
    "\n",
    "    df[\"row_key\"] = df[\"map_id\"].astype(str).str.zfill(4) + \"::\" + df[\"prompt_id\"].astype(str)\n",
    "\n",
    "    # Build index arrays in the current df order\n",
    "    train_idx = df.index[df[\"row_key\"].isin(train_keys)].to_numpy()\n",
    "    val_idx   = df.index[df[\"row_key\"].isin(val_keys)].to_numpy()\n",
    "    test_idx  = df.index[df[\"row_key\"].isin(test_keys)].to_numpy()\n",
    "\n",
    "    # Check full coverage: each experiment must contain all row_keys\n",
    "    missing = (train_keys | val_keys | test_keys) - set(df[\"row_key\"].tolist())\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"Experiment '{exp_name}' is missing {len(missing)} rows from the shared split \"\n",
    "            f\"(first few: {list(sorted(missing))[:5]}). \"\n",
    "            \"This usually means the pairs table differs between experiments.\"\n",
    "        )\n",
    "\n",
    "    X_train, X_val, X_test = X[train_idx], X[val_idx], X[test_idx]\n",
    "    df_train = df.loc[train_idx].reset_index(drop=True)\n",
    "    df_val   = df.loc[val_idx].reset_index(drop=True)\n",
    "    df_test  = df.loc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    SPLITS[exp_name] = {\n",
    "        \"train_idx\": train_idx,\n",
    "        \"val_idx\": val_idx,\n",
    "        \"test_idx\": test_idx,\n",
    "        \"X_train\": X_train, \"X_val\": X_val, \"X_test\": X_test,\n",
    "        \"df_train\": df_train, \"df_val\": df_val, \"df_test\": df_test,\n",
    "    }\n",
    "\n",
    "    print(f\"\\nðŸ§ª {exp_name}\")\n",
    "    print(\"Rows -> Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187473d",
   "metadata": {},
   "source": [
    "## Step 8 â€” Modality-Aware Preprocessing (Experiment-Aware)\n",
    "\n",
    "This step applies preprocessing tailored to the input modalities **separately for each\n",
    "experiment**, using **training data only** to fit preprocessing parameters.  \n",
    "A preprocessing bundle is saved per experiment so the exact same transformations can be\n",
    "reused during evaluation and inference.\n",
    "\n",
    "---\n",
    "\n",
    "### Prompt embeddings (all feature modes)\n",
    "\n",
    "Prompt vectors are normalized using **row-wise L2 normalization** to ensure consistent scale\n",
    "across samples and embedding backends (e.g., USE vs OpenAI).\n",
    "\n",
    "---\n",
    "\n",
    "### Map embeddings (only for fused prompt + map experiments)\n",
    "\n",
    "When the feature mode includes map vectors (`prompt_plus_map` / fused), the map block is\n",
    "processed using a robust pipeline:\n",
    "\n",
    "1. Replace non-finite values (`Â±inf`) with `NaN`\n",
    "2. Impute missing values using the **median** (fit on training data only)\n",
    "3. Clip each feature to training-set **5thâ€“95th percentiles** to reduce outlier impact\n",
    "4. Drop zero-variance (or near-constant) features based on training data\n",
    "5. Apply **RobustScaler** using quantile range **(5, 95)**\n",
    "\n",
    "The prompt block remains L2-normalized and is concatenated with the processed map block.\n",
    "\n",
    "---\n",
    "\n",
    "### Outputs (per experiment)\n",
    "\n",
    "For each experiment we obtain:\n",
    "\n",
    "- `X_train_s`, `X_val_s`, `X_test_s` â€” preprocessed matrices ready for training\n",
    "\n",
    "A preprocessing bundle is saved into the experimentâ€™s model output folder as:\n",
    "\n",
    "- `preproc.joblib`\n",
    "\n",
    "This ensures the exact same preprocessing can be reused for:\n",
    "- reproducible training\n",
    "- consistent evaluation across experiments\n",
    "- deployment-time inference (operator + parameter prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41942b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fitting modality-aware preprocessing per experiment ===\n",
      "\n",
      "ðŸ§ª Experiment: prompt_only\n",
      "   Feature mode : prompt_only -> preproc_mode=prompt_only\n",
      "   map_dim      : 0\n",
      "   prompt_dim   : 512\n",
      "   Save preproc : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_prompt_only/preproc.joblib\n",
      "   âœ… Preprocessing complete.\n",
      "   Shapes: (448, 512) (57, 512) (57, 512)\n",
      "\n",
      "ðŸ§ª Experiment: map_only\n",
      "   Feature mode : map_only -> preproc_mode=prompt_plus_map\n",
      "   map_dim      : 165\n",
      "   prompt_dim   : 0\n",
      "   Save preproc : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_map_only/preproc.joblib\n",
      "   âœ… Preprocessing complete.\n",
      "   Shapes: (448, 165) (57, 165) (57, 165)\n",
      "\n",
      "ðŸ§ª Experiment: use_map\n",
      "   Feature mode : prompt_plus_map -> preproc_mode=prompt_plus_map\n",
      "   map_dim      : 165\n",
      "   prompt_dim   : 512\n",
      "   Save preproc : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_use_map/preproc.joblib\n",
      "   âœ… Preprocessing complete.\n",
      "   Shapes: (448, 677) (57, 677) (57, 677)\n",
      "\n",
      "ðŸ§ª Experiment: openai_map\n",
      "   Feature mode : prompt_plus_map -> preproc_mode=prompt_plus_map\n",
      "   map_dim      : 165\n",
      "   prompt_dim   : 1536\n",
      "   Save preproc : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_openai_map/preproc.joblib\n",
      "   âœ… Preprocessing complete.\n",
      "   Shapes: (448, 1701) (57, 1701) (57, 1701)\n",
      "\n",
      "âœ… All preprocessing finished.\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 8 â€” Modality-aware preprocessing (per experiment, incl. map-only) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "from src.train.preprocessing import fit_transform_modality_preproc\n",
    "\n",
    "PREPROC = {}  # exp_name -> dict with scaled arrays + bundle path\n",
    "\n",
    "print(\"\\n=== Fitting modality-aware preprocessing per experiment ===\")\n",
    "\n",
    "def _to_preproc_mode(feature_mode: str) -> str:\n",
    "    \"\"\"\n",
    "    fit_transform_modality_preproc expects:\n",
    "      - \"prompt_only\"\n",
    "      - \"prompt_plus_map\"\n",
    "    For map_only we use \"prompt_plus_map\" semantics but with prompt_dim=0.\n",
    "    \"\"\"\n",
    "    fm = str(feature_mode).strip().lower()\n",
    "    if fm == \"prompt_only\":\n",
    "        return \"prompt_only\"\n",
    "    if fm in {\"prompt_plus_map\", \"map_only\"}:\n",
    "        return \"prompt_plus_map\"\n",
    "    raise ValueError(f\"Unsupported feature_mode for preprocessing: {feature_mode}\")\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "\n",
    "    split = SPLITS[exp_name]\n",
    "    feature_mode = cfg[\"feature_mode\"]\n",
    "    preproc_mode = _to_preproc_mode(feature_mode)\n",
    "\n",
    "    # dims inferred in Cell 4 (map_only sets prompt_dim=0, prompt_only sets map_dim=0)\n",
    "    map_dim    = int(cfg[\"map_dim\"])\n",
    "    prompt_dim = int(cfg[\"prompt_dim\"])\n",
    "\n",
    "    model_out_dir = Path(cfg[\"model_out\"])\n",
    "    model_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    preproc_path = model_out_dir / \"preproc.joblib\"\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   Feature mode : {feature_mode} -> preproc_mode={preproc_mode}\")\n",
    "    print(f\"   map_dim      : {map_dim}\")\n",
    "    print(f\"   prompt_dim   : {prompt_dim}\")\n",
    "    print(f\"   Save preproc : {preproc_path}\")\n",
    "\n",
    "    # Safety checks: X dims must match the experiment dims\n",
    "    Xtr = split[\"X_train\"]\n",
    "    if Xtr.shape[1] != (map_dim + prompt_dim):\n",
    "        raise ValueError(\n",
    "            f\"Dim mismatch in {exp_name}: X_train has {Xtr.shape[1]} cols, \"\n",
    "            f\"but map_dim+prompt_dim={map_dim + prompt_dim} (map_dim={map_dim}, prompt_dim={prompt_dim}).\"\n",
    "        )\n",
    "\n",
    "    res = fit_transform_modality_preproc(\n",
    "        X_train=split[\"X_train\"],\n",
    "        X_val=split[\"X_val\"],\n",
    "        X_test=split[\"X_test\"],\n",
    "        feature_mode=preproc_mode,   # \"prompt_only\" or \"prompt_plus_map\"\n",
    "        map_dim=map_dim,\n",
    "        prompt_dim=prompt_dim,\n",
    "        eps=1e-12,\n",
    "        clip_q=(5, 95),\n",
    "        impute_strategy=\"median\",\n",
    "        robust_qrange=(5, 95),\n",
    "        save_path=preproc_path,\n",
    "    )\n",
    "\n",
    "    PREPROC[exp_name] = {\n",
    "        \"X_train_s\": res.X_train_s,\n",
    "        \"X_val_s\":   res.X_val_s,\n",
    "        \"X_test_s\":  res.X_test_s,\n",
    "        \"bundle_path\": res.bundle_path,\n",
    "    }\n",
    "\n",
    "    print(\"   âœ… Preprocessing complete.\")\n",
    "    print(\"   Shapes:\", res.X_train_s.shape, res.X_val_s.shape, res.X_test_s.shape)\n",
    "\n",
    "print(\"\\nâœ… All preprocessing finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c32c2",
   "metadata": {},
   "source": [
    "## Step 9 â€” Build Class Labels and Sample Weights (Experiment-Aware)\n",
    "\n",
    "In this step, we construct the classification labels and training sample weights **for each\n",
    "experiment**, using the same fixed class order and the same split definition. This guarantees\n",
    "that differences in performance across experiments are due to the feature representation,\n",
    "not label encoding or sampling artifacts.\n",
    "\n",
    "---\n",
    "\n",
    "### Fixed class encoding\n",
    "\n",
    "Operator labels are encoded using a fixed global class order:\n",
    "\n",
    "`[simplify, select, aggregate, displace]`\n",
    "\n",
    "This guarantees consistent label indices across:\n",
    "- training\n",
    "- saved model bundles\n",
    "- evaluation and inference code\n",
    "\n",
    "Because the split is shared across experiments, this encoding remains stable and comparable.\n",
    "\n",
    "---\n",
    "\n",
    "### Sample weighting\n",
    "\n",
    "Training samples are weighted to address two common sources of bias:\n",
    "\n",
    "1. **Class imbalance**\n",
    "   - Balanced class weights are computed from the training distribution to prevent majority\n",
    "     classes from dominating learning.\n",
    "\n",
    "2. **Map-level prompt multiplicity**\n",
    "   - Some `map_id`s have multiple prompts.\n",
    "   - To prevent such maps from contributing disproportionately, each map contributes\n",
    "     approximately equal total weight by assigning each prompt a map-weight of:\n",
    "\n",
    "   `map_weight = 1 / (#prompts for that map_id)`\n",
    "\n",
    "---\n",
    "\n",
    "### Final weight definition\n",
    "\n",
    "The final per-sample weight used during training is:\n",
    "\n",
    "`sample_w = class_weight(operator) Ã— map_weight(map_id)`\n",
    "\n",
    "These weights are used during classifier training (and optionally regression training)\n",
    "to improve robustness and ensure fair learning across operators and maps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0df2b2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Building labels and sample weights per experiment ===\n",
      "\n",
      "ðŸ§ª prompt_only\n",
      "Classes (fixed order): [np.str_('simplify'), np.str_('select'), np.str_('aggregate'), np.str_('displace')]\n",
      "Class weights: {'simplify': 1.0275229357798166, 'select': 0.7777777777777778, 'aggregate': 0.835820895522388, 'displace': 1.8360655737704918}\n",
      "y_train/y_val/y_test shapes: (448,) (57,) (57,)\n",
      "Sample weight summary: {'min': 0.025925925925925925, 'max': 1.8360655737704918, 'mean': 0.6487687942076353}\n",
      "\n",
      "ðŸ§ª map_only\n",
      "Classes (fixed order): [np.str_('simplify'), np.str_('select'), np.str_('aggregate'), np.str_('displace')]\n",
      "Class weights: {'simplify': 1.0275229357798166, 'select': 0.7777777777777778, 'aggregate': 0.835820895522388, 'displace': 1.8360655737704918}\n",
      "y_train/y_val/y_test shapes: (448,) (57,) (57,)\n",
      "Sample weight summary: {'min': 0.025925925925925925, 'max': 1.8360655737704918, 'mean': 0.6487687942076353}\n",
      "\n",
      "ðŸ§ª use_map\n",
      "Classes (fixed order): [np.str_('simplify'), np.str_('select'), np.str_('aggregate'), np.str_('displace')]\n",
      "Class weights: {'simplify': 1.0275229357798166, 'select': 0.7777777777777778, 'aggregate': 0.835820895522388, 'displace': 1.8360655737704918}\n",
      "y_train/y_val/y_test shapes: (448,) (57,) (57,)\n",
      "Sample weight summary: {'min': 0.025925925925925925, 'max': 1.8360655737704918, 'mean': 0.6487687942076353}\n",
      "\n",
      "ðŸ§ª openai_map\n",
      "Classes (fixed order): [np.str_('simplify'), np.str_('select'), np.str_('aggregate'), np.str_('displace')]\n",
      "Class weights: {'simplify': 1.0275229357798166, 'select': 0.7777777777777778, 'aggregate': 0.835820895522388, 'displace': 1.8360655737704918}\n",
      "y_train/y_val/y_test shapes: (448,) (57,) (57,)\n",
      "Sample weight summary: {'min': 0.025925925925925925, 'max': 1.8360655737704918, 'mean': 0.6487687942076353}\n",
      "\n",
      "âœ… Label build complete for all experiments (class order consistent).\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 9 â€” Build labels + sample weights (per experiment, incl. map-only) =====================\n",
    "\n",
    "import numpy as np\n",
    "from src.train.labels_and_weights import build_labels_and_sample_weights\n",
    "\n",
    "OP_COL = PATHS.OPERATOR_COL  # usually \"operator\"\n",
    "\n",
    "LABELS = {}  # exp_name -> labels, weights, class_names, etc.\n",
    "\n",
    "print(\"\\n=== Building labels and sample weights per experiment ===\")\n",
    "\n",
    "for exp_name, split in SPLITS.items():\n",
    "\n",
    "    df_train = split[\"df_train\"].copy()\n",
    "    df_val   = split[\"df_val\"].copy()\n",
    "    df_test  = split[\"df_test\"].copy()\n",
    "\n",
    "    # Fail early if operator missing (this should NOT happen if Cell 6 merge worked)\n",
    "    for part_name, dfi in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n",
    "        if OP_COL not in dfi.columns:\n",
    "            raise ValueError(f\"{exp_name}: df_{part_name} missing operator column '{OP_COL}'.\")\n",
    "        n_miss = int(dfi[OP_COL].isna().sum())\n",
    "        if n_miss:\n",
    "            raise ValueError(\n",
    "                f\"{exp_name}: df_{part_name} has {n_miss} missing operator labels. \"\n",
    "                \"Fix the label merge in the data-loading step before training.\"\n",
    "            )\n",
    "\n",
    "    lab = build_labels_and_sample_weights(\n",
    "        df_train=df_train,\n",
    "        df_val=df_val,\n",
    "        df_test=df_test,\n",
    "        op_col=OP_COL,\n",
    "        map_id_col=\"map_id\",\n",
    "        fixed_classes=FIXED_CLASSES,\n",
    "        use_map_weight=True,\n",
    "        class_weight_mode=\"balanced\",\n",
    "    )\n",
    "\n",
    "    class_names = np.array(lab.class_names)\n",
    "\n",
    "    LABELS[exp_name] = {\n",
    "        \"class_names\": class_names,\n",
    "        \"y_train_cls\": lab.y_train,\n",
    "        \"y_val_cls\":   lab.y_val,\n",
    "        \"y_test_cls\":  lab.y_test,\n",
    "        \"sample_w\":    lab.sample_w,\n",
    "        \"class_weight_map\": lab.class_weight_map,\n",
    "    }\n",
    "\n",
    "    print(f\"\\nðŸ§ª {exp_name}\")\n",
    "    print(\"Classes (fixed order):\", list(class_names))\n",
    "    print(\"Class weights:\", lab.class_weight_map)\n",
    "    print(\"y_train/y_val/y_test shapes:\", lab.y_train.shape, lab.y_val.shape, lab.y_test.shape)\n",
    "    sw = lab.sample_w\n",
    "    print(\"Sample weight summary:\", {\"min\": float(sw.min()), \"max\": float(sw.max()), \"mean\": float(sw.mean())})\n",
    "\n",
    "# Sanity: class order must match across experiments\n",
    "first = next(iter(LABELS.keys()))\n",
    "base_classes = LABELS[first][\"class_names\"].tolist()\n",
    "for exp_name in LABELS.keys():\n",
    "    if LABELS[exp_name][\"class_names\"].tolist() != base_classes:\n",
    "        raise ValueError(f\"Class order differs in experiment {exp_name}. This would break fair comparison.\")\n",
    "\n",
    "print(\"\\nâœ… Label build complete for all experiments (class order consistent).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988f274",
   "metadata": {},
   "source": [
    "## Step 10 â€” Operator Classification Model (MLP, Experiment-Aware)\n",
    "\n",
    "This step trains the operator classifier that predicts one of the four map generalization\n",
    "operators:\n",
    "\n",
    "`{simplify, select, aggregate, displace}`\n",
    "\n",
    "The same training protocol is applied **independently for each experiment** (prompt-only,\n",
    "USE + map, OpenAI + map) using the **shared split**. This ensures that differences in\n",
    "performance across experiments are attributable to the feature representation rather than\n",
    "changes in training procedure.\n",
    "\n",
    "---\n",
    "\n",
    "### Model and training strategy\n",
    "\n",
    "- We use an **MLPClassifier** (multi-layer perceptron).\n",
    "- Hyperparameters are explored via a lightweight random search over:\n",
    "  - hidden layer sizes\n",
    "  - weight decay (`alpha`)\n",
    "  - learning rate schedule\n",
    "  - batch size / optimization settings (as implemented in the helper)\n",
    "\n",
    "---\n",
    "\n",
    "### Validation protocol (leakage-free)\n",
    "\n",
    "To prevent leakage, we perform **grouped cross-validation** using `map_id`:\n",
    "\n",
    "- prompts from the same map are never split across folds\n",
    "\n",
    "This is critical because multiple prompts may refer to the same map and would otherwise\n",
    "inflate performance due to memorization.\n",
    "\n",
    "---\n",
    "\n",
    "### Model selection and evaluation\n",
    "\n",
    "The best configuration is selected using validation performance (with grouped CV used for\n",
    "reliable hyperparameter tuning). The selected model is then retrained on the full training\n",
    "split and evaluated on validation and test splits.\n",
    "\n",
    "---\n",
    "\n",
    "### Outputs (per experiment)\n",
    "\n",
    "For each experiment, the trained classifier is saved into the experimentâ€™s model folder as:\n",
    "\n",
    "- `classifier.joblib`\n",
    "\n",
    "This classifier is later used to:\n",
    "1. predict the operator class\n",
    "2. route each sample to the correct operator-specific parameter regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95133825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training operator classifiers for all experiments ===\n",
      "\n",
      "ðŸ§ª Experiment: prompt_only\n",
      "   Classes   : ['simplify', 'select', 'aggregate', 'displace']\n",
      "   Train X   : (448, 512)\n",
      "   Val X     : (57, 512)\n",
      "   Test X    : (57, 512)\n",
      "   Model out : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_prompt_only\n",
      "\n",
      "Searching 50 MLP configs...\n",
      "[01/50] cvF1=0.809Â±0.048 | VAL F1=0.944 acc=0.947 | (128, 64), Î±=2.02e-02, lr=1.2e-03, bs=16\n",
      "[02/50] cvF1=0.813Â±0.059 | VAL F1=0.944 acc=0.947 | (256, 128), Î±=3.49e-05, lr=1.7e-04, bs=64\n",
      "[03/50] cvF1=0.808Â±0.064 | VAL F1=0.944 acc=0.947 | (256,), Î±=1.03e-02, lr=7.7e-04, bs=128\n",
      "[04/50] cvF1=0.804Â±0.066 | VAL F1=0.944 acc=0.947 | (256,), Î±=1.18e-05, lr=2.7e-03, bs=128\n",
      "[05/50] cvF1=0.806Â±0.058 | VAL F1=0.944 acc=0.947 | (256, 128, 64), Î±=5.47e-05, lr=1.9e-04, bs=16\n",
      "[06/50] cvF1=0.804Â±0.066 | VAL F1=0.944 acc=0.947 | (64,), Î±=1.14e-04, lr=6.0e-04, bs=128\n",
      "[07/50] cvF1=0.800Â±0.062 | VAL F1=0.944 acc=0.947 | (64,), Î±=1.03e-04, lr=8.0e-04, bs=32\n",
      "[08/50] cvF1=0.803Â±0.062 | VAL F1=0.944 acc=0.947 | (128, 64), Î±=2.43e-02, lr=2.2e-04, bs=32\n",
      "[09/50] cvF1=0.805Â±0.060 | VAL F1=0.944 acc=0.947 | (256, 128, 64), Î±=4.95e-05, lr=5.7e-04, bs=128\n",
      "[10/50] cvF1=0.802Â±0.065 | VAL F1=0.947 acc=0.947 | (64,), Î±=1.45e-05, lr=7.9e-04, bs=16\n",
      "[11/50] cvF1=0.802Â±0.060 | VAL F1=0.944 acc=0.947 | (64,), Î±=1.68e-05, lr=2.5e-03, bs=128\n",
      "[12/50] cvF1=0.809Â±0.072 | VAL F1=0.944 acc=0.947 | (256, 128, 64), Î±=6.47e-03, lr=2.8e-04, bs=16\n",
      "[13/50] cvF1=0.806Â±0.056 | VAL F1=0.944 acc=0.947 | (128,), Î±=2.39e-03, lr=4.5e-04, bs=64\n",
      "[14/50] cvF1=0.812Â±0.052 | VAL F1=0.944 acc=0.947 | (128, 64), Î±=5.27e-04, lr=1.1e-04, bs=32\n",
      "[15/50] cvF1=0.799Â±0.062 | VAL F1=0.944 acc=0.947 | (64,), Î±=7.94e-05, lr=9.5e-04, bs=32\n",
      "[16/50] cvF1=0.811Â±0.052 | VAL F1=0.944 acc=0.947 | (256, 128, 64), Î±=6.43e-04, lr=6.4e-04, bs=32\n",
      "[17/50] cvF1=0.790Â±0.053 | VAL F1=0.944 acc=0.947 | (256, 128), Î±=2.35e-02, lr=1.4e-03, bs=32\n",
      "[18/50] cvF1=0.807Â±0.058 | VAL F1=0.944 acc=0.947 | (128,), Î±=1.29e-02, lr=7.6e-04, bs=128\n",
      "[19/50] cvF1=0.819Â±0.062 | VAL F1=0.944 acc=0.947 | (256, 128, 64), Î±=4.80e-05, lr=1.2e-04, bs=128\n",
      "[20/50] cvF1=0.814Â±0.064 | VAL F1=0.947 acc=0.947 | (256, 128), Î±=2.25e-04, lr=2.5e-04, bs=16\n",
      "[21/50] cvF1=0.814Â±0.051 | VAL F1=0.944 acc=0.947 | (128,), Î±=2.27e-02, lr=7.9e-04, bs=16\n",
      "[22/50] cvF1=0.801Â±0.060 | VAL F1=0.944 acc=0.947 | (64,), Î±=1.07e-04, lr=1.8e-04, bs=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/50] cvF1=0.817Â±0.054 | VAL F1=0.944 acc=0.947 | (64,), Î±=4.84e-03, lr=2.0e-04, bs=128\n",
      "[24/50] cvF1=0.804Â±0.066 | VAL F1=0.944 acc=0.947 | (256,), Î±=4.91e-05, lr=1.1e-03, bs=64\n",
      "[25/50] cvF1=0.806Â±0.061 | VAL F1=0.944 acc=0.947 | (64,), Î±=1.28e-03, lr=2.3e-03, bs=32\n",
      "[26/50] cvF1=0.808Â±0.061 | VAL F1=0.944 acc=0.947 | (64,), Î±=1.52e-02, lr=1.8e-03, bs=128\n",
      "[27/50] cvF1=0.801Â±0.060 | VAL F1=0.944 acc=0.947 | (128,), Î±=2.15e-05, lr=3.5e-04, bs=32\n",
      "[28/50] cvF1=0.799Â±0.066 | VAL F1=0.944 acc=0.947 | (256, 128), Î±=3.44e-03, lr=8.7e-04, bs=64\n",
      "[29/50] cvF1=0.807Â±0.065 | VAL F1=0.944 acc=0.947 | (256,), Î±=4.38e-04, lr=1.5e-04, bs=32\n",
      "[30/50] cvF1=0.808Â±0.064 | VAL F1=0.944 acc=0.947 | (256,), Î±=4.42e-03, lr=6.7e-04, bs=64\n",
      "[31/50] cvF1=0.806Â±0.061 | VAL F1=0.944 acc=0.947 | (256, 128, 64), Î±=5.21e-04, lr=5.9e-04, bs=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32/50] cvF1=0.804Â±0.055 | VAL F1=0.944 acc=0.947 | (128,), Î±=1.23e-05, lr=1.4e-04, bs=64\n",
      "[33/50] cvF1=0.806Â±0.063 | VAL F1=0.944 acc=0.947 | (64,), Î±=1.24e-04, lr=5.6e-04, bs=32\n",
      "[34/50] cvF1=0.810Â±0.060 | VAL F1=0.947 acc=0.947 | (256, 128), Î±=7.36e-05, lr=4.0e-04, bs=64\n",
      "[35/50] cvF1=0.807Â±0.056 | VAL F1=0.944 acc=0.947 | (256, 128), Î±=6.25e-05, lr=1.3e-04, bs=64\n",
      "[36/50] cvF1=0.807Â±0.059 | VAL F1=0.944 acc=0.947 | (256,), Î±=3.64e-05, lr=2.4e-03, bs=16\n",
      "[37/50] cvF1=0.814Â±0.051 | VAL F1=0.944 acc=0.947 | (256, 128, 64), Î±=1.59e-03, lr=1.9e-03, bs=128\n",
      "[38/50] cvF1=0.807Â±0.052 | VAL F1=0.947 acc=0.947 | (128, 64), Î±=4.45e-05, lr=2.1e-03, bs=64\n",
      "[39/50] cvF1=0.801Â±0.060 | VAL F1=0.944 acc=0.947 | (128,), Î±=2.66e-05, lr=3.4e-04, bs=32\n",
      "[40/50] cvF1=0.804Â±0.069 | VAL F1=0.947 acc=0.947 | (64,), Î±=8.84e-05, lr=9.1e-04, bs=16\n",
      "[41/50] cvF1=0.809Â±0.049 | VAL F1=0.947 acc=0.947 | (128, 64), Î±=1.68e-04, lr=2.8e-04, bs=32\n",
      "[42/50] cvF1=0.810Â±0.060 | VAL F1=0.944 acc=0.947 | (256,), Î±=2.83e-04, lr=2.1e-04, bs=64\n",
      "[43/50] cvF1=0.805Â±0.058 | VAL F1=0.944 acc=0.947 | (128,), Î±=1.49e-04, lr=2.5e-03, bs=32\n",
      "[44/50] cvF1=0.816Â±0.051 | VAL F1=0.944 acc=0.947 | (64,), Î±=2.54e-04, lr=1.2e-04, bs=32\n",
      "[45/50] cvF1=0.803Â±0.059 | VAL F1=0.947 acc=0.947 | (128, 64), Î±=7.22e-05, lr=1.1e-03, bs=64\n",
      "[46/50] cvF1=0.804Â±0.051 | VAL F1=0.944 acc=0.947 | (64,), Î±=3.27e-05, lr=3.0e-03, bs=64\n",
      "[47/50] cvF1=0.805Â±0.059 | VAL F1=0.944 acc=0.947 | (64,), Î±=2.49e-02, lr=4.0e-04, bs=64\n",
      "[48/50] cvF1=0.805Â±0.066 | VAL F1=0.947 acc=0.947 | (256, 128), Î±=1.58e-04, lr=8.6e-04, bs=32\n",
      "[49/50] cvF1=0.801Â±0.059 | VAL F1=0.944 acc=0.947 | (128,), Î±=7.02e-04, lr=4.6e-04, bs=32\n",
      "[50/50] cvF1=0.802Â±0.064 | VAL F1=0.947 acc=0.947 | (128, 64), Î±=1.91e-05, lr=3.5e-04, bs=16\n",
      "\n",
      "=== Top candidates (by VAL macro-F1) ===\n",
      "VAL F1=0.947 (acc=0.947) | cvF1=0.814Â±0.064 | params={'hidden_layer_sizes': (256, 128), 'alpha': 0.00022463533171675809, 'learning_rate_init': 0.0002516607127550297, 'batch_size': 16, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.947 (acc=0.947) | cvF1=0.810Â±0.060 | params={'hidden_layer_sizes': (256, 128), 'alpha': 7.35900856867979e-05, 'learning_rate_init': 0.0004038176882071837, 'batch_size': 64, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.947 (acc=0.947) | cvF1=0.809Â±0.049 | params={'hidden_layer_sizes': (128, 64), 'alpha': 0.00016823821974707638, 'learning_rate_init': 0.0002819673835884168, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.947 (acc=0.947) | cvF1=0.807Â±0.052 | params={'hidden_layer_sizes': (128, 64), 'alpha': 4.45375904389123e-05, 'learning_rate_init': 0.0020816986844858954, 'batch_size': 64, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.947 (acc=0.947) | cvF1=0.805Â±0.066 | params={'hidden_layer_sizes': (256, 128), 'alpha': 0.0001584362559438067, 'learning_rate_init': 0.0008649955121393716, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "\n",
      "ðŸ† Selected params:\n",
      "{'hidden_layer_sizes': (256, 128), 'alpha': 0.00022463533171675809, 'learning_rate_init': 0.0002516607127550297, 'batch_size': 16, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "\n",
      "===== VAL =====\n",
      "VAL: acc=0.9474  f1_macro=0.9472\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    simplify       1.00      0.92      0.96        13\n",
      "      select       0.90      1.00      0.95        19\n",
      "   aggregate       0.94      0.94      0.94        16\n",
      "    displace       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.95        57\n",
      "   macro avg       0.96      0.94      0.95        57\n",
      "weighted avg       0.95      0.95      0.95        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[12  0  1  0]\n",
      " [ 0 19  0  0]\n",
      " [ 0  1 15  0]\n",
      " [ 0  1  0  8]]\n",
      "\n",
      "===== TEST =====\n",
      "TEST: acc=0.8772  f1_macro=0.8762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    simplify       0.81      1.00      0.90        13\n",
      "      select       0.89      0.89      0.89        19\n",
      "   aggregate       0.87      0.81      0.84        16\n",
      "    displace       1.00      0.78      0.88         9\n",
      "\n",
      "    accuracy                           0.88        57\n",
      "   macro avg       0.89      0.87      0.88        57\n",
      "weighted avg       0.88      0.88      0.88        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[13  0  0  0]\n",
      " [ 0 17  2  0]\n",
      " [ 1  2 13  0]\n",
      " [ 2  0  0  7]]\n",
      "\n",
      "âœ… Saved classifier to: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_prompt_only/classifier.joblib\n",
      "   âœ… Classifier training done.\n",
      "   Saved to: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_prompt_only/classifier.joblib\n",
      "   Best VAL: {'macro_f1': None, 'acc': 0.9473684210526315}\n",
      "   TEST     : {'macro_f1': None, 'acc': 0.8771929824561403}\n",
      "   (debug) Result fields: ['model_path', 'best_params', 'class_names', 'val_acc', 'val_f1_macro', 'test_acc', 'test_f1_macro']\n",
      "\n",
      "ðŸ§ª Experiment: map_only\n",
      "   Classes   : ['simplify', 'select', 'aggregate', 'displace']\n",
      "   Train X   : (448, 165)\n",
      "   Val X     : (57, 165)\n",
      "   Test X    : (57, 165)\n",
      "   Model out : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_map_only\n",
      "\n",
      "Searching 50 MLP configs...\n",
      "[01/50] cvF1=0.253Â±0.071 | VAL F1=0.184 acc=0.193 | (128, 64), Î±=2.02e-02, lr=1.2e-03, bs=16\n",
      "[02/50] cvF1=0.172Â±0.069 | VAL F1=0.281 acc=0.281 | (256, 128), Î±=3.49e-05, lr=1.7e-04, bs=64\n",
      "[03/50] cvF1=0.272Â±0.099 | VAL F1=0.244 acc=0.246 | (256,), Î±=1.03e-02, lr=7.7e-04, bs=128\n",
      "[04/50] cvF1=0.268Â±0.101 | VAL F1=0.233 acc=0.246 | (256,), Î±=1.18e-05, lr=2.7e-03, bs=128\n",
      "[05/50] cvF1=0.306Â±0.108 | VAL F1=0.280 acc=0.281 | (256, 128, 64), Î±=5.47e-05, lr=1.9e-04, bs=16\n",
      "[06/50] cvF1=0.293Â±0.109 | VAL F1=0.333 acc=0.333 | (64,), Î±=1.14e-04, lr=6.0e-04, bs=128\n",
      "[07/50] cvF1=0.263Â±0.105 | VAL F1=0.342 acc=0.351 | (64,), Î±=1.03e-04, lr=8.0e-04, bs=32\n",
      "[08/50] cvF1=0.265Â±0.110 | VAL F1=0.296 acc=0.298 | (128, 64), Î±=2.43e-02, lr=2.2e-04, bs=32\n",
      "[09/50] cvF1=0.268Â±0.091 | VAL F1=0.280 acc=0.281 | (256, 128, 64), Î±=4.95e-05, lr=5.7e-04, bs=128\n",
      "[10/50] cvF1=0.267Â±0.114 | VAL F1=0.379 acc=0.386 | (64,), Î±=1.45e-05, lr=7.9e-04, bs=16\n",
      "[11/50] cvF1=0.296Â±0.112 | VAL F1=0.296 acc=0.316 | (64,), Î±=1.68e-05, lr=2.5e-03, bs=128\n",
      "[12/50] cvF1=0.306Â±0.102 | VAL F1=0.198 acc=0.193 | (256, 128, 64), Î±=6.47e-03, lr=2.8e-04, bs=16\n",
      "[13/50] cvF1=0.206Â±0.091 | VAL F1=0.209 acc=0.211 | (128,), Î±=2.39e-03, lr=4.5e-04, bs=64\n",
      "[14/50] cvF1=0.204Â±0.088 | VAL F1=0.254 acc=0.246 | (128, 64), Î±=5.27e-04, lr=1.1e-04, bs=32\n",
      "[15/50] cvF1=0.256Â±0.116 | VAL F1=0.328 acc=0.351 | (64,), Î±=7.94e-05, lr=9.5e-04, bs=32\n",
      "[16/50] cvF1=0.266Â±0.125 | VAL F1=0.267 acc=0.263 | (256, 128, 64), Î±=6.43e-04, lr=6.4e-04, bs=32\n",
      "[17/50] cvF1=0.254Â±0.108 | VAL F1=0.164 acc=0.158 | (256, 128), Î±=2.35e-02, lr=1.4e-03, bs=32\n",
      "[18/50] cvF1=0.206Â±0.090 | VAL F1=0.216 acc=0.211 | (128,), Î±=1.29e-02, lr=7.6e-04, bs=128\n",
      "[19/50] cvF1=0.278Â±0.068 | VAL F1=0.263 acc=0.263 | (256, 128, 64), Î±=4.80e-05, lr=1.2e-04, bs=128\n",
      "[20/50] cvF1=0.182Â±0.073 | VAL F1=0.254 acc=0.246 | (256, 128), Î±=2.25e-04, lr=2.5e-04, bs=16\n",
      "[21/50] cvF1=0.249Â±0.135 | VAL F1=0.180 acc=0.175 | (128,), Î±=2.27e-02, lr=7.9e-04, bs=16\n",
      "[22/50] cvF1=0.251Â±0.118 | VAL F1=0.326 acc=0.333 | (64,), Î±=1.07e-04, lr=1.8e-04, bs=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/50] cvF1=0.261Â±0.080 | VAL F1=0.323 acc=0.333 | (64,), Î±=4.84e-03, lr=2.0e-04, bs=128\n",
      "[24/50] cvF1=0.245Â±0.120 | VAL F1=0.258 acc=0.263 | (256,), Î±=4.91e-05, lr=1.1e-03, bs=64\n",
      "[25/50] cvF1=0.240Â±0.131 | VAL F1=0.297 acc=0.316 | (64,), Î±=1.28e-03, lr=2.3e-03, bs=32\n",
      "[26/50] cvF1=0.278Â±0.108 | VAL F1=0.377 acc=0.386 | (64,), Î±=1.52e-02, lr=1.8e-03, bs=128\n",
      "[27/50] cvF1=0.247Â±0.125 | VAL F1=0.223 acc=0.228 | (128,), Î±=2.15e-05, lr=3.5e-04, bs=32\n",
      "[28/50] cvF1=0.170Â±0.087 | VAL F1=0.265 acc=0.263 | (256, 128), Î±=3.44e-03, lr=8.7e-04, bs=64\n",
      "[29/50] cvF1=0.266Â±0.102 | VAL F1=0.258 acc=0.263 | (256,), Î±=4.38e-04, lr=1.5e-04, bs=32\n",
      "[30/50] cvF1=0.262Â±0.120 | VAL F1=0.256 acc=0.263 | (256,), Î±=4.42e-03, lr=6.7e-04, bs=64\n",
      "[31/50] cvF1=0.301Â±0.115 | VAL F1=0.279 acc=0.281 | (256, 128, 64), Î±=5.21e-04, lr=5.9e-04, bs=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32/50] cvF1=0.212Â±0.091 | VAL F1=0.247 acc=0.246 | (128,), Î±=1.23e-05, lr=1.4e-04, bs=64\n",
      "[33/50] cvF1=0.208Â±0.078 | VAL F1=0.340 acc=0.351 | (64,), Î±=1.24e-04, lr=5.6e-04, bs=32\n",
      "[34/50] cvF1=0.189Â±0.073 | VAL F1=0.255 acc=0.246 | (256, 128), Î±=7.36e-05, lr=4.0e-04, bs=64\n",
      "[35/50] cvF1=0.177Â±0.074 | VAL F1=0.273 acc=0.263 | (256, 128), Î±=6.25e-05, lr=1.3e-04, bs=64\n",
      "[36/50] cvF1=0.225Â±0.078 | VAL F1=0.196 acc=0.193 | (256,), Î±=3.64e-05, lr=2.4e-03, bs=16\n",
      "[37/50] cvF1=0.249Â±0.093 | VAL F1=0.261 acc=0.263 | (256, 128, 64), Î±=1.59e-03, lr=1.9e-03, bs=128\n",
      "[38/50] cvF1=0.213Â±0.105 | VAL F1=0.278 acc=0.298 | (128, 64), Î±=4.45e-05, lr=2.1e-03, bs=64\n",
      "[39/50] cvF1=0.249Â±0.129 | VAL F1=0.223 acc=0.228 | (128,), Î±=2.66e-05, lr=3.4e-04, bs=32\n",
      "[40/50] cvF1=0.277Â±0.118 | VAL F1=0.327 acc=0.351 | (64,), Î±=8.84e-05, lr=9.1e-04, bs=16\n",
      "[41/50] cvF1=0.199Â±0.097 | VAL F1=0.262 acc=0.263 | (128, 64), Î±=1.68e-04, lr=2.8e-04, bs=32\n",
      "[42/50] cvF1=0.280Â±0.111 | VAL F1=0.245 acc=0.246 | (256,), Î±=2.83e-04, lr=2.1e-04, bs=64\n",
      "[43/50] cvF1=0.274Â±0.146 | VAL F1=0.192 acc=0.193 | (128,), Î±=1.49e-04, lr=2.5e-03, bs=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44/50] cvF1=0.244Â±0.089 | VAL F1=0.345 acc=0.368 | (64,), Î±=2.54e-04, lr=1.2e-04, bs=32\n",
      "[45/50] cvF1=0.201Â±0.087 | VAL F1=0.248 acc=0.263 | (128, 64), Î±=7.22e-05, lr=1.1e-03, bs=64\n",
      "[46/50] cvF1=0.248Â±0.098 | VAL F1=0.319 acc=0.333 | (64,), Î±=3.27e-05, lr=3.0e-03, bs=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47/50] cvF1=0.276Â±0.123 | VAL F1=0.271 acc=0.281 | (64,), Î±=2.49e-02, lr=4.0e-04, bs=64\n",
      "[48/50] cvF1=0.186Â±0.074 | VAL F1=0.268 acc=0.263 | (256, 128), Î±=1.58e-04, lr=8.6e-04, bs=32\n",
      "[49/50] cvF1=0.254Â±0.137 | VAL F1=0.208 acc=0.211 | (128,), Î±=7.02e-04, lr=4.6e-04, bs=32\n",
      "[50/50] cvF1=0.217Â±0.096 | VAL F1=0.239 acc=0.246 | (128, 64), Î±=1.91e-05, lr=3.5e-04, bs=16\n",
      "\n",
      "=== Top candidates (by VAL macro-F1) ===\n",
      "VAL F1=0.379 (acc=0.386) | cvF1=0.267Â±0.114 | params={'hidden_layer_sizes': (64,), 'alpha': 1.4504865877614242e-05, 'learning_rate_init': 0.0007896186801026691, 'batch_size': 16, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.377 (acc=0.386) | cvF1=0.278Â±0.108 | params={'hidden_layer_sizes': (64,), 'alpha': 0.015185382476939244, 'learning_rate_init': 0.0018013995527645626, 'batch_size': 128, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.345 (acc=0.368) | cvF1=0.244Â±0.089 | params={'hidden_layer_sizes': (64,), 'alpha': 0.0002536222399285845, 'learning_rate_init': 0.00012469634313376035, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.342 (acc=0.351) | cvF1=0.263Â±0.105 | params={'hidden_layer_sizes': (64,), 'alpha': 0.00010295300642650056, 'learning_rate_init': 0.0008012737503998539, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.340 (acc=0.351) | cvF1=0.208Â±0.078 | params={'hidden_layer_sizes': (64,), 'alpha': 0.00012389502377355922, 'learning_rate_init': 0.0005639239991667559, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "\n",
      "ðŸ† Selected params:\n",
      "{'hidden_layer_sizes': (64,), 'alpha': 1.4504865877614242e-05, 'learning_rate_init': 0.0007896186801026691, 'batch_size': 16, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "\n",
      "===== VAL =====\n",
      "VAL: acc=0.3860  f1_macro=0.3785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    simplify       0.45      0.38      0.42        13\n",
      "      select       0.40      0.32      0.35        19\n",
      "   aggregate       0.40      0.50      0.44        16\n",
      "    displace       0.27      0.33      0.30         9\n",
      "\n",
      "    accuracy                           0.39        57\n",
      "   macro avg       0.38      0.38      0.38        57\n",
      "weighted avg       0.39      0.39      0.38        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[5 5 3 0]\n",
      " [2 6 6 5]\n",
      " [3 2 8 3]\n",
      " [1 2 3 3]]\n",
      "\n",
      "===== TEST =====\n",
      "TEST: acc=0.2632  f1_macro=0.2572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    simplify       0.21      0.23      0.22        13\n",
      "      select       0.38      0.32      0.34        19\n",
      "   aggregate       0.21      0.25      0.23        16\n",
      "    displace       0.25      0.22      0.24         9\n",
      "\n",
      "    accuracy                           0.26        57\n",
      "   macro avg       0.26      0.25      0.26        57\n",
      "weighted avg       0.27      0.26      0.27        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[3 2 5 3]\n",
      " [2 6 9 2]\n",
      " [6 5 4 1]\n",
      " [3 3 1 2]]\n",
      "\n",
      "âœ… Saved classifier to: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_map_only/classifier.joblib\n",
      "   âœ… Classifier training done.\n",
      "   Saved to: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_map_only/classifier.joblib\n",
      "   Best VAL: {'macro_f1': None, 'acc': 0.38596491228070173}\n",
      "   TEST     : {'macro_f1': None, 'acc': 0.2631578947368421}\n",
      "\n",
      "ðŸ§ª Experiment: use_map\n",
      "   Classes   : ['simplify', 'select', 'aggregate', 'displace']\n",
      "   Train X   : (448, 677)\n",
      "   Val X     : (57, 677)\n",
      "   Test X    : (57, 677)\n",
      "   Model out : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_use_map\n",
      "\n",
      "Searching 50 MLP configs...\n",
      "[01/50] cvF1=0.627Â±0.086 | VAL F1=0.814 acc=0.825 | (128, 64), Î±=2.02e-02, lr=1.2e-03, bs=16\n",
      "[02/50] cvF1=0.706Â±0.098 | VAL F1=0.873 acc=0.877 | (256, 128), Î±=3.49e-05, lr=1.7e-04, bs=64\n",
      "[03/50] cvF1=0.675Â±0.113 | VAL F1=0.788 acc=0.807 | (256,), Î±=1.03e-02, lr=7.7e-04, bs=128\n",
      "[04/50] cvF1=0.718Â±0.113 | VAL F1=0.814 acc=0.825 | (256,), Î±=1.18e-05, lr=2.7e-03, bs=128\n",
      "[05/50] cvF1=0.711Â±0.117 | VAL F1=0.884 acc=0.895 | (256, 128, 64), Î±=5.47e-05, lr=1.9e-04, bs=16\n",
      "[06/50] cvF1=0.722Â±0.103 | VAL F1=0.833 acc=0.842 | (64,), Î±=1.14e-04, lr=6.0e-04, bs=128\n",
      "[07/50] cvF1=0.727Â±0.093 | VAL F1=0.833 acc=0.842 | (64,), Î±=1.03e-04, lr=8.0e-04, bs=32\n",
      "[08/50] cvF1=0.644Â±0.122 | VAL F1=0.808 acc=0.825 | (128, 64), Î±=2.43e-02, lr=2.2e-04, bs=32\n",
      "[09/50] cvF1=0.709Â±0.126 | VAL F1=0.833 acc=0.842 | (256, 128, 64), Î±=4.95e-05, lr=5.7e-04, bs=128\n",
      "[10/50] cvF1=0.731Â±0.088 | VAL F1=0.851 acc=0.860 | (64,), Î±=1.45e-05, lr=7.9e-04, bs=16\n",
      "[11/50] cvF1=0.725Â±0.098 | VAL F1=0.833 acc=0.842 | (64,), Î±=1.68e-05, lr=2.5e-03, bs=128\n",
      "[12/50] cvF1=0.651Â±0.106 | VAL F1=0.827 acc=0.842 | (256, 128, 64), Î±=6.47e-03, lr=2.8e-04, bs=16\n",
      "[13/50] cvF1=0.719Â±0.107 | VAL F1=0.853 acc=0.860 | (128,), Î±=2.39e-03, lr=4.5e-04, bs=64\n",
      "[14/50] cvF1=0.741Â±0.081 | VAL F1=0.871 acc=0.877 | (128, 64), Î±=5.27e-04, lr=1.1e-04, bs=32\n",
      "[15/50] cvF1=0.731Â±0.088 | VAL F1=0.851 acc=0.860 | (64,), Î±=7.94e-05, lr=9.5e-04, bs=32\n",
      "[16/50] cvF1=0.692Â±0.121 | VAL F1=0.870 acc=0.877 | (256, 128, 64), Î±=6.43e-04, lr=6.4e-04, bs=32\n",
      "[17/50] cvF1=0.696Â±0.079 | VAL F1=0.826 acc=0.842 | (256, 128), Î±=2.35e-02, lr=1.4e-03, bs=32\n",
      "[18/50] cvF1=0.698Â±0.117 | VAL F1=0.827 acc=0.842 | (128,), Î±=1.29e-02, lr=7.6e-04, bs=128\n",
      "[19/50] cvF1=0.695Â±0.111 | VAL F1=0.853 acc=0.860 | (256, 128, 64), Î±=4.80e-05, lr=1.2e-04, bs=128\n",
      "[20/50] cvF1=0.710Â±0.101 | VAL F1=0.866 acc=0.877 | (256, 128), Î±=2.25e-04, lr=2.5e-04, bs=16\n",
      "[21/50] cvF1=0.630Â±0.107 | VAL F1=0.774 acc=0.789 | (128,), Î±=2.27e-02, lr=7.9e-04, bs=16\n",
      "[22/50] cvF1=0.729Â±0.087 | VAL F1=0.833 acc=0.842 | (64,), Î±=1.07e-04, lr=1.8e-04, bs=16\n",
      "[23/50] cvF1=0.703Â±0.113 | VAL F1=0.851 acc=0.860 | (64,), Î±=4.84e-03, lr=2.0e-04, bs=128\n",
      "[24/50] cvF1=0.724Â±0.118 | VAL F1=0.834 acc=0.842 | (256,), Î±=4.91e-05, lr=1.1e-03, bs=64\n",
      "[25/50] cvF1=0.672Â±0.107 | VAL F1=0.814 acc=0.825 | (64,), Î±=1.28e-03, lr=2.3e-03, bs=32\n",
      "[26/50] cvF1=0.660Â±0.108 | VAL F1=0.827 acc=0.842 | (64,), Î±=1.52e-02, lr=1.8e-03, bs=128\n",
      "[27/50] cvF1=0.744Â±0.087 | VAL F1=0.871 acc=0.877 | (128,), Î±=2.15e-05, lr=3.5e-04, bs=32\n",
      "[28/50] cvF1=0.642Â±0.104 | VAL F1=0.827 acc=0.842 | (256, 128), Î±=3.44e-03, lr=8.7e-04, bs=64\n",
      "[29/50] cvF1=0.720Â±0.113 | VAL F1=0.834 acc=0.842 | (256,), Î±=4.38e-04, lr=1.5e-04, bs=32\n",
      "[30/50] cvF1=0.676Â±0.119 | VAL F1=0.788 acc=0.807 | (256,), Î±=4.42e-03, lr=6.7e-04, bs=64\n",
      "[31/50] cvF1=0.706Â±0.128 | VAL F1=0.853 acc=0.860 | (256, 128, 64), Î±=5.21e-04, lr=5.9e-04, bs=64\n",
      "[32/50] cvF1=0.743Â±0.088 | VAL F1=0.871 acc=0.877 | (128,), Î±=1.23e-05, lr=1.4e-04, bs=64\n",
      "[33/50] cvF1=0.726Â±0.085 | VAL F1=0.833 acc=0.842 | (64,), Î±=1.24e-04, lr=5.6e-04, bs=32\n",
      "[34/50] cvF1=0.700Â±0.094 | VAL F1=0.853 acc=0.860 | (256, 128), Î±=7.36e-05, lr=4.0e-04, bs=64\n",
      "[35/50] cvF1=0.707Â±0.098 | VAL F1=0.873 acc=0.877 | (256, 128), Î±=6.25e-05, lr=1.3e-04, bs=64\n",
      "[36/50] cvF1=0.738Â±0.084 | VAL F1=0.873 acc=0.877 | (256,), Î±=3.64e-05, lr=2.4e-03, bs=16\n",
      "[37/50] cvF1=0.678Â±0.100 | VAL F1=0.833 acc=0.842 | (256, 128, 64), Î±=1.59e-03, lr=1.9e-03, bs=128\n",
      "[38/50] cvF1=0.733Â±0.080 | VAL F1=0.889 acc=0.895 | (128, 64), Î±=4.45e-05, lr=2.1e-03, bs=64\n",
      "[39/50] cvF1=0.741Â±0.088 | VAL F1=0.871 acc=0.877 | (128,), Î±=2.66e-05, lr=3.4e-04, bs=32\n",
      "[40/50] cvF1=0.735Â±0.088 | VAL F1=0.854 acc=0.860 | (64,), Î±=8.84e-05, lr=9.1e-04, bs=16\n",
      "[41/50] cvF1=0.747Â±0.084 | VAL F1=0.871 acc=0.877 | (128, 64), Î±=1.68e-04, lr=2.8e-04, bs=32\n",
      "[42/50] cvF1=0.729Â±0.110 | VAL F1=0.853 acc=0.860 | (256,), Î±=2.83e-04, lr=2.1e-04, bs=64\n",
      "[43/50] cvF1=0.726Â±0.108 | VAL F1=0.833 acc=0.842 | (128,), Î±=1.49e-04, lr=2.5e-03, bs=32\n",
      "[44/50] cvF1=0.729Â±0.086 | VAL F1=0.851 acc=0.860 | (64,), Î±=2.54e-04, lr=1.2e-04, bs=32\n",
      "[45/50] cvF1=0.741Â±0.084 | VAL F1=0.854 acc=0.860 | (128, 64), Î±=7.22e-05, lr=1.1e-03, bs=64\n",
      "[46/50] cvF1=0.730Â±0.093 | VAL F1=0.833 acc=0.842 | (64,), Î±=3.27e-05, lr=3.0e-03, bs=64\n",
      "[47/50] cvF1=0.654Â±0.133 | VAL F1=0.788 acc=0.807 | (64,), Î±=2.49e-02, lr=4.0e-04, bs=64\n",
      "[48/50] cvF1=0.718Â±0.085 | VAL F1=0.873 acc=0.877 | (256, 128), Î±=1.58e-04, lr=8.6e-04, bs=32\n",
      "[49/50] cvF1=0.736Â±0.087 | VAL F1=0.853 acc=0.860 | (128,), Î±=7.02e-04, lr=4.6e-04, bs=32\n",
      "[50/50] cvF1=0.752Â±0.090 | VAL F1=0.854 acc=0.860 | (128, 64), Î±=1.91e-05, lr=3.5e-04, bs=16\n",
      "\n",
      "=== Top candidates (by VAL macro-F1) ===\n",
      "VAL F1=0.889 (acc=0.895) | cvF1=0.733Â±0.080 | params={'hidden_layer_sizes': (128, 64), 'alpha': 4.45375904389123e-05, 'learning_rate_init': 0.0020816986844858954, 'batch_size': 64, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.884 (acc=0.895) | cvF1=0.711Â±0.117 | params={'hidden_layer_sizes': (256, 128, 64), 'alpha': 5.474303040596574e-05, 'learning_rate_init': 0.00018559980846490572, 'batch_size': 16, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.873 (acc=0.877) | cvF1=0.738Â±0.084 | params={'hidden_layer_sizes': (256,), 'alpha': 3.6356826243945064e-05, 'learning_rate_init': 0.0023619797107806524, 'batch_size': 16, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.873 (acc=0.877) | cvF1=0.718Â±0.085 | params={'hidden_layer_sizes': (256, 128), 'alpha': 0.0001584362559438067, 'learning_rate_init': 0.0008649955121393716, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.873 (acc=0.877) | cvF1=0.707Â±0.098 | params={'hidden_layer_sizes': (256, 128), 'alpha': 6.245381382257146e-05, 'learning_rate_init': 0.00012992976740520352, 'batch_size': 64, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "\n",
      "ðŸ† Selected params:\n",
      "{'hidden_layer_sizes': (128, 64), 'alpha': 4.45375904389123e-05, 'learning_rate_init': 0.0020816986844858954, 'batch_size': 64, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "\n",
      "===== VAL =====\n",
      "VAL: acc=0.8947  f1_macro=0.8892\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    simplify       1.00      0.85      0.92        13\n",
      "      select       0.90      0.95      0.92        19\n",
      "   aggregate       0.88      0.88      0.88        16\n",
      "    displace       0.80      0.89      0.84         9\n",
      "\n",
      "    accuracy                           0.89        57\n",
      "   macro avg       0.89      0.89      0.89        57\n",
      "weighted avg       0.90      0.89      0.90        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[11  1  1  0]\n",
      " [ 0 18  1  0]\n",
      " [ 0  0 14  2]\n",
      " [ 0  1  0  8]]\n",
      "\n",
      "===== TEST =====\n",
      "TEST: acc=0.7544  f1_macro=0.7667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    simplify       0.83      0.77      0.80        13\n",
      "      select       0.82      0.74      0.78        19\n",
      "   aggregate       0.61      0.69      0.65        16\n",
      "    displace       0.80      0.89      0.84         9\n",
      "\n",
      "    accuracy                           0.75        57\n",
      "   macro avg       0.77      0.77      0.77        57\n",
      "weighted avg       0.76      0.75      0.76        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[10  1  2  0]\n",
      " [ 0 14  5  0]\n",
      " [ 1  2 11  2]\n",
      " [ 1  0  0  8]]\n",
      "\n",
      "âœ… Saved classifier to: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_use_map/classifier.joblib\n",
      "   âœ… Classifier training done.\n",
      "   Saved to: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_use_map/classifier.joblib\n",
      "   Best VAL: {'macro_f1': None, 'acc': 0.8947368421052632}\n",
      "   TEST     : {'macro_f1': None, 'acc': 0.7543859649122807}\n",
      "\n",
      "ðŸ§ª Experiment: openai_map\n",
      "   Classes   : ['simplify', 'select', 'aggregate', 'displace']\n",
      "   Train X   : (448, 1701)\n",
      "   Val X     : (57, 1701)\n",
      "   Test X    : (57, 1701)\n",
      "   Model out : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_openai_map\n",
      "\n",
      "Searching 50 MLP configs...\n",
      "[01/50] cvF1=0.652Â±0.116 | VAL F1=0.794 acc=0.807 | (128, 64), Î±=2.02e-02, lr=1.2e-03, bs=16\n",
      "[02/50] cvF1=0.796Â±0.089 | VAL F1=0.944 acc=0.947 | (256, 128), Î±=3.49e-05, lr=1.7e-04, bs=64\n",
      "[03/50] cvF1=0.711Â±0.114 | VAL F1=0.830 acc=0.842 | (256,), Î±=1.03e-02, lr=7.7e-04, bs=128\n",
      "[04/50] cvF1=0.784Â±0.084 | VAL F1=0.920 acc=0.930 | (256,), Î±=1.18e-05, lr=2.7e-03, bs=128\n",
      "[05/50] cvF1=0.790Â±0.082 | VAL F1=0.917 acc=0.930 | (256, 128, 64), Î±=5.47e-05, lr=1.9e-04, bs=16\n",
      "[06/50] cvF1=0.818Â±0.068 | VAL F1=0.944 acc=0.947 | (64,), Î±=1.14e-04, lr=6.0e-04, bs=128\n",
      "[07/50] cvF1=0.813Â±0.066 | VAL F1=0.944 acc=0.947 | (64,), Î±=1.03e-04, lr=8.0e-04, bs=32\n",
      "[08/50] cvF1=0.661Â±0.123 | VAL F1=0.808 acc=0.825 | (128, 64), Î±=2.43e-02, lr=2.2e-04, bs=32\n",
      "[09/50] cvF1=0.779Â±0.084 | VAL F1=0.940 acc=0.947 | (256, 128, 64), Î±=4.95e-05, lr=5.7e-04, bs=128\n",
      "[10/50] cvF1=0.833Â±0.064 | VAL F1=0.920 acc=0.930 | (64,), Î±=1.45e-05, lr=7.9e-04, bs=16\n",
      "[11/50] cvF1=0.816Â±0.065 | VAL F1=0.927 acc=0.930 | (64,), Î±=1.68e-05, lr=2.5e-03, bs=128\n",
      "[12/50] cvF1=0.707Â±0.096 | VAL F1=0.807 acc=0.825 | (256, 128, 64), Î±=6.47e-03, lr=2.8e-04, bs=16\n",
      "[13/50] cvF1=0.785Â±0.081 | VAL F1=0.903 acc=0.912 | (128,), Î±=2.39e-03, lr=4.5e-04, bs=64\n",
      "[14/50] cvF1=0.808Â±0.065 | VAL F1=0.927 acc=0.930 | (128, 64), Î±=5.27e-04, lr=1.1e-04, bs=32\n",
      "[15/50] cvF1=0.812Â±0.068 | VAL F1=0.944 acc=0.947 | (64,), Î±=7.94e-05, lr=9.5e-04, bs=32\n",
      "[16/50] cvF1=0.732Â±0.092 | VAL F1=0.884 acc=0.895 | (256, 128, 64), Î±=6.43e-04, lr=6.4e-04, bs=32\n",
      "[17/50] cvF1=0.691Â±0.068 | VAL F1=0.890 acc=0.895 | (256, 128), Î±=2.35e-02, lr=1.4e-03, bs=32\n",
      "[18/50] cvF1=0.716Â±0.112 | VAL F1=0.829 acc=0.842 | (128,), Î±=1.29e-02, lr=7.6e-04, bs=128\n",
      "[19/50] cvF1=0.791Â±0.082 | VAL F1=0.917 acc=0.930 | (256, 128, 64), Î±=4.80e-05, lr=1.2e-04, bs=128\n",
      "[20/50] cvF1=0.785Â±0.076 | VAL F1=0.903 acc=0.912 | (256, 128), Î±=2.25e-04, lr=2.5e-04, bs=16\n",
      "[21/50] cvF1=0.622Â±0.093 | VAL F1=0.735 acc=0.754 | (128,), Î±=2.27e-02, lr=7.9e-04, bs=16\n",
      "[22/50] cvF1=0.827Â±0.062 | VAL F1=0.920 acc=0.930 | (64,), Î±=1.07e-04, lr=1.8e-04, bs=16\n",
      "[23/50] cvF1=0.804Â±0.070 | VAL F1=0.944 acc=0.947 | (64,), Î±=4.84e-03, lr=2.0e-04, bs=128\n",
      "[24/50] cvF1=0.822Â±0.072 | VAL F1=0.920 acc=0.930 | (256,), Î±=4.91e-05, lr=1.1e-03, bs=64\n",
      "[25/50] cvF1=0.671Â±0.097 | VAL F1=0.825 acc=0.842 | (64,), Î±=1.28e-03, lr=2.3e-03, bs=32\n",
      "[26/50] cvF1=0.666Â±0.123 | VAL F1=0.793 acc=0.807 | (64,), Î±=1.52e-02, lr=1.8e-03, bs=128\n",
      "[27/50] cvF1=0.822Â±0.050 | VAL F1=0.944 acc=0.947 | (128,), Î±=2.15e-05, lr=3.5e-04, bs=32\n",
      "[28/50] cvF1=0.682Â±0.125 | VAL F1=0.863 acc=0.877 | (256, 128), Î±=3.44e-03, lr=8.7e-04, bs=64\n",
      "[29/50] cvF1=0.817Â±0.084 | VAL F1=0.920 acc=0.930 | (256,), Î±=4.38e-04, lr=1.5e-04, bs=32\n",
      "[30/50] cvF1=0.701Â±0.117 | VAL F1=0.830 acc=0.842 | (256,), Î±=4.42e-03, lr=6.7e-04, bs=64\n",
      "[31/50] cvF1=0.781Â±0.090 | VAL F1=0.917 acc=0.930 | (256, 128, 64), Î±=5.21e-04, lr=5.9e-04, bs=64\n",
      "[32/50] cvF1=0.819Â±0.050 | VAL F1=0.927 acc=0.930 | (128,), Î±=1.23e-05, lr=1.4e-04, bs=64\n",
      "[33/50] cvF1=0.827Â±0.061 | VAL F1=0.944 acc=0.947 | (64,), Î±=1.24e-04, lr=5.6e-04, bs=32\n",
      "[34/50] cvF1=0.782Â±0.092 | VAL F1=0.944 acc=0.947 | (256, 128), Î±=7.36e-05, lr=4.0e-04, bs=64\n",
      "[35/50] cvF1=0.793Â±0.096 | VAL F1=0.944 acc=0.947 | (256, 128), Î±=6.25e-05, lr=1.3e-04, bs=64\n",
      "[36/50] cvF1=0.796Â±0.083 | VAL F1=0.927 acc=0.930 | (256,), Î±=3.64e-05, lr=2.4e-03, bs=16\n",
      "[37/50] cvF1=0.756Â±0.094 | VAL F1=0.863 acc=0.877 | (256, 128, 64), Î±=1.59e-03, lr=1.9e-03, bs=128\n",
      "[38/50] cvF1=0.806Â±0.053 | VAL F1=0.927 acc=0.930 | (128, 64), Î±=4.45e-05, lr=2.1e-03, bs=64\n",
      "[39/50] cvF1=0.818Â±0.057 | VAL F1=0.944 acc=0.947 | (128,), Î±=2.66e-05, lr=3.4e-04, bs=32\n",
      "[40/50] cvF1=0.818Â±0.060 | VAL F1=0.920 acc=0.930 | (64,), Î±=8.84e-05, lr=9.1e-04, bs=16\n",
      "[41/50] cvF1=0.803Â±0.063 | VAL F1=0.927 acc=0.930 | (128, 64), Î±=1.68e-04, lr=2.8e-04, bs=32\n",
      "[42/50] cvF1=0.818Â±0.064 | VAL F1=0.927 acc=0.930 | (256,), Î±=2.83e-04, lr=2.1e-04, bs=64\n",
      "[43/50] cvF1=0.815Â±0.069 | VAL F1=0.903 acc=0.912 | (128,), Î±=1.49e-04, lr=2.5e-03, bs=32\n",
      "[44/50] cvF1=0.817Â±0.064 | VAL F1=0.944 acc=0.947 | (64,), Î±=2.54e-04, lr=1.2e-04, bs=32\n",
      "[45/50] cvF1=0.803Â±0.066 | VAL F1=0.927 acc=0.930 | (128, 64), Î±=7.22e-05, lr=1.1e-03, bs=64\n",
      "[46/50] cvF1=0.816Â±0.058 | VAL F1=0.944 acc=0.947 | (64,), Î±=3.27e-05, lr=3.0e-03, bs=64\n",
      "[47/50] cvF1=0.701Â±0.091 | VAL F1=0.810 acc=0.825 | (64,), Î±=2.49e-02, lr=4.0e-04, bs=64\n",
      "[48/50] cvF1=0.790Â±0.088 | VAL F1=0.920 acc=0.930 | (256, 128), Î±=1.58e-04, lr=8.6e-04, bs=32\n",
      "[49/50] cvF1=0.795Â±0.081 | VAL F1=0.903 acc=0.912 | (128,), Î±=7.02e-04, lr=4.6e-04, bs=32\n",
      "[50/50] cvF1=0.819Â±0.061 | VAL F1=0.944 acc=0.947 | (128, 64), Î±=1.91e-05, lr=3.5e-04, bs=16\n",
      "\n",
      "=== Top candidates (by VAL macro-F1) ===\n",
      "VAL F1=0.944 (acc=0.947) | cvF1=0.827Â±0.061 | params={'hidden_layer_sizes': (64,), 'alpha': 0.00012389502377355922, 'learning_rate_init': 0.0005639239991667559, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.944 (acc=0.947) | cvF1=0.822Â±0.050 | params={'hidden_layer_sizes': (128,), 'alpha': 2.1466070134458155e-05, 'learning_rate_init': 0.00035297465462888, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.944 (acc=0.947) | cvF1=0.819Â±0.061 | params={'hidden_layer_sizes': (128, 64), 'alpha': 1.910455030763905e-05, 'learning_rate_init': 0.0003515802365051835, 'batch_size': 16, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.944 (acc=0.947) | cvF1=0.818Â±0.068 | params={'hidden_layer_sizes': (64,), 'alpha': 0.00011425814516827707, 'learning_rate_init': 0.0005958389350068958, 'batch_size': 128, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.944 (acc=0.947) | cvF1=0.818Â±0.057 | params={'hidden_layer_sizes': (128,), 'alpha': 2.6577530317759453e-05, 'learning_rate_init': 0.0003359658305322318, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "\n",
      "ðŸ† Selected params:\n",
      "{'hidden_layer_sizes': (64,), 'alpha': 0.00012389502377355922, 'learning_rate_init': 0.0005639239991667559, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "\n",
      "===== VAL =====\n",
      "VAL: acc=0.9474  f1_macro=0.9444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    simplify       0.87      1.00      0.93        13\n",
      "      select       0.95      1.00      0.97        19\n",
      "   aggregate       1.00      0.88      0.93        16\n",
      "    displace       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.95        57\n",
      "   macro avg       0.95      0.94      0.94        57\n",
      "weighted avg       0.95      0.95      0.95        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[13  0  0  0]\n",
      " [ 0 19  0  0]\n",
      " [ 2  0 14  0]\n",
      " [ 0  1  0  8]]\n",
      "\n",
      "===== TEST =====\n",
      "TEST: acc=0.8596  f1_macro=0.8661\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    simplify       0.73      0.85      0.79        13\n",
      "      select       0.94      0.84      0.89        19\n",
      "   aggregate       0.82      0.88      0.85        16\n",
      "    displace       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.86        57\n",
      "   macro avg       0.87      0.86      0.87        57\n",
      "weighted avg       0.87      0.86      0.86        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[11  0  2  0]\n",
      " [ 2 16  1  0]\n",
      " [ 1  1 14  0]\n",
      " [ 1  0  0  8]]\n",
      "\n",
      "âœ… Saved classifier to: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_openai_map/classifier.joblib\n",
      "   âœ… Classifier training done.\n",
      "   Saved to: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_openai_map/classifier.joblib\n",
      "   Best VAL: {'macro_f1': None, 'acc': 0.9473684210526315}\n",
      "   TEST     : {'macro_f1': None, 'acc': 0.8596491228070176}\n",
      "\n",
      "âœ… All classifiers trained.\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 10 â€” Train classifier (per experiment, MLP search + final fit) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from dataclasses import asdict, is_dataclass\n",
    "\n",
    "from src.train.train_classifier import train_mlp_classifier_with_search\n",
    "\n",
    "CLF_RESULTS = {}  # exp_name -> ClassifierTrainResult\n",
    "\n",
    "def _safe_get(obj, *names, default=None):\n",
    "    for n in names:\n",
    "        if hasattr(obj, n):\n",
    "            return getattr(obj, n)\n",
    "    return default\n",
    "\n",
    "print(\"\\n=== Training operator classifiers for all experiments ===\")\n",
    "\n",
    "printed_debug_fields = False\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "\n",
    "    split = SPLITS[exp_name]\n",
    "    pre   = PREPROC[exp_name]\n",
    "    lab   = LABELS[exp_name]\n",
    "\n",
    "    X_train_s = pre[\"X_train_s\"]\n",
    "    X_val_s   = pre[\"X_val_s\"]\n",
    "    X_test_s  = pre[\"X_test_s\"]\n",
    "\n",
    "    y_train = lab[\"y_train_cls\"]\n",
    "    y_val   = lab[\"y_val_cls\"]\n",
    "    y_test  = lab[\"y_test_cls\"]\n",
    "    sample_w = lab[\"sample_w\"]\n",
    "\n",
    "    class_names = [str(x) for x in lab[\"class_names\"]]\n",
    "\n",
    "    # Sanity checks\n",
    "    if X_train_s.shape[0] != len(y_train):\n",
    "        raise ValueError(f\"{exp_name}: X_train rows {X_train_s.shape[0]} != y_train {len(y_train)}\")\n",
    "    if X_val_s.shape[0] != len(y_val):\n",
    "        raise ValueError(f\"{exp_name}: X_val rows {X_val_s.shape[0]} != y_val {len(y_val)}\")\n",
    "    if X_test_s.shape[0] != len(y_test):\n",
    "        raise ValueError(f\"{exp_name}: X_test rows {X_test_s.shape[0]} != y_test {len(y_test)}\")\n",
    "\n",
    "    # Grouped CV: group by map_id to avoid leakage across folds\n",
    "    groups_tr = split[\"df_train\"][\"map_id\"].astype(str).to_numpy()\n",
    "\n",
    "    model_out_dir = Path(cfg[\"model_out\"])\n",
    "    model_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   Classes   : {class_names}\")\n",
    "    print(f\"   Train X   : {X_train_s.shape}\")\n",
    "    print(f\"   Val X     : {X_val_s.shape}\")\n",
    "    print(f\"   Test X    : {X_test_s.shape}\")\n",
    "    print(f\"   Model out : {model_out_dir}\")\n",
    "\n",
    "    res_clf = train_mlp_classifier_with_search(\n",
    "        exp_name=exp_name,\n",
    "        X_train=X_train_s,\n",
    "        y_train=y_train,\n",
    "        groups_train=groups_tr,\n",
    "        sample_w=sample_w,\n",
    "        X_val=X_val_s,\n",
    "        y_val=y_val,\n",
    "        X_test=X_test_s,\n",
    "        y_test=y_test,\n",
    "        class_names=class_names,\n",
    "        out_dir=model_out_dir,\n",
    "        n_iter=50,\n",
    "        n_splits=5,\n",
    "        seed=int(CFG.SEED),\n",
    "        verbose=True,\n",
    "        save_name=\"classifier.joblib\",\n",
    "    )\n",
    "\n",
    "    CLF_RESULTS[exp_name] = res_clf\n",
    "\n",
    "    # ---- robust reporting (no assumptions about field names) ----\n",
    "    model_path    = _safe_get(res_clf, \"model_path\", \"path\", default=str(model_out_dir / \"classifier.joblib\"))\n",
    "    best_val_f1   = _safe_get(res_clf, \"best_val_f1\", \"val_f1\", \"best_f1\", default=None)\n",
    "    best_val_acc  = _safe_get(res_clf, \"best_val_acc\", \"val_acc\", \"best_accuracy\", default=None)\n",
    "    test_f1       = _safe_get(res_clf, \"test_f1\", default=None)\n",
    "    test_acc      = _safe_get(res_clf, \"test_acc\", \"accuracy_test\", default=None)\n",
    "\n",
    "    print(\"   âœ… Classifier training done.\")\n",
    "    print(\"   Saved to:\", model_path)\n",
    "    if best_val_f1 is not None or best_val_acc is not None:\n",
    "        print(\"   Best VAL:\", {\"macro_f1\": best_val_f1, \"acc\": best_val_acc})\n",
    "    if test_f1 is not None or test_acc is not None:\n",
    "        print(\"   TEST     :\", {\"macro_f1\": test_f1, \"acc\": test_acc})\n",
    "\n",
    "    # Save lightweight meta for evaluation / reporting\n",
    "    clf_meta = {\n",
    "        \"experiment\": exp_name,\n",
    "        \"feature_mode\": cfg[\"feature_mode\"],\n",
    "        \"class_names\": class_names,\n",
    "        \"best_val\": {\"macro_f1\": best_val_f1, \"acc\": best_val_acc},\n",
    "        \"test\": {\"macro_f1\": test_f1, \"acc\": test_acc},\n",
    "        \"model_path\": str(model_path),\n",
    "    }\n",
    "    (model_out_dir / \"classifier_meta.json\").write_text(json.dumps(clf_meta, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # Print available fields once for debugging\n",
    "    if not printed_debug_fields:\n",
    "        printed_debug_fields = True\n",
    "        if is_dataclass(res_clf):\n",
    "            print(\"   (debug) Result fields:\", list(asdict(res_clf).keys()))\n",
    "        else:\n",
    "            print(\"   (debug) Result attrs :\", [a for a in dir(res_clf) if not a.startswith(\"_\")])\n",
    "\n",
    "print(\"\\nâœ… All classifiers trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f878f54",
   "metadata": {},
   "source": [
    "## Step 11 â€” Parameter Regression (Per-Operator) and Final Model Bundle (Experiment-Aware)\n",
    "\n",
    "This step trains **operator-specific regressors** to predict the generalization parameter in a\n",
    "**scale-independent form**, and then packages all trained components into a single,\n",
    "experiment-scoped model bundle.\n",
    "\n",
    "The same procedure is applied **independently for each experiment** (prompt-only,\n",
    "USE + map, OpenAI + map), using the shared data split and preprocessing pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### Regression target\n",
    "\n",
    "Regressors are trained on the normalized target `param_norm`, defined as:\n",
    "\n",
    "- **Distance-based operators** (`simplify`, `aggregate`, `displace`):  \n",
    "  `param_norm = param_value / extent_diag_m`\n",
    "\n",
    "- **Area-based operators** (`select`):  \n",
    "  `param_norm = param_value / extent_area_m2`\n",
    "\n",
    "This normalization allows each regressor to generalize across maps of different spatial\n",
    "extent while preserving physical meaning. During inference, predictions are converted back\n",
    "to real-world units using the same per-map extent references.\n",
    "\n",
    "---\n",
    "\n",
    "### Training strategy\n",
    "\n",
    "- One **MLPRegressor per operator**\n",
    "- Training data restricted to samples of the corresponding operator\n",
    "- **Grouped cross-validation** (`GroupKFold`) by `map_id` to prevent spatial leakage\n",
    "- Hyperparameter optimization via `RandomizedSearchCV` for each operator independently\n",
    "\n",
    "---\n",
    "\n",
    "### Final model bundle\n",
    "\n",
    "For each experiment, the trained components are stored together in a single bundle:\n",
    "\n",
    "- `cls_plus_regressors.joblib`\n",
    "\n",
    "This bundle contains:\n",
    "- the trained operator classifier\n",
    "- the dictionary of operator-specific regressors\n",
    "- the fixed class order\n",
    "- normalization metadata (operator groups and extent columns)\n",
    "\n",
    "Along with the experimentâ€™s `preproc.joblib`, this bundle is sufficient for the evaluation\n",
    "notebook to compute:\n",
    "\n",
    "1. **Classifier-only metrics**  \n",
    "2. **Regressor-only metrics** (oracle operator routing)  \n",
    "3. **End-to-end pipeline metrics** (predicted operator routing)\n",
    "\n",
    "This design keeps evaluation simple, reproducible, and fully decoupled from the training\n",
    "notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea7905b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training per-operator regressors and saving final bundles ===\n",
      "\n",
      "ðŸ§ª Experiment: prompt_only\n",
      "   Model out: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_prompt_only\n",
      "   Train X  : (448, 512) | df_train: (448, 16)\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'simplify' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 1.001749518487077\n",
      "best CV RMSE (param_norm units): 0.0043470648673623385\n",
      "best params: {'alpha': np.float64(0.003904209851777714), 'hidden_layer_sizes': (64,), 'learning_rate_init': np.float64(0.00010546221020664906)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'select' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 1.024593862422861\n",
      "best CV RMSE (param_norm units): 0.00036238325020281493\n",
      "best params: {'alpha': np.float64(0.003904209851777714), 'hidden_layer_sizes': (64,), 'learning_rate_init': np.float64(0.00010546221020664906)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'aggregate' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 1.0133264116251575\n",
      "best CV RMSE (param_norm units): 0.0034985184250531753\n",
      "best params: {'alpha': np.float64(0.003904209851777714), 'hidden_layer_sizes': (64,), 'learning_rate_init': np.float64(0.00010546221020664906)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'displace' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 1.034262775822706\n",
      "best CV RMSE (param_norm units): 0.003668900656611833\n",
      "best params: {'alpha': np.float64(0.003904209851777714), 'hidden_layer_sizes': (64,), 'learning_rate_init': np.float64(0.00010546221020664906)}\n",
      "   âœ… Saved bundle: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_prompt_only/cls_plus_regressors.joblib\n",
      "   âœ… Regressors trained for: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "ðŸ§ª Experiment: map_only\n",
      "   Model out: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_map_only\n",
      "   Train X  : (448, 165) | df_train: (448, 16)\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'simplify' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 0.9718750161193167\n",
      "best CV RMSE (param_norm units): 0.004217425274553791\n",
      "best params: {'alpha': np.float64(0.000522114714225509), 'hidden_layer_sizes': (256, 128), 'learning_rate_init': np.float64(0.00016149614799999194)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'select' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 0.7144460751936476\n",
      "best CV RMSE (param_norm units): 0.0002526886997069152\n",
      "best params: {'alpha': np.float64(8.00057734834173e-06), 'hidden_layer_sizes': (64,), 'learning_rate_init': np.float64(0.0002913009501549591)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'aggregate' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 1.0595340897855707\n",
      "best CV RMSE (param_norm units): 0.003658050843796577\n",
      "best params: {'alpha': np.float64(0.003904209851777714), 'hidden_layer_sizes': (64,), 'learning_rate_init': np.float64(0.00010546221020664906)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'displace' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 0.9184685031073186\n",
      "best CV RMSE (param_norm units): 0.0032581368805884364\n",
      "best params: {'alpha': np.float64(3.5186816415472715e-06), 'hidden_layer_sizes': (256, 128), 'learning_rate_init': np.float64(0.0017011002697669686)}\n",
      "   âœ… Saved bundle: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_map_only/cls_plus_regressors.joblib\n",
      "   âœ… Regressors trained for: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "ðŸ§ª Experiment: use_map\n",
      "   Model out: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_use_map\n",
      "   Train X  : (448, 677) | df_train: (448, 16)\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'simplify' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 1.01289160632171\n",
      "best CV RMSE (param_norm units): 0.004395415655339905\n",
      "best params: {'alpha': np.float64(1.9255661420887873e-06), 'hidden_layer_sizes': (256, 128), 'learning_rate_init': np.float64(0.001195960383019184)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'select' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 0.7029201831130865\n",
      "best CV RMSE (param_norm units): 0.00024861216715404223\n",
      "best params: {'alpha': np.float64(3.5186816415472715e-06), 'hidden_layer_sizes': (256, 128), 'learning_rate_init': np.float64(0.0017011002697669686)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'aggregate' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 1.074545323756762\n",
      "best CV RMSE (param_norm units): 0.003709877262242308\n",
      "best params: {'alpha': np.float64(3.11927680501103e-05), 'hidden_layer_sizes': (256,), 'learning_rate_init': np.float64(0.00010725209743172001)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'displace' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 0.9332264158910322\n",
      "best CV RMSE (param_norm units): 0.0033104884851981225\n",
      "best params: {'alpha': np.float64(0.0041619125396912095), 'hidden_layer_sizes': (64,), 'learning_rate_init': np.float64(0.00010558059144381523)}\n",
      "   âœ… Saved bundle: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_use_map/cls_plus_regressors.joblib\n",
      "   âœ… Regressors trained for: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "ðŸ§ª Experiment: openai_map\n",
      "   Model out: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_openai_map\n",
      "   Train X  : (448, 1701) | df_train: (448, 16)\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'simplify' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 1.0696427836660816\n",
      "best CV RMSE (param_norm units): 0.00464168585029618\n",
      "best params: {'alpha': np.float64(3.11927680501103e-05), 'hidden_layer_sizes': (256,), 'learning_rate_init': np.float64(0.00010725209743172001)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'select' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 0.7148081953130067\n",
      "best CV RMSE (param_norm units): 0.0002528167760800323\n",
      "best params: {'alpha': np.float64(1.9255661420887873e-06), 'hidden_layer_sizes': (256, 128), 'learning_rate_init': np.float64(0.001195960383019184)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'aggregate' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 1.100979706754777\n",
      "best CV RMSE (param_norm units): 0.0038011422040345076\n",
      "best params: {'alpha': np.float64(0.003904209851777714), 'hidden_layer_sizes': (64,), 'learning_rate_init': np.float64(0.00010546221020664906)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'displace' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 0.9549161120803993\n",
      "best CV RMSE (param_norm units): 0.003387429609302275\n",
      "best params: {'alpha': np.float64(2.1453931225439485e-06), 'hidden_layer_sizes': (128,), 'learning_rate_init': np.float64(0.0001483039268456802)}\n",
      "   âœ… Saved bundle: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_openai_map/cls_plus_regressors.joblib\n",
      "   âœ… Regressors trained for: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "âœ… All bundles saved.\n",
      " - prompt_only : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_prompt_only/cls_plus_regressors.joblib\n",
      " - map_only    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_map_only/cls_plus_regressors.joblib\n",
      " - use_map     : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_use_map/cls_plus_regressors.joblib\n",
      " - openai_map  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models/exp_openai_map/cls_plus_regressors.joblib\n"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 11 â€” Train per-operator regressors + save final bundle (per experiment) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from src.train.train_regressors import train_regressors_per_operator\n",
    "from src.train.save_bundle import save_cls_plus_regressors_bundle\n",
    "\n",
    "BUNDLES = {}     # exp_name -> bundle path\n",
    "REG_RESULTS = {} # exp_name -> regressor training result\n",
    "\n",
    "print(\"\\n=== Training per-operator regressors and saving final bundles ===\")\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "\n",
    "    split = SPLITS[exp_name]\n",
    "    pre   = PREPROC[exp_name]\n",
    "    lab   = LABELS[exp_name]\n",
    "    res_clf = CLF_RESULTS[exp_name]\n",
    "\n",
    "    X_train_s = pre[\"X_train_s\"]\n",
    "    df_train  = split[\"df_train\"]\n",
    "    y_train_cls = lab[\"y_train_cls\"]\n",
    "    sample_w = lab[\"sample_w\"]\n",
    "\n",
    "    # Make sure class_names is list[str] (stable ordering)\n",
    "    cn = [str(x) for x in lab[\"class_names\"]]\n",
    "\n",
    "    model_out_dir = Path(cfg[\"model_out\"])\n",
    "    model_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   Model out: {model_out_dir}\")\n",
    "    print(f\"   Train X  : {X_train_s.shape} | df_train: {df_train.shape}\")\n",
    "\n",
    "    # ---- (1) Train per-operator regressors on TRAIN only ----\n",
    "    reg_res = train_regressors_per_operator(\n",
    "        X_train_s=X_train_s,\n",
    "        df_train=df_train,\n",
    "        y_train_cls=y_train_cls,\n",
    "        class_names=cn,\n",
    "        sample_w=sample_w,\n",
    "        group_col=\"map_id\",\n",
    "        target_col=\"param_norm\",\n",
    "        use_log1p=False,\n",
    "        n_splits=5,\n",
    "        n_iter=40,\n",
    "        random_state=int(CFG.SEED),\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    REG_RESULTS[exp_name] = reg_res\n",
    "\n",
    "    # ---- (2) Load the trained classifier model (from Cell 10 output) ----\n",
    "    clf_pack = joblib.load(Path(res_clf.model_path))\n",
    "    final_clf = clf_pack[\"model\"]\n",
    "\n",
    "    # ---- (3) Save combined bundle for evaluation notebook ----\n",
    "    bundle_res = save_cls_plus_regressors_bundle(\n",
    "        exp_name=exp_name,\n",
    "        out_dir=model_out_dir,\n",
    "        classifier=final_clf,\n",
    "        regressors_by_class=reg_res.regressors_by_class,\n",
    "        class_names=cn,\n",
    "        use_log1p=reg_res.use_log1p,\n",
    "        cv_summary=reg_res.cv_summary,\n",
    "        distance_ops=DISTANCE_OPS,\n",
    "        area_ops=AREA_OPS,\n",
    "        diag_col=\"extent_diag_m\",\n",
    "        area_col=\"extent_area_m2\",\n",
    "        save_name=\"cls_plus_regressors.joblib\",  # fixed name inside each experiment folder\n",
    "    )\n",
    "\n",
    "    BUNDLES[exp_name] = bundle_res.bundle_path\n",
    "\n",
    "    print(\"   âœ… Saved bundle:\", bundle_res.bundle_path)\n",
    "    print(\"   âœ… Regressors trained for:\", sorted(list(reg_res.regressors_by_class.keys())))\n",
    "\n",
    "print(\"\\nâœ… All bundles saved.\")\n",
    "for k, v in BUNDLES.items():\n",
    "    print(f\" - {k:12s}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c73a28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
