{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c1f56a2c",
      "metadata": {},
      "source": [
        "# MapVec end-to-end pipeline ðŸ“’\n",
        "\n",
        "This notebook runs the **entire pipeline**:\n",
        "1. Prompt embeddings (Universal Sentence Encoder)\n",
        "2. Map embeddings (handcrafted polygon features)\n",
        "3. Concatenation into a training matrix\n",
        "4. Helper cells to inspect vectors by `prompt_id` or `map_id`\n",
        "\n",
        "**Edit the Parameters** in the next cell to match your project layout.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(PosixPath('..'), PosixPath('../data'))"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ===================== PARAMETERS (EDIT ME) =====================\n",
        "from pathlib import Path\n",
        "\n",
        "# Project root that contains `src/` and `data/`\n",
        "PROJ_ROOT = Path('../')  # e.g., Path('/Users/you/Documents/Semester_5/Thesis/CODES')\n",
        "\n",
        "# Data locations\n",
        "DATA_DIR    = PROJ_ROOT / 'data'\n",
        "PROMPTS_CSV = DATA_DIR / 'input' / 'prompts.csv'           # CSV with columns prompt_id,text (or id,text)\n",
        "PAIRS_CSV   = DATA_DIR / 'input' / 'pairs.csv'             # CSV with map_id,prompt_id\n",
        "MAPS_ROOT   = DATA_DIR / 'input' / 'samples' / 'pairs'     # Folder with *_input.geojson files\n",
        "INPUT_MAPS_PATTERN = '*_input.geojson'\n",
        "OUTPUT_MAPS_PATTERN = '*_generalized.geojson'\n",
        "\n",
        "# Output directories\n",
        "PROMPT_OUT = DATA_DIR / 'output' / 'prompt_out'\n",
        "MAP_OUT    = DATA_DIR / 'output' / 'map_out'\n",
        "TRAIN_OUT  = DATA_DIR / 'output' / 'train_out'\n",
        "PAIR_MAP_OUT    = DATA_DIR / 'output' / 'pair_map_out'\n",
        "SPLIT_OUT   = DATA_DIR / \"train_out\" / \"splits\"\n",
        "MODEL_OUT   = DATA_DIR / \"models\"\n",
        "MODEL_OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "# USE model: 'dan' or 'transformer'\n",
        "USE_MODEL = 'dan'\n",
        "\n",
        "# Expected dims (do not change unless you know what you're doing)\n",
        "MAP_DIM = 249\n",
        "PROMPT_DIM = 512\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "PROJ_ROOT, DATA_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1e8f95c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ§¹ Removing old directory: ../data/output/prompt_out\n",
            "ðŸ§¹ Removing old directory: ../data/output/map_out\n",
            "ðŸ§¹ Removing old directory: ../data/output/train_out\n",
            "âœ… All output folders cleaned and recreated fresh.\n"
          ]
        }
      ],
      "source": [
        "# ===================== CLEAN PREVIOUS OUTPUTS =====================\n",
        "import shutil\n",
        "\n",
        "for d in [PROMPT_OUT, MAP_OUT, TRAIN_OUT]:\n",
        "    if d.exists():\n",
        "        print(f\"ðŸ§¹ Removing old directory: {d}\")\n",
        "        shutil.rmtree(d)\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"âœ… All output folders cleaned and recreated fresh.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PYTHONPATH updated with: ..\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(PosixPath('../data/output/prompt_out'),\n",
              " PosixPath('../data/output/map_out'),\n",
              " PosixPath('../data/output/train_out'))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Make sure Python can import your local modules (src/)\n",
        "import sys\n",
        "sys.path.insert(0, str(PROJ_ROOT))\n",
        "print('PYTHONPATH updated with:', PROJ_ROOT)\n",
        "\n",
        "# Create output folders\n",
        "PROMPT_OUT.mkdir(parents=True, exist_ok=True)\n",
        "MAP_OUT.mkdir(parents=True, exist_ok=True)\n",
        "TRAIN_OUT.mkdir(parents=True, exist_ok=True)\n",
        "PROMPT_OUT, MAP_OUT, TRAIN_OUT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Dependency check (Parquet engine)\n",
        "We ensure `pyarrow` or `fastparquet` is available for `pandas.to_parquet`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parquet engine: OK\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "ok = importlib.util.find_spec('pyarrow') or importlib.util.find_spec('fastparquet')\n",
        "if not ok:\n",
        "    raise SystemExit('Missing parquet engine (pyarrow/fastparquet). Install with: conda install pyarrow -y')\n",
        "print('Parquet engine: OK')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Prompt embeddings\n",
        "Runs `src/mapvec/prompts/prompt_embeddings.py` using your chosen USE model and saves artifacts to `PROMPT_OUT`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python ../src/mapvec/prompts/prompt_embeddings.py --input ../data/input/prompts.csv --model dan --l2 --out_dir ../data/output/prompt_out -v\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20:46:31 | DEBUG | FILE_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/src/mapvec/prompts\n",
            "20:46:31 | DEBUG | PROJECT_ROOT=/Users/amirdonyadide/Documents/GitHub/Thesis\n",
            "20:46:31 | DEBUG | DEFAULT_DATA_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
            "20:46:31 | INFO | DATA_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
            "20:46:31 | INFO | INPUT=/Users/amirdonyadide/Documents/GitHub/Thesis/data/input/prompts.csv\n",
            "20:46:31 | INFO | OUT_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
            "20:46:31 | INFO | Reading CSV: /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/prompts.csv\n",
            "20:46:31 | INFO | Loaded 500 prompts (id_col=prompt_id). Sample IDs: p001, p002, p003â€¦\n",
            "20:46:31 | INFO | Using local USE-dan at /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/model_dan\n",
            "20:46:31 | INFO | Loading USE-dan from local path: /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/model_dan â€¦\n",
            "20:46:34 | INFO | Fingerprint not found. Saved model loading will continue.\n",
            "20:46:34 | INFO | path_and_singleprint metric could not be logged. Saved model loading will continue.\n",
            "20:46:34 | INFO | Model loaded in 3.48s\n",
            "20:46:34 | INFO | Embedding 500 prompts (batch_size=512, l2=True)â€¦\n",
            "20:46:34 | DEBUG |   embedded rows [1:500)\n",
            "20:46:34 | INFO | Done embedding in 0.15s (dim=512).\n",
            "20:46:34 | INFO | Writing outputs to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
            "20:46:34 | INFO |   saved embeddings.npz (shape=(500, 512))\n",
            "20:46:34 | INFO |   saved prompts.parquet (rows=500)\n",
            "20:46:34 | INFO |   saved meta.json\n",
            "20:46:34 | INFO | All done âœ…\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt embeddings completed.\n"
          ]
        }
      ],
      "source": [
        "import subprocess, shlex\n",
        "cmd = (\n",
        "    f\"python {shlex.quote(str(PROJ_ROOT / 'src' / 'mapvec' / 'prompts' / 'prompt_embeddings.py'))} \"\n",
        "    f\"--input {shlex.quote(str(PROMPTS_CSV))} --model {shlex.quote(str(USE_MODEL))} --l2 --out_dir {shlex.quote(str(PROMPT_OUT))} -v\"\n",
        ")\n",
        "print(cmd)\n",
        "res = subprocess.run(cmd, shell=True)\n",
        "if res.returncode != 0:\n",
        "    raise SystemExit('Prompt embedding step failed.')\n",
        "print('Prompt embeddings completed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Map embeddings\n",
        "Runs the map embedding module on the GeoJSON inputs. Skips problematic features, logs warnings, and writes `embeddings.npz` to `PAIR_MAP_OUT`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "fa2b07a9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CMD: /opt/anaconda3/envs/thesis/bin/python -m src.mapvec.maps.pair_map_embeddings --root data/input/samples/pairs --input_pattern *_input.geojson --gen_pattern *_generalized.geojson --out_dir data/output/pair_map_out -v\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "19:34:37 | DEBUG | PROJECT_ROOT=/Users/amirdonyadide/Documents/Semester_5/Thesis/CODES\n",
            "19:34:37 | DEBUG | DATA_DIR=/Users/amirdonyadide/Documents/Semester_5/Thesis/CODES/data\n",
            "19:34:37 | INFO | Scanning /Users/amirdonyadide/Documents/Semester_5/Thesis/CODES/data/input/samples/pairs (in=*_input.geojson, gen=*_generalized.geojson)â€¦\n",
            "19:34:41 | INFO | OK  map_id=0073  -> pair_vec[996] (per_map_dim=249)\n",
            "19:34:42 | INFO | OK  map_id=0080  -> pair_vec[996] (per_map_dim=249)\n",
            "19:34:44 | INFO | OK  map_id=0093  -> pair_vec[996] (per_map_dim=249)\n",
            "19:34:48 | INFO | OK  map_id=0122  -> pair_vec[996] (per_map_dim=249)\n",
            "19:34:50 | INFO | OK  map_id=0123  -> pair_vec[996] (per_map_dim=249)\n",
            "19:34:51 | INFO | OK  map_id=0127  -> pair_vec[996] (per_map_dim=249)\n",
            "19:34:52 | INFO | OK  map_id=0158  -> pair_vec[996] (per_map_dim=249)\n",
            "19:34:57 | INFO | OK  map_id=0159  -> pair_vec[996] (per_map_dim=249)\n",
            "19:34:58 | INFO | OK  map_id=0160  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:01 | INFO | OK  map_id=0165  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:03 | INFO | OK  map_id=0167  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:05 | INFO | OK  map_id=0168  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:06 | INFO | OK  map_id=0171  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:11 | INFO | OK  map_id=0208  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:18 | INFO | OK  map_id=0209  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:19 | INFO | OK  map_id=0215  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:21 | INFO | OK  map_id=0240  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:22 | INFO | OK  map_id=0256  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:23 | INFO | OK  map_id=0257  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:24 | INFO | OK  map_id=0262  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:27 | INFO | OK  map_id=0285  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:30 | INFO | OK  map_id=0286  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:34 | INFO | OK  map_id=0288  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:39 | INFO | OK  map_id=0289  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:40 | INFO | OK  map_id=0313  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:42 | INFO | OK  map_id=0341  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:43 | INFO | OK  map_id=0362  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:48 | INFO | OK  map_id=0363  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:51 | INFO | OK  map_id=0364  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:53 | INFO | OK  map_id=0379  -> pair_vec[996] (per_map_dim=249)\n",
            "19:35:56 | INFO | OK  map_id=0389  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:00 | INFO | OK  map_id=0390  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:02 | INFO | OK  map_id=0409  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:05 | INFO | OK  map_id=0410  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:07 | INFO | OK  map_id=0412  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:09 | INFO | OK  map_id=0413  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:12 | INFO | OK  map_id=0414  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:13 | INFO | OK  map_id=0417  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:14 | INFO | OK  map_id=0421  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:17 | INFO | OK  map_id=0426  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:18 | INFO | OK  map_id=0427  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:19 | INFO | OK  map_id=0432  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:20 | INFO | OK  map_id=0433  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:24 | INFO | OK  map_id=0437  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:26 | INFO | OK  map_id=0438  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:29 | INFO | OK  map_id=0439  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:31 | INFO | OK  map_id=0454  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:32 | INFO | OK  map_id=0458  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:34 | INFO | OK  map_id=0459  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:36 | INFO | OK  map_id=0460  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:39 | INFO | OK  map_id=0466  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:41 | INFO | OK  map_id=0469  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:44 | INFO | OK  map_id=0471  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:47 | INFO | OK  map_id=0472  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:53 | INFO | OK  map_id=0474  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:55 | INFO | OK  map_id=0475  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:58 | INFO | OK  map_id=0479  -> pair_vec[996] (per_map_dim=249)\n",
            "19:36:59 | INFO | OK  map_id=0480  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:01 | INFO | OK  map_id=0481  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:02 | INFO | OK  map_id=0482  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:04 | INFO | OK  map_id=0508  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:07 | INFO | OK  map_id=0509  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:09 | INFO | OK  map_id=0518  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:18 | INFO | OK  map_id=0520  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:21 | INFO | OK  map_id=0521  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:23 | INFO | OK  map_id=0523  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:26 | INFO | OK  map_id=0527  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:30 | INFO | OK  map_id=0528  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:31 | INFO | OK  map_id=0529  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:34 | INFO | OK  map_id=0530  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:36 | INFO | OK  map_id=0553  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:38 | INFO | OK  map_id=0557  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:40 | INFO | OK  map_id=0575  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:41 | INFO | OK  map_id=0576  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:43 | INFO | OK  map_id=0594  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:46 | INFO | OK  map_id=0595  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:48 | INFO | OK  map_id=0600  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:52 | INFO | OK  map_id=0605  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:55 | INFO | OK  map_id=0606  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:56 | INFO | OK  map_id=0608  -> pair_vec[996] (per_map_dim=249)\n",
            "19:37:59 | INFO | OK  map_id=0609  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:01 | INFO | OK  map_id=0611  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:05 | INFO | OK  map_id=0614  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:07 | INFO | OK  map_id=0615  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:09 | INFO | OK  map_id=0618  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:12 | INFO | OK  map_id=0623  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:13 | INFO | OK  map_id=0624  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:15 | INFO | OK  map_id=0645  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:17 | INFO | OK  map_id=0646  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:18 | INFO | OK  map_id=0655  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:19 | INFO | OK  map_id=0656  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:22 | INFO | OK  map_id=0657  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:26 | INFO | OK  map_id=0658  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:27 | INFO | OK  map_id=0659  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:29 | INFO | OK  map_id=0667  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:31 | INFO | OK  map_id=0672  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:33 | INFO | OK  map_id=0699  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:36 | INFO | OK  map_id=0700  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:39 | INFO | OK  map_id=0701  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:42 | INFO | OK  map_id=0706  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:43 | INFO | OK  map_id=0707  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:44 | INFO | OK  map_id=0715  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:47 | INFO | OK  map_id=0721  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:49 | INFO | OK  map_id=0747  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:52 | INFO | OK  map_id=0748  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:53 | INFO | OK  map_id=0749  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:55 | INFO | OK  map_id=0755  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:56 | INFO | OK  map_id=0758  -> pair_vec[996] (per_map_dim=249)\n",
            "19:38:58 | INFO | OK  map_id=0759  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:00 | INFO | OK  map_id=0762  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:01 | INFO | OK  map_id=0770  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:03 | INFO | OK  map_id=0804  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:05 | INFO | OK  map_id=0807  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:06 | INFO | OK  map_id=0808  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:09 | INFO | OK  map_id=0809  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:12 | INFO | OK  map_id=0819  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:13 | INFO | OK  map_id=0848  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:18 | INFO | OK  map_id=0853  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:21 | INFO | OK  map_id=0854  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:23 | INFO | OK  map_id=0856  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:24 | INFO | OK  map_id=0857  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:37 | INFO | OK  map_id=0858  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:40 | INFO | OK  map_id=0859  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:41 | INFO | OK  map_id=0867  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:48 | INFO | OK  map_id=0868  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:49 | INFO | OK  map_id=0869  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:51 | INFO | OK  map_id=0901  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:55 | INFO | OK  map_id=0903  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:58 | INFO | OK  map_id=0904  -> pair_vec[996] (per_map_dim=249)\n",
            "19:39:59 | INFO | OK  map_id=0905  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:12 | INFO | OK  map_id=0906  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:13 | INFO | OK  map_id=0907  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:15 | INFO | OK  map_id=0908  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:18 | INFO | OK  map_id=0917  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:21 | INFO | OK  map_id=0918  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:25 | INFO | OK  map_id=0926  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:27 | INFO | OK  map_id=0947  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:33 | INFO | OK  map_id=0948  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:38 | INFO | OK  map_id=0949  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:39 | INFO | OK  map_id=0950  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:40 | INFO | OK  map_id=0951  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:43 | INFO | OK  map_id=0952  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:46 | INFO | OK  map_id=0966  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:52 | INFO | OK  map_id=0967  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:53 | INFO | OK  map_id=0970  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:56 | INFO | OK  map_id=0971  -> pair_vec[996] (per_map_dim=249)\n",
            "19:40:59 | INFO | OK  map_id=0974  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:01 | INFO | OK  map_id=0975  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:03 | INFO | OK  map_id=0976  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:04 | INFO | OK  map_id=0994  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:07 | INFO | OK  map_id=0995  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:09 | INFO | OK  map_id=0997  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:14 | INFO | OK  map_id=0998  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:16 | INFO | OK  map_id=1019  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:23 | INFO | OK  map_id=1020  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:27 | INFO | OK  map_id=1052  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:31 | INFO | OK  map_id=1053  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:35 | INFO | OK  map_id=1054  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:36 | INFO | OK  map_id=1055  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:38 | INFO | OK  map_id=1056  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:40 | INFO | OK  map_id=1057  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:45 | INFO | OK  map_id=1069  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:46 | INFO | OK  map_id=1070  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:49 | INFO | OK  map_id=1090  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:52 | INFO | OK  map_id=1091  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:53 | INFO | OK  map_id=1092  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:54 | INFO | OK  map_id=1100  -> pair_vec[996] (per_map_dim=249)\n",
            "19:41:57 | INFO | OK  map_id=1103  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:00 | INFO | OK  map_id=1105  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:03 | INFO | OK  map_id=1106  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:05 | INFO | OK  map_id=1118  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:07 | INFO | OK  map_id=1119  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:10 | INFO | OK  map_id=1120  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:12 | INFO | OK  map_id=1139  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:14 | INFO | OK  map_id=1140  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:16 | INFO | OK  map_id=1148  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:17 | INFO | OK  map_id=1155  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:21 | INFO | OK  map_id=1157  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:26 | INFO | OK  map_id=1168  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:28 | INFO | OK  map_id=1169  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:30 | INFO | OK  map_id=1170  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:33 | INFO | OK  map_id=1197  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:36 | INFO | OK  map_id=1198  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:38 | INFO | OK  map_id=1202  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:44 | INFO | OK  map_id=1203  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:46 | INFO | OK  map_id=1204  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:47 | INFO | OK  map_id=1217  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:52 | INFO | OK  map_id=1218  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:56 | INFO | OK  map_id=1219  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:57 | INFO | OK  map_id=1221  -> pair_vec[996] (per_map_dim=249)\n",
            "19:42:58 | INFO | OK  map_id=1222  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:00 | INFO | OK  map_id=1231  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:02 | INFO | OK  map_id=1233  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:04 | INFO | OK  map_id=1234  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:06 | INFO | OK  map_id=1261  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:08 | INFO | OK  map_id=1269  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:17 | INFO | OK  map_id=1270  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:18 | INFO | OK  map_id=1271  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:20 | INFO | OK  map_id=1276  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:23 | INFO | OK  map_id=1277  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:25 | INFO | OK  map_id=1283  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:28 | INFO | OK  map_id=1284  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:30 | INFO | OK  map_id=1285  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:35 | INFO | OK  map_id=1295  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:40 | INFO | OK  map_id=1296  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:41 | INFO | OK  map_id=1297  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:43 | INFO | OK  map_id=1303  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:45 | INFO | OK  map_id=1304  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:46 | INFO | OK  map_id=1310  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:52 | INFO | OK  map_id=1319  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:55 | INFO | OK  map_id=1333  -> pair_vec[996] (per_map_dim=249)\n",
            "19:43:56 | INFO | OK  map_id=1334  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:02 | INFO | OK  map_id=1344  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:03 | INFO | OK  map_id=1349  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:06 | INFO | OK  map_id=1364  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:12 | INFO | OK  map_id=1365  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:16 | INFO | OK  map_id=1366  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:20 | INFO | OK  map_id=1367  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:22 | INFO | OK  map_id=1368  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:24 | INFO | OK  map_id=1369  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:25 | INFO | OK  map_id=1377  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:27 | INFO | OK  map_id=1378  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:29 | INFO | OK  map_id=1385  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:32 | INFO | OK  map_id=1386  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:34 | INFO | OK  map_id=1399  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:35 | INFO | OK  map_id=1401  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:38 | INFO | OK  map_id=1408  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:42 | INFO | OK  map_id=1409  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:44 | INFO | OK  map_id=1410  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:46 | INFO | OK  map_id=1413  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:51 | INFO | OK  map_id=1414  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:53 | INFO | OK  map_id=1415  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:55 | INFO | OK  map_id=1416  -> pair_vec[996] (per_map_dim=249)\n",
            "19:44:56 | INFO | OK  map_id=1417  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:01 | INFO | OK  map_id=1418  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:03 | INFO | OK  map_id=1434  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:04 | INFO | OK  map_id=1438  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:09 | INFO | OK  map_id=1439  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:11 | INFO | OK  map_id=1450  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:13 | INFO | OK  map_id=1451  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:19 | INFO | OK  map_id=1458  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:22 | INFO | OK  map_id=1459  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:23 | INFO | OK  map_id=1460  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:25 | INFO | OK  map_id=1465  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:29 | INFO | OK  map_id=1466  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:31 | INFO | OK  map_id=1467  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:34 | INFO | OK  map_id=1473  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:36 | INFO | OK  map_id=1474  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:38 | INFO | OK  map_id=1476  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:41 | INFO | OK  map_id=1479  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:42 | INFO | OK  map_id=1486  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:44 | INFO | OK  map_id=1487  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:46 | INFO | OK  map_id=1496  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:48 | INFO | OK  map_id=1500  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:50 | INFO | OK  map_id=1501  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:53 | INFO | OK  map_id=1507  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:55 | INFO | OK  map_id=1508  -> pair_vec[996] (per_map_dim=249)\n",
            "19:45:59 | INFO | OK  map_id=1509  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:01 | INFO | OK  map_id=1514  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:04 | INFO | OK  map_id=1515  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:06 | INFO | OK  map_id=1557  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:08 | INFO | OK  map_id=1563  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:13 | INFO | OK  map_id=1564  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:17 | INFO | OK  map_id=1565  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:19 | INFO | OK  map_id=1570  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:20 | INFO | OK  map_id=1579  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:22 | INFO | OK  map_id=1580  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:24 | INFO | OK  map_id=1583  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:25 | INFO | OK  map_id=1584  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:27 | INFO | OK  map_id=1598  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:29 | INFO | OK  map_id=1613  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:30 | INFO | OK  map_id=1614  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:33 | INFO | OK  map_id=1618  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:34 | INFO | OK  map_id=1619  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:36 | INFO | OK  map_id=1629  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:42 | INFO | OK  map_id=1630  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:43 | INFO | OK  map_id=1631  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:45 | INFO | OK  map_id=1647  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:47 | INFO | OK  map_id=1649  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:48 | INFO | OK  map_id=1650  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:50 | INFO | OK  map_id=1653  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:52 | INFO | OK  map_id=1666  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:55 | INFO | OK  map_id=1667  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:57 | INFO | OK  map_id=1672  -> pair_vec[996] (per_map_dim=249)\n",
            "19:46:59 | INFO | OK  map_id=1673  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:01 | INFO | OK  map_id=1679  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:03 | INFO | OK  map_id=1691  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:05 | INFO | OK  map_id=1696  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:11 | INFO | OK  map_id=1700  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:16 | INFO | OK  map_id=1702  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:20 | INFO | OK  map_id=1703  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:22 | INFO | OK  map_id=1709  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:23 | INFO | OK  map_id=1710  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:25 | INFO | OK  map_id=1748  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:27 | INFO | OK  map_id=1749  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:29 | INFO | OK  map_id=1750  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:32 | INFO | OK  map_id=1751  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:33 | INFO | OK  map_id=1752  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:35 | INFO | OK  map_id=1755  -> pair_vec[996] (per_map_dim=249)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pair map embeddings completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "19:47:37 | INFO | OK  map_id=1757  -> pair_vec[996] (per_map_dim=249)\n",
            "19:47:37 | INFO | Saved 300 pair vectors (failed=0) to /Users/amirdonyadide/Documents/Semester_5/Thesis/CODES/data/output/pair_map_out\n"
          ]
        }
      ],
      "source": [
        "# notebook snippet\n",
        "import sys, subprocess, pathlib\n",
        "cmd = [\n",
        "    sys.executable, \"-m\", \"src.mapvec.maps.pair_map_embeddings\",\n",
        "    \"--root\", str(MAPS_ROOT),\n",
        "    \"--input_pattern\", str(INPUT_MAPS_PATTERN),\n",
        "    \"--gen_pattern\", str(OUTPUT_MAPS_PATTERN),\n",
        "    \"--out_dir\", str(PAIR_MAP_OUT),\n",
        "    \"-v\"\n",
        "]\n",
        "print(\"CMD:\", \" \".join(cmd))\n",
        "res = subprocess.run(cmd, cwd=str(PROJ_ROOT))\n",
        "if res.returncode != 0:\n",
        "    raise SystemExit(\"Pair map embedding step failed.\")\n",
        "print(\"Pair map embeddings completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Concatenate\n",
        "Joins map & prompt vectors using `pairs.csv` and writes `X_concat.npy` and `train_pairs.parquet` to `TRAIN_OUT`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CMD: /opt/anaconda3/envs/thesis/bin/python -m src.mapvec.concat.concat_embeddings --pairs ../data/input/pairs.csv --map_npz ../data/output/pair_map_out/embeddings.npz --prompt_npz ../data/output/prompt_out/embeddings.npz --out_dir ../data/output/train_out --drop_dupes\n",
            "Concatenation completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20:46:50 | INFO | Map  embeddings: (300, 996) from /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/pair_map_out/embeddings.npz\n",
            "20:46:50 | INFO | Prompt embeddings: (500, 512) from /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/embeddings.npz\n",
            "20:46:50 | INFO | X shape = (450, 1508)  (map_dim=996, prompt_dim=512)\n",
            "20:46:50 | INFO | Saved to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out in 0.03s\n"
          ]
        }
      ],
      "source": [
        "import sys, subprocess\n",
        "\n",
        "cmd = [\n",
        "    sys.executable, \"-m\", \"src.mapvec.concat.concat_embeddings\",\n",
        "    \"--pairs\",      str(PAIRS_CSV),\n",
        "    \"--map_npz\",    str(PAIR_MAP_OUT / \"embeddings.npz\"),   # from pair_map_out\n",
        "    \"--prompt_npz\", str(PROMPT_OUT / \"embeddings.npz\"),\n",
        "    \"--out_dir\",    str(TRAIN_OUT),\n",
        "    \"--drop_dupes\",                                   # optional: drop duplicate (map_id,prompt_id)\n",
        "    # \"--fail_on_missing\",                            # optional: stop instead of skipping missing IDs\n",
        "]\n",
        "print(\"CMD:\", \" \".join(cmd))\n",
        "\n",
        "# Run from the project root so src/ is importable\n",
        "res = subprocess.run(cmd, cwd=str(PROJ_ROOT))\n",
        "if res.returncode != 0:\n",
        "    raise SystemExit(\"Concatenation step failed.\")\n",
        "print(\"Concatenation completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b997cab7",
      "metadata": {},
      "source": [
        "## 4) Split dataset  \n",
        "Splits the concatenated feature matrix `X_concat.npy` and its metadata `train_pairs.parquet` into separate **training**, **validation**, and **test** subsets.  \n",
        "Each split preserves row alignment between features and metadata, and the resulting files are saved under `TRAIN_OUT/splits/` as:  \n",
        "\n",
        "- `X_train.npy`, `pairs_train.parquet`  \n",
        "- `X_val.npy`, `pairs_val.parquet`  \n",
        "- `X_test.npy`, `pairs_test.parquet`  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3bc0897d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded X: (450, 1508), pairs: (450, 2)\n",
            "Train: (315, 1508), Val: (67, 1508), Test: (68, 1508)\n",
            "Saved splits to ../data/output/train_out/splits\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ==== CONFIG ====\n",
        "TRAIN_OUT = Path(TRAIN_OUT)\n",
        "VAL_RATIO  = 0.15     # 15% validation\n",
        "TEST_RATIO = 0.15     # 15% test (remaining 70% train)\n",
        "SEED       = 42       # reproducibility\n",
        "# =================\n",
        "\n",
        "# Load data\n",
        "X = np.load(TRAIN_OUT / \"X_concat.npy\")\n",
        "pairs_df = pd.read_parquet(TRAIN_OUT / \"train_pairs.parquet\")\n",
        "\n",
        "print(f\"Loaded X: {X.shape}, pairs: {pairs_df.shape}\")\n",
        "\n",
        "# --- Step 1: Train/Test split\n",
        "X_train, X_temp, df_train, df_temp = train_test_split(\n",
        "    X, pairs_df, test_size=VAL_RATIO + TEST_RATIO, random_state=SEED, shuffle=True\n",
        ")\n",
        "\n",
        "# --- Step 2: Split temp into Val/Test\n",
        "relative_test_ratio = TEST_RATIO / (VAL_RATIO + TEST_RATIO)\n",
        "X_val, X_test, df_val, df_test = train_test_split(\n",
        "    X_temp, df_temp, test_size=relative_test_ratio, random_state=SEED, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "# --- Save splits\n",
        "split_dir = TRAIN_OUT / \"splits\"\n",
        "split_dir.mkdir(exist_ok=True)\n",
        "\n",
        "np.save(split_dir / \"X_train.npy\", X_train)\n",
        "np.save(split_dir / \"X_val.npy\",   X_val)\n",
        "np.save(split_dir / \"X_test.npy\",  X_test)\n",
        "\n",
        "df_train.to_parquet(split_dir / \"pairs_train.parquet\", index=False)\n",
        "df_val.to_parquet(split_dir / \"pairs_val.parquet\", index=False)\n",
        "df_test.to_parquet(split_dir / \"pairs_test.parquet\", index=False)\n",
        "\n",
        "print(f\"Saved splits to {split_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9ace9893",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Balanced sizes -> train: (1254, 1508) val: (266, 1508) test: (270, 1508) | class balance (train): [939 315]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'val': {'accuracy': 0.4849624060150376,\n",
              "  'f1': 0.33170731707317075,\n",
              "  'auroc': 0.493662341558539},\n",
              " 'test': {'accuracy': 0.5185185185185185,\n",
              "  'f1': 0.3434343434343434,\n",
              "  'auroc': 0.49919918462434487}}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 5) Self-supervised baseline (no labels.csv needed)\n",
        "# Build features for positives and for shuffled negatives directly from embeddings.\n",
        "\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "NEG_PER_POS = 3\n",
        "RNG_SEED    = 42\n",
        "\n",
        "# --- paths to the original embeddings used for concatenation\n",
        "MAP_NPZ = DATA_DIR / 'output' / \"pair_map_out\" / \"embeddings.npz\"\n",
        "PRM_NPZ = DATA_DIR / \"output\" / \"prompt_out\" / \"embeddings.npz\"\n",
        "\n",
        "def load_npz(npz_path: Path):\n",
        "    z = np.load(npz_path, allow_pickle=True)\n",
        "    E   = z[\"E\"]\n",
        "    ids = [str(x) for x in z[\"ids\"].tolist()]\n",
        "    return E, ids\n",
        "\n",
        "E_map, map_ids = load_npz(MAP_NPZ)   # shape (M, Dm)\n",
        "E_prm, prm_ids = load_npz(PRM_NPZ)   # shape (P, Dp)\n",
        "idx_map = {k:i for i,k in enumerate(map_ids)}\n",
        "idx_prm = {k:i for i,k in enumerate(prm_ids)}\n",
        "\n",
        "# --- load split ID tables (created in the previous cell)\n",
        "J_tr = pd.read_parquet(split_dir / \"pairs_train.parquet\")\n",
        "J_va = pd.read_parquet(split_dir / \"pairs_val.parquet\")\n",
        "J_te = pd.read_parquet(split_dir / \"pairs_test.parquet\")\n",
        "\n",
        "rng = np.random.default_rng(RNG_SEED)\n",
        "\n",
        "def make_pos_neg_tbl(join_df: pd.DataFrame, neg_per_pos: int) -> pd.DataFrame:\n",
        "    join_df = join_df[[\"map_id\",\"prompt_id\"]].astype(str).copy()\n",
        "    pos = join_df.copy()\n",
        "    pos[\"y\"] = 1\n",
        "\n",
        "    negl = []\n",
        "    maps = join_df[\"map_id\"].to_numpy()\n",
        "    prms = join_df[\"prompt_id\"].to_numpy()\n",
        "    for _ in range(neg_per_pos):\n",
        "        shuf = prms.copy()\n",
        "        rng.shuffle(shuf)\n",
        "        neg = pd.DataFrame({\"map_id\": maps, \"prompt_id\": shuf})\n",
        "        # drop any accidental positives\n",
        "        neg = neg.merge(pos[[\"map_id\",\"prompt_id\"]], on=[\"map_id\",\"prompt_id\"],\n",
        "                        how=\"left\", indicator=True)\n",
        "        neg = neg[neg[\"_merge\"]==\"left_only\"].drop(columns=\"_merge\")\n",
        "        negl.append(neg)\n",
        "\n",
        "    neg_all = pd.concat(negl, ignore_index=True)\n",
        "    neg_all[\"y\"] = 0\n",
        "\n",
        "    all_tbl = pd.concat([pos, neg_all], ignore_index=True)\n",
        "    all_tbl = all_tbl.sample(frac=1.0, random_state=RNG_SEED).reset_index(drop=True)\n",
        "    return all_tbl\n",
        "\n",
        "def table_to_features(tbl: pd.DataFrame) -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Map (map_id,prompt_id) pairs to (E_map | E_prm) features.\"\"\"\n",
        "    m_idx = [idx_map.get(m, -1) for m in tbl[\"map_id\"].astype(str)]\n",
        "    p_idx = [idx_prm.get(p, -1) for p in tbl[\"prompt_id\"].astype(str)]\n",
        "    keep  = [i for i,(im,ip) in enumerate(zip(m_idx, p_idx)) if im >= 0 and ip >= 0]\n",
        "    if not keep:\n",
        "        raise SystemExit(\"No pairs could be mapped to embeddings. Check IDs.\")\n",
        "    m_idx = np.asarray([m_idx[i] for i in keep], dtype=int)\n",
        "    p_idx = np.asarray([p_idx[i] for i in keep], dtype=int)\n",
        "    X = np.hstack([E_map[m_idx], E_prm[p_idx]]).astype(np.float32, copy=False)\n",
        "    y = tbl.iloc[keep][\"y\"].to_numpy(dtype=int)\n",
        "    return X, y\n",
        "\n",
        "# Build features/labels for each split\n",
        "Y_tr_tbl = make_pos_neg_tbl(J_tr, NEG_PER_POS)\n",
        "Y_va_tbl = make_pos_neg_tbl(J_va, NEG_PER_POS)\n",
        "Y_te_tbl = make_pos_neg_tbl(J_te, NEG_PER_POS)\n",
        "\n",
        "X_tr_b, y_tr = table_to_features(Y_tr_tbl)\n",
        "X_va_b, y_va = table_to_features(Y_va_tbl)\n",
        "X_te_b, y_te = table_to_features(Y_te_tbl)\n",
        "\n",
        "print(\"Balanced sizes ->\",\n",
        "      \"train:\", X_tr_b.shape, \"val:\", X_va_b.shape, \"test:\", X_te_b.shape,\n",
        "      \"| class balance (train):\", np.bincount(y_tr))\n",
        "\n",
        "# --- scale (fit on train only)\n",
        "scaler = StandardScaler()\n",
        "X_tr_s = scaler.fit_transform(X_tr_b)\n",
        "X_va_s = scaler.transform(X_va_b)\n",
        "X_te_s = scaler.transform(X_te_b)\n",
        "\n",
        "# --- train + eval\n",
        "clf = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
        "clf.fit(X_tr_s, y_tr)\n",
        "\n",
        "def eval_split(Xs, ys):\n",
        "    pred = clf.predict(Xs)\n",
        "    proba = clf.predict_proba(Xs)[:, 1]\n",
        "    return {\n",
        "        \"accuracy\": float(accuracy_score(ys, pred)),\n",
        "        \"f1\": float(f1_score(ys, pred)),\n",
        "        \"auroc\": float(roc_auc_score(ys, proba)),\n",
        "    }\n",
        "\n",
        "metrics = {\"val\": eval_split(X_va_s, y_va),\n",
        "           \"test\": eval_split(X_te_s, y_te)}\n",
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b6e8991e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GradientBoosting: {'val': {'accuracy': 0.40601503759398494, 'f1': 0.3875968992248062, 'auroc': 0.538438460961524}, 'test': {'accuracy': 0.4666666666666667, 'f1': 0.3898305084745763, 'auroc': 0.5446272568433315}}\n",
            "LogisticCV: {'val': {'accuracy': 0.45112781954887216, 'f1': 0.3706896551724138, 'auroc': 0.5031125778144454}, 'test': {'accuracy': 0.4962962962962963, 'f1': 0.3644859813084112, 'auroc': 0.505824111822947}}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import resample\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# ---- set this to your actual map dimension ----\n",
        "MAP_DIM = 996  # map | prompt -> 996 + 512 = 1508\n",
        "# -----------------------------------------------\n",
        "\n",
        "def split_blocks(X):\n",
        "    Xm = X[:, :MAP_DIM]\n",
        "    Xp = X[:, MAP_DIM:]\n",
        "    return Xm, Xp\n",
        "\n",
        "def block_scale_fit(X):\n",
        "    Xm, Xp = split_blocks(X)\n",
        "    sm = StandardScaler().fit(Xm)\n",
        "    sp = StandardScaler().fit(Xp)\n",
        "    Xs = np.hstack([sm.transform(Xm), sp.transform(Xp)])\n",
        "    return sm, sp, Xs\n",
        "\n",
        "def block_scale_transform(X, sm, sp):\n",
        "    Xm, Xp = split_blocks(X)\n",
        "    return np.hstack([sm.transform(Xm), sp.transform(Xp)])\n",
        "\n",
        "# We already have: X_tr_b, y_tr, X_va_b, y_va, X_te_b, y_te\n",
        "sm, sp, X_tr_s = block_scale_fit(X_tr_b)\n",
        "X_va_s = block_scale_transform(X_va_b, sm, sp)\n",
        "X_te_s = block_scale_transform(X_te_b, sm, sp)\n",
        "\n",
        "# ---- Rebalance train to 1:1 (downsample the majority class) ----\n",
        "pos_mask = (y_tr == 1)\n",
        "neg_mask = ~pos_mask\n",
        "X_pos, y_pos = X_tr_s[pos_mask], y_tr[pos_mask]\n",
        "X_neg, y_neg = X_tr_s[neg_mask], y_tr[neg_mask]\n",
        "\n",
        "if len(X_pos) > 0 and len(X_neg) > 0:\n",
        "    if len(X_neg) > len(X_pos):\n",
        "        X_neg_b, y_neg_b = resample(X_neg, y_neg, replace=False, n_samples=len(X_pos), random_state=42)\n",
        "        X_bal = np.vstack([X_pos, X_neg_b])\n",
        "        y_bal = np.hstack([y_pos, y_neg_b])\n",
        "    else:\n",
        "        X_pos_b, y_pos_b = resample(X_pos, y_pos, replace=False, n_samples=len(X_neg), random_state=42)\n",
        "        X_bal = np.vstack([X_pos_b, X_neg])\n",
        "        y_bal = np.hstack([y_pos_b, y_neg])\n",
        "else:\n",
        "    # fallback if something went wrong\n",
        "    X_bal, y_bal = X_tr_s, y_tr\n",
        "\n",
        "# ---- Try a stronger baseline (Gradient Boosting) ----\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "gb.fit(X_bal, y_bal)\n",
        "\n",
        "def eval_model(model, Xs, ys):\n",
        "    pred = model.predict(Xs)\n",
        "    try:\n",
        "        proba = model.predict_proba(Xs)[:, 1]\n",
        "        auroc = float(roc_auc_score(ys, proba))\n",
        "    except Exception:\n",
        "        auroc = None\n",
        "    out = {\n",
        "        \"accuracy\": float(accuracy_score(ys, pred)),\n",
        "        \"f1\": float(f1_score(ys, pred)),\n",
        "    }\n",
        "    if auroc is not None and len(np.unique(ys)) == 2:\n",
        "        out[\"auroc\"] = auroc\n",
        "    return out\n",
        "\n",
        "metrics_gb = {\n",
        "    \"val\":  eval_model(gb, X_va_s, y_va),\n",
        "    \"test\": eval_model(gb, X_te_s, y_te),\n",
        "}\n",
        "print(\"GradientBoosting:\", metrics_gb)\n",
        "\n",
        "# ---- Also try a tuned logistic as a comparison ----\n",
        "logcv = LogisticRegressionCV(\n",
        "    Cs=10, cv=5, max_iter=3000, class_weight=\"balanced\", scoring=\"roc_auc\", n_jobs=None\n",
        ")\n",
        "logcv.fit(X_bal, y_bal)\n",
        "\n",
        "metrics_lr = {\n",
        "    \"val\":  eval_model(logcv, X_va_s, y_va),\n",
        "    \"test\": eval_model(logcv, X_te_s, y_te),\n",
        "}\n",
        "print(\"LogisticCV:\", metrics_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e3b9fe0",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "thesis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
