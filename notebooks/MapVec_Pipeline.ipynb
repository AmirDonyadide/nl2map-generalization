{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c1f56a2c",
      "metadata": {},
      "source": [
        "# MapVec end-to-end pipeline üìí\n",
        "\n",
        "This notebook runs the **entire pipeline**:\n",
        "1. Prompt embeddings (Universal Sentence Encoder)\n",
        "2. Map embeddings (handcrafted polygon features)\n",
        "3. Concatenation into a training matrix\n",
        "4. Helper cells to inspect vectors by `prompt_id` or `map_id`\n",
        "\n",
        "**Edit the Parameters** in the next cell to match your project layout.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================== PARAMETERS =====================\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shlex\n",
        "import joblib\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, mean_squared_error, mean_absolute_error\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "PROJ_ROOT = Path(\"../\").resolve()       # adjust if your notebook sits elsewhere\n",
        "SRC_DIR   = PROJ_ROOT / \"src\"\n",
        "# Put project root on sys.path\n",
        "if str(PROJ_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJ_ROOT))\n",
        "\n",
        "from src.config import PATHS, CFG, print_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9ed0df45",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== CONFIG SUMMARY ===\n",
            "PROJ_ROOT  : /Users/amirdonyadide/Documents/GitHub/Thesis\n",
            "DATA_DIR   : /Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
            "INPUT_DIR  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input\n",
            "OUTPUT_DIR : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output\n",
            "MAPS_ROOT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/samples/pairs\n",
            "INPUT PAT. : *_input.geojson\n",
            "PROMPTS_CSV: /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/prompts.csv\n",
            "PAIRS_CSV  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/pairs.csv\n",
            "PROMPT_OUT : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
            "MAP_OUT    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n",
            "TRAIN_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out\n",
            "MODEL_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models\n",
            "SPLIT_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/splits\n",
            "PRM_NPZ    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/prompts_embeddings.npz\n",
            "--- Model ---\n",
            "USE_MODEL  : dan\n",
            "MAP_DIM    : 249\n",
            "PROMPT_DIM : 512\n",
            "FUSED_DIM  : 761\n",
            "BATCH_SIZE : 512\n",
            "VAL/TEST   : 0.15 0.15\n",
            "SEED       : 42\n"
          ]
        }
      ],
      "source": [
        "print_summary()  # optional\n",
        "\n",
        "# Access paths like:\n",
        "MAPS_ROOT = PATHS.MAPS_ROOT\n",
        "INPUT_MAPS_PATTERN = PATHS.INPUT_MAPS_PATTERN\n",
        "PROMPTS_CSV = PATHS.PROMPTS_CSV\n",
        "PRM_NPZ = PATHS.PRM_NPZ\n",
        "\n",
        "# Dims (auto-inferred if available; else None until you set them)\n",
        "MAP_DIM = CFG.MAP_DIM or 996         # fallback\n",
        "PROMPT_DIM = CFG.PROMPT_DIM or 512   # fallback\n",
        "FUSED_DIM = CFG.FUSED_DIM or (MAP_DIM + PROMPT_DIM)\n",
        "BATCH_SIZE = CFG.BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "1e8f95c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
            "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n",
            "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out\n",
            "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models\n",
            "‚úÖ All output folders cleaned and recreated fresh.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "PATHS.clean_outputs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Prompt embeddings\n",
        "Runs `src/mapvec/prompts/prompt_embeddings.py` using your chosen USE model and saves artifacts to `PROMPT_OUT`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CMD: /opt/anaconda3/envs/thesis/bin/python -m src.mapvec.prompts.prompt_embeddings --input /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/prompts.csv --model dan --l2 --out_dir /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out -v\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "15:59:39 | DEBUG | FILE_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/src/mapvec/prompts\n",
            "15:59:39 | DEBUG | PROJECT_ROOT=/Users/amirdonyadide/Documents/GitHub/Thesis\n",
            "15:59:39 | DEBUG | DEFAULT_DATA_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
            "15:59:39 | INFO | DATA_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
            "15:59:39 | INFO | INPUT=/Users/amirdonyadide/Documents/GitHub/Thesis/data/input/prompts.csv\n",
            "15:59:39 | INFO | OUT_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
            "15:59:39 | INFO | Reading CSV: /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/prompts.csv\n",
            "15:59:39 | INFO | Loaded 500 prompts (id_col=prompt_id). Sample IDs: p001, p002, p003‚Ä¶\n",
            "15:59:39 | INFO | Using local USE-dan at /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/model_dan\n",
            "15:59:39 | INFO | Loading USE-dan from local path: /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/model_dan ‚Ä¶\n",
            "15:59:42 | INFO | Fingerprint not found. Saved model loading will continue.\n",
            "15:59:42 | INFO | path_and_singleprint metric could not be logged. Saved model loading will continue.\n",
            "15:59:42 | INFO | Model loaded in 2.83s\n",
            "15:59:42 | INFO | Embedding 500 prompts (batch_size=512, l2=True)‚Ä¶\n",
            "15:59:42 | DEBUG |   embedded rows [1:500)\n",
            "15:59:42 | INFO | Done embedding in 0.12s (dim=512).\n",
            "15:59:42 | INFO | Writing outputs to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
            "15:59:42 | INFO |   saved prompts_embeddings.npz (shape=(500, 512))\n",
            "15:59:42 | INFO |   saved prompts.parquet (rows=500)\n",
            "15:59:42 | INFO |   saved meta.json\n",
            "15:59:42 | INFO | All done ‚úÖ\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt embeddings completed.\n"
          ]
        }
      ],
      "source": [
        "cmd = [\n",
        "    sys.executable, \"-m\", \"src.mapvec.prompts.prompt_embeddings\",\n",
        "    \"--input\",    str(PATHS.PROMPTS_CSV),\n",
        "    \"--model\",    str(CFG.USE_MODEL),\n",
        "    \"--l2\",       \n",
        "    \"--out_dir\",  str(PATHS.PROMPT_OUT),\n",
        "    \"-v\"\n",
        "]\n",
        "print(\"CMD:\", \" \".join(cmd))\n",
        "res = subprocess.run(cmd, cwd=str(PATHS.PROJ_ROOT))  # shell=False by default\n",
        "if res.returncode != 0:\n",
        "    raise SystemExit(\"Prompt embedding step failed.\")\n",
        "print(\"Prompt embeddings completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Map embeddings\n",
        "Runs the map embedding module on the GeoJSON inputs. Skips problematic features, logs warnings, and writes `embeddings.npz` to `PAIR_MAP_OUT`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "fa2b07a9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CMD: /opt/anaconda3/envs/thesis/bin/python -m src.mapvec.maps.map_embeddings --root /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/samples/pairs --pattern *_input.geojson --out_dir /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out -v\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "15:42:14 | DEBUG | PROJECT_ROOT=/Users/amirdonyadide/Documents/GitHub/Thesis\n",
            "15:42:14 | DEBUG | DATA_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
            "15:42:14 | INFO | Scanning /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/samples/pairs (pattern=*_input.geojson)‚Ä¶\n",
            "15:42:15 | INFO | OK  map_id=0073  -> vector[249]\n",
            "15:42:17 | INFO | OK  map_id=0080  -> vector[249]\n",
            "15:42:17 | INFO | OK  map_id=0093  -> vector[249]\n",
            "15:42:21 | INFO | OK  map_id=0122  -> vector[249]\n",
            "15:42:22 | INFO | OK  map_id=0123  -> vector[249]\n",
            "15:42:23 | INFO | OK  map_id=0127  -> vector[249]\n",
            "15:42:24 | INFO | OK  map_id=0158  -> vector[249]\n",
            "15:42:26 | INFO | OK  map_id=0159  -> vector[249]\n",
            "15:42:27 | INFO | OK  map_id=0160  -> vector[249]\n",
            "15:42:28 | INFO | OK  map_id=0165  -> vector[249]\n",
            "15:42:29 | INFO | OK  map_id=0167  -> vector[249]\n",
            "15:42:30 | INFO | OK  map_id=0168  -> vector[249]\n",
            "15:42:31 | INFO | OK  map_id=0171  -> vector[249]\n",
            "15:42:34 | INFO | OK  map_id=0208  -> vector[249]\n",
            "15:42:38 | INFO | OK  map_id=0209  -> vector[249]\n",
            "15:42:39 | INFO | OK  map_id=0215  -> vector[249]\n",
            "15:42:40 | INFO | OK  map_id=0240  -> vector[249]\n",
            "15:42:41 | INFO | OK  map_id=0256  -> vector[249]\n",
            "15:42:41 | INFO | OK  map_id=0257  -> vector[249]\n",
            "15:42:42 | INFO | OK  map_id=0262  -> vector[249]\n",
            "15:42:44 | INFO | OK  map_id=0285  -> vector[249]\n",
            "15:42:46 | INFO | OK  map_id=0286  -> vector[249]\n",
            "15:42:49 | INFO | OK  map_id=0288  -> vector[249]\n",
            "15:42:52 | INFO | OK  map_id=0289  -> vector[249]\n",
            "15:42:53 | INFO | OK  map_id=0313  -> vector[249]\n",
            "15:42:53 | INFO | OK  map_id=0341  -> vector[249]\n",
            "15:42:54 | INFO | OK  map_id=0362  -> vector[249]\n",
            "15:42:57 | INFO | OK  map_id=0363  -> vector[249]\n",
            "15:42:58 | INFO | OK  map_id=0364  -> vector[249]\n",
            "15:42:59 | INFO | OK  map_id=0379  -> vector[249]\n",
            "15:43:00 | INFO | OK  map_id=0389  -> vector[249]\n",
            "15:43:02 | INFO | OK  map_id=0390  -> vector[249]\n",
            "15:43:04 | INFO | OK  map_id=0409  -> vector[249]\n",
            "15:43:05 | INFO | OK  map_id=0410  -> vector[249]\n",
            "15:43:07 | INFO | OK  map_id=0412  -> vector[249]\n",
            "15:43:08 | INFO | OK  map_id=0413  -> vector[249]\n",
            "15:43:09 | INFO | OK  map_id=0414  -> vector[249]\n",
            "15:43:10 | INFO | OK  map_id=0417  -> vector[249]\n",
            "15:43:11 | INFO | OK  map_id=0421  -> vector[249]\n",
            "15:43:12 | INFO | OK  map_id=0426  -> vector[249]\n",
            "15:43:13 | INFO | OK  map_id=0427  -> vector[249]\n",
            "15:43:14 | INFO | OK  map_id=0432  -> vector[249]\n",
            "15:43:15 | INFO | OK  map_id=0433  -> vector[249]\n",
            "15:43:17 | INFO | OK  map_id=0437  -> vector[249]\n",
            "15:43:19 | INFO | OK  map_id=0438  -> vector[249]\n",
            "15:43:20 | INFO | OK  map_id=0439  -> vector[249]\n",
            "15:43:22 | INFO | OK  map_id=0454  -> vector[249]\n",
            "15:43:23 | INFO | OK  map_id=0458  -> vector[249]\n",
            "15:43:25 | INFO | OK  map_id=0459  -> vector[249]\n",
            "15:43:26 | INFO | OK  map_id=0460  -> vector[249]\n",
            "15:43:27 | INFO | OK  map_id=0466  -> vector[249]\n",
            "15:43:28 | INFO | OK  map_id=0469  -> vector[249]\n",
            "15:43:31 | INFO | OK  map_id=0471  -> vector[249]\n",
            "15:43:32 | INFO | OK  map_id=0472  -> vector[249]\n",
            "15:43:35 | INFO | OK  map_id=0474  -> vector[249]\n",
            "15:43:37 | INFO | OK  map_id=0475  -> vector[249]\n",
            "15:43:38 | INFO | OK  map_id=0479  -> vector[249]\n",
            "15:43:39 | INFO | OK  map_id=0480  -> vector[249]\n",
            "15:43:40 | INFO | OK  map_id=0481  -> vector[249]\n",
            "15:43:41 | INFO | OK  map_id=0482  -> vector[249]\n",
            "15:43:41 | INFO | OK  map_id=0508  -> vector[249]\n",
            "15:43:43 | INFO | OK  map_id=0509  -> vector[249]\n",
            "15:43:44 | INFO | OK  map_id=0518  -> vector[249]\n",
            "15:43:50 | INFO | OK  map_id=0520  -> vector[249]\n",
            "15:43:52 | INFO | OK  map_id=0521  -> vector[249]\n",
            "15:43:53 | INFO | OK  map_id=0523  -> vector[249]\n",
            "15:43:55 | INFO | OK  map_id=0527  -> vector[249]\n",
            "15:43:56 | INFO | OK  map_id=0528  -> vector[249]\n",
            "15:43:58 | INFO | OK  map_id=0529  -> vector[249]\n",
            "15:43:59 | INFO | OK  map_id=0530  -> vector[249]\n",
            "15:44:00 | INFO | OK  map_id=0553  -> vector[249]\n",
            "15:44:01 | INFO | OK  map_id=0557  -> vector[249]\n",
            "15:44:02 | INFO | OK  map_id=0575  -> vector[249]\n",
            "15:44:03 | INFO | OK  map_id=0576  -> vector[249]\n",
            "15:44:04 | INFO | OK  map_id=0594  -> vector[249]\n",
            "15:44:06 | INFO | OK  map_id=0595  -> vector[249]\n",
            "15:44:06 | INFO | OK  map_id=0600  -> vector[249]\n",
            "15:44:08 | INFO | OK  map_id=0605  -> vector[249]\n",
            "15:44:10 | INFO | OK  map_id=0606  -> vector[249]\n",
            "15:44:11 | INFO | OK  map_id=0608  -> vector[249]\n",
            "15:44:12 | INFO | OK  map_id=0609  -> vector[249]\n",
            "15:44:13 | INFO | OK  map_id=0611  -> vector[249]\n",
            "15:44:15 | INFO | OK  map_id=0614  -> vector[249]\n",
            "15:44:16 | INFO | OK  map_id=0615  -> vector[249]\n",
            "15:44:17 | INFO | OK  map_id=0618  -> vector[249]\n",
            "15:44:19 | INFO | OK  map_id=0623  -> vector[249]\n",
            "15:44:19 | INFO | OK  map_id=0624  -> vector[249]\n",
            "15:44:20 | INFO | OK  map_id=0645  -> vector[249]\n",
            "15:44:21 | INFO | OK  map_id=0646  -> vector[249]\n",
            "15:44:22 | INFO | OK  map_id=0655  -> vector[249]\n",
            "15:44:23 | INFO | OK  map_id=0656  -> vector[249]\n",
            "15:44:24 | INFO | OK  map_id=0657  -> vector[249]\n",
            "15:44:26 | INFO | OK  map_id=0658  -> vector[249]\n",
            "15:44:28 | INFO | OK  map_id=0659  -> vector[249]\n",
            "15:44:29 | INFO | OK  map_id=0667  -> vector[249]\n",
            "15:44:30 | INFO | OK  map_id=0672  -> vector[249]\n",
            "15:44:31 | INFO | OK  map_id=0699  -> vector[249]\n",
            "15:44:33 | INFO | OK  map_id=0700  -> vector[249]\n",
            "15:44:35 | INFO | OK  map_id=0701  -> vector[249]\n",
            "15:44:36 | INFO | OK  map_id=0706  -> vector[249]\n",
            "15:44:37 | INFO | OK  map_id=0707  -> vector[249]\n",
            "15:44:38 | INFO | OK  map_id=0715  -> vector[249]\n",
            "15:44:39 | INFO | OK  map_id=0721  -> vector[249]\n",
            "15:44:41 | INFO | OK  map_id=0747  -> vector[249]\n",
            "15:44:42 | INFO | OK  map_id=0748  -> vector[249]\n",
            "15:44:43 | INFO | OK  map_id=0749  -> vector[249]\n",
            "15:44:45 | INFO | OK  map_id=0755  -> vector[249]\n",
            "15:44:46 | INFO | OK  map_id=0758  -> vector[249]\n",
            "15:44:47 | INFO | OK  map_id=0759  -> vector[249]\n",
            "15:44:47 | INFO | OK  map_id=0762  -> vector[249]\n",
            "15:44:48 | INFO | OK  map_id=0770  -> vector[249]\n",
            "15:44:50 | INFO | OK  map_id=0804  -> vector[249]\n",
            "15:44:51 | INFO | OK  map_id=0807  -> vector[249]\n",
            "15:44:52 | INFO | OK  map_id=0808  -> vector[249]\n",
            "15:44:54 | INFO | OK  map_id=0809  -> vector[249]\n",
            "15:44:57 | INFO | OK  map_id=0819  -> vector[249]\n",
            "15:44:57 | INFO | OK  map_id=0848  -> vector[249]\n",
            "15:45:00 | INFO | OK  map_id=0853  -> vector[249]\n",
            "15:45:02 | INFO | OK  map_id=0854  -> vector[249]\n",
            "15:45:02 | INFO | OK  map_id=0856  -> vector[249]\n",
            "15:45:04 | INFO | OK  map_id=0857  -> vector[249]\n",
            "15:45:10 | INFO | OK  map_id=0858  -> vector[249]\n",
            "15:45:11 | INFO | OK  map_id=0859  -> vector[249]\n",
            "15:45:12 | INFO | OK  map_id=0867  -> vector[249]\n",
            "15:45:16 | INFO | OK  map_id=0868  -> vector[249]\n",
            "15:45:16 | INFO | OK  map_id=0869  -> vector[249]\n",
            "15:45:17 | INFO | OK  map_id=0901  -> vector[249]\n",
            "15:45:19 | INFO | OK  map_id=0903  -> vector[249]\n",
            "15:45:20 | INFO | OK  map_id=0904  -> vector[249]\n",
            "15:45:21 | INFO | OK  map_id=0905  -> vector[249]\n",
            "15:45:27 | INFO | OK  map_id=0906  -> vector[249]\n",
            "15:45:28 | INFO | OK  map_id=0907  -> vector[249]\n",
            "15:45:30 | INFO | OK  map_id=0908  -> vector[249]\n",
            "15:45:33 | INFO | OK  map_id=0917  -> vector[249]\n",
            "15:45:34 | INFO | OK  map_id=0918  -> vector[249]\n",
            "15:45:37 | INFO | OK  map_id=0926  -> vector[249]\n",
            "15:45:38 | INFO | OK  map_id=0947  -> vector[249]\n",
            "15:45:41 | INFO | OK  map_id=0948  -> vector[249]\n",
            "15:45:43 | INFO | OK  map_id=0949  -> vector[249]\n",
            "15:45:44 | INFO | OK  map_id=0950  -> vector[249]\n",
            "15:45:45 | INFO | OK  map_id=0951  -> vector[249]\n",
            "15:45:46 | INFO | OK  map_id=0952  -> vector[249]\n",
            "15:45:49 | INFO | OK  map_id=0966  -> vector[249]\n",
            "15:45:52 | INFO | OK  map_id=0967  -> vector[249]\n",
            "15:45:53 | INFO | OK  map_id=0970  -> vector[249]\n",
            "15:45:54 | INFO | OK  map_id=0971  -> vector[249]\n",
            "15:45:55 | INFO | OK  map_id=0974  -> vector[249]\n",
            "15:45:57 | INFO | OK  map_id=0975  -> vector[249]\n",
            "15:45:58 | INFO | OK  map_id=0976  -> vector[249]\n",
            "15:45:58 | INFO | OK  map_id=0994  -> vector[249]\n",
            "15:46:00 | INFO | OK  map_id=0995  -> vector[249]\n",
            "15:46:01 | INFO | OK  map_id=0997  -> vector[249]\n",
            "15:46:04 | INFO | OK  map_id=0998  -> vector[249]\n",
            "15:46:05 | INFO | OK  map_id=1019  -> vector[249]\n",
            "15:46:09 | INFO | OK  map_id=1020  -> vector[249]\n",
            "15:46:10 | INFO | OK  map_id=1052  -> vector[249]\n",
            "15:46:12 | INFO | OK  map_id=1053  -> vector[249]\n",
            "15:46:14 | INFO | OK  map_id=1054  -> vector[249]\n",
            "15:46:15 | INFO | OK  map_id=1055  -> vector[249]\n",
            "15:46:17 | INFO | OK  map_id=1056  -> vector[249]\n",
            "15:46:18 | INFO | OK  map_id=1057  -> vector[249]\n",
            "15:46:20 | INFO | OK  map_id=1069  -> vector[249]\n",
            "15:46:21 | INFO | OK  map_id=1070  -> vector[249]\n",
            "15:46:22 | INFO | OK  map_id=1090  -> vector[249]\n",
            "15:46:24 | INFO | OK  map_id=1091  -> vector[249]\n",
            "15:46:24 | INFO | OK  map_id=1092  -> vector[249]\n",
            "15:46:25 | INFO | OK  map_id=1100  -> vector[249]\n",
            "15:46:26 | INFO | OK  map_id=1103  -> vector[249]\n",
            "15:46:28 | INFO | OK  map_id=1105  -> vector[249]\n",
            "15:46:29 | INFO | OK  map_id=1106  -> vector[249]\n",
            "15:46:31 | INFO | OK  map_id=1118  -> vector[249]\n",
            "15:46:32 | INFO | OK  map_id=1119  -> vector[249]\n",
            "15:46:34 | INFO | OK  map_id=1120  -> vector[249]\n",
            "15:46:36 | INFO | OK  map_id=1139  -> vector[249]\n",
            "15:46:38 | INFO | OK  map_id=1140  -> vector[249]\n",
            "15:46:39 | INFO | OK  map_id=1148  -> vector[249]\n",
            "15:46:40 | INFO | OK  map_id=1155  -> vector[249]\n",
            "15:46:42 | INFO | OK  map_id=1157  -> vector[249]\n",
            "15:46:44 | INFO | OK  map_id=1168  -> vector[249]\n",
            "15:46:46 | INFO | OK  map_id=1169  -> vector[249]\n",
            "15:46:47 | INFO | OK  map_id=1170  -> vector[249]\n",
            "15:46:49 | INFO | OK  map_id=1197  -> vector[249]\n",
            "15:46:51 | INFO | OK  map_id=1198  -> vector[249]\n",
            "15:46:52 | INFO | OK  map_id=1202  -> vector[249]\n",
            "15:46:56 | INFO | OK  map_id=1203  -> vector[249]\n",
            "15:46:57 | INFO | OK  map_id=1204  -> vector[249]\n",
            "15:46:58 | INFO | OK  map_id=1217  -> vector[249]\n",
            "15:47:00 | INFO | OK  map_id=1218  -> vector[249]\n",
            "15:47:01 | INFO | OK  map_id=1219  -> vector[249]\n",
            "15:47:03 | INFO | OK  map_id=1221  -> vector[249]\n",
            "15:47:04 | INFO | OK  map_id=1222  -> vector[249]\n",
            "15:47:05 | INFO | OK  map_id=1231  -> vector[249]\n",
            "15:47:06 | INFO | OK  map_id=1233  -> vector[249]\n",
            "15:47:08 | INFO | OK  map_id=1234  -> vector[249]\n",
            "15:47:10 | INFO | OK  map_id=1261  -> vector[249]\n",
            "15:47:11 | INFO | OK  map_id=1269  -> vector[249]\n",
            "15:47:15 | INFO | OK  map_id=1270  -> vector[249]\n",
            "15:47:16 | INFO | OK  map_id=1271  -> vector[249]\n",
            "15:47:17 | INFO | OK  map_id=1276  -> vector[249]\n",
            "15:47:18 | INFO | OK  map_id=1277  -> vector[249]\n",
            "15:47:19 | INFO | OK  map_id=1283  -> vector[249]\n",
            "15:47:21 | INFO | OK  map_id=1284  -> vector[249]\n",
            "15:47:22 | INFO | OK  map_id=1285  -> vector[249]\n",
            "15:47:24 | INFO | OK  map_id=1295  -> vector[249]\n",
            "15:47:26 | INFO | OK  map_id=1296  -> vector[249]\n",
            "15:47:28 | INFO | OK  map_id=1297  -> vector[249]\n",
            "15:47:29 | INFO | OK  map_id=1303  -> vector[249]\n",
            "15:47:31 | INFO | OK  map_id=1304  -> vector[249]\n",
            "15:47:32 | INFO | OK  map_id=1310  -> vector[249]\n",
            "15:47:35 | INFO | OK  map_id=1319  -> vector[249]\n",
            "15:47:36 | INFO | OK  map_id=1333  -> vector[249]\n",
            "15:47:37 | INFO | OK  map_id=1334  -> vector[249]\n",
            "15:47:40 | INFO | OK  map_id=1344  -> vector[249]\n",
            "15:47:40 | INFO | OK  map_id=1349  -> vector[249]\n",
            "15:47:43 | INFO | OK  map_id=1364  -> vector[249]\n",
            "15:47:46 | INFO | OK  map_id=1365  -> vector[249]\n",
            "15:47:49 | INFO | OK  map_id=1366  -> vector[249]\n",
            "15:47:52 | INFO | OK  map_id=1367  -> vector[249]\n",
            "15:47:54 | INFO | OK  map_id=1368  -> vector[249]\n",
            "15:47:55 | INFO | OK  map_id=1369  -> vector[249]\n",
            "15:47:56 | INFO | OK  map_id=1377  -> vector[249]\n",
            "15:47:57 | INFO | OK  map_id=1378  -> vector[249]\n",
            "15:47:58 | INFO | OK  map_id=1385  -> vector[249]\n",
            "15:47:59 | INFO | OK  map_id=1386  -> vector[249]\n",
            "15:48:00 | INFO | OK  map_id=1399  -> vector[249]\n",
            "15:48:01 | INFO | OK  map_id=1401  -> vector[249]\n",
            "15:48:03 | INFO | OK  map_id=1408  -> vector[249]\n",
            "15:48:06 | INFO | OK  map_id=1409  -> vector[249]\n",
            "15:48:08 | INFO | OK  map_id=1410  -> vector[249]\n",
            "15:48:10 | INFO | OK  map_id=1413  -> vector[249]\n",
            "15:48:12 | INFO | OK  map_id=1414  -> vector[249]\n",
            "15:48:13 | INFO | OK  map_id=1415  -> vector[249]\n",
            "15:48:15 | INFO | OK  map_id=1416  -> vector[249]\n",
            "15:48:16 | INFO | OK  map_id=1417  -> vector[249]\n",
            "15:48:19 | INFO | OK  map_id=1418  -> vector[249]\n",
            "15:48:20 | INFO | OK  map_id=1434  -> vector[249]\n",
            "15:48:21 | INFO | OK  map_id=1438  -> vector[249]\n",
            "15:48:23 | INFO | OK  map_id=1439  -> vector[249]\n",
            "15:48:24 | INFO | OK  map_id=1450  -> vector[249]\n",
            "15:48:26 | INFO | OK  map_id=1451  -> vector[249]\n",
            "15:48:29 | INFO | OK  map_id=1458  -> vector[249]\n",
            "15:48:30 | INFO | OK  map_id=1459  -> vector[249]\n",
            "15:48:32 | INFO | OK  map_id=1460  -> vector[249]\n",
            "15:48:33 | INFO | OK  map_id=1465  -> vector[249]\n",
            "15:48:35 | INFO | OK  map_id=1466  -> vector[249]\n",
            "15:48:37 | INFO | OK  map_id=1467  -> vector[249]\n",
            "15:48:40 | INFO | OK  map_id=1473  -> vector[249]\n",
            "15:48:41 | INFO | OK  map_id=1474  -> vector[249]\n",
            "15:48:42 | INFO | OK  map_id=1476  -> vector[249]\n",
            "15:48:43 | INFO | OK  map_id=1479  -> vector[249]\n",
            "15:48:44 | INFO | OK  map_id=1486  -> vector[249]\n",
            "15:48:46 | INFO | OK  map_id=1487  -> vector[249]\n",
            "15:48:48 | INFO | OK  map_id=1496  -> vector[249]\n",
            "15:48:50 | INFO | OK  map_id=1500  -> vector[249]\n",
            "15:48:51 | INFO | OK  map_id=1501  -> vector[249]\n",
            "15:48:52 | INFO | OK  map_id=1507  -> vector[249]\n",
            "15:48:54 | INFO | OK  map_id=1508  -> vector[249]\n",
            "15:48:56 | INFO | OK  map_id=1509  -> vector[249]\n",
            "15:48:57 | INFO | OK  map_id=1514  -> vector[249]\n",
            "15:48:59 | INFO | OK  map_id=1515  -> vector[249]\n",
            "15:49:00 | INFO | OK  map_id=1557  -> vector[249]\n",
            "15:49:01 | INFO | OK  map_id=1563  -> vector[249]\n",
            "15:49:04 | INFO | OK  map_id=1564  -> vector[249]\n",
            "15:49:06 | INFO | OK  map_id=1565  -> vector[249]\n",
            "15:49:07 | INFO | OK  map_id=1570  -> vector[249]\n",
            "15:49:08 | INFO | OK  map_id=1579  -> vector[249]\n",
            "15:49:09 | INFO | OK  map_id=1580  -> vector[249]\n",
            "15:49:10 | INFO | OK  map_id=1583  -> vector[249]\n",
            "15:49:11 | INFO | OK  map_id=1584  -> vector[249]\n",
            "15:49:12 | INFO | OK  map_id=1598  -> vector[249]\n",
            "15:49:14 | INFO | OK  map_id=1613  -> vector[249]\n",
            "15:49:15 | INFO | OK  map_id=1614  -> vector[249]\n",
            "15:49:16 | INFO | OK  map_id=1618  -> vector[249]\n",
            "15:49:18 | INFO | OK  map_id=1619  -> vector[249]\n",
            "15:49:18 | INFO | OK  map_id=1629  -> vector[249]\n",
            "15:49:21 | INFO | OK  map_id=1630  -> vector[249]\n",
            "15:49:22 | INFO | OK  map_id=1631  -> vector[249]\n",
            "15:49:24 | INFO | OK  map_id=1647  -> vector[249]\n",
            "15:49:25 | INFO | OK  map_id=1649  -> vector[249]\n",
            "15:49:26 | INFO | OK  map_id=1650  -> vector[249]\n",
            "15:49:27 | INFO | OK  map_id=1653  -> vector[249]\n",
            "15:49:27 | INFO | OK  map_id=1666  -> vector[249]\n",
            "15:49:29 | INFO | OK  map_id=1667  -> vector[249]\n",
            "15:49:31 | INFO | OK  map_id=1672  -> vector[249]\n",
            "15:49:33 | INFO | OK  map_id=1673  -> vector[249]\n",
            "15:49:34 | INFO | OK  map_id=1679  -> vector[249]\n",
            "15:49:35 | INFO | OK  map_id=1691  -> vector[249]\n",
            "15:49:36 | INFO | OK  map_id=1696  -> vector[249]\n",
            "15:49:39 | INFO | OK  map_id=1700  -> vector[249]\n",
            "15:49:42 | INFO | OK  map_id=1702  -> vector[249]\n",
            "15:49:45 | INFO | OK  map_id=1703  -> vector[249]\n",
            "15:49:46 | INFO | OK  map_id=1709  -> vector[249]\n",
            "15:49:47 | INFO | OK  map_id=1710  -> vector[249]\n",
            "15:49:49 | INFO | OK  map_id=1748  -> vector[249]\n",
            "15:49:51 | INFO | OK  map_id=1749  -> vector[249]\n",
            "15:49:53 | INFO | OK  map_id=1750  -> vector[249]\n",
            "15:49:54 | INFO | OK  map_id=1751  -> vector[249]\n",
            "15:49:55 | INFO | OK  map_id=1752  -> vector[249]\n",
            "15:49:56 | INFO | OK  map_id=1755  -> vector[249]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Input map embeddings completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "15:49:57 | INFO | OK  map_id=1757  -> vector[249]\n",
            "15:49:57 | INFO | Saved 300 vectors (failed=0) to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n"
          ]
        }
      ],
      "source": [
        "cmd = [\n",
        "    sys.executable, \"-m\", \"src.mapvec.maps.map_embeddings\",\n",
        "    \"--root\", str(PATHS.MAPS_ROOT),\n",
        "    \"--pattern\", PATHS.INPUT_MAPS_PATTERN,\n",
        "    \"--out_dir\", str(PATHS.MAP_OUT),\n",
        "    \"-v\"\n",
        "]\n",
        "print(\"CMD:\", \" \".join(cmd))\n",
        "res = subprocess.run(cmd, cwd=str(PATHS.PROJ_ROOT)) \n",
        "if res.returncode != 0:\n",
        "    raise SystemExit(\"Map embedding step failed.\")\n",
        "print(\"‚úÖ Input map embeddings completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Concatenate\n",
        "Joins map & prompt vectors using `pairs.csv` and writes `X_concat.npy` and `train_pairs.parquet` to `TRAIN_OUT`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CMD: /opt/anaconda3/envs/thesis/bin/python -m src.mapvec.concat.concat_embeddings --pairs /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/pairs.csv --map_npz /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/maps_embeddings.npz --prompt_npz /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/prompts_embeddings.npz --out_dir /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out --drop_dupes\n",
            "‚úÖ Concatenation completed successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "16:15:14 | INFO | Map  embeddings: (300, 249) from /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/maps_embeddings.npz\n",
            "16:15:14 | INFO | Prompt embeddings: (500, 512) from /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/prompts_embeddings.npz\n",
            "16:15:14 | INFO | X shape = (450, 761)  (map_dim=249, prompt_dim=512)\n",
            "16:15:14 | INFO | Saved to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out in 0.02s\n"
          ]
        }
      ],
      "source": [
        "cmd = [\n",
        "    sys.executable, \"-m\", \"src.mapvec.concat.concat_embeddings\",\n",
        "    \"--pairs\",      str(PATHS.PAIRS_CSV),\n",
        "    \"--map_npz\",    str(PATHS.MAP_OUT / \"maps_embeddings.npz\"),   # single-map embeddings\n",
        "    \"--prompt_npz\", str(PATHS.PROMPT_OUT / \"prompts_embeddings.npz\"),\n",
        "    \"--out_dir\",    str(PATHS.TRAIN_OUT),\n",
        "    \"--drop_dupes\",          # optional flag (keep if you want)\n",
        "    # \"--fail_on_missing\",   # optional: uncomment if you prefer hard failure on missing IDs\n",
        "]\n",
        "\n",
        "print(\"CMD:\", \" \".join(cmd))\n",
        "res = subprocess.run(cmd, cwd=str(PATHS.PROJ_ROOT))  # run from project root\n",
        "if res.returncode != 0:\n",
        "    raise SystemExit(\"Concatenation step failed.\")\n",
        "print(\"‚úÖ Concatenation completed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b997cab7",
      "metadata": {},
      "source": [
        "## 4) Split dataset  \n",
        "Splits the concatenated feature matrix `X_concat.npy` and its metadata `train_pairs.parquet` into separate **training**, **validation**, and **test** subsets.  \n",
        "Each split preserves row alignment between features and metadata, and the resulting files are saved under `TRAIN_OUT/splits/` as:  \n",
        "\n",
        "- `X_train.npy`, `pairs_train.parquet`  \n",
        "- `X_val.npy`, `pairs_val.parquet`  \n",
        "- `X_test.npy`, `pairs_test.parquet`  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "3bc0897d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded X: (450, 761), pairs: (450, 4)\n",
            "Train: (315, 761), Val: (67, 761), Test: (68, 761)\n",
            "Saved splits to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/splits\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "X = np.load(PATHS.TRAIN_OUT / \"X_concat.npy\")\n",
        "pairs_df = pd.read_parquet(PATHS.TRAIN_OUT / \"train_pairs.parquet\")\n",
        "\n",
        "print(f\"Loaded X: {X.shape}, pairs: {pairs_df.shape}\")\n",
        "\n",
        "# --- Step 1: Train/Test split\n",
        "X_train, X_temp, df_train, df_temp = train_test_split(\n",
        "    X, pairs_df, test_size= CFG.VAL_RATIO + CFG.TEST_RATIO, random_state= CFG.SEED, shuffle=True\n",
        ")\n",
        "\n",
        "# --- Step 2: Split temp into Val/Test\n",
        "relative_test_ratio = CFG.TEST_RATIO / (CFG.VAL_RATIO + CFG.TEST_RATIO)\n",
        "X_val, X_test, df_val, df_test = train_test_split(\n",
        "    X_temp, df_temp, test_size=relative_test_ratio, random_state= CFG.SEED, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "# --- Save splits\n",
        "split_dir = PATHS.TRAIN_OUT / \"splits\"\n",
        "split_dir.mkdir(exist_ok=True)\n",
        "\n",
        "np.save(split_dir / \"X_train.npy\", X_train)\n",
        "np.save(split_dir / \"X_val.npy\",   X_val)\n",
        "np.save(split_dir / \"X_test.npy\",  X_test)\n",
        "\n",
        "df_train.to_parquet(split_dir / \"pairs_train.parquet\", index=False)\n",
        "df_val.to_parquet(split_dir / \"pairs_val.parquet\", index=False)\n",
        "df_test.to_parquet(split_dir / \"pairs_test.parquet\", index=False)\n",
        "\n",
        "print(f\"Saved splits to {split_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "41942b1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================== MODEL TRAINING PARAMETERS =====================\n",
        "# --------- CONFIG ----------\n",
        "OP_COL         = \"operator\"\n",
        "PARAM_COLS     = [\"param\"]                     # keep list so y has shape (n,1)\n",
        "FIXED_CLASSES  = [\"simplify\", \"select\", \"aggregate\", \"displace\"]\n",
        "CLIP_ABS = None  # set to 8.0 to enable, or None to disable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "bb7d5a8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_X_and_df(X, df):\n",
        "    \"\"\"Filter rows that have operator and param; normalize operator; keep X aligned.\"\"\"\n",
        "    df2 = df.copy()\n",
        "    df2[OP_COL] = df2[OP_COL].astype(str).str.strip().str.lower()\n",
        "    mask = df2[OP_COL].notna()\n",
        "    for c in PARAM_COLS:\n",
        "        mask &= df2[c].notna()\n",
        "    mask = mask.values  # numpy bool array aligned with X\n",
        "    X2   = X[mask]\n",
        "    df2  = df2.loc[mask].reset_index(drop=True)\n",
        "    return X2.astype(\"float32\", copy=False), df2\n",
        "\n",
        "# Apply to your splits produced earlier\n",
        "X_train, df_train = filter_X_and_df(X_train, df_train)\n",
        "X_val,   df_val   = filter_X_and_df(X_val,   df_val)\n",
        "X_test,  df_test  = filter_X_and_df(X_test,  df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "25b55b49",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes: ['aggregate', 'displace', 'select', 'simplify']\n"
          ]
        }
      ],
      "source": [
        "# Fixed class order (even if a class is missing in train)\n",
        "le = LabelEncoder()\n",
        "le.fit(FIXED_CLASSES)\n",
        "classes = le.classes_.tolist()\n",
        "print(\"Classes:\", classes)\n",
        "\n",
        "y_train_cls = le.transform(df_train[OP_COL])\n",
        "y_val_cls   = le.transform(df_val[OP_COL])\n",
        "y_test_cls  = le.transform(df_test[OP_COL])\n",
        "\n",
        "# Single-parameter targets -> 2D arrays (n,1)\n",
        "y_train_reg = df_train[PARAM_COLS].to_numpy(dtype=\"float32\")\n",
        "y_val_reg   = df_val[PARAM_COLS].to_numpy(dtype=\"float32\")\n",
        "y_test_reg  = df_test[PARAM_COLS].to_numpy(dtype=\"float32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "144ee7e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace inf with NaN so imputer can handle them\n",
        "for X in (X_train, X_val, X_test):\n",
        "    X[~np.isfinite(X)] = np.nan\n",
        "\n",
        "imp = SimpleImputer(strategy=\"median\")\n",
        "X_train_imp = imp.fit_transform(X_train)\n",
        "X_val_imp   = imp.transform(X_val)\n",
        "X_test_imp  = imp.transform(X_test)\n",
        "\n",
        "scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5, 95))\n",
        "X_train_s = scaler.fit_transform(X_train_imp)\n",
        "X_val_s   = scaler.transform(X_val_imp)\n",
        "X_test_s  = scaler.transform(X_test_imp)\n",
        "\n",
        "# Clip extremes to stabilize optimizer\n",
        "if CLIP_ABS is not None:\n",
        "    X_train_s = np.clip(X_train_s, -CLIP_ABS, CLIP_ABS)\n",
        "    X_val_s   = np.clip(X_val_s,   -CLIP_ABS, CLIP_ABS)\n",
        "    X_test_s  = np.clip(X_test_s,  -CLIP_ABS, CLIP_ABS)\n",
        "\n",
        "assert np.isfinite(X_train_s).all() and np.isfinite(X_val_s).all() and np.isfinite(X_test_s).all(), \"Non-finite after scaling.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "229a52d7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class weights: {'aggregate': np.float64(0.9375), 'displace': np.float64(0.9603658536585366), 'select': np.float64(1.0227272727272727), 'simplify': np.float64(1.09375)}\n",
            "Iteration 1, loss = nan\n",
            "Validation score: 0.282862\n",
            "Iteration 2, loss = nan\n",
            "Validation score: 0.232634\n",
            "Iteration 3, loss = nan\n",
            "Validation score: 0.232634\n",
            "Iteration 4, loss = nan\n",
            "Validation score: 0.232634\n",
            "Iteration 5, loss = nan\n",
            "Validation score: 0.232634\n",
            "Iteration 6, loss = nan\n",
            "Validation score: 0.232634\n",
            "Iteration 7, loss = nan\n",
            "Validation score: 0.232634\n",
            "Iteration 8, loss = nan\n",
            "Validation score: 0.232634\n",
            "Iteration 9, loss = nan\n",
            "Validation score: 0.232634\n",
            "Iteration 10, loss = nan\n",
            "Validation score: 0.232634\n",
            "Iteration 11, loss = nan\n",
            "Validation score: 0.232634\n",
            "Iteration 12, loss = nan\n",
            "Validation score: 0.232634\n",
            "Iteration 13, loss = nan\n",
            "Validation score: 0.232634\n",
            "Iteration 14, loss = nan\n",
            "Validation score: 0.232634\n",
            "Iteration 15, loss = nan\n",
            "Validation score: 0.232634\n",
            "Iteration 16, loss = nan\n",
            "Validation score: 0.232634\n",
            "Iteration 17, loss = nan\n",
            "Validation score: 0.232634\n",
            "Iteration 18, loss = nan\n",
            "Validation score: 0.232634\n",
            "Iteration 19, loss = nan\n",
            "Validation score: 0.232634\n",
            "Iteration 20, loss = nan\n",
            "Validation score: 0.232634\n",
            "Iteration 21, loss = nan\n",
            "Validation score: 0.232634\n",
            "Iteration 22, loss = nan\n",
            "Validation score: 0.232634\n",
            "Validation score did not improve more than tol=0.000100 for 20 consecutive epochs. Stopping.\n",
            "‚ö†Ô∏è Adam crashed (Solver produced non-finite parameter weights. The input data may contain large values and need to be preprocessed.). Falling back to lbfgs (no sample_weight).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_stochastic_optimizers.py:275: RuntimeWarning: overflow encountered in square\n",
            "  self.beta_2 * v + (1 - self.beta_2) * (grad**2)\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:173: RuntimeWarning: invalid value encountered in add\n",
            "  activations[i + 1] += self.intercepts_[i]\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:173: RuntimeWarning: invalid value encountered in add\n",
            "  activations[i + 1] += self.intercepts_[i]\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:602: ConvergenceWarning: lbfgs failed to converge after 0 iteration(s) (status=2):\n",
            "ABNORMAL: \n",
            "\n",
            "You might also want to scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        }
      ],
      "source": [
        "# Class weights for imbalance\n",
        "cls_w    = compute_class_weight(\"balanced\", classes=np.arange(len(classes)), y=y_train_cls)\n",
        "sample_w = np.array([cls_w[c] for c in y_train_cls], dtype=\"float32\")\n",
        "print(\"Class weights:\", dict(zip(classes, cls_w)))\n",
        "\n",
        "clf = MLPClassifier(\n",
        "    hidden_layer_sizes=(256, 128),\n",
        "    activation=\"relu\",\n",
        "    solver=\"adam\",\n",
        "    alpha=1e-3,\n",
        "    learning_rate_init=1e-4,\n",
        "    batch_size=128,\n",
        "    max_iter=400,\n",
        "    early_stopping=True,\n",
        "    n_iter_no_change=20,\n",
        "    random_state=CFG.SEED,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "try:\n",
        "    clf.fit(X_train_s, y_train_cls, sample_weight=sample_w)\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Adam crashed ({e}). Falling back to lbfgs (no sample_weight).\")\n",
        "    clf = MLPClassifier(\n",
        "        hidden_layer_sizes=(256, 128),\n",
        "        activation=\"relu\",\n",
        "        solver=\"lbfgs\",\n",
        "        alpha=1e-3,\n",
        "        max_iter=500,\n",
        "        random_state= CFG.SEED,\n",
        "        verbose=True\n",
        "    )\n",
        "    clf.fit(X_train_s, y_train_cls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "44b82400",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- X_train ---\n",
            "shape: (315, 1508) dtype: float32\n",
            "finite: True\n",
            "min/max: -552.0 4054873900.0\n",
            "mean/std: 24875.152 8824922.0\n",
            "all-NaN cols: 0 zero-variance cols: 58\n",
            "--- X_val ---\n",
            "shape: (67, 1508) dtype: float32\n",
            "finite: True\n",
            "min/max: -505.0 96049.9\n",
            "mean/std: 22.502472 675.2408\n",
            "all-NaN cols: 0 zero-variance cols: 72\n",
            "--- X_test ---\n",
            "shape: (68, 1508) dtype: float32\n",
            "finite: True\n",
            "min/max: -552.0 4054873900.0\n",
            "mean/std: 115146.09 18993522.0\n",
            "all-NaN cols: 0 zero-variance cols: 67\n",
            "--- X_train_imp ---\n",
            "shape: (315, 1508) dtype: float32\n",
            "finite: True\n",
            "min/max: -552.0 4054873900.0\n",
            "mean/std: 24875.152 8824922.0\n",
            "all-NaN cols: 0 zero-variance cols: 58\n",
            "--- X_train_s ---\n",
            "shape: (315, 1508) dtype: float32\n",
            "finite: True\n",
            "min/max: -20.723267 142484560.0\n",
            "mean/std: 1179.2317 357503.12\n",
            "all-NaN cols: 0 zero-variance cols: 58\n",
            "y_train_cls has NaN? False\n",
            "classes present in train: ['aggregate', 'displace', 'select', 'simplify']\n"
          ]
        }
      ],
      "source": [
        "def check_matrix(name, X):\n",
        "    print(f\"--- {name} ---\")\n",
        "    print(\"shape:\", X.shape, \"dtype:\", X.dtype)\n",
        "    print(\"finite:\", np.isfinite(X).all())\n",
        "    print(\"min/max:\", np.nanmin(X), np.nanmax(X))\n",
        "    print(\"mean/std:\", np.nanmean(X), np.nanstd(X))\n",
        "    # any all-NaN or all-constant columns before scaling?\n",
        "    col_nan = np.isnan(X).all(axis=0).sum()\n",
        "    col_zero_var = (np.nanstd(X, axis=0) == 0).sum()\n",
        "    print(\"all-NaN cols:\", col_nan, \"zero-variance cols:\", col_zero_var)\n",
        "\n",
        "check_matrix(\"X_train\", X_train)\n",
        "check_matrix(\"X_val\",   X_val)\n",
        "check_matrix(\"X_test\",  X_test)\n",
        "\n",
        "check_matrix(\"X_train_imp\", X_train_imp)\n",
        "check_matrix(\"X_train_s\",   X_train_s)\n",
        "\n",
        "print(\"y_train_cls has NaN?\", np.isnan(y_train_cls).any() if hasattr(y_train_cls, \"__len__\") else False)\n",
        "print(\"classes present in train:\", sorted(set(df_train[OP_COL])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "13fca12f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL] acc=0.224  f1_macro=0.191\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   aggregate       0.00      0.00      0.00        17\n",
            "    displace       0.42      0.42      0.42        12\n",
            "      select       0.50      0.04      0.08        24\n",
            "    simplify       0.17      0.64      0.27        14\n",
            "\n",
            "    accuracy                           0.22        67\n",
            "   macro avg       0.27      0.28      0.19        67\n",
            "weighted avg       0.29      0.22      0.16        67\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        }
      ],
      "source": [
        "y_val_pred  = clf.predict(X_val_s)\n",
        "val_acc     = accuracy_score(y_val_cls, y_val_pred)\n",
        "val_f1m     = f1_score(y_val_cls, y_val_pred, average=\"macro\")\n",
        "print(f\"[VAL] acc={val_acc:.3f}  f1_macro={val_f1m:.3f}\")\n",
        "print(classification_report(y_val_cls, y_val_pred, target_names=classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "1980b8b6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1771: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:214: RuntimeWarning: invalid value encountered in add\n",
            "  activation += self.intercepts_[i]\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (600) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1771: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Regressor trained for 'aggregate' on 84 samples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (600) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1771: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Regressor trained for 'displace' on 82 samples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (600) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1771: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Regressor trained for 'select' on 77 samples.\n",
            "‚úÖ Regressor trained for 'simplify' on 72 samples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (600) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "regressors = {}\n",
        "for idx, name in enumerate(classes):\n",
        "    sel = (y_train_cls == idx)\n",
        "    n = int(sel.sum())\n",
        "    if n < 5:\n",
        "        print(f\"‚ö†Ô∏è  Skipping regressor for '{name}' (only {n} samples).\")\n",
        "        continue\n",
        "\n",
        "    reg = MLPRegressor(\n",
        "        hidden_layer_sizes=(128, 64),\n",
        "        activation=\"relu\",\n",
        "        solver=\"adam\",\n",
        "        alpha=1e-3,\n",
        "        learning_rate_init=5e-4,\n",
        "        max_iter=600,\n",
        "        early_stopping=True,\n",
        "        n_iter_no_change=30,\n",
        "        random_state=SEED,\n",
        "        verbose=False\n",
        "    )\n",
        "    reg.fit(X_train_s[sel], y_train_reg[sel])  # (n,d) -> (n,1)\n",
        "    regressors[name] = reg\n",
        "    print(f\"‚úÖ Regressor trained for '{name}' on {n} samples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "b5603571",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TEST] acc=0.279  f1_macro=0.214\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   aggregate       0.00      0.00      0.00        20\n",
            "    displace       0.42      0.33      0.37        15\n",
            "      select       1.00      0.07      0.13        14\n",
            "    simplify       0.24      0.68      0.35        19\n",
            "\n",
            "    accuracy                           0.28        68\n",
            "   macro avg       0.41      0.27      0.21        68\n",
            "weighted avg       0.36      0.28      0.21        68\n",
            "\n",
            "Parameter evaluation on 19/68 samples with correct operator prediction.\n",
            "[TEST Param | correct-ops] MSE=390.9869  MAE=7.3307\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        }
      ],
      "source": [
        "# Classification\n",
        "y_test_pred = clf.predict(X_test_s)\n",
        "test_acc    = accuracy_score(y_test_cls, y_test_pred)\n",
        "test_f1m    = f1_score(y_test_cls, y_test_pred, average=\"macro\")\n",
        "print(f\"[TEST] acc={test_acc:.3f}  f1_macro={test_f1m:.3f}\")\n",
        "print(classification_report(y_test_cls, y_test_pred, target_names=classes))\n",
        "\n",
        "# Parameter regression (conditioned on correct operator)\n",
        "mask = (y_test_pred == y_test_cls)\n",
        "print(f\"Parameter evaluation on {int(mask.sum())}/{len(mask)} samples with correct operator prediction.\")\n",
        "y_pred_params = np.full_like(y_test_reg, np.nan, dtype=\"float32\")  # (n,1)\n",
        "\n",
        "for i, ok in enumerate(mask):\n",
        "    if not ok:\n",
        "        continue\n",
        "    cls_name = classes[y_test_pred[i]]\n",
        "    reg = regressors.get(cls_name)\n",
        "    if reg is None:\n",
        "        continue\n",
        "    pred = reg.predict(X_test_s[i:i+1])[0]\n",
        "    y_pred_params[i] = pred if hasattr(pred, \"__len__\") else [float(pred)]\n",
        "\n",
        "valid = np.isfinite(y_pred_params).all(axis=1) & mask\n",
        "if valid.any():\n",
        "    mse = mean_squared_error(y_test_reg[valid], y_pred_params[valid])\n",
        "    mae = mean_absolute_error(y_test_reg[valid], y_pred_params[valid])\n",
        "    print(f\"[TEST Param | correct-ops] MSE={mse:.4f}  MAE={mae:.4f}\")\n",
        "else:\n",
        "    print(\"No valid parameter predictions to score.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "50f905d8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models saved to ../data/output/models\n"
          ]
        }
      ],
      "source": [
        "joblib.dump(imp,        MODEL_OUT / \"imputer.joblib\")\n",
        "joblib.dump(scaler,     MODEL_OUT / \"scaler.joblib\")\n",
        "joblib.dump(le,         MODEL_OUT / \"label_encoder.joblib\")\n",
        "joblib.dump(clf,        MODEL_OUT / \"mlp_classifier.joblib\")\n",
        "joblib.dump(regressors, MODEL_OUT / \"per_class_regressors.joblib\")\n",
        "print(f\"Models saved to {MODEL_OUT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "10fd82f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_pipeline(X_batch):\n",
        "    \"\"\"\n",
        "    X_batch: (n, 1508) raw concatenated features\n",
        "    Returns: dict with operator_probs, operator_label, params_pred\n",
        "    \"\"\"\n",
        "    Xb = np.asarray(X_batch, dtype=\"float32\")\n",
        "    Xb[~np.isfinite(Xb)] = np.nan\n",
        "    Xb = imp.transform(Xb)\n",
        "    Xb = scaler.transform(Xb)\n",
        "    Xb = np.clip(Xb, -CLIP_ABS, CLIP_ABS)\n",
        "\n",
        "    proba = clf.predict_proba(Xb)                    # (n, 4)\n",
        "    pred_idx = np.argmax(proba, axis=1)\n",
        "    pred_labels = le.inverse_transform(pred_idx).tolist()\n",
        "\n",
        "    params = []\n",
        "    for i, lbl in enumerate(pred_labels):\n",
        "        reg = regressors.get(lbl)\n",
        "        if reg is None:\n",
        "            params.append([np.nan])                 # single param\n",
        "        else:\n",
        "            pred = reg.predict(Xb[i:i+1])[0]\n",
        "            params.append(pred.tolist() if hasattr(pred, \"tolist\") else [float(pred)])\n",
        "\n",
        "    return {\"operator_probs\": proba, \"operator_label\": pred_labels, \"params_pred\": np.array(params)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d21771b",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "thesis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
