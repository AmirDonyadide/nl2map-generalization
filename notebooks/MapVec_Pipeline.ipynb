{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ccd509",
   "metadata": {},
   "source": [
    "### Cell 0 â€” Repository Bootstrap & Experiment Registry (Required)\n",
    "\n",
    "This cell ensures that the project repository is discoverable by Python **and**\n",
    "defines the registry of experiments that will be executed in this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why this is needed\n",
    "\n",
    "- The notebook lives inside the `notebooks/` directory  \n",
    "- Python does not automatically know where the project root is  \n",
    "- All project code lives under the `src/` directory  \n",
    "- Multiple experimental configurations (prompt-only, USE + map, OpenAI + map)\n",
    "  must be executed in a single, reproducible workflow  \n",
    "\n",
    "---\n",
    "\n",
    "#### What this cell does\n",
    "\n",
    "- Walks up the directory tree starting from the current notebook location  \n",
    "- Finds the repository root (identified by the presence of a `src/` folder)  \n",
    "- Adds that directory to `sys.path` so imports such as  \n",
    "  `from src.config import ...` work correctly  \n",
    "- Defines a central **experiment registry** describing:\n",
    "  - which feature representation is used\n",
    "  - where training data is read from\n",
    "  - where trained models and metadata are saved  \n",
    "\n",
    "---\n",
    "\n",
    "#### Design principles\n",
    "\n",
    "- Executed once at the very top of the notebook  \n",
    "- Contains no learning or model-specific logic  \n",
    "- Provides a single source of truth for all experiment configurations  \n",
    "- Enables fair and controlled comparison between different feature setups  \n",
    "\n",
    "---\n",
    "\n",
    "This cell must be executed **before any imports from `src.*`**.  \n",
    "All subsequent cells rely on the repository path and experiment definitions\n",
    "established here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2152ca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: /Users/amirdonyadide/Documents/GitHub/Thesis\n",
      "ðŸ§ª Will run experiments: openai_prompt_only, use_prompt_only, map_only, use_map, openai_map\n",
      " - openai_prompt_only | mode=prompt_only    | prompt=openai-small | train_out=train_out_openai_prompt_only | model_out=exp_openai_prompt_only\n",
      " - use_prompt_only | mode=prompt_only    | prompt=dan          | train_out=train_out_use_prompt_only | model_out=exp_use_prompt_only\n",
      " - map_only     | mode=map_only       | prompt=-            | train_out=train_out_map_only | model_out=exp_map_only\n",
      " - use_map      | mode=prompt_plus_map | prompt=dan          | train_out=train_out_use_map | model_out=exp_use_map\n",
      " - openai_map   | mode=prompt_plus_map | prompt=openai-small | train_out=train_out_openai_map | model_out=exp_openai_map\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 0: bootstrap so \"src\" is importable + run-all-experiments config ---\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Make repo root importable (find folder containing \"src\")\n",
    "# -------------------------------------------------\n",
    "p = Path.cwd().resolve()\n",
    "for candidate in [p, *p.parents]:\n",
    "    if (candidate / \"src\").is_dir():\n",
    "        if str(candidate) not in sys.path:\n",
    "            sys.path.insert(0, str(candidate))\n",
    "        REPO_ROOT = candidate\n",
    "        print(\"Repo root:\", candidate)\n",
    "        break\n",
    "else:\n",
    "    raise RuntimeError(\"Could not find repo root (no 'src' folder found in parents).\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Experiment registry (ALL experiments will run)\n",
    "# -------------------------------------------------\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "\n",
    "# NOTE:\n",
    "# - feature_mode strings are aligned with helpers:\n",
    "#     \"prompt_only\", \"map_only\", \"prompt_plus_map\"\n",
    "# - prompt_encoder_kind must be one of:\n",
    "#     \"dan\", \"transformer\", \"openai-small\", \"openai-large\"\n",
    "EXPERIMENTS = {\n",
    "    \"openai_prompt_only\": {\n",
    "        \"train_out\": DATA_DIR / \"output\" / \"train_out_openai_prompt_only\",\n",
    "        \"model_out\": DATA_DIR / \"output\" / \"models\" / \"exp_openai_prompt_only\",\n",
    "        \"feature_mode\": \"prompt_only\",\n",
    "        \"prompt_encoder_kind\": \"openai-small\",\n",
    "    },\n",
    "    \"use_prompt_only\": {\n",
    "        \"train_out\": DATA_DIR / \"output\" / \"train_out_use_prompt_only\",\n",
    "        \"model_out\": DATA_DIR / \"output\" / \"models\" / \"exp_use_prompt_only\",\n",
    "        \"feature_mode\": \"prompt_only\",\n",
    "        \"prompt_encoder_kind\": \"dan\",\n",
    "    },\n",
    "    \"map_only\": {\n",
    "        \"train_out\": DATA_DIR / \"output\" / \"train_out_map_only\",\n",
    "        \"model_out\": DATA_DIR / \"output\" / \"models\" / \"exp_map_only\",\n",
    "        \"feature_mode\": \"map_only\",\n",
    "    },\n",
    "    \"use_map\": {\n",
    "        \"train_out\": DATA_DIR / \"output\" / \"train_out_use_map\",\n",
    "        \"model_out\": DATA_DIR / \"output\" / \"models\" / \"exp_use_map\",\n",
    "        \"feature_mode\": \"prompt_plus_map\",\n",
    "        \"prompt_encoder_kind\": \"dan\",\n",
    "    },\n",
    "    \"openai_map\": {\n",
    "        \"train_out\": DATA_DIR / \"output\" / \"train_out_openai_map\",\n",
    "        \"model_out\": DATA_DIR / \"output\" / \"models\" / \"exp_openai_map\",\n",
    "        \"feature_mode\": \"prompt_plus_map\",\n",
    "        \"prompt_encoder_kind\": \"openai-small\",\n",
    "    },\n",
    "}\n",
    "\n",
    "for exp_name, exp_cfg in EXPERIMENTS.items():\n",
    "    exp_cfg[\"train_out\"] = Path(exp_cfg[\"train_out\"])\n",
    "    exp_cfg[\"model_out\"] = Path(exp_cfg[\"model_out\"])\n",
    "    exp_cfg[\"train_out\"].mkdir(parents=True, exist_ok=True)\n",
    "    exp_cfg[\"model_out\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"ðŸ§ª Will run experiments:\", \", \".join(EXPERIMENTS.keys()))\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    pe = cfg.get(\"prompt_encoder_kind\", \"-\")\n",
    "    print(\n",
    "        f\" - {exp_name:12s} | \"\n",
    "        f\"mode={cfg['feature_mode']:14s} | \"\n",
    "        f\"prompt={pe:12s} | \"\n",
    "        f\"train_out={cfg['train_out'].name} | \"\n",
    "        f\"model_out={cfg['model_out'].name}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f56a2c",
   "metadata": {},
   "source": [
    "### Cell 1 â€” Experiment Setup & Global Configuration\n",
    "\n",
    "This cell initializes the experiment environment and loads the global configuration required\n",
    "for training and evaluation. It is designed so the notebook can run **all experiment\n",
    "configurations in one pass** (prompt-only, USE + map, OpenAI + map) without manual edits.\n",
    "\n",
    "**What this cell does:**\n",
    "\n",
    "- **Loads global project configuration**\n",
    "  - Paths (`PATHS`)\n",
    "  - Runtime settings (`CFG`)\n",
    "  - Operator groups (`DISTANCE_OPS`, `AREA_OPS`)\n",
    "  - Dynamic extent configuration flags (`USE_DYNAMIC_EXTENT_REFS`, `ALLOW_FALLBACK_EXTENT`)\n",
    "  - Extent reference column names (`EXTENT_DIAG_COL`, `EXTENT_AREA_COL`)\n",
    "\n",
    "- **Defines a central experiment registry**\n",
    "  - `EXPERIMENTS` â€” a dictionary where each experiment specifies:\n",
    "    - `feature_mode`:\n",
    "      - `prompt_only` (uses prompt embeddings only)\n",
    "      - `fused` (uses concatenated map + prompt embeddings)\n",
    "    - `train_out` â€” where the prepared matrices (`X_*`) and `train_pairs.parquet` are read from\n",
    "    - `model_out` â€” where trained artifacts are saved\n",
    "\n",
    "- **Sets embedding dimensions**\n",
    "  - `MAP_DIM`, `PROMPT_DIM`\n",
    "  - `FUSED_DIM = MAP_DIM + PROMPT_DIM`\n",
    "  - The effective input dimension is derived per experiment:\n",
    "    - `prompt_only` â†’ `PROMPT_DIM`\n",
    "    - `fused` â†’ `FUSED_DIM`\n",
    "\n",
    "- **Validates experiment folders**\n",
    "  - Ensures each experimentâ€™s `train_out` and `model_out` directories exist\n",
    "  - Performs basic schema checks (required keys, valid feature modes)\n",
    "\n",
    "**Design principles**\n",
    "\n",
    "- Executed once near the top of the notebook (before data loading/training)\n",
    "- Contains no model training logic\n",
    "- Provides a single source of truth for experiment configuration\n",
    "- Prevents accidental overwrites by saving each experiment into its own output folder\n",
    "\n",
    "This cell must be executed **before any training or evaluation cells**. All experiment\n",
    "comparisons depend on the consistent configuration established here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "367f89f6ac45439b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:21:53.302406Z",
     "start_time": "2025-10-27T11:21:53.298709Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PATHS' from 'src.config' (/Users/amirdonyadide/Documents/GitHub/Thesis/src/config/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ===================== CELL 1 â€” PARAMETERS =====================\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      6\u001b[39m     PATHS, CFG, print_summary,\n\u001b[32m      7\u001b[39m     DISTANCE_OPS, AREA_OPS,\n\u001b[32m      8\u001b[39m     USE_DYNAMIC_EXTENT_REFS, ALLOW_FALLBACK_EXTENT,\n\u001b[32m      9\u001b[39m     EXTENT_DIAG_COL, EXTENT_AREA_COL,\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m print_summary()\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUSE_DYNAMIC_EXTENT_REFS:\u001b[39m\u001b[33m\"\u001b[39m, USE_DYNAMIC_EXTENT_REFS)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'PATHS' from 'src.config' (/Users/amirdonyadide/Documents/GitHub/Thesis/src/config/__init__.py)"
     ]
    }
   ],
   "source": [
    "# ===================== CELL 1 â€” PARAMETERS =====================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from src.config import (\n",
    "    PATHS, CFG, print_summary,\n",
    "    DISTANCE_OPS, AREA_OPS,\n",
    "    USE_DYNAMIC_EXTENT_REFS, ALLOW_FALLBACK_EXTENT,\n",
    "    EXTENT_DIAG_COL, EXTENT_AREA_COL,\n",
    ")\n",
    "\n",
    "print_summary()\n",
    "print(\"USE_DYNAMIC_EXTENT_REFS:\", USE_DYNAMIC_EXTENT_REFS)\n",
    "print(\"ALLOW_FALLBACK_EXTENT  :\", ALLOW_FALLBACK_EXTENT)\n",
    "print(\"EXTENT_DIAG_COL:\", EXTENT_DIAG_COL, \" EXTENT_AREA_COL:\", EXTENT_AREA_COL)\n",
    "\n",
    "MAP_DIM_CFG = int(CFG.MAP_DIM)\n",
    "PROMPT_DIM_CFG = int(CFG.PROMPT_DIM)\n",
    "FUSED_DIM_CFG = MAP_DIM_CFG + PROMPT_DIM_CFG\n",
    "BATCH_SIZE = int(CFG.BATCH_SIZE)\n",
    "\n",
    "print(\"CFG dims -> MAP_DIM:\", MAP_DIM_CFG, \"| PROMPT_DIM:\", PROMPT_DIM_CFG, \"| FUSED_DIM:\", FUSED_DIM_CFG)\n",
    "print(\"BATCH_SIZE:\", BATCH_SIZE)\n",
    "\n",
    "# Validate experiment registry (from Cell 0)\n",
    "required_keys = {\"train_out\", \"model_out\", \"feature_mode\"}\n",
    "allowed_modes = {\"prompt_only\", \"map_only\", \"prompt_plus_map\"}\n",
    "\n",
    "for exp_name, exp_cfg in EXPERIMENTS.items():\n",
    "    missing = required_keys - set(exp_cfg.keys())\n",
    "    if missing:\n",
    "        raise ValueError(f\"Experiment '{exp_name}' is missing keys: {missing}\")\n",
    "\n",
    "    mode = str(exp_cfg[\"feature_mode\"]).strip().lower()\n",
    "    if mode not in allowed_modes:\n",
    "        raise ValueError(\n",
    "            f\"Experiment '{exp_name}' has invalid feature_mode='{exp_cfg['feature_mode']}'. \"\n",
    "            f\"Allowed: {sorted(allowed_modes)}\"\n",
    "        )\n",
    "    exp_cfg[\"feature_mode\"] = mode\n",
    "\n",
    "    exp_cfg[\"train_out\"] = Path(exp_cfg[\"train_out\"])\n",
    "    exp_cfg[\"model_out\"] = Path(exp_cfg[\"model_out\"])\n",
    "\n",
    "    exp_cfg[\"train_out\"].mkdir(parents=True, exist_ok=True)\n",
    "    exp_cfg[\"model_out\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Prompt encoder required whenever prompts are part of features\n",
    "    if mode in {\"prompt_only\", \"prompt_plus_map\"}:\n",
    "        if \"prompt_encoder_kind\" not in exp_cfg:\n",
    "            raise ValueError(\n",
    "                f\"Experiment '{exp_name}' needs 'prompt_encoder_kind' because feature_mode='{mode}'.\"\n",
    "            )\n",
    "\n",
    "    if mode == \"map_only\":\n",
    "        exp_cfg.pop(\"prompt_encoder_kind\", None)\n",
    "\n",
    "\n",
    "print(\"\\nðŸ§ª Experiments to be executed:\")\n",
    "for exp_name, exp_cfg in EXPERIMENTS.items():\n",
    "    pe = exp_cfg.get(\"prompt_encoder_kind\", \"-\")\n",
    "    print(\n",
    "        f\" - {exp_name:12s} | \"\n",
    "        f\"mode={exp_cfg['feature_mode']:14s} | \"\n",
    "        f\"prompt={pe:12s} | \"\n",
    "        f\"train_out={exp_cfg['train_out'].name} | \"\n",
    "        f\"model_out={exp_cfg['model_out'].name}\"\n",
    "    )\n",
    "\n",
    "def get_feature_dims_from_cfg(feature_mode: str):\n",
    "    fm = str(feature_mode).strip().lower()\n",
    "    if fm == \"USE_prompt_only\":\n",
    "        return 0, PROMPT_DIM_CFG, PROMPT_DIM_CFG\n",
    "    if fm == \"map_only\":\n",
    "        return MAP_DIM_CFG, 0, MAP_DIM_CFG\n",
    "    if fm in {\"prompt_plus_map\", \"use_map\", \"openai_map\"}:\n",
    "        return MAP_DIM_CFG, PROMPT_DIM_CFG, FUSED_DIM_CFG\n",
    "    raise ValueError(f\"Unknown feature_mode: {feature_mode}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a2350e",
   "metadata": {},
   "source": [
    "## Step 2 â€” Prompt Embedding Generation (All Experiments)\n",
    "\n",
    "In this step, we generate vector embeddings for all user prompts for **each experiment\n",
    "configuration** (e.g., prompt-only, USE + map, OpenAI + map). The embedding backend is\n",
    "selected per experiment via `PROMPT_ENCODER` (e.g., **USE-DAN/Transformer** or **OpenAI\n",
    "text-embedding-3-* models**).\n",
    "\n",
    "### What happens here?\n",
    "\n",
    "For each entry in the experiment registry:\n",
    "\n",
    "- Prompts are loaded from the user study source file.\n",
    "- Only valid prompts are kept (`complete == True`, `remove == False`).\n",
    "- The prompt embedding model is selected from the experiment configuration\n",
    "  (mapped to `CFG.PROMPT_ENCODER`).\n",
    "- All prompts are embedded in batches (with optional L2 normalization).\n",
    "- The resulting embeddings and metadata are saved to an **experiment-specific folder**\n",
    "  so no configuration overwrites another.\n",
    "\n",
    "**Outputs per experiment** (written under `PATHS.PROMPT_OUT/<experiment_name>/`):\n",
    "\n",
    "- `prompts_embeddings.npz` â€” matrix `E` and `ids`\n",
    "- `prompts.parquet` â€” prompt_id, text, tile_id\n",
    "- `meta.json` â€” model label, dimensionality, and export info\n",
    "\n",
    "### Why this is encapsulated in a helper\n",
    "\n",
    "To keep the notebook clean and reproducible, all logic related to:\n",
    "\n",
    "- loading and filtering prompts,\n",
    "- selecting the embedding backend,\n",
    "- batching and normalization,\n",
    "- saving outputs in a consistent format,\n",
    "\n",
    "is encapsulated in `src/train/run_prompt_embeddings.py`.\n",
    "\n",
    "The notebook only *orchestrates* experiments by calling this helper with an\n",
    "experiment-specific configuration.\n",
    "\n",
    "This design ensures:\n",
    "\n",
    "- consistent prompt embeddings across training and evaluation,\n",
    "- easy comparison between USE and OpenAI backends,\n",
    "- clean separation between experiment orchestration (notebook) and implementation (src),\n",
    "- safe parallel storage of artifacts for multiple experiment runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0df45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:21:55.572701Z",
     "start_time": "2025-10-27T11:21:55.570071Z"
    }
   },
   "outputs": [],
   "source": [
    "# ===================== CELL 2 â€” Prompt embeddings (experiment-scoped) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "from dataclasses import replace\n",
    "\n",
    "import src.config as CONFIG\n",
    "from src.train.run_prompt_embeddings import run_prompt_embeddings_from_config\n",
    "\n",
    "print(\"\\n=== Running prompt embeddings for experiments that require prompts ===\")\n",
    "\n",
    "prompt_meta_by_experiment = {}\n",
    "\n",
    "# IMPORTANT: because prompt_id is now read from Excel, old artifacts are stale.\n",
    "FORCE_REBUILD_PROMPTS = False  # set False later when stable\n",
    "\n",
    "for exp_name, exp_cfg in EXPERIMENTS.items():\n",
    "    feature_mode = exp_cfg[\"feature_mode\"]\n",
    "\n",
    "    if feature_mode == \"map_only\":\n",
    "        print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "        print(\"   (skip) feature_mode=map_only â†’ no prompt embeddings required.\")\n",
    "        continue\n",
    "\n",
    "    prompt_encoder_kind = exp_cfg.get(\"prompt_encoder_kind\", CONFIG.CFG.PROMPT_ENCODER)\n",
    "    CFG_EXP = replace(CONFIG.CFG, PROMPT_ENCODER=prompt_encoder_kind)\n",
    "\n",
    "    prompt_out_dir = Path(CONFIG.PATHS.PROMPT_OUT) / exp_name\n",
    "    prompt_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    emb_npz = prompt_out_dir / \"prompts_embeddings.npz\"\n",
    "    prm_pq  = prompt_out_dir / \"prompts.parquet\"\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   feature_mode   : {feature_mode}\")\n",
    "    print(f\"   PROMPT_ENCODER : {CFG_EXP.PROMPT_ENCODER}\")\n",
    "    print(f\"   Output dir     : {prompt_out_dir}\")\n",
    "\n",
    "    if (not FORCE_REBUILD_PROMPTS) and emb_npz.exists() and prm_pq.exists():\n",
    "        print(\"   âœ… Prompt embeddings already exist â€” skipping recomputation.\")\n",
    "        meta = {\n",
    "            \"out_dir\": str(prompt_out_dir),\n",
    "            \"embeddings_path\": str(emb_npz),\n",
    "            \"prompts_parquet_path\": str(prm_pq),\n",
    "            \"skipped\": True,\n",
    "        }\n",
    "    else:\n",
    "        meta = run_prompt_embeddings_from_config(\n",
    "            input_path=Path(CONFIG.PATHS.USER_STUDY_XLSX),\n",
    "            out_dir=prompt_out_dir,\n",
    "            cfg=CFG_EXP,\n",
    "            paths=CONFIG.PATHS,\n",
    "            verbosity=1,\n",
    "            l2_normalize=True,\n",
    "            also_save_embeddings_csv=False,\n",
    "        )\n",
    "        print(\"   âœ… Prompt embeddings completed.\")\n",
    "\n",
    "    prompt_meta_by_experiment[exp_name] = meta\n",
    "\n",
    "print(\"\\nâœ… Prompt embedding step finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c0b51",
   "metadata": {},
   "source": [
    "## Step 3 â€” Map Embeddings (Dynamic, Shared Across Experiments)\n",
    "\n",
    "In this step, we compute **map embeddings** for all GeoJSON tiles that are eligible for the user study.  \n",
    "Because map embeddings depend only on the input map data (not on the prompt encoder), they are computed **once** and stored in a **shared output folder**, then reused across all experiments.\n",
    "\n",
    "### What this step does\n",
    "\n",
    "1. **Filters tiles using the user study Excel**\n",
    "   - Keeps only rows marked as *complete*\n",
    "   - Excludes rows marked as *removed*\n",
    "   - Extracts the set of allowed `tile_id`s (used to select which map folders to embed)\n",
    "\n",
    "2. **Discovers and embeds GeoJSON maps**\n",
    "   - Finds all GeoJSON files under `PATHS.MAPS_ROOT`\n",
    "   - Keeps only those whose `map_id` is in the allowed set\n",
    "   - Counts valid polygons per map to determine a dataset-wide `max_polygons`\n",
    "     (used to normalize the `poly_count` feature safely)\n",
    "\n",
    "3. **Computes map embeddings**\n",
    "   - Uses `norm=\"extent\"` for **dynamic per-map normalization**\n",
    "   - Ensures all vectors have consistent dimensionality\n",
    "   - Skips maps with invalid geometries or degenerate extents\n",
    "\n",
    "4. **Stores dynamic extent references (required for parameter scaling)**\n",
    "   - `extent_diag_m`\n",
    "   - `extent_area_m2`\n",
    "\n",
    "   These are saved alongside embeddings and are later used to convert\n",
    "   normalized parameters (`param_norm`) into real-world units:\n",
    "   - distance operators â†’ meters via `extent_diag_m`\n",
    "   - area operators â†’ mÂ² via `extent_area_m2`\n",
    "\n",
    "5. **Writes outputs once to a shared directory**\n",
    "   - Prevents redundant computation across experiments\n",
    "   - Guarantees every experiment uses the **same map representation**\n",
    "   - Avoids accidental overwrites while keeping artifacts reusable\n",
    "\n",
    "### Why this is important\n",
    "\n",
    "- Ensures **consistent normalization** between training and evaluation  \n",
    "- Provides the necessary per-map reference scales for parameter un-normalization  \n",
    "- Improves reproducibility and efficiency by reusing identical map embeddings  \n",
    "- Supports fair comparison between:\n",
    "  - prompt-only baselines (which ignore map embeddings)\n",
    "  - fused prompt + map hybrids\n",
    "  - different prompt backends (USE vs OpenAI)\n",
    "\n",
    "At the end of this step, the repository contains a self-contained set of map embeddings\n",
    "ready to be concatenated with prompt embeddings in the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca0c3d8b71fc70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:26:59.687350Z",
     "start_time": "2025-10-27T11:26:19.901557Z"
    }
   },
   "outputs": [],
   "source": [
    "# ===================== CELL 3 â€” Map embeddings (shared) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "import src.config as CONFIG\n",
    "from src.train.run_map_embeddings import run_map_embeddings_from_config\n",
    "\n",
    "# Map embeddings do NOT depend on prompt backend, so compute once and reuse.\n",
    "MAP_EMB_DIR = Path(CONFIG.PATHS.MAP_OUT) / \"shared_extent\"\n",
    "MAP_EMB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "maps_npz = MAP_EMB_DIR / \"maps_embeddings.npz\"\n",
    "maps_pq  = MAP_EMB_DIR / \"maps.parquet\"\n",
    "\n",
    "print(\"\\n=== Map embeddings (shared across all experiments) ===\")\n",
    "print(\"Target dir:\", MAP_EMB_DIR)\n",
    "\n",
    "# If you changed anything about map embedding logic, set this True once.\n",
    "FORCE_REBUILD_MAPS = False\n",
    "\n",
    "if (not FORCE_REBUILD_MAPS) and maps_npz.exists() and maps_pq.exists():\n",
    "    print(\"âœ… Map embeddings already exist â€” skipping recomputation.\")\n",
    "    map_meta = {\"out_dir\": str(MAP_EMB_DIR), \"skipped\": True}\n",
    "else:\n",
    "    map_meta = run_map_embeddings_from_config(\n",
    "        maps_root=Path(CONFIG.PATHS.MAPS_ROOT),\n",
    "        input_pattern=CONFIG.PATHS.INPUT_MAPS_PATTERN,\n",
    "        user_study_xlsx=Path(CONFIG.PATHS.USER_STUDY_XLSX),\n",
    "        responses_sheet=CONFIG.PATHS.RESPONSES_SHEET,\n",
    "        tile_id_col=CONFIG.PATHS.TILE_ID_COL,\n",
    "        complete_col=CONFIG.PATHS.COMPLETE_COL,\n",
    "        remove_col=CONFIG.PATHS.REMOVE_COL,\n",
    "        only_complete=False,\n",
    "        exclude_removed=False,\n",
    "        out_dir=MAP_EMB_DIR,\n",
    "        verbosity=1,\n",
    "        norm=\"extent\",\n",
    "    )\n",
    "    print(\"âœ… Map embeddings completed.\")\n",
    "\n",
    "if not maps_npz.exists():\n",
    "    raise FileNotFoundError(f\"Missing maps_embeddings.npz at: {maps_npz}\")\n",
    "if not maps_pq.exists():\n",
    "    raise FileNotFoundError(f\"Missing maps.parquet at: {maps_pq}\")\n",
    "\n",
    "print(\"âœ… Map embedding artifacts ready:\")\n",
    "print(\" -\", maps_npz)\n",
    "print(\" -\", maps_pq)\n",
    "print(\"MAP_EMB_DIR:\", MAP_EMB_DIR)\n",
    "print(map_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaf32c5",
   "metadata": {},
   "source": [
    "### ðŸ”¢ Inferring Embedding Dimensions (Experiment-Aware)\n",
    "\n",
    "In this step, we **infer embedding dimensionalities directly from the saved embedding files**\n",
    "rather than relying on configuration defaults. Dimensions are inferred **per experiment** to\n",
    "account for different prompt embedding backends (e.g., USE vs OpenAI), while map embeddings\n",
    "are shared across experiments.\n",
    "\n",
    "This ensures:\n",
    "- A **single source of truth** for feature dimensions\n",
    "- **Consistency** between training and evaluation pipelines\n",
    "- Robustness to changes in embedding models or backend configurations\n",
    "- Correct handling of mixed feature modes (prompt-only vs. fused prompt + map)\n",
    "\n",
    "Specifically, we:\n",
    "\n",
    "- Load **prompt embeddings** from  \n",
    "  `PATHS.PROMPT_OUT/<experiment_name>/prompts_embeddings.npz`\n",
    "- Load **map embeddings** from the shared map embedding directory  \n",
    "  `PATHS.MAP_OUT/<shared_folder>/maps_embeddings.npz`\n",
    "- Infer dimensions as follows:\n",
    "  - `PROMPT_DIM` â€” from prompt embeddings (per experiment)\n",
    "  - `MAP_DIM` â€” from map embeddings (shared)\n",
    "  - `FUSED_DIM` â€” computed per experiment:\n",
    "    - `prompt_only` â†’ `PROMPT_DIM`\n",
    "    - `fused` â†’ `MAP_DIM + PROMPT_DIM`\n",
    "\n",
    "If inferred dimensions differ from those defined in the global configuration (`CFG`),\n",
    "the inferred values take precedence for all downstream processing.\n",
    "\n",
    "The inferred dimensions are stored in the experiment registry and used consistently by:\n",
    "- feature preprocessing\n",
    "- operator classification\n",
    "- parameter regression\n",
    "- evaluation and inference\n",
    "\n",
    "This guarantees that all downstream models operate on **correctly shaped feature vectors**\n",
    "and that comparisons between experiments remain valid and reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8cdf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== CELL 4 â€” Infer embedding dimensions (multi-experiment, incl. map-only) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import src.config as CONFIG\n",
    "\n",
    "def _infer_dim_from_npz(npz_path: Path) -> int:\n",
    "    if not npz_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing embeddings file: {npz_path}\")\n",
    "    with np.load(npz_path, allow_pickle=True) as z:\n",
    "        if \"E\" not in z:\n",
    "            raise ValueError(f\"{npz_path} missing array 'E'\")\n",
    "        E = z[\"E\"]\n",
    "    if E.ndim != 2 or E.shape[1] <= 0:\n",
    "        raise ValueError(f\"Invalid embedding matrix in {npz_path}: shape={E.shape}\")\n",
    "    return int(E.shape[1])\n",
    "\n",
    "# -------------------------------\n",
    "# Map dim (shared across all experiments)\n",
    "# -------------------------------\n",
    "maps_npz = Path(MAP_EMB_DIR) / \"maps_embeddings.npz\"\n",
    "MAP_DIM_INF = _infer_dim_from_npz(maps_npz)\n",
    "print(\"âœ… Inferred MAP_DIM from shared maps:\", MAP_DIM_INF)\n",
    "\n",
    "# -------------------------------\n",
    "# Prompt dim per experiment + final input dim per experiment\n",
    "# -------------------------------\n",
    "dims_by_experiment = {}\n",
    "\n",
    "PROMPT_BASED_MODES = {\"prompt_only\", \"prompt_plus_map\"}\n",
    "\n",
    "for exp_name, exp_cfg in EXPERIMENTS.items():\n",
    "    feature_mode = str(exp_cfg[\"feature_mode\"]).strip().lower()\n",
    "\n",
    "    # infer prompt dim if this mode uses prompts\n",
    "    PROMPT_DIM_INF = 0\n",
    "    if feature_mode in PROMPT_BASED_MODES:\n",
    "        prm_npz = Path(CONFIG.PATHS.PROMPT_OUT) / exp_name / \"prompts_embeddings.npz\"\n",
    "        PROMPT_DIM_INF = _infer_dim_from_npz(prm_npz)\n",
    "\n",
    "    if feature_mode == \"prompt_only\":\n",
    "        map_dim = 0\n",
    "        prompt_dim = PROMPT_DIM_INF\n",
    "        fused_dim = PROMPT_DIM_INF\n",
    "\n",
    "    elif feature_mode == \"map_only\":\n",
    "        map_dim = MAP_DIM_INF\n",
    "        prompt_dim = 0\n",
    "        fused_dim = MAP_DIM_INF\n",
    "\n",
    "    elif feature_mode == \"prompt_plus_map\":\n",
    "        map_dim = MAP_DIM_INF\n",
    "        prompt_dim = PROMPT_DIM_INF\n",
    "        fused_dim = MAP_DIM_INF + PROMPT_DIM_INF\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown feature_mode for {exp_name}: {feature_mode}\")\n",
    "\n",
    "    exp_cfg[\"map_dim\"] = int(map_dim)\n",
    "    exp_cfg[\"prompt_dim\"] = int(prompt_dim)\n",
    "    exp_cfg[\"fused_dim\"] = int(fused_dim)\n",
    "\n",
    "    dims_by_experiment[exp_name] = {\n",
    "        \"feature_mode\": feature_mode,\n",
    "        \"MAP_DIM\": int(map_dim),\n",
    "        \"PROMPT_DIM\": int(prompt_dim),\n",
    "        \"FUSED_DIM\": int(fused_dim),\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"\\nâœ… Inferred dims per experiment:\")\n",
    "for exp_name, d in dims_by_experiment.items():\n",
    "    print(\n",
    "        f\" - {exp_name:12s} | mode={d['feature_mode']:14s} | \"\n",
    "        f\"MAP_DIM={d['MAP_DIM']:4d} | PROMPT_DIM={d['PROMPT_DIM']:4d} | FUSED_DIM={d['FUSED_DIM']:4d}\"\n",
    "    )\n",
    "\n",
    "if MAP_DIM_INF != int(CONFIG.CFG.MAP_DIM):\n",
    "    print(\"\\nâš ï¸ CONFIG.CFG.MAP_DIM differs from inferred MAP_DIM (using inferred for experiments).\")\n",
    "    print(f\"   inferred MAP_DIM={MAP_DIM_INF} vs CONFIG.CFG.MAP_DIM={CONFIG.CFG.MAP_DIM}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd186319e89f445",
   "metadata": {},
   "source": [
    "### ðŸ”— Feature Construction (Prompt-Only vs. Fused Map + Prompt Embeddings)\n",
    "\n",
    "In this step, we construct the final feature matrices used for training and evaluation by\n",
    "aligning prompt embeddings with map embeddings and exporting **experiment-scoped** artifacts.\n",
    "\n",
    "#### What this step does (per experiment)\n",
    "\n",
    "- Loads **prompt embeddings** from  \n",
    "  `PATHS.PROMPT_OUT/<experiment_name>/prompts_embeddings.npz`\n",
    "- Loads **map embeddings** from the shared map embedding directory  \n",
    "  `PATHS.MAP_OUT/<shared_folder>/maps_embeddings.npz`\n",
    "- Aligns samples via the authoritative pairing table (`prompts.parquet` â†’ `map_id/tile_id` + `prompt_id`)\n",
    "- Merges **dynamic map extent metadata** (e.g., `extent_diag_m`, `extent_area_m2`) from `maps.parquet`\n",
    "  so downstream regression can convert normalized parameters into real-world units.\n",
    "\n",
    "#### Feature modes supported\n",
    "\n",
    "- **`prompt_only`**  \n",
    "  Uses only prompt vectors:  \n",
    "  `X = prompt_embedding`\n",
    "\n",
    "- **`fused`**  \n",
    "  Concatenates map and prompt vectors:  \n",
    "  `X = [map_embedding | prompt_embedding]`\n",
    "\n",
    "#### Outputs written per experiment\n",
    "\n",
    "All artifacts are saved into the experimentâ€™s `train_out` directory (to prevent overwrites):\n",
    "\n",
    "- `X_prompt.npy` or `X_concat.npy` â€” final feature matrix (depending on feature mode)\n",
    "- `train_pairs.parquet` â€” aligned metadata (including `operator`, `param_value`, and extent references)\n",
    "- `meta.json` â€” provenance (sources, options) and shape information\n",
    "\n",
    "#### Why this design\n",
    "\n",
    "- Keeps **training and evaluation perfectly aligned** by exporting a single, consistent pairing table\n",
    "- Avoids hard-coded dimensions by relying on the saved embedding files\n",
    "- Supports **multiple experiments side-by-side** without overwriting artifacts\n",
    "- Enables **dynamic extent-aware** parameter regression (meters / mÂ² scaling) downstream\n",
    "- Ensures fair comparison: prompt-only baselines vs. fused map+prompt models\n",
    "\n",
    "After this step, each experiment has a complete, self-contained dataset ready for:\n",
    "1. Operator classification  \n",
    "2. Per-operator parameter regression  \n",
    "3. End-to-end evaluation in the evaluation notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2b07a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== CELL 5 â€” Feature construction (multi-experiment) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "import src.config as CONFIG\n",
    "\n",
    "from src.train.run_concat_features import run_concat_features_from_dirs\n",
    "\n",
    "print(\"\\n=== Building feature matrices for all experiments ===\")\n",
    "\n",
    "concat_meta_by_experiment = {}\n",
    "\n",
    "# Because prompt_id changed, you should rebuild features at least once.\n",
    "FORCE_REBUILD_FEATURES = True  # set False later\n",
    "\n",
    "# Choose a canonical source of prompts.parquet (pairs table) for map_only.\n",
    "# Any prompt-based experiment works as long as it produced prompts.parquet.\n",
    "PAIRS_SOURCE_EXP = \"use_prompt_only\"\n",
    "PAIRS_PARQUET_CANON = Path(CONFIG.PATHS.PROMPT_OUT) / PAIRS_SOURCE_EXP / \"prompts.parquet\"\n",
    "\n",
    "if not PAIRS_PARQUET_CANON.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Expected pairs parquet for map_only at {PAIRS_PARQUET_CANON}. \"\n",
    "        f\"Run Cell 2 (prompt embeddings) for '{PAIRS_SOURCE_EXP}' first.\"\n",
    "    )\n",
    "\n",
    "for exp_name, exp_cfg in EXPERIMENTS.items():\n",
    "    feature_mode = exp_cfg[\"feature_mode\"]\n",
    "\n",
    "    train_out_dir = Path(exp_cfg[\"train_out\"])\n",
    "    train_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    map_out_dir = MAP_EMB_DIR\n",
    "    prompt_out_dir = Path(CONFIG.PATHS.PROMPT_OUT) / exp_name  # per-experiment prompt embeddings dir\n",
    "\n",
    "    # âœ… NEW: for map_only we still need a pairs table, but not prompt embeddings\n",
    "    pairs_parquet = None\n",
    "    if feature_mode == \"map_only\":\n",
    "        pairs_parquet = PAIRS_PARQUET_CANON\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   Feature mode : {feature_mode}\")\n",
    "    print(f\"   Prompt out   : {prompt_out_dir}\")\n",
    "    if pairs_parquet is not None:\n",
    "        print(f\"   Pairs parquet: {pairs_parquet}  (shared)\")\n",
    "    print(f\"   Map out      : {map_out_dir}\")\n",
    "    print(f\"   Train out    : {train_out_dir}\")\n",
    "\n",
    "    # Expected outputs for this experiment\n",
    "    X_expected = train_out_dir / f\"X_{exp_name}.npy\"\n",
    "    pairs_expected = train_out_dir / f\"train_pairs_{exp_name}.parquet\"\n",
    "\n",
    "    if (not FORCE_REBUILD_FEATURES) and X_expected.exists() and pairs_expected.exists():\n",
    "        print(\"   âœ… Features already exist â€” skipping recomputation.\")\n",
    "        meta = {\n",
    "            \"skipped\": True,\n",
    "            \"X_path\": str(X_expected),\n",
    "            \"pairs_path\": str(pairs_expected),\n",
    "        }\n",
    "    else:\n",
    "        meta = run_concat_features_from_dirs(\n",
    "            prompt_out_dir=prompt_out_dir,\n",
    "            map_out_dir=map_out_dir,\n",
    "            out_dir=train_out_dir,\n",
    "            exp_name=exp_name,\n",
    "            feature_mode=feature_mode,\n",
    "            verbosity=1,\n",
    "            prompt_id_width=4,\n",
    "            pairs_parquet=pairs_parquet,\n",
    "        )\n",
    "        print(\"   âœ… Feature construction completed.\")\n",
    "\n",
    "    concat_meta_by_experiment[exp_name] = meta\n",
    "\n",
    "print(\"\\nâœ… All feature construction finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5142d4b68c273d37",
   "metadata": {},
   "source": [
    "## Step 6 â€” Load Training Data and Build Normalized Regression Target (Experiment-Aware)\n",
    "\n",
    "In this step, we load the **experiment-specific training data** produced by the feature\n",
    "construction stage and build the final learning targets for both classification and regression.\n",
    "\n",
    "Because this notebook runs **multiple experiments**, the same procedure is applied\n",
    "**independently for each experiment**, using its own `train_out` directory.\n",
    "\n",
    "### What happens here (per experiment)\n",
    "\n",
    "1. **Load the feature matrix**\n",
    "   - `prompt_only` experiments load prompt-only features (e.g., `X_prompt.npy`)\n",
    "   - fused experiments load concatenated features (e.g., `X_concat.npy`)\n",
    "\n",
    "2. **Load the paired metadata table**\n",
    "   - `train_pairs.parquet` containing aligned `(map_id, prompt_id)` rows and dynamic extent references\n",
    "\n",
    "3. **Attach labels and apply consistent filtering**\n",
    "   - Ensures only valid user study rows are included (e.g., `complete == True`, `remove == False`)\n",
    "   - Ensures `operator` and `param_value` are present (and prompt text if required)\n",
    "\n",
    "4. **Validate dynamic extent references**\n",
    "   - Confirms the presence of per-map reference scales required for normalization:\n",
    "     - `extent_diag_m`\n",
    "     - `extent_area_m2`\n",
    "\n",
    "5. **Compute the normalized regression target `param_norm`**\n",
    "   Normalization depends on the operator group:\n",
    "\n",
    "   - **Distance-based operators** (`aggregate`, `displace`, `simplify`):  \n",
    "     `param_norm = param_value / extent_diag_m`\n",
    "\n",
    "   - **Area-based operators** (`select`):  \n",
    "     `param_norm = param_value / extent_area_m2`\n",
    "\n",
    "### Why this normalization is used\n",
    "\n",
    "This step converts parameters from heterogeneous, map-scale-dependent units into a\n",
    "scale-aware normalized target. It allows per-operator regressors to generalize across\n",
    "maps of different extents while preserving physical meaning during inference\n",
    "(when `param_norm` is converted back to meters or mÂ² using the same extent references).\n",
    "\n",
    "### Outputs\n",
    "\n",
    "For each experiment, we obtain:\n",
    "\n",
    "- `X` â€” feature matrix aligned with labels  \n",
    "- `df` â€” cleaned metadata table including `operator`, `param_value`, `param_norm`, and extent references  \n",
    "\n",
    "These outputs feed directly into the subsequent training stages:\n",
    "1. Operator classification  \n",
    "2. Per-operator parameter regression  \n",
    "3. End-to-end evaluation across experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a494fd27dfe7681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== CELL 6 â€” Load training data + compute param_norm (collision-proof) =====================\n",
    "\n",
    "from dataclasses import replace\n",
    "from pathlib import Path\n",
    "\n",
    "import src.config as CONFIG   # âœ… use a name you will NOT reuse accidentally\n",
    "\n",
    "from src.train.load_training_data import load_training_data_with_dynamic_param_norm\n",
    "\n",
    "TRAIN_DATA = {}\n",
    "\n",
    "print(\"\\n=== Loading training data for all experiments (unified loader) ===\")\n",
    "print(\"CONFIG type:\", type(CONFIG))\n",
    "\n",
    "for exp_name, exp_cfg in EXPERIMENTS.items():\n",
    "    train_out_dir = Path(exp_cfg[\"train_out\"])\n",
    "    if not train_out_dir.exists():\n",
    "        raise FileNotFoundError(f\"Missing train_out for {exp_name}: {train_out_dir}\")\n",
    "\n",
    "    feature_mode = str(exp_cfg[\"feature_mode\"]).strip().lower()\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   train_out : {train_out_dir}\")\n",
    "    print(f\"   mode      : {feature_mode}\")\n",
    "\n",
    "    PATHS_EXP = replace(CONFIG.PATHS, TRAIN_OUT=str(train_out_dir))\n",
    "\n",
    "    data = load_training_data_with_dynamic_param_norm(\n",
    "        exp_name=exp_name,\n",
    "        feature_mode=feature_mode,\n",
    "        paths=PATHS_EXP,\n",
    "        cfg=CONFIG,\n",
    "        distance_ops=CONFIG.DISTANCE_OPS,\n",
    "        area_ops=CONFIG.AREA_OPS,\n",
    "        require_text=True,\n",
    "    )\n",
    "\n",
    "    X = data.X\n",
    "    df = data.df\n",
    "\n",
    "    print(f\"   âœ… Loaded: X={X.shape} | df={df.shape}\")\n",
    "    print(\"   Operators:\", sorted(df[PATHS_EXP.OPERATOR_COL].dropna().unique().tolist()))\n",
    "\n",
    "    TRAIN_DATA[exp_name] = {\"X\": X, \"df\": df, \"paths\": PATHS_EXP}\n",
    "\n",
    "first_key = next(iter(TRAIN_DATA.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997cab7",
   "metadata": {},
   "source": [
    "## Step 7 â€” Train/Validation/Test Split (Shared, Leakage-Free by Map)\n",
    "\n",
    "In this step, we construct a **reproducible and fair** train/validation/test split that is\n",
    "**shared across all experiments** (prompt-only, USE + map, OpenAI + map).  \n",
    "The split is computed **once** and then applied to every experiment to ensure that\n",
    "performance differences are attributable solely to the feature representation.\n",
    "\n",
    "---\n",
    "\n",
    "### Constraints enforced\n",
    "\n",
    "- **No leakage by map**  \n",
    "  The same `map_id` never appears in more than one split (train / validation / test).\n",
    "\n",
    "- **Multi-prompt maps are forced into TRAIN**  \n",
    "  If a map has multiple prompts, *all* corresponding samples are assigned to the\n",
    "  training set.  \n",
    "  As a result, validation and test sets contain **single-prompt maps only**.\n",
    "\n",
    "- **Consistency across experiments**  \n",
    "  The exact same samples (identified by `(map_id, prompt_id)`) are used for\n",
    "  train/validation/test in every experiment.\n",
    "\n",
    "---\n",
    "\n",
    "### Stratification strategy\n",
    "\n",
    "To obtain balanced splits while respecting the above constraints, stratification is applied as:\n",
    "\n",
    "- Primary: `operator Ã— intensity` (if sufficient samples exist), otherwise\n",
    "- Fallback: `operator` only (automatically selected if finer stratification is infeasible)\n",
    "\n",
    "---\n",
    "\n",
    "### Coverage requirement\n",
    "\n",
    "Each split is required to contain **all operators** in the fixed class set:\n",
    "\n",
    "- `simplify`\n",
    "- `select`\n",
    "- `aggregate`\n",
    "- `displace`\n",
    "\n",
    "This guarantees that classification and regression models can be trained and evaluated\n",
    "for every operator.\n",
    "\n",
    "---\n",
    "\n",
    "### Outputs\n",
    "\n",
    "- A single shared split definition is saved to disk as:  \n",
    "  `splits_shared.json`\n",
    "- For each experiment, the split is applied to slice:\n",
    "  - `X` â€” the feature matrix\n",
    "  - `df` â€” the aligned metadata table\n",
    "\n",
    "The resulting subsets (`train`, `val`, `test`) are then used in all downstream\n",
    "training and evaluation steps.\n",
    "\n",
    "---\n",
    "\n",
    "This design ensures:\n",
    "- **Leakage-free evaluation**\n",
    "- **Fair, apples-to-apples comparison** between experiments\n",
    "- **Reproducibility**, since the split is deterministic and persisted to disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc0897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== CELL 7 â€” Shared Train/Val/Test Split (fair across experiments, incl. map-only) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import src.config as CONFIG\n",
    "from src.train.splitting import make_splits_multi_prompt_to_train\n",
    "\n",
    "FIXED_CLASSES = [\"simplify\", \"select\", \"aggregate\", \"displace\"]\n",
    "USE_INTENSITY_FOR_STRAT = True\n",
    "\n",
    "OP_COL  = CONFIG.PATHS.OPERATOR_COL\n",
    "INT_COL = CONFIG.PATHS.INTENSITY_COL\n",
    "\n",
    "# Where to save ONE shared split (used by all experiments)\n",
    "SPLITS_DIR = Path(CONFIG.PATHS.SPLIT_OUT)\n",
    "SPLITS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "split_path = SPLITS_DIR / \"splits_shared.json\"\n",
    "\n",
    "# -------------------------------\n",
    "# Choose a stable reference experiment\n",
    "# Prefer prompt-based; avoid map_only if possible.\n",
    "# -------------------------------\n",
    "preferred_order = [\"use_prompt_only\", \"use_map\", \"openai_map\", \"map_only\"]\n",
    "ref_exp = next((name for name in preferred_order if name in TRAIN_DATA), None)\n",
    "if ref_exp is None:\n",
    "    ref_exp = next(iter(TRAIN_DATA.keys()))\n",
    "\n",
    "ref_df = TRAIN_DATA[ref_exp][\"df\"].copy()\n",
    "ref_X  = TRAIN_DATA[ref_exp][\"X\"]\n",
    "\n",
    "# Must have keys for stable matching across experiments\n",
    "if not {\"map_id\", \"prompt_id\"}.issubset(ref_df.columns):\n",
    "    raise ValueError(\"Expected columns {'map_id','prompt_id'} in df for split mapping.\")\n",
    "\n",
    "# Must have operator for stratification constraints\n",
    "if OP_COL not in ref_df.columns:\n",
    "    raise ValueError(f\"Reference df missing operator column '{OP_COL}'. Cannot build stratified split.\")\n",
    "\n",
    "# Build a stable key per row for mapping splits across experiments\n",
    "ref_df[\"row_key\"] = ref_df[\"map_id\"].astype(str).str.zfill(4) + \"::\" + ref_df[\"prompt_id\"].astype(str)\n",
    "\n",
    "print(f\"\\n=== Computing shared split using reference experiment: {ref_exp} ===\")\n",
    "print(\"ref_df:\", ref_df.shape, \"| ref_X:\", ref_X.shape)\n",
    "\n",
    "split = make_splits_multi_prompt_to_train(\n",
    "    df=ref_df,\n",
    "    X=ref_X,\n",
    "    op_col=OP_COL,\n",
    "    intensity_col=INT_COL if (USE_INTENSITY_FOR_STRAT and INT_COL in ref_df.columns) else None,\n",
    "    map_id_col=\"map_id\",\n",
    "    fixed_classes=FIXED_CLASSES,\n",
    "    use_intensity_for_strat=USE_INTENSITY_FOR_STRAT,\n",
    "    seed=int(CONFIG.CFG.SEED),\n",
    "    val_ratio=float(CONFIG.CFG.VAL_RATIO),\n",
    "    test_ratio=float(CONFIG.CFG.TEST_RATIO),\n",
    "    max_attempts=500,\n",
    "    save_splits_json=split_path,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "train_idx_ref, val_idx_ref, test_idx_ref = split.train_idx, split.val_idx, split.test_idx\n",
    "\n",
    "# Convert indices -> row_key sets (transfer across experiments)\n",
    "train_keys = set(ref_df.loc[train_idx_ref, \"row_key\"].tolist())\n",
    "val_keys   = set(ref_df.loc[val_idx_ref,   \"row_key\"].tolist()) if len(val_idx_ref) else set()\n",
    "test_keys  = set(ref_df.loc[test_idx_ref,  \"row_key\"].tolist()) if len(test_idx_ref) else set()\n",
    "\n",
    "# Sanity: no overlap\n",
    "assert train_keys.isdisjoint(val_keys)\n",
    "assert train_keys.isdisjoint(test_keys)\n",
    "assert val_keys.isdisjoint(test_keys)\n",
    "\n",
    "print(\"\\nâœ… Shared split created:\")\n",
    "print(f\"   Train keys: {len(train_keys)} | Val keys: {len(val_keys)} | Test keys: {len(test_keys)}\")\n",
    "print(f\"   Saved to  : {split_path}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Apply the SAME split to every experiment by mapping row_key -> indices\n",
    "# -------------------------------\n",
    "SPLITS = {}  # exp_name -> dict with X_train/X_val/X_test and df_train/df_val/df_test\n",
    "\n",
    "for exp_name, pack in TRAIN_DATA.items():\n",
    "    df = pack[\"df\"].copy()\n",
    "    X  = pack[\"X\"]\n",
    "\n",
    "    if not {\"map_id\", \"prompt_id\"}.issubset(df.columns):\n",
    "        raise ValueError(f\"Experiment '{exp_name}' df missing map_id/prompt_id needed for split mapping.\")\n",
    "\n",
    "    df[\"row_key\"] = df[\"map_id\"].astype(str).str.zfill(4) + \"::\" + df[\"prompt_id\"].astype(str)\n",
    "\n",
    "    # Build index arrays in the current df order\n",
    "    train_idx = df.index[df[\"row_key\"].isin(train_keys)].to_numpy()\n",
    "    val_idx   = df.index[df[\"row_key\"].isin(val_keys)].to_numpy() if val_keys else df.index[df[\"row_key\"].isin([])].to_numpy()\n",
    "    test_idx  = df.index[df[\"row_key\"].isin(test_keys)].to_numpy() if test_keys else df.index[df[\"row_key\"].isin([])].to_numpy()\n",
    "\n",
    "    # Check full coverage ONLY for keys that exist (train always exists; val/test may be empty in fallback)\n",
    "    needed_keys = train_keys | val_keys | test_keys\n",
    "    missing = needed_keys - set(df[\"row_key\"].tolist())\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"Experiment '{exp_name}' is missing {len(missing)} rows from the shared split \"\n",
    "            f\"(first few: {list(sorted(missing))[:5]}). \"\n",
    "            \"This usually means the pairs table differs between experiments.\"\n",
    "        )\n",
    "\n",
    "    X_train, X_val, X_test = X[train_idx], X[val_idx], X[test_idx]\n",
    "    df_train = df.loc[train_idx].reset_index(drop=True)\n",
    "    df_val   = df.loc[val_idx].reset_index(drop=True)\n",
    "    df_test  = df.loc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    SPLITS[exp_name] = {\n",
    "        \"train_idx\": train_idx,\n",
    "        \"val_idx\": val_idx,\n",
    "        \"test_idx\": test_idx,\n",
    "        \"X_train\": X_train, \"X_val\": X_val, \"X_test\": X_test,\n",
    "        \"df_train\": df_train, \"df_val\": df_val, \"df_test\": df_test,\n",
    "    }\n",
    "\n",
    "    print(f\"\\nðŸ§ª {exp_name}\")\n",
    "    print(\"Rows -> Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187473d",
   "metadata": {},
   "source": [
    "## Step 8 â€” Modality-Aware Preprocessing (Experiment-Aware)\n",
    "\n",
    "This step applies preprocessing tailored to the input modalities **separately for each\n",
    "experiment**, using **training data only** to fit preprocessing parameters.  \n",
    "A preprocessing bundle is saved per experiment so the exact same transformations can be\n",
    "reused during evaluation and inference.\n",
    "\n",
    "---\n",
    "\n",
    "### Prompt embeddings (all feature modes)\n",
    "\n",
    "Prompt vectors are normalized using **row-wise L2 normalization** to ensure consistent scale\n",
    "across samples and embedding backends (e.g., USE vs OpenAI).\n",
    "\n",
    "---\n",
    "\n",
    "### Map embeddings (only for fused prompt + map experiments)\n",
    "\n",
    "When the feature mode includes map vectors (`prompt_plus_map` / fused), the map block is\n",
    "processed using a robust pipeline:\n",
    "\n",
    "1. Replace non-finite values (`Â±inf`) with `NaN`\n",
    "2. Impute missing values using the **median** (fit on training data only)\n",
    "3. Clip each feature to training-set **5thâ€“95th percentiles** to reduce outlier impact\n",
    "4. Drop zero-variance (or near-constant) features based on training data\n",
    "5. Apply **RobustScaler** using quantile range **(5, 95)**\n",
    "\n",
    "The prompt block remains L2-normalized and is concatenated with the processed map block.\n",
    "\n",
    "---\n",
    "\n",
    "### Outputs (per experiment)\n",
    "\n",
    "For each experiment we obtain:\n",
    "\n",
    "- `X_train_s`, `X_val_s`, `X_test_s` â€” preprocessed matrices ready for training\n",
    "\n",
    "A preprocessing bundle is saved into the experimentâ€™s model output folder as:\n",
    "\n",
    "- `preproc.joblib`\n",
    "\n",
    "This ensures the exact same preprocessing can be reused for:\n",
    "- reproducible training\n",
    "- consistent evaluation across experiments\n",
    "- deployment-time inference (operator + parameter prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41942b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== CELL 8 â€” Modality-aware preprocessing (per experiment, incl. map-only) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "from src.train.preprocessing import fit_transform_modality_preproc\n",
    "\n",
    "PREPROC = {}  # exp_name -> dict with scaled arrays + bundle path\n",
    "\n",
    "print(\"\\n=== Fitting modality-aware preprocessing per experiment ===\")\n",
    "\n",
    "def _to_preproc_mode(feature_mode: str) -> str:\n",
    "    \"\"\"\n",
    "    fit_transform_modality_preproc expects:\n",
    "      - \"prompt_only\"\n",
    "      - \"prompt_plus_map\"\n",
    "    For map_only we use \"prompt_plus_map\" semantics but with prompt_dim=0.\n",
    "    \"\"\"\n",
    "    fm = str(feature_mode).strip().lower()\n",
    "    if fm == \"prompt_only\":\n",
    "        return \"prompt_only\"\n",
    "    if fm in {\"prompt_plus_map\", \"map_only\"}:\n",
    "        return \"prompt_plus_map\"\n",
    "    raise ValueError(f\"Unsupported feature_mode for preprocessing: {feature_mode}\")\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "\n",
    "    split = SPLITS[exp_name]\n",
    "    feature_mode = cfg[\"feature_mode\"]\n",
    "    preproc_mode = _to_preproc_mode(feature_mode)\n",
    "\n",
    "    # dims inferred in Cell 4 (map_only sets prompt_dim=0, prompt_only sets map_dim=0)\n",
    "    map_dim    = int(cfg[\"map_dim\"])\n",
    "    prompt_dim = int(cfg[\"prompt_dim\"])\n",
    "\n",
    "    model_out_dir = Path(cfg[\"model_out\"])\n",
    "    model_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    preproc_path = model_out_dir / \"preproc.joblib\"\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   Feature mode : {feature_mode} -> preproc_mode={preproc_mode}\")\n",
    "    print(f\"   map_dim      : {map_dim}\")\n",
    "    print(f\"   prompt_dim   : {prompt_dim}\")\n",
    "    print(f\"   Save preproc : {preproc_path}\")\n",
    "\n",
    "    # Safety checks: X dims must match the experiment dims\n",
    "    Xtr = split[\"X_train\"]\n",
    "    if Xtr.shape[1] != (map_dim + prompt_dim):\n",
    "        raise ValueError(\n",
    "            f\"Dim mismatch in {exp_name}: X_train has {Xtr.shape[1]} cols, \"\n",
    "            f\"but map_dim+prompt_dim={map_dim + prompt_dim} (map_dim={map_dim}, prompt_dim={prompt_dim}).\"\n",
    "        )\n",
    "\n",
    "    res = fit_transform_modality_preproc(\n",
    "        X_train=split[\"X_train\"],\n",
    "        X_val=split[\"X_val\"],\n",
    "        X_test=split[\"X_test\"],\n",
    "        feature_mode=preproc_mode,\n",
    "        map_dim=map_dim,\n",
    "        prompt_dim=prompt_dim,\n",
    "        eps=1e-12,\n",
    "        clip_q=(5, 95),\n",
    "        impute_strategy=\"median\",\n",
    "        robust_qrange=(5, 95),\n",
    "        save_path=preproc_path,\n",
    "    )\n",
    "\n",
    "    PREPROC[exp_name] = {\n",
    "        \"X_train_s\": res.X_train_s,\n",
    "        \"X_val_s\":   res.X_val_s,\n",
    "        \"X_test_s\":  res.X_test_s,\n",
    "        \"bundle_path\": res.bundle_path,\n",
    "    }\n",
    "\n",
    "    print(\"   âœ… Preprocessing complete.\")\n",
    "    print(\"   Shapes:\", res.X_train_s.shape, res.X_val_s.shape, res.X_test_s.shape)\n",
    "\n",
    "print(\"\\nâœ… All preprocessing finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c32c2",
   "metadata": {},
   "source": [
    "## Step 9 â€” Build Class Labels and Sample Weights (Experiment-Aware)\n",
    "\n",
    "In this step, we construct the classification labels and training sample weights **for each\n",
    "experiment**, using the same fixed class order and the same split definition. This guarantees\n",
    "that differences in performance across experiments are due to the feature representation,\n",
    "not label encoding or sampling artifacts.\n",
    "\n",
    "---\n",
    "\n",
    "### Fixed class encoding\n",
    "\n",
    "Operator labels are encoded using a fixed global class order:\n",
    "\n",
    "`[simplify, select, aggregate, displace]`\n",
    "\n",
    "This guarantees consistent label indices across:\n",
    "- training\n",
    "- saved model bundles\n",
    "- evaluation and inference code\n",
    "\n",
    "Because the split is shared across experiments, this encoding remains stable and comparable.\n",
    "\n",
    "---\n",
    "\n",
    "### Sample weighting\n",
    "\n",
    "Training samples are weighted to address two common sources of bias:\n",
    "\n",
    "1. **Class imbalance**\n",
    "   - Balanced class weights are computed from the training distribution to prevent majority\n",
    "     classes from dominating learning.\n",
    "\n",
    "2. **Map-level prompt multiplicity**\n",
    "   - Some `map_id`s have multiple prompts.\n",
    "   - To prevent such maps from contributing disproportionately, each map contributes\n",
    "     approximately equal total weight by assigning each prompt a map-weight of:\n",
    "\n",
    "   `map_weight = 1 / (#prompts for that map_id)`\n",
    "\n",
    "---\n",
    "\n",
    "### Final weight definition\n",
    "\n",
    "The final per-sample weight used during training is:\n",
    "\n",
    "`sample_w = class_weight(operator) Ã— map_weight(map_id)`\n",
    "\n",
    "These weights are used during classifier training (and optionally regression training)\n",
    "to improve robustness and ensure fair learning across operators and maps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df2b2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== CELL 9 â€” Build labels + sample weights (per experiment, incl. map-only) =====================\n",
    "\n",
    "import numpy as np\n",
    "from src.train.labels_and_weights import build_labels_and_sample_weights\n",
    "\n",
    "OP_COL = PATHS.OPERATOR_COL  # usually \"operator\"\n",
    "\n",
    "LABELS = {}  # exp_name -> labels, weights, class_names, etc.\n",
    "\n",
    "print(\"\\n=== Building labels and sample weights per experiment ===\")\n",
    "\n",
    "for exp_name, split in SPLITS.items():\n",
    "\n",
    "    df_train = split[\"df_train\"].copy()\n",
    "    df_val   = split[\"df_val\"].copy()\n",
    "    df_test  = split[\"df_test\"].copy()\n",
    "\n",
    "    # Fail early if operator missing (this should NOT happen if Cell 6 merge worked)\n",
    "    for part_name, dfi in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n",
    "        if OP_COL not in dfi.columns:\n",
    "            raise ValueError(f\"{exp_name}: df_{part_name} missing operator column '{OP_COL}'.\")\n",
    "        n_miss = int(dfi[OP_COL].isna().sum())\n",
    "        if n_miss:\n",
    "            raise ValueError(\n",
    "                f\"{exp_name}: df_{part_name} has {n_miss} missing operator labels. \"\n",
    "                \"Fix the label merge in the data-loading step before training.\"\n",
    "            )\n",
    "\n",
    "    lab = build_labels_and_sample_weights(\n",
    "        df_train=df_train,\n",
    "        df_val=df_val,\n",
    "        df_test=df_test,\n",
    "        op_col=OP_COL,\n",
    "        map_id_col=\"map_id\",\n",
    "        fixed_classes=FIXED_CLASSES,\n",
    "        use_map_weight=True,\n",
    "        class_weight_mode=\"balanced\",\n",
    "    )\n",
    "\n",
    "    class_names = np.array(lab.class_names)\n",
    "\n",
    "    LABELS[exp_name] = {\n",
    "        \"class_names\": class_names,\n",
    "        \"y_train_cls\": lab.y_train,\n",
    "        \"y_val_cls\":   lab.y_val,\n",
    "        \"y_test_cls\":  lab.y_test,\n",
    "        \"sample_w\":    lab.sample_w,\n",
    "        \"class_weight_map\": lab.class_weight_map,\n",
    "    }\n",
    "\n",
    "    print(f\"\\nðŸ§ª {exp_name}\")\n",
    "    print(\"Classes (fixed order):\", list(class_names))\n",
    "    print(\"Class weights:\", lab.class_weight_map)\n",
    "    print(\"y_train/y_val/y_test shapes:\", lab.y_train.shape, lab.y_val.shape, lab.y_test.shape)\n",
    "    sw = lab.sample_w\n",
    "    print(\"Sample weight summary:\", {\"min\": float(sw.min()), \"max\": float(sw.max()), \"mean\": float(sw.mean())})\n",
    "\n",
    "# Sanity: class order must match across experiments\n",
    "first = next(iter(LABELS.keys()))\n",
    "base_classes = LABELS[first][\"class_names\"].tolist()\n",
    "for exp_name in LABELS.keys():\n",
    "    if LABELS[exp_name][\"class_names\"].tolist() != base_classes:\n",
    "        raise ValueError(f\"Class order differs in experiment {exp_name}. This would break fair comparison.\")\n",
    "\n",
    "print(\"\\nâœ… Label build complete for all experiments (class order consistent).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988f274",
   "metadata": {},
   "source": [
    "## Step 10 â€” Operator Classification Model (MLP, Experiment-Aware)\n",
    "\n",
    "This step trains the operator classifier that predicts one of the four map generalization\n",
    "operators:\n",
    "\n",
    "`{simplify, select, aggregate, displace}`\n",
    "\n",
    "The same training protocol is applied **independently for each experiment** (prompt-only,\n",
    "USE + map, OpenAI + map) using the **shared split**. This ensures that differences in\n",
    "performance across experiments are attributable to the feature representation rather than\n",
    "changes in training procedure.\n",
    "\n",
    "---\n",
    "\n",
    "### Model and training strategy\n",
    "\n",
    "- We use an **MLPClassifier** (multi-layer perceptron).\n",
    "- Hyperparameters are explored via a lightweight random search over:\n",
    "  - hidden layer sizes\n",
    "  - weight decay (`alpha`)\n",
    "  - learning rate schedule\n",
    "  - batch size / optimization settings (as implemented in the helper)\n",
    "\n",
    "---\n",
    "\n",
    "### Validation protocol (leakage-free)\n",
    "\n",
    "To prevent leakage, we perform **grouped cross-validation** using `map_id`:\n",
    "\n",
    "- prompts from the same map are never split across folds\n",
    "\n",
    "This is critical because multiple prompts may refer to the same map and would otherwise\n",
    "inflate performance due to memorization.\n",
    "\n",
    "---\n",
    "\n",
    "### Model selection and evaluation\n",
    "\n",
    "The best configuration is selected using validation performance (with grouped CV used for\n",
    "reliable hyperparameter tuning). The selected model is then retrained on the full training\n",
    "split and evaluated on validation and test splits.\n",
    "\n",
    "---\n",
    "\n",
    "### Outputs (per experiment)\n",
    "\n",
    "For each experiment, the trained classifier is saved into the experimentâ€™s model folder as:\n",
    "\n",
    "- `classifier.joblib`\n",
    "\n",
    "This classifier is later used to:\n",
    "1. predict the operator class\n",
    "2. route each sample to the correct operator-specific parameter regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95133825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== CELL 10 â€” Train classifier (per experiment, MLP search + final fit) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from dataclasses import asdict, is_dataclass\n",
    "\n",
    "from src.train.train_classifier import train_mlp_classifier_with_search\n",
    "\n",
    "CLF_RESULTS = {}\n",
    "\n",
    "def _safe_get(obj, *names, default=None):\n",
    "    for n in names:\n",
    "        if hasattr(obj, n):\n",
    "            return getattr(obj, n)\n",
    "    return default\n",
    "\n",
    "print(\"\\n=== Training operator classifiers for all experiments ===\")\n",
    "\n",
    "printed_debug_fields = False\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "\n",
    "    split = SPLITS[exp_name]\n",
    "    pre   = PREPROC[exp_name]\n",
    "    lab   = LABELS[exp_name]\n",
    "\n",
    "    X_train_s = pre[\"X_train_s\"]\n",
    "    X_val_s   = pre[\"X_val_s\"]\n",
    "    X_test_s  = pre[\"X_test_s\"]\n",
    "\n",
    "    y_train = lab[\"y_train_cls\"]\n",
    "    y_val   = lab[\"y_val_cls\"]\n",
    "    y_test  = lab[\"y_test_cls\"]\n",
    "    sample_w = lab[\"sample_w\"]\n",
    "\n",
    "    class_names = [str(x) for x in lab[\"class_names\"]]\n",
    "\n",
    "    # Sanity checks\n",
    "    if X_train_s.shape[0] != len(y_train):\n",
    "        raise ValueError(f\"{exp_name}: X_train rows {X_train_s.shape[0]} != y_train {len(y_train)}\")\n",
    "    if X_val_s.shape[0] != len(y_val):\n",
    "        raise ValueError(f\"{exp_name}: X_val rows {X_val_s.shape[0]} != y_val {len(y_val)}\")\n",
    "    if X_test_s.shape[0] != len(y_test):\n",
    "        raise ValueError(f\"{exp_name}: X_test rows {X_test_s.shape[0]} != y_test {len(y_test)}\")\n",
    "\n",
    "    # Grouped CV: group by map_id to avoid leakage across folds\n",
    "    groups_tr = split[\"df_train\"][\"map_id\"].astype(str).to_numpy()\n",
    "\n",
    "    model_out_dir = Path(cfg[\"model_out\"])\n",
    "    model_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   Classes   : {class_names}\")\n",
    "    print(f\"   Train X   : {X_train_s.shape}\")\n",
    "    print(f\"   Val X     : {X_val_s.shape}\")\n",
    "    print(f\"   Test X    : {X_test_s.shape}\")\n",
    "    print(f\"   Model out : {model_out_dir}\")\n",
    "\n",
    "    res_clf = train_mlp_classifier_with_search(\n",
    "        exp_name=exp_name,\n",
    "        X_train=X_train_s,\n",
    "        y_train=y_train,\n",
    "        groups_train=groups_tr,\n",
    "        sample_w=sample_w,\n",
    "        X_val=X_val_s,\n",
    "        y_val=y_val,\n",
    "        X_test=X_test_s,\n",
    "        y_test=y_test,\n",
    "        class_names=class_names,\n",
    "        out_dir=model_out_dir,\n",
    "        n_iter=5,  #50\n",
    "        n_splits=5,\n",
    "        seed=int(CFG.SEED),\n",
    "        verbose=True,\n",
    "        save_name=\"classifier.joblib\",\n",
    "    )\n",
    "\n",
    "    CLF_RESULTS[exp_name] = res_clf\n",
    "\n",
    "    # ---- robust reporting (no assumptions about field names) ----\n",
    "    model_path    = _safe_get(res_clf, \"model_path\", \"path\", default=str(model_out_dir / \"classifier.joblib\"))\n",
    "    best_val_f1   = _safe_get(res_clf, \"val_f1_macro\", \"best_val_f1\", \"val_f1\", \"best_f1\", default=None)\n",
    "    best_val_acc  = _safe_get(res_clf, \"val_acc\", \"best_val_acc\", \"best_accuracy\", default=None)\n",
    "    test_f1       = _safe_get(res_clf, \"test_f1_macro\", \"test_f1\", default=None)\n",
    "    test_acc      = _safe_get(res_clf, \"test_acc\", \"accuracy_test\", default=None)\n",
    "\n",
    "    print(\"   âœ… Classifier training done.\")\n",
    "    print(\"   Saved to:\", model_path)\n",
    "    if best_val_f1 is not None or best_val_acc is not None:\n",
    "        print(\"   Best VAL:\", {\"macro_f1\": best_val_f1, \"acc\": best_val_acc})\n",
    "    if test_f1 is not None or test_acc is not None:\n",
    "        print(\"   TEST     :\", {\"macro_f1\": test_f1, \"acc\": test_acc})\n",
    "\n",
    "    # Save lightweight meta for evaluation / reporting\n",
    "    clf_meta = {\n",
    "        \"experiment\": exp_name,\n",
    "        \"feature_mode\": cfg[\"feature_mode\"],\n",
    "        \"class_names\": class_names,\n",
    "        \"best_val\": {\"macro_f1\": best_val_f1, \"acc\": best_val_acc},\n",
    "        \"test\": {\"macro_f1\": test_f1, \"acc\": test_acc},\n",
    "        \"model_path\": str(model_path),\n",
    "    }\n",
    "    (model_out_dir / \"classifier_meta.json\").write_text(json.dumps(clf_meta, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # Print available fields once for debugging\n",
    "    if not printed_debug_fields:\n",
    "        printed_debug_fields = True\n",
    "        if is_dataclass(res_clf):\n",
    "            print(\"   (debug) Result fields:\", list(asdict(res_clf).keys()))\n",
    "        else:\n",
    "            print(\"   (debug) Result attrs :\", [a for a in dir(res_clf) if not a.startswith(\"_\")])\n",
    "\n",
    "print(\"\\nâœ… All classifiers trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f878f54",
   "metadata": {},
   "source": [
    "## Step 11 â€” Parameter Regression (Per-Operator) and Final Model Bundle (Experiment-Aware)\n",
    "\n",
    "This step trains **operator-specific regressors** to predict the generalization parameter in a\n",
    "**scale-independent form**, and then packages all trained components into a single,\n",
    "experiment-scoped model bundle.\n",
    "\n",
    "The same procedure is applied **independently for each experiment** (prompt-only,\n",
    "USE + map, OpenAI + map), using the shared data split and preprocessing pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### Regression target\n",
    "\n",
    "Regressors are trained on the normalized target `param_norm`, defined as:\n",
    "\n",
    "- **Distance-based operators** (`simplify`, `aggregate`, `displace`):  \n",
    "  `param_norm = param_value / extent_diag_m`\n",
    "\n",
    "- **Area-based operators** (`select`):  \n",
    "  `param_norm = param_value / extent_area_m2`\n",
    "\n",
    "This normalization allows each regressor to generalize across maps of different spatial\n",
    "extent while preserving physical meaning. During inference, predictions are converted back\n",
    "to real-world units using the same per-map extent references.\n",
    "\n",
    "---\n",
    "\n",
    "### Training strategy\n",
    "\n",
    "- One **MLPRegressor per operator**\n",
    "- Training data restricted to samples of the corresponding operator\n",
    "- **Grouped cross-validation** (`GroupKFold`) by `map_id` to prevent spatial leakage\n",
    "- Hyperparameter optimization via `RandomizedSearchCV` for each operator independently\n",
    "\n",
    "---\n",
    "\n",
    "### Final model bundle\n",
    "\n",
    "For each experiment, the trained components are stored together in a single bundle:\n",
    "\n",
    "- `cls_plus_regressors.joblib`\n",
    "\n",
    "This bundle contains:\n",
    "- the trained operator classifier\n",
    "- the dictionary of operator-specific regressors\n",
    "- the fixed class order\n",
    "- normalization metadata (operator groups and extent columns)\n",
    "\n",
    "Along with the experimentâ€™s `preproc.joblib`, this bundle is sufficient for the evaluation\n",
    "notebook to compute:\n",
    "\n",
    "1. **Classifier-only metrics**  \n",
    "2. **Regressor-only metrics** (oracle operator routing)  \n",
    "3. **End-to-end pipeline metrics** (predicted operator routing)\n",
    "\n",
    "This design keeps evaluation simple, reproducible, and fully decoupled from the training\n",
    "notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7905b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== CELL 11 â€” Train per-operator regressors + save final bundle (per experiment) =====================\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "from src.train.train_regressors import train_regressors_per_operator\n",
    "from src.train.save_bundle import save_cls_plus_regressors_bundle\n",
    "\n",
    "BUNDLES = {}     # exp_name -> bundle path\n",
    "REG_RESULTS = {} # exp_name -> regressor training result\n",
    "\n",
    "print(\"\\n=== Training per-operator regressors and saving final bundles ===\")\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "\n",
    "    split = SPLITS[exp_name]\n",
    "    pre   = PREPROC[exp_name]\n",
    "    lab   = LABELS[exp_name]\n",
    "    res_clf = CLF_RESULTS[exp_name]\n",
    "\n",
    "    X_train_s = pre[\"X_train_s\"]\n",
    "    df_train  = split[\"df_train\"]\n",
    "    y_train_cls = lab[\"y_train_cls\"]\n",
    "    sample_w = lab[\"sample_w\"]\n",
    "\n",
    "    # Make sure class_names is list[str] (stable ordering)\n",
    "    cn = [str(x) for x in lab[\"class_names\"]]\n",
    "\n",
    "    model_out_dir = Path(cfg[\"model_out\"])\n",
    "    model_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nðŸ§ª Experiment: {exp_name}\")\n",
    "    print(f\"   Model out: {model_out_dir}\")\n",
    "    print(f\"   Train X  : {X_train_s.shape} | df_train: {df_train.shape}\")\n",
    "\n",
    "    # ---- (1) Train per-operator regressors on TRAIN only ----\n",
    "    reg_res = train_regressors_per_operator(\n",
    "        X_train_s=X_train_s,\n",
    "        df_train=df_train,\n",
    "        y_train_cls=y_train_cls,\n",
    "        class_names=cn,\n",
    "        sample_w=sample_w,\n",
    "        group_col=\"map_id\",\n",
    "        target_col=\"param_norm\",\n",
    "        use_log1p=False,\n",
    "        n_splits=5,\n",
    "        n_iter=40,\n",
    "        random_state=int(CFG.SEED),\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    REG_RESULTS[exp_name] = reg_res\n",
    "\n",
    "    # ---- (2) Load the trained classifier model (from Cell 10 output) ----\n",
    "    clf_pack = joblib.load(Path(res_clf.model_path))\n",
    "    final_clf = clf_pack[\"model\"]\n",
    "\n",
    "    # ---- (3) Save combined bundle for evaluation notebook ----\n",
    "    bundle_res = save_cls_plus_regressors_bundle(\n",
    "        exp_name=exp_name,\n",
    "        out_dir=model_out_dir,\n",
    "        classifier=final_clf,\n",
    "        regressors_by_class=reg_res.regressors_by_class,\n",
    "        class_names=cn,\n",
    "        use_log1p=reg_res.use_log1p,\n",
    "        cv_summary=reg_res.cv_summary,\n",
    "        distance_ops=DISTANCE_OPS,\n",
    "        area_ops=AREA_OPS,\n",
    "        diag_col=\"extent_diag_m\",\n",
    "        area_col=\"extent_area_m2\",\n",
    "        save_name=\"cls_plus_regressors.joblib\",  # fixed name inside each experiment folder\n",
    "    )\n",
    "\n",
    "    BUNDLES[exp_name] = bundle_res.bundle_path\n",
    "\n",
    "    print(\"   âœ… Saved bundle:\", bundle_res.bundle_path)\n",
    "    print(\"   âœ… Regressors trained for:\", sorted(list(reg_res.regressors_by_class.keys())))\n",
    "\n",
    "print(\"\\nâœ… All bundles saved.\")\n",
    "for k, v in BUNDLES.items():\n",
    "    print(f\" - {k:12s}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eea1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== CELL 12A â€” Classifier comparison table =====================\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "rows = []\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    meta_path = Path(cfg[\"model_out\"]) / \"classifier_meta.json\"\n",
    "    meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    rows.append({\n",
    "        \"experiment\": exp_name,\n",
    "        \"val_acc\": meta[\"best_val\"][\"acc\"],\n",
    "        \"val_f1_macro\": meta[\"best_val\"][\"macro_f1\"],\n",
    "        \"test_acc\": meta[\"test\"][\"acc\"],\n",
    "        \"test_f1_macro\": meta[\"test\"][\"macro_f1\"],\n",
    "        \"model_path\": meta[\"model_path\"],\n",
    "    })\n",
    "\n",
    "df_clf = pd.DataFrame(rows).sort_values(\"test_f1_macro\", ascending=False)\n",
    "print(df_clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40e5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== CELL 12B (revised) â€” One clear regressor comparison table =====================\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) Load cv_summary from bundles\n",
    "bund_cv = {}\n",
    "for exp_name, bundle_path in BUNDLES.items():\n",
    "    pack = joblib.load(bundle_path)\n",
    "    cv_summary = pack.get(\"cv_summary\", None)\n",
    "    if cv_summary is None:\n",
    "        raise ValueError(f\"{exp_name}: bundle has no cv_summary. Check save_bundle.py\")\n",
    "    bund_cv[exp_name] = cv_summary\n",
    "\n",
    "# 2) Extract RMSE for each operator\n",
    "def get_rmse_param(cv_summary, op_name):\n",
    "    d = cv_summary.get(op_name, cv_summary.get(str(op_name), {}))\n",
    "    for k in [\"best_rmse_param\", \"rmse_param\", \"rmse_param_units\", \"best_cv_rmse_param\"]:\n",
    "        if k in d and d[k] is not None:\n",
    "            return float(d[k])\n",
    "    # fallback search\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, (int, float)) and \"rmse\" in k.lower() and \"param\" in k.lower():\n",
    "            return float(v)\n",
    "    return np.nan\n",
    "\n",
    "ops = [\"simplify\", \"select\", \"aggregate\", \"displace\"]\n",
    "\n",
    "rows = []\n",
    "for exp_name, cv_summary in bund_cv.items():\n",
    "    row = {\"experiment\": exp_name}\n",
    "    for op in ops:\n",
    "        row[op] = get_rmse_param(cv_summary, op)\n",
    "    rows.append(row)\n",
    "\n",
    "df_rmse = pd.DataFrame(rows).set_index(\"experiment\")\n",
    "\n",
    "# 3) Add summary stats\n",
    "df_rmse[\"mean_rmse\"] = df_rmse[ops].mean(axis=1)\n",
    "\n",
    "# 4) Sort by mean_rmse (lower is better)\n",
    "df_rmse_sorted = df_rmse.sort_values(\"mean_rmse\")\n",
    "\n",
    "# 5) Create a human-friendly display version (percent of [0,1] range)\n",
    "df_pct = (df_rmse_sorted * 100).round(3)   # e.g. 0.0043 -> 0.43%\n",
    "df_pct = df_pct.rename(columns={op: f\"{op} RMSE (%)\" for op in ops} | {\"mean_rmse\": \"Mean RMSE (%)\"})\n",
    "\n",
    "df_rmse_sorted.round(6), df_pct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79040ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== CELL 12C â€” End-to-end evaluation (TEST) [FIXED: scaler is y-scaler] =====================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "TOL = 0.05  # tolerance in param_norm units (0..1)\n",
    "\n",
    "def _predict_param(reg_and_scaler, Xi):\n",
    "    \"\"\"\n",
    "    reg_and_scaler is either:\n",
    "      - regressor\n",
    "      - (regressor, y_scaler) where y_scaler was fit on target y (shape [n,1])\n",
    "    Xi is a 2D row: shape (1, n_features)\n",
    "    \"\"\"\n",
    "    # unpack\n",
    "    if isinstance(reg_and_scaler, (tuple, list)):\n",
    "        reg = reg_and_scaler[0]\n",
    "        y_scaler = reg_and_scaler[1] if len(reg_and_scaler) > 1 else None\n",
    "    else:\n",
    "        reg = reg_and_scaler\n",
    "        y_scaler = None\n",
    "\n",
    "    # predict (in scaled-y space if y_scaler exists)\n",
    "    y_hat = float(reg.predict(Xi)[0])\n",
    "\n",
    "    # if scaler looks like a y-scaler, inverse-transform\n",
    "    if y_scaler is not None:\n",
    "        # y_scaler fitted on y => expects 1 feature\n",
    "        try:\n",
    "            y_hat = float(y_scaler.inverse_transform(np.array([[y_hat]], dtype=float))[0, 0])\n",
    "        except Exception:\n",
    "            # if inverse_transform fails for any reason, keep raw prediction\n",
    "            pass\n",
    "\n",
    "    return y_hat\n",
    "\n",
    "rows = []\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    bundle = joblib.load(BUNDLES[exp_name])\n",
    "\n",
    "    clf = bundle[\"classifier\"]\n",
    "    regs = bundle[\"regressors_by_class\"]      # op -> (regressor, y_scaler)\n",
    "    class_names = [str(x) for x in bundle[\"class_names\"]]\n",
    "\n",
    "    pre   = PREPROC[exp_name]\n",
    "    split = SPLITS[exp_name]\n",
    "    lab   = LABELS[exp_name]\n",
    "\n",
    "    X_test = pre[\"X_test_s\"]\n",
    "    y_true_cls = lab[\"y_test_cls\"]\n",
    "    df_test = split[\"df_test\"]\n",
    "\n",
    "    if \"param_norm\" not in df_test.columns:\n",
    "        raise KeyError(f\"{exp_name}: df_test has no 'param_norm' column. Available: {list(df_test.columns)}\")\n",
    "    y_true_param = df_test[\"param_norm\"].to_numpy(dtype=float)\n",
    "\n",
    "    # ---- Predict operator ----\n",
    "    y_pred_cls = clf.predict(X_test)\n",
    "    op_acc = float((y_pred_cls == y_true_cls).mean())\n",
    "\n",
    "    pred_names = [class_names[int(i)] for i in y_pred_cls]\n",
    "    true_names = [class_names[int(i)] for i in y_true_cls]\n",
    "\n",
    "    # ---- Predict parameter using regressor of the PREDICTED operator ----\n",
    "    y_pred_param = np.zeros_like(y_true_param, dtype=float)\n",
    "    for i, op in enumerate(pred_names):\n",
    "        Xi = X_test[i:i+1]  # shape (1, n_features)\n",
    "        y_pred_param[i] = _predict_param(regs[op], Xi)\n",
    "\n",
    "    # errors\n",
    "    abs_err = np.abs(y_pred_param - y_true_param)\n",
    "    correct_mask = (np.array(pred_names) == np.array(true_names))\n",
    "\n",
    "    # Param RMSE/MAE only when operator correct\n",
    "    if correct_mask.any():\n",
    "        rmse_cond = float(np.sqrt(np.mean((y_pred_param[correct_mask] - y_true_param[correct_mask])**2)))\n",
    "        mae_cond  = float(np.mean(abs_err[correct_mask]))\n",
    "    else:\n",
    "        rmse_cond, mae_cond = np.nan, np.nan\n",
    "\n",
    "    # Joint metric: operator correct AND parameter close enough\n",
    "    joint_success = float(np.mean(correct_mask & (abs_err <= TOL)))\n",
    "\n",
    "    # Penalized RMSE over ALL: if operator wrong, set error = 1.0 (max on [0,1])\n",
    "    penalized_err = abs_err.copy()\n",
    "    penalized_err[~correct_mask] = 1.0\n",
    "    rmse_penalized = float(np.sqrt(np.mean(penalized_err**2)))\n",
    "\n",
    "    rows.append({\n",
    "        \"experiment\": exp_name,\n",
    "        \"op_acc\": op_acc,\n",
    "        \"param_rmse_if_op_correct\": rmse_cond,\n",
    "        \"param_mae_if_op_correct\": mae_cond,\n",
    "        f\"joint_success@{TOL}\": joint_success,\n",
    "        \"rmse_penalized_all\": rmse_penalized,\n",
    "        \"n_test\": int(len(y_true_cls)),\n",
    "        \"n_op_correct\": int(correct_mask.sum()),\n",
    "    })\n",
    "\n",
    "df_e2e = pd.DataFrame(rows).sort_values(\"rmse_penalized_all\")\n",
    "df_e2e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2eb472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
