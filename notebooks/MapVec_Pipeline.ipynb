{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f56a2c",
   "metadata": {},
   "source": [
    "## üß© 0) Setup & Imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "367f89f6ac45439b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:21:53.302406Z",
     "start_time": "2025-10-27T11:21:53.298709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFIG SUMMARY ===\n",
      "PROJ_ROOT  : /Users/amirdonyadide/Documents/GitHub/Thesis\n",
      "DATA_DIR   : /Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
      "INPUT_DIR  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input\n",
      "OUTPUT_DIR : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output\n",
      "MAPS_ROOT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/samples/pairs\n",
      "INPUT PAT. : *_input.geojson\n",
      "--- User Study ---\n",
      "USER_STUDY_XLSX : /Users/amirdonyadide/Documents/GitHub/Thesis/data/userstudy/UserStudy.xlsx\n",
      "RESPONSES_SHEET : Responses\n",
      "TILE_ID_COL     : tile_id\n",
      "COMPLETE_COL    : complete\n",
      "REMOVE_COL      : remove\n",
      "TEXT_COL        : cleaned_text\n",
      "PARAM_VALUE_COL : param_value\n",
      "OPERATOR_COL    : operator\n",
      "INTENSITY_COL   : intensity\n",
      "--- Filters / IDs / Split ---\n",
      "ONLY_COMPLETE   : True\n",
      "EXCLUDE_REMOVED : True\n",
      "PROMPT_ID       : r{i:08d}\n",
      "SPLIT_BY        : tile\n",
      "--- Outputs ---\n",
      "PROMPT_OUT : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "MAP_OUT    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n",
      "TRAIN_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out\n",
      "MODEL_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models\n",
      "SPLIT_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/splits\n",
      "PRM_NPZ    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/prompts_embeddings.npz\n",
      "--- Model ---\n",
      "PROMPT_ENCODER: openai-small\n",
      "MAP_DIM       : 165\n",
      "PROMPT_DIM    : 1536\n",
      "FUSED_DIM     : 1701\n",
      "BATCH_SIZE    : 512\n",
      "VAL/TEST      : 0.15 0.15\n",
      "SEED          : 42\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models\n",
      "‚úÖ All output folders cleaned and recreated fresh.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================== PARAMETERS / IMPORTS =====================\n",
    "from pathlib import Path\n",
    "import sys, subprocess, numpy as np, pandas as pd, joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, make_scorer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from scipy.stats import loguniform, randint\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Project config\n",
    "PROJ_ROOT = Path(\"../\").resolve()\n",
    "SRC_DIR   = PROJ_ROOT / \"src\"\n",
    "if str(PROJ_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJ_ROOT))\n",
    "\n",
    "from src.config import PATHS, CFG, print_summary\n",
    "print_summary()\n",
    "\n",
    "# Dims (fallbacks if CFG unset)\n",
    "MAP_DIM = PROMPT_DIM = FUSED_DIM = None\n",
    "\n",
    "BATCH_SIZE  = CFG.BATCH_SIZE\n",
    "\n",
    "# Clean outputs for a fresh run\n",
    "PATHS.clean_outputs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a2350e",
   "metadata": {},
   "source": [
    "## üìö 1) Build Prompt Embeddings (USE) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0df45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:21:55.572701Z",
     "start_time": "2025-10-27T11:21:55.570071Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 21:24:23 | INFO | Reading Excel: /Users/amirdonyadide/Documents/GitHub/Thesis/data/userstudy/UserStudy.xlsx (sheet=Responses)\n",
      "2026-01-21 21:24:23 | INFO | Filtered Excel rows: 786 ‚Üí 564 (only_complete=True, exclude_removed=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 564 prompts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 21:24:23 | INFO | Embedding 564 prompts with OpenAI model=text-embedding-3-small (batch_size=512, l2=True)‚Ä¶\n",
      "2026-01-21 21:24:25 | INFO | HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-21 21:24:26 | INFO | HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-21 21:24:26 | INFO | Done OpenAI embedding in 2.31s (dim=1536).\n",
      "2026-01-21 21:24:26 | INFO | Writing outputs to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "2026-01-21 21:24:26 | INFO |   saved prompts_embeddings.npz (shape=(564, 1536))\n",
      "2026-01-21 21:24:26 | INFO |   saved prompts.parquet (rows=564)\n",
      "2026-01-21 21:24:26 | INFO |   saved meta.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prompt embeddings completed.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Import the module\n",
    "from src.mapvec.prompts import prompt_embeddings as pe\n",
    "\n",
    "# Choose paths\n",
    "data_dir = PATHS.DATA_DIR  # or Path(\"../data\").resolve()\n",
    "in_path  = PATHS.USER_STUDY_XLSX\n",
    "out_dir  = PATHS.PROMPT_OUT\n",
    "\n",
    "# Logging (match what CLI does)\n",
    "pe.setup_logging(verbosity=1)\n",
    "\n",
    "# Load prompts (will filter complete==True & remove==False because you updated the function)\n",
    "ids, texts, id_colname = pe.load_prompts_from_source(\n",
    "    input_path=Path(in_path),\n",
    "    sheet_name=PATHS.RESPONSES_SHEET,\n",
    "    tile_id_col=PATHS.TILE_ID_COL,\n",
    "    complete_col=PATHS.COMPLETE_COL,\n",
    "    remove_col=PATHS.REMOVE_COL,\n",
    "    text_col=PATHS.TEXT_COL,\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(texts)} prompts.\")\n",
    "\n",
    "# Get embedder based on CFG.PROMPT_ENCODER (dan/transformer/openai-small/openai-large)\n",
    "embed_fn, model_label = pe.get_embedder(\n",
    "    kind=CFG.PROMPT_ENCODER,\n",
    "    data_dir=Path(data_dir),\n",
    "    l2_normalize=True,\n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    ")\n",
    "\n",
    "# Embed\n",
    "E = embed_fn(texts)\n",
    "\n",
    "# Save outputs in the same format as before\n",
    "pe.save_outputs(\n",
    "    out_dir=Path(out_dir),\n",
    "    ids=ids,\n",
    "    texts=texts,\n",
    "    E=E,\n",
    "    model_name=model_label,\n",
    "    l2_normalized=True,\n",
    "    id_colname=\"prompt_id\",\n",
    "    also_save_embeddings_csv=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Prompt embeddings completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c0b51",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è 2) Build Map Embeddings (geometric) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ca0c3d8b71fc70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:26:59.687350Z",
     "start_time": "2025-10-27T11:26:19.901557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed tiles from Excel: 401\n",
      "Maps to embed after filtering: 401\n",
      "‚úÖ Map embeddings completed.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.mapvec.maps import map_embeddings as me\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Load allowed tile_ids from Excel\n",
    "# ------------------------------------------------------------\n",
    "dfu = pd.read_excel(PATHS.USER_STUDY_XLSX, sheet_name=PATHS.RESPONSES_SHEET)\n",
    "\n",
    "dfu[PATHS.COMPLETE_COL] = dfu[PATHS.COMPLETE_COL].astype(bool)\n",
    "dfu[PATHS.REMOVE_COL]   = dfu[PATHS.REMOVE_COL].astype(bool)\n",
    "\n",
    "mask = pd.Series(True, index=dfu.index)\n",
    "if PATHS.ONLY_COMPLETE:\n",
    "    mask &= (dfu[PATHS.COMPLETE_COL] == True)\n",
    "if PATHS.EXCLUDE_REMOVED:\n",
    "    mask &= (dfu[PATHS.REMOVE_COL] == False)\n",
    "dfu = dfu[mask].copy()\n",
    "\n",
    "\n",
    "tile_raw = dfu[PATHS.TILE_ID_COL]\n",
    "tile_num = pd.to_numeric(tile_raw, errors=\"coerce\")\n",
    "if tile_num.notna().all():\n",
    "    allowed_tile_ids = set(tile_num.astype(int).astype(str).str.zfill(4).tolist())\n",
    "else:\n",
    "    allowed_tile_ids = set(tile_raw.astype(str).str.strip().str.zfill(4).tolist())\n",
    "\n",
    "print(f\"Allowed tiles from Excel: {len(allowed_tile_ids)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Discover GeoJSONs and filter by tile_id\n",
    "# ------------------------------------------------------------\n",
    "me.setup_logging(verbosity=1)\n",
    "\n",
    "pairs = list(me.find_geojsons(PATHS.MAPS_ROOT, PATHS.INPUT_MAPS_PATTERN))\n",
    "\n",
    "pairs = [\n",
    "    (map_id, path)\n",
    "    for (map_id, path) in pairs\n",
    "    if str(map_id) in allowed_tile_ids\n",
    "]\n",
    "\n",
    "if not pairs:\n",
    "    raise RuntimeError(\"No maps left after Excel filtering.\")\n",
    "\n",
    "print(f\"Maps to embed after filtering: {len(pairs)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) First pass: polygon counting\n",
    "# ------------------------------------------------------------\n",
    "counts = {}\n",
    "for map_id, path in pairs:\n",
    "    try:\n",
    "        counts[map_id] = me._count_valid_polygons(path)\n",
    "    except Exception:\n",
    "        counts[map_id] = 0\n",
    "\n",
    "max_polygons = max(max(counts.values()), 1)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Second pass: embed maps\n",
    "# ------------------------------------------------------------\n",
    "ids, vecs, rows = [], [], []\n",
    "feat_names = None\n",
    "first_dim = None\n",
    "\n",
    "for map_id, path in pairs:\n",
    "    vec, names = me.embed_one_map(\n",
    "        path,\n",
    "        max_polygons=max_polygons,\n",
    "        norm=\"fixed\",\n",
    "        norm_wh=\"400x400\",\n",
    "    )\n",
    "\n",
    "    if first_dim is None:\n",
    "        first_dim = vec.shape[0]\n",
    "        feat_names = names\n",
    "    elif vec.shape[0] != first_dim:\n",
    "        print(f\"Skipping {map_id}: dim mismatch\")\n",
    "        continue\n",
    "\n",
    "    ids.append(map_id)\n",
    "    vecs.append(vec)\n",
    "\n",
    "    rows.append({\n",
    "        \"map_id\": map_id,\n",
    "        \"geojson\": str(path),\n",
    "        \"n_polygons\": counts.get(map_id, 0),\n",
    "    })\n",
    "\n",
    "E = np.vstack(vecs).astype(np.float32)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Save outputs (same format as script)\n",
    "# ------------------------------------------------------------\n",
    "me.save_outputs(\n",
    "    out_dir=PATHS.MAP_OUT,\n",
    "    rows=rows,\n",
    "    E=E,\n",
    "    ids=ids,\n",
    "    feat_names=feat_names or [],\n",
    "    save_csv=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Map embeddings completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e8cdf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inferred dims: {'MAP_DIM': 165, 'PROMPT_DIM': 1536, 'FUSED_DIM': 1701}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def infer_dims(paths):\n",
    "    prm_npz = paths.PROMPT_OUT / \"prompts_embeddings.npz\"\n",
    "    map_npz = paths.MAP_OUT / \"maps_embeddings.npz\"\n",
    "\n",
    "    if not prm_npz.exists():\n",
    "        raise FileNotFoundError(f\"Missing {prm_npz} (run prompt embeddings first)\")\n",
    "    z = np.load(prm_npz, allow_pickle=True)\n",
    "    PROMPT_DIM = int(z[\"E\"].shape[1])\n",
    "\n",
    "    if not map_npz.exists():\n",
    "        raise FileNotFoundError(f\"Missing {map_npz} (run map embeddings first)\")\n",
    "    z2 = np.load(map_npz, allow_pickle=True)\n",
    "    MAP_DIM = int(z2[\"E\"].shape[1])\n",
    "\n",
    "    FUSED_DIM = MAP_DIM + PROMPT_DIM\n",
    "    return MAP_DIM, PROMPT_DIM, FUSED_DIM\n",
    "\n",
    "MAP_DIM, PROMPT_DIM, FUSED_DIM = infer_dims(PATHS)\n",
    "print(\"‚úÖ Inferred dims:\", {\"MAP_DIM\": MAP_DIM, \"PROMPT_DIM\": PROMPT_DIM, \"FUSED_DIM\": FUSED_DIM})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd186319e89f445",
   "metadata": {},
   "source": [
    "## üîó 3) Concatenate (pairs ‚Üí fused rows) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa2b07a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Concatenation completed.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from src.mapvec.concat import concat_embeddings as ce\n",
    "ce.setup_logging(verbosity=1)\n",
    "\n",
    "map_npz_path = Path(PATHS.MAP_OUT / \"maps_embeddings.npz\")\n",
    "prm_npz_path = Path(PATHS.PROMPT_OUT / \"prompts_embeddings.npz\")\n",
    "prompts_pq   = Path(PATHS.PROMPT_OUT / \"prompts.parquet\")\n",
    "out_dir      = Path(PATHS.TRAIN_OUT)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- build pairs from prompts.parquet (authoritative) ----\n",
    "pairs = pd.read_parquet(prompts_pq)\n",
    "\n",
    "if \"prompt_id\" not in pairs.columns or \"tile_id\" not in pairs.columns:\n",
    "    raise RuntimeError(\"prompts.parquet must contain columns: prompt_id, tile_id\")\n",
    "\n",
    "pairs = pairs.rename(columns={\"tile_id\": \"map_id\"})[[\"map_id\", \"prompt_id\"]].copy()\n",
    "pairs[\"map_id\"] = pairs[\"map_id\"].astype(str).str.strip()\n",
    "pairs[\"prompt_id\"] = pairs[\"prompt_id\"].astype(str).str.strip()\n",
    "pairs = pairs.dropna(subset=[\"map_id\", \"prompt_id\"])\n",
    "pairs = pairs[(pairs[\"map_id\"] != \"\") & (pairs[\"prompt_id\"] != \"\")]\n",
    "pairs = pairs.drop_duplicates(subset=[\"map_id\", \"prompt_id\"])\n",
    "\n",
    "# ---- load embeddings ----\n",
    "E_map, map_ids = ce.load_npz(map_npz_path)\n",
    "E_prm, prm_ids = ce.load_npz(prm_npz_path)\n",
    "\n",
    "idx_map = {k: i for i, k in enumerate(map_ids)}\n",
    "idx_prm = {k: i for i, k in enumerate(prm_ids)}\n",
    "\n",
    "# ---- match & build X ----\n",
    "chosen_rows, im_list, ip_list = [], [], []\n",
    "missing = 0\n",
    "\n",
    "for i, row in enumerate(pairs.itertuples(index=False), start=0):\n",
    "    im = idx_map.get(row.map_id)\n",
    "    ip = idx_prm.get(row.prompt_id)\n",
    "    if im is None or ip is None:\n",
    "        missing += 1\n",
    "        continue\n",
    "    chosen_rows.append(i)\n",
    "    im_list.append(im)\n",
    "    ip_list.append(ip)\n",
    "\n",
    "if not im_list:\n",
    "    raise RuntimeError(\"No valid pairs after ID matching.\")\n",
    "\n",
    "if missing:\n",
    "    print(f\"‚ö†Ô∏è Skipped {missing} rows with missing IDs\")\n",
    "\n",
    "X_map = E_map[np.asarray(im_list, dtype=int)].astype(np.float32, copy=False)\n",
    "X_prm = E_prm[np.asarray(ip_list, dtype=int)].astype(np.float32, copy=False)\n",
    "X = np.hstack([X_map, X_prm]).astype(np.float32, copy=False)\n",
    "\n",
    "np.save(out_dir / \"X_concat.npy\", X)\n",
    "join_df = pairs.iloc[chosen_rows].reset_index(drop=True)\n",
    "join_df.to_parquet(out_dir / \"train_pairs.parquet\", index=False)\n",
    "\n",
    "meta = {\n",
    "    \"shape\": [int(X.shape[0]), int(X.shape[1])],\n",
    "    \"map_dim\": int(E_map.shape[1]),\n",
    "    \"prompt_dim\": int(E_prm.shape[1]),\n",
    "    \"rows\": int(X.shape[0]),\n",
    "    \"skipped_pairs\": int(missing),\n",
    "    \"sources\": {\n",
    "        \"prompts_parquet\": str(prompts_pq),\n",
    "        \"map_npz\": str(map_npz_path),\n",
    "        \"prompt_npz\": str(prm_npz_path),\n",
    "    },\n",
    "}\n",
    "(out_dir / \"meta.json\").write_text(json.dumps(meta, indent=2))\n",
    "\n",
    "print(\"‚úÖ Concatenation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5142d4b68c273d37",
   "metadata": {},
   "source": [
    "## üì• 4) Load & Basic Cleaning ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a494fd27dfe7681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded X: (564, 1701), pairs: (564, 2)\n",
      "After cleaning: X=(564, 1701), df=(564, 5), ops=['aggregate', 'displace', 'select', 'simplify']\n",
      "Example rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>map_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>operator</th>\n",
       "      <th>param_value</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1304</td>\n",
       "      <td>r00000000</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1690</td>\n",
       "      <td>r00000001</td>\n",
       "      <td>select</td>\n",
       "      <td>47.584</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1646</td>\n",
       "      <td>r00000002</td>\n",
       "      <td>select</td>\n",
       "      <td>129.722</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1663</td>\n",
       "      <td>r00000005</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0856</td>\n",
       "      <td>r00000006</td>\n",
       "      <td>simplify</td>\n",
       "      <td>16.917</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>1697</td>\n",
       "      <td>r00000781</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>0078</td>\n",
       "      <td>r00000782</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>4.000</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>1525</td>\n",
       "      <td>r00000783</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>7.000</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>1288</td>\n",
       "      <td>r00000784</td>\n",
       "      <td>select</td>\n",
       "      <td>80.000</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0363</td>\n",
       "      <td>r00000785</td>\n",
       "      <td>displace</td>\n",
       "      <td>6.285</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>564 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    map_id  prompt_id   operator  param_value intensity\n",
       "0     1304  r00000000  aggregate        0.000    medium\n",
       "1     1690  r00000001     select       47.584       low\n",
       "2     1646  r00000002     select      129.722    medium\n",
       "3     1663  r00000005  aggregate        0.000    medium\n",
       "4     0856  r00000006   simplify       16.917      high\n",
       "..     ...        ...        ...          ...       ...\n",
       "559   1697  r00000781  aggregate        0.000       low\n",
       "560   0078  r00000782  aggregate        4.000      high\n",
       "561   1525  r00000783  aggregate        7.000      high\n",
       "562   1288  r00000784     select       80.000    medium\n",
       "563   0363  r00000785   displace        6.285      high\n",
       "\n",
       "[564 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === LOAD FUSED DATA (operator + param_value only) ===\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = np.load(PATHS.TRAIN_OUT / \"X_concat.npy\")\n",
    "pairs_df = pd.read_parquet(PATHS.TRAIN_OUT / \"train_pairs.parquet\")\n",
    "print(f\"Loaded X: {X.shape}, pairs: {pairs_df.shape}\")\n",
    "\n",
    "# --- Rebuild labels from Excel and merge (since train_pairs.parquet has only ids) ---\n",
    "dfu = pd.read_excel(PATHS.USER_STUDY_XLSX, sheet_name=PATHS.RESPONSES_SHEET)\n",
    "\n",
    "dfu[PATHS.COMPLETE_COL] = dfu[PATHS.COMPLETE_COL].astype(bool)\n",
    "dfu[PATHS.REMOVE_COL]   = dfu[PATHS.REMOVE_COL].astype(bool)\n",
    "\n",
    "mask_excel = pd.Series(True, index=dfu.index)\n",
    "if PATHS.ONLY_COMPLETE:\n",
    "    mask_excel &= (dfu[PATHS.COMPLETE_COL] == True)\n",
    "if PATHS.EXCLUDE_REMOVED:\n",
    "    mask_excel &= (dfu[PATHS.REMOVE_COL] == False)\n",
    "dfu = dfu[mask_excel].copy()\n",
    "\n",
    "# match prompt_embeddings.py: prompt_id uses original Excel row index\n",
    "dfu = dfu.reset_index(drop=False).rename(columns={\"index\": \"_row\"})\n",
    "prefix = PATHS.PROMPT_ID_PREFIX\n",
    "width  = PATHS.PROMPT_ID_WIDTH\n",
    "dfu[\"prompt_id\"] = dfu[\"_row\"].apply(lambda r: f\"{prefix}{int(r):0{width}d}\")\n",
    "\n",
    "# normalize tile_id -> map_id (4-digit folder ids like 0001, 0345)\n",
    "tile_raw = dfu[PATHS.TILE_ID_COL]\n",
    "tile_num = pd.to_numeric(tile_raw, errors=\"coerce\")\n",
    "if tile_num.notna().all():\n",
    "    dfu[\"map_id\"] = tile_num.astype(int).astype(str).str.zfill(4)\n",
    "else:\n",
    "    dfu[\"map_id\"] = tile_raw.astype(str).str.strip().str.zfill(4)\n",
    "\n",
    "labels = dfu[[\n",
    "    \"map_id\",\n",
    "    \"prompt_id\",\n",
    "    PATHS.OPERATOR_COL,\n",
    "    PATHS.PARAM_VALUE_COL,\n",
    "    PATHS.INTENSITY_COL,   # <-- ADD THIS\n",
    "]].copy()\n",
    "\n",
    "\n",
    "df = pairs_df.merge(labels, on=[\"map_id\", \"prompt_id\"], how=\"left\")\n",
    "\n",
    "OP_COL = PATHS.OPERATOR_COL               # \"operator\"\n",
    "PARAM_COL = PATHS.PARAM_VALUE_COL         # \"param_value\"\n",
    "\n",
    "# clean target columns\n",
    "df[OP_COL] = df[OP_COL].astype(str).str.strip().str.lower()\n",
    "df[PARAM_COL] = pd.to_numeric(df[PARAM_COL], errors=\"coerce\")\n",
    "\n",
    "# keep only rows that have both targets\n",
    "mask = df[OP_COL].notna() & df[PARAM_COL].notna()\n",
    "\n",
    "X  = X[mask.values].astype(\"float64\", copy=False)\n",
    "df = df.loc[mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"After cleaning: X={X.shape}, df={df.shape}, ops={sorted(df[OP_COL].unique())}\")\n",
    "print(\"Example rows:\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997cab7",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è 5) Split & Targets ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc0897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET SUMMARY ===\n",
      "Total rows (prompts): 564\n",
      "Unique maps: 401\n",
      "Multi-prompt maps (>1 prompt): 22\n",
      "Single-prompt maps (=1 prompt): 379\n",
      "\n",
      "Top 10 maps by prompt count:\n",
      "map_id\n",
      "1646    30\n",
      "1304    29\n",
      "1755    26\n",
      "1532    13\n",
      "0127    10\n",
      "0168     8\n",
      "0142     7\n",
      "0078     6\n",
      "0080     6\n",
      "0001     6\n",
      "dtype: int64\n",
      "\n",
      "=== SPLIT SUMMARY ===\n",
      "‚úÖ Split found (seed=42)\n",
      "Train maps: 287  (includes multi-prompt maps: 22)\n",
      "Val maps:   57\n",
      "Test maps:  57\n",
      "Rows -> Train: (450, 1701), Val: (57, 1701), Test: (57, 1701)\n",
      "‚úÖ Verified: no map_id leakage across splits.\n",
      "‚úÖ Verified: all multi-prompt maps are in TRAIN.\n",
      "\n",
      "TRAIN ‚Äî Operator counts\n",
      "operator\n",
      "select       146\n",
      "aggregate    134\n",
      "simplify     109\n",
      "displace      61\n",
      "Name: count, dtype: int64\n",
      "\n",
      "VAL ‚Äî Operator counts\n",
      "operator\n",
      "select       19\n",
      "aggregate    16\n",
      "simplify     13\n",
      "displace      9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "TEST ‚Äî Operator counts\n",
      "operator\n",
      "select       19\n",
      "aggregate    16\n",
      "simplify     13\n",
      "displace      9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "TRAIN ‚Äî Operator √ó Intensity table (counts)\n",
      "intensity  high  low  medium\n",
      "operator                    \n",
      "aggregate    37   38      59\n",
      "displace     13   20      28\n",
      "select       35   35      76\n",
      "simplify     25   26      58\n",
      "\n",
      "TRAIN ‚Äî Operator totals (row sums)\n",
      "operator\n",
      "aggregate    134\n",
      "displace      61\n",
      "select       146\n",
      "simplify     109\n",
      "dtype: int64\n",
      "\n",
      "TRAIN ‚Äî Intensity totals (col sums)\n",
      "intensity\n",
      "high      110\n",
      "low       119\n",
      "medium    221\n",
      "dtype: int64\n",
      "\n",
      "VAL ‚Äî Operator √ó Intensity table (counts)\n",
      "intensity  high  low  medium\n",
      "operator                    \n",
      "aggregate     5    6       5\n",
      "displace      3    3       3\n",
      "select        6    6       7\n",
      "simplify      4    4       5\n",
      "\n",
      "VAL ‚Äî Operator totals (row sums)\n",
      "operator\n",
      "aggregate    16\n",
      "displace      9\n",
      "select       19\n",
      "simplify     13\n",
      "dtype: int64\n",
      "\n",
      "VAL ‚Äî Intensity totals (col sums)\n",
      "intensity\n",
      "high      18\n",
      "low       19\n",
      "medium    20\n",
      "dtype: int64\n",
      "\n",
      "TEST ‚Äî Operator √ó Intensity table (counts)\n",
      "intensity  high  low  medium\n",
      "operator                    \n",
      "aggregate     5    6       5\n",
      "displace      3    3       3\n",
      "select        6    6       7\n",
      "simplify      4    4       5\n",
      "\n",
      "TEST ‚Äî Operator totals (row sums)\n",
      "operator\n",
      "aggregate    16\n",
      "displace      9\n",
      "select       19\n",
      "simplify     13\n",
      "dtype: int64\n",
      "\n",
      "TEST ‚Äî Intensity totals (col sums)\n",
      "intensity\n",
      "high      18\n",
      "low       19\n",
      "medium    20\n",
      "dtype: int64\n",
      "\n",
      "‚úÖ TRAIN: All operator√óintensity combos present.\n",
      "\n",
      "‚úÖ VAL: All operator√óintensity combos present.\n",
      "\n",
      "‚úÖ TEST: All operator√óintensity combos present.\n",
      "\n",
      "TRAIN ‚Äî prompts per map statistics\n",
      "TRAIN ‚Äî #maps with >1 prompt: 22\n",
      "\n",
      "VAL ‚Äî prompts per map statistics\n",
      "VAL ‚Äî #maps with >1 prompt: 0\n",
      "\n",
      "TEST ‚Äî prompts per map statistics\n",
      "TEST ‚Äî #maps with >1 prompt: 0\n",
      "\n",
      "TRAIN ‚Äî Top multi-prompt maps (forced to train):\n",
      "map_id\n",
      "1646    30\n",
      "1304    29\n",
      "1755    26\n",
      "1532    13\n",
      "0127    10\n",
      "0168     8\n",
      "0142     7\n",
      "0080     6\n",
      "0001     6\n",
      "0078     6\n",
      "0073     6\n",
      "0159     5\n",
      "0074     5\n",
      "0079     4\n",
      "0081     4\n",
      "0077     4\n",
      "0025     3\n",
      "0120     3\n",
      "0240     3\n",
      "0118     3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "FIXED_CLASSES = [\"simplify\", \"select\", \"aggregate\", \"displace\"]\n",
    "USE_INTENSITY_FOR_STRAT = True  # will fall back to operator-only if needed\n",
    "\n",
    "OP_COL   = PATHS.OPERATOR_COL        # \"operator\"\n",
    "PARAM_COL = PATHS.PARAM_VALUE_COL    # \"param_value\"\n",
    "INT_COL  = PATHS.INTENSITY_COL       # \"intensity\"\n",
    "\n",
    "df = df.copy()\n",
    "df[OP_COL] = df[OP_COL].astype(str).str.strip().str.lower()\n",
    "if INT_COL in df.columns:\n",
    "    df[INT_COL] = df[INT_COL].astype(str).str.strip().str.lower()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Group stats: prompts per map_id\n",
    "# ------------------------------------------------------------\n",
    "prompt_counts = df.groupby(\"map_id\").size()\n",
    "multi_map_ids = prompt_counts[prompt_counts > 1].index.tolist()\n",
    "single_map_ids = prompt_counts[prompt_counts == 1].index.tolist()\n",
    "\n",
    "print(\"=== DATASET SUMMARY ===\")\n",
    "print(f\"Total rows (prompts): {len(df)}\")\n",
    "print(f\"Unique maps: {prompt_counts.shape[0]}\")\n",
    "print(f\"Multi-prompt maps (>1 prompt): {len(multi_map_ids)}\")\n",
    "print(f\"Single-prompt maps (=1 prompt): {len(single_map_ids)}\")\n",
    "print(\"\\nTop 10 maps by prompt count:\")\n",
    "print(prompt_counts.sort_values(ascending=False).head(10))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Map-level table for single maps (one row per map_id)\n",
    "# ------------------------------------------------------------\n",
    "df_single = df[df[\"map_id\"].isin(single_map_ids)].copy()\n",
    "map_level = df_single.groupby(\"map_id\").first().reset_index()\n",
    "\n",
    "# Build strat label: operator√óintensity if feasible, else operator only\n",
    "if USE_INTENSITY_FOR_STRAT and INT_COL in map_level.columns:\n",
    "    map_level[\"_strat\"] = map_level[OP_COL] + \"__\" + map_level[INT_COL]\n",
    "    vc = map_level[\"_strat\"].value_counts()\n",
    "    if (vc < 2).any():\n",
    "        print(\"\\n‚ö†Ô∏è Some operator√óintensity groups too rare (<2 single-maps). Falling back to operator-only stratification.\")\n",
    "        map_level[\"_strat\"] = map_level[OP_COL]\n",
    "else:\n",
    "    map_level[\"_strat\"] = map_level[OP_COL]\n",
    "\n",
    "def has_all_ops(dfx: pd.DataFrame) -> bool:\n",
    "    return set(dfx[OP_COL].unique()) >= set(FIXED_CLASSES)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Split single maps into train/val/test with retries\n",
    "# ------------------------------------------------------------\n",
    "test_ratio = CFG.TEST_RATIO\n",
    "val_ratio = CFG.VAL_RATIO\n",
    "val_rel = val_ratio / (1.0 - test_ratio)\n",
    "\n",
    "X_idx = np.arange(len(map_level))\n",
    "y_strat = map_level[\"_strat\"].to_numpy()\n",
    "map_ids_arr = map_level[\"map_id\"].to_numpy()\n",
    "\n",
    "best = None\n",
    "for attempt in range(500):\n",
    "    rs = CFG.SEED + attempt\n",
    "\n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=test_ratio, random_state=rs)\n",
    "    trainval_i, test_i = next(sss1.split(X_idx, y_strat))\n",
    "\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_rel, random_state=rs + 999)\n",
    "    train_i, val_i = next(sss2.split(trainval_i, y_strat[trainval_i]))\n",
    "\n",
    "    single_train_maps = set(map_ids_arr[trainval_i[train_i]])\n",
    "    single_val_maps   = set(map_ids_arr[trainval_i[val_i]])\n",
    "    single_test_maps  = set(map_ids_arr[test_i])\n",
    "\n",
    "    train_maps = set(multi_map_ids) | single_train_maps\n",
    "    val_maps   = single_val_maps\n",
    "    test_maps  = single_test_maps\n",
    "\n",
    "    # leakage check\n",
    "    if (train_maps & val_maps) or (train_maps & test_maps) or (val_maps & test_maps):\n",
    "        continue\n",
    "\n",
    "    df_train_tmp = df[df[\"map_id\"].isin(train_maps)]\n",
    "    df_val_tmp   = df[df[\"map_id\"].isin(val_maps)]\n",
    "    df_test_tmp  = df[df[\"map_id\"].isin(test_maps)]\n",
    "\n",
    "    # must contain all operators in each split\n",
    "    if not (has_all_ops(df_train_tmp) and has_all_ops(df_val_tmp) and has_all_ops(df_test_tmp)):\n",
    "        continue\n",
    "\n",
    "    best = (train_maps, val_maps, test_maps, rs)\n",
    "    break\n",
    "\n",
    "if best is None:\n",
    "    raise RuntimeError(\n",
    "        \"Could not find a leakage-safe split with operator coverage in all splits \"\n",
    "        \"and multi-prompt maps forced to TRAIN. \"\n",
    "        \"Try: USE_INTENSITY_FOR_STRAT=False, or adjust VAL/TEST ratios.\"\n",
    "    )\n",
    "\n",
    "train_maps, val_maps, test_maps, used_seed = best\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Build row-level splits (no leakage)\n",
    "# ------------------------------------------------------------\n",
    "train_idx = df.index[df[\"map_id\"].isin(train_maps)].to_numpy()\n",
    "val_idx   = df.index[df[\"map_id\"].isin(val_maps)].to_numpy()\n",
    "test_idx  = df.index[df[\"map_id\"].isin(test_maps)].to_numpy()\n",
    "\n",
    "X_train, X_val, X_test = X[train_idx], X[val_idx], X[test_idx]\n",
    "df_train = df.loc[train_idx].reset_index(drop=True)\n",
    "df_val   = df.loc[val_idx].reset_index(drop=True)\n",
    "df_test  = df.loc[test_idx].reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== SPLIT SUMMARY ===\")\n",
    "print(f\"‚úÖ Split found (seed={used_seed})\")\n",
    "print(f\"Train maps: {len(train_maps)}  (includes multi-prompt maps: {len(set(multi_map_ids))})\")\n",
    "print(f\"Val maps:   {len(val_maps)}\")\n",
    "print(f\"Test maps:  {len(test_maps)}\")\n",
    "print(f\"Rows -> Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Hard guarantees\n",
    "assert set(df_train[\"map_id\"]).isdisjoint(df_val[\"map_id\"])\n",
    "assert set(df_train[\"map_id\"]).isdisjoint(df_test[\"map_id\"])\n",
    "assert set(df_val[\"map_id\"]).isdisjoint(df_test[\"map_id\"])\n",
    "assert set(multi_map_ids).issubset(train_maps)\n",
    "print(\"‚úÖ Verified: no map_id leakage across splits.\")\n",
    "print(\"‚úÖ Verified: all multi-prompt maps are in TRAIN.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Detailed diagnostics: operator + intensity coverage\n",
    "# ------------------------------------------------------------\n",
    "def print_operator_counts(dfx, name):\n",
    "    print(f\"\\n{name} ‚Äî Operator counts\")\n",
    "    print(dfx[OP_COL].value_counts())\n",
    "\n",
    "def print_op_intensity_table(dfx, name):\n",
    "    if INT_COL not in dfx.columns:\n",
    "        print(f\"\\n{name} ‚Äî intensity column missing; skipping op√óintensity table.\")\n",
    "        return\n",
    "    print(f\"\\n{name} ‚Äî Operator √ó Intensity table (counts)\")\n",
    "    tab = (\n",
    "        dfx.groupby([OP_COL, INT_COL]).size()\n",
    "        .unstack(fill_value=0)\n",
    "        .sort_index()\n",
    "    )\n",
    "    print(tab)\n",
    "    print(f\"\\n{name} ‚Äî Operator totals (row sums)\")\n",
    "    print(tab.sum(axis=1))\n",
    "    print(f\"\\n{name} ‚Äî Intensity totals (col sums)\")\n",
    "    print(tab.sum(axis=0))\n",
    "\n",
    "def report_missing_combos(dfx, name, all_ops, all_ints):\n",
    "    if INT_COL not in dfx.columns:\n",
    "        return\n",
    "    present = set(zip(dfx[OP_COL], dfx[INT_COL]))\n",
    "    missing = [(op, it) for op in all_ops for it in all_ints if (op, it) not in present]\n",
    "    if missing:\n",
    "        print(f\"\\n‚ö†Ô∏è {name}: Missing operator√óintensity combos ({len(missing)}):\")\n",
    "        print(missing[:40], \"...\" if len(missing) > 40 else \"\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ {name}: All operator√óintensity combos present.\")\n",
    "\n",
    "ALL_OPS = FIXED_CLASSES[:]  # enforce fixed order\n",
    "ALL_INTS = sorted(df[INT_COL].unique()) if INT_COL in df.columns else []\n",
    "\n",
    "print_operator_counts(df_train, \"TRAIN\")\n",
    "print_operator_counts(df_val,   \"VAL\")\n",
    "print_operator_counts(df_test,  \"TEST\")\n",
    "\n",
    "print_op_intensity_table(df_train, \"TRAIN\")\n",
    "print_op_intensity_table(df_val,   \"VAL\")\n",
    "print_op_intensity_table(df_test,  \"TEST\")\n",
    "\n",
    "report_missing_combos(df_train, \"TRAIN\", ALL_OPS, ALL_INTS)\n",
    "report_missing_combos(df_val,   \"VAL\",   ALL_OPS, ALL_INTS)\n",
    "report_missing_combos(df_test,  \"TEST\",  ALL_OPS, ALL_INTS)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Map-level prompt multiplicity info per split\n",
    "# ------------------------------------------------------------\n",
    "def map_prompt_stats(map_set, name):\n",
    "    sub_counts = prompt_counts.loc[list(map_set)]\n",
    "    print(f\"\\n{name} ‚Äî prompts per map statistics\")\n",
    "    print(f\"{name} ‚Äî #maps with >1 prompt:\", int((sub_counts > 1).sum()))\n",
    "\n",
    "map_prompt_stats(train_maps, \"TRAIN\")\n",
    "map_prompt_stats(val_maps,   \"VAL\")\n",
    "map_prompt_stats(test_maps,  \"TEST\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) Optional: show top multi-prompt maps in TRAIN\n",
    "# ------------------------------------------------------------\n",
    "top_multi = prompt_counts.loc[multi_map_ids].sort_values(ascending=False).head(20)\n",
    "print(\"\\nTRAIN ‚Äî Top multi-prompt maps (forced to train):\")\n",
    "print(top_multi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187473d",
   "metadata": {},
   "source": [
    "## üßº 6) Modality-Aware Preprocessing (map only) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41942b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modality-aware preprocessing complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/preproc.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === MODALITY-AWARE PREPROCESSING ===\n",
    "def split_blocks(X):\n",
    "    X_map    = X[:, :MAP_DIM].astype(np.float64, copy=True)\n",
    "    X_prompt = X[:, MAP_DIM:MAP_DIM+PROMPT_DIM].astype(np.float64, copy=True)\n",
    "    return X_map, X_prompt\n",
    "\n",
    "def l2_normalize_rows(A, eps=1e-12):\n",
    "    nrm = np.sqrt((A * A).sum(axis=1, keepdims=True))\n",
    "    return A / np.maximum(nrm, eps)\n",
    "\n",
    "# split\n",
    "Xm_tr, Xp_tr = split_blocks(X_train)\n",
    "Xm_va, Xp_va = split_blocks(X_val)\n",
    "Xm_te, Xp_te = split_blocks(X_test)\n",
    "\n",
    "# prompts: L2 only\n",
    "Xp_tr = l2_normalize_rows(Xp_tr)\n",
    "Xp_va = l2_normalize_rows(Xp_va)\n",
    "Xp_te = l2_normalize_rows(Xp_te)\n",
    "\n",
    "# maps: inf‚ÜíNaN\n",
    "for A in (Xm_tr, Xm_va, Xm_te):\n",
    "    A[~np.isfinite(A)] = np.nan\n",
    "\n",
    "# impute (train)\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "Xm_tr_imp = imp.fit_transform(Xm_tr)\n",
    "Xm_va_imp = imp.transform(Xm_va)\n",
    "Xm_te_imp = imp.transform(Xm_te)\n",
    "\n",
    "# clip (5‚Äì95%) train thresholds\n",
    "q_lo = np.nanpercentile(Xm_tr_imp, 5, axis=0)\n",
    "q_hi = np.nanpercentile(Xm_tr_imp, 95, axis=0)\n",
    "def clip_to_q(A, lo, hi): return np.clip(A, lo, hi)\n",
    "\n",
    "Xm_tr_imp = clip_to_q(Xm_tr_imp, q_lo, q_hi)\n",
    "Xm_va_imp = clip_to_q(Xm_va_imp, q_lo, q_hi)\n",
    "Xm_te_imp = clip_to_q(Xm_te_imp, q_lo, q_hi)\n",
    "\n",
    "# drop zero-variance cols on train\n",
    "stds = np.nanstd(Xm_tr_imp, axis=0)\n",
    "keep_mask = stds > 1e-12\n",
    "\n",
    "# scale kept columns (train fit)\n",
    "scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5, 95))\n",
    "Xm_tr_kept = scaler.fit_transform(Xm_tr_imp[:, keep_mask])\n",
    "Xm_va_kept = scaler.transform(Xm_va_imp[:, keep_mask])\n",
    "Xm_te_kept = scaler.transform(Xm_te_imp[:, keep_mask])\n",
    "\n",
    "# rebuild full map dim (dropped cols = 0)\n",
    "Xm_tr_s = np.zeros_like(Xm_tr_imp, dtype=np.float64)\n",
    "Xm_va_s = np.zeros_like(Xm_va_imp, dtype=np.float64)\n",
    "Xm_te_s = np.zeros_like(Xm_te_imp, dtype=np.float64)\n",
    "Xm_tr_s[:, keep_mask] = Xm_tr_kept.astype(np.float64)\n",
    "Xm_va_s[:, keep_mask] = Xm_va_kept.astype(np.float64)\n",
    "Xm_te_s[:, keep_mask] = Xm_te_kept.astype(np.float64)\n",
    "\n",
    "# fuse back\n",
    "X_train_s = np.concatenate([Xm_tr_s, Xp_tr], axis=1).astype(np.float64)\n",
    "X_val_s   = np.concatenate([Xm_va_s, Xp_va], axis=1).astype(np.float64)\n",
    "X_test_s  = np.concatenate([Xm_te_s, Xp_te], axis=1).astype(np.float64)\n",
    "\n",
    "assert np.isfinite(X_train_s).all() and np.isfinite(X_val_s).all() and np.isfinite(X_test_s).all(), \"Non-finite after preprocessing.\"\n",
    "print(\"‚úÖ Modality-aware preprocessing complete.\")\n",
    "\n",
    "# save preprocessing bundle\n",
    "joblib.dump({\n",
    "    \"imp\": imp, \"q_lo\": q_lo, \"q_hi\": q_hi,\n",
    "    \"keep_mask\": keep_mask, \"scaler\": scaler,\n",
    "    \"map_dim\": MAP_DIM, \"prompt_dim\": PROMPT_DIM\n",
    "}, PATHS.TRAIN_OUT / \"preproc.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c32c2",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è 7) Class Weights ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0df2b2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator classes: ['aggregate', 'displace', 'select', 'simplify']\n",
      "Class weights: {'aggregate': np.float64(0.8395522388059702), 'displace': np.float64(1.8442622950819672), 'select': np.float64(0.7705479452054794), 'simplify': np.float64(1.0321100917431192)}\n",
      "Sample weight summary: {'min': 0.025684931506849314, 'max': 1.8442622950819672, 'mean': 0.6500530407829777}\n"
     ]
    }
   ],
   "source": [
    "# === BUILD y_train_cls + CLASS + MAP-AWARE SAMPLE WEIGHTS ===\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "OP_COL = PATHS.OPERATOR_COL  # \"operator\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Build y_train_cls directly from df_train\n",
    "# ------------------------------------------------------------\n",
    "# factorize gives stable integer labels starting at 0\n",
    "y_train_cls, class_names = pd.factorize(df_train[OP_COL], sort=True)\n",
    "n_classes = len(class_names)\n",
    "\n",
    "print(\"Operator classes:\", list(class_names))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Class weights (operator imbalance)\n",
    "# ------------------------------------------------------------\n",
    "classes = np.arange(n_classes)\n",
    "\n",
    "cls_w = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=y_train_cls\n",
    ")\n",
    "cls_w = np.asarray(cls_w, dtype=\"float64\")\n",
    "\n",
    "class_weight_map = dict(zip(class_names, cls_w))\n",
    "print(\"Class weights:\", class_weight_map)\n",
    "\n",
    "# per-sample class weight\n",
    "w_class = np.array([cls_w[c] for c in y_train_cls], dtype=\"float64\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Map-level weighting (prompt multiplicity correction)\n",
    "# ------------------------------------------------------------\n",
    "map_counts = df_train[\"map_id\"].value_counts()\n",
    "\n",
    "# each map contributes ~1 total weight\n",
    "w_map = df_train[\"map_id\"].map(lambda m: 1.0 / map_counts[m]).to_numpy(dtype=\"float64\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Final sample weights\n",
    "# ------------------------------------------------------------\n",
    "sample_w = w_class * w_map\n",
    "\n",
    "print(\n",
    "    \"Sample weight summary:\",\n",
    "    {\n",
    "        \"min\": float(sample_w.min()),\n",
    "        \"max\": float(sample_w.max()),\n",
    "        \"mean\": float(sample_w.mean()),\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988f274",
   "metadata": {},
   "source": [
    "## üß† 8) Train MLP ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95133825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching 50 MLP configs...\n",
      "[01/50] cvF1=0.223¬±0.019 | VAL F1=0.727 acc=0.731 | (128, 64), Œ±=2.02e-02, lr=1.2e-03, bs=16\n",
      "[02/50] cvF1=0.222¬±0.031 | VAL F1=0.642 acc=0.657 | (256, 128), Œ±=3.49e-05, lr=1.7e-04, bs=64\n",
      "[03/50] cvF1=0.232¬±0.042 | VAL F1=0.755 acc=0.761 | (256,), Œ±=1.03e-02, lr=7.7e-04, bs=128\n",
      "[04/50] cvF1=0.216¬±0.050 | VAL F1=0.643 acc=0.657 | (256,), Œ±=1.18e-05, lr=2.7e-03, bs=128\n",
      "[05/50] cvF1=0.239¬±0.039 | VAL F1=0.648 acc=0.657 | (256, 128, 64), Œ±=5.47e-05, lr=1.9e-04, bs=16\n",
      "[06/50] cvF1=0.214¬±0.057 | VAL F1=0.643 acc=0.657 | (64,), Œ±=1.14e-04, lr=6.0e-04, bs=128\n",
      "[07/50] cvF1=0.210¬±0.060 | VAL F1=0.656 acc=0.672 | (64,), Œ±=1.03e-04, lr=8.0e-04, bs=32\n",
      "[08/50] cvF1=0.244¬±0.044 | VAL F1=0.759 acc=0.761 | (128, 64), Œ±=2.43e-02, lr=2.2e-04, bs=32\n",
      "[09/50] cvF1=0.238¬±0.047 | VAL F1=0.630 acc=0.642 | (256, 128, 64), Œ±=4.95e-05, lr=5.7e-04, bs=128\n",
      "[10/50] cvF1=0.224¬±0.056 | VAL F1=0.661 acc=0.672 | (64,), Œ±=1.45e-05, lr=7.9e-04, bs=16\n",
      "[11/50] cvF1=0.213¬±0.049 | VAL F1=0.643 acc=0.657 | (64,), Œ±=1.68e-05, lr=2.5e-03, bs=128\n",
      "[12/50] cvF1=0.260¬±0.047 | VAL F1=0.743 acc=0.746 | (256, 128, 64), Œ±=6.47e-03, lr=2.8e-04, bs=16\n",
      "[13/50] cvF1=0.220¬±0.028 | VAL F1=0.656 acc=0.672 | (128,), Œ±=2.39e-03, lr=4.5e-04, bs=64\n",
      "[14/50] cvF1=0.220¬±0.029 | VAL F1=0.628 acc=0.642 | (128, 64), Œ±=5.27e-04, lr=1.1e-04, bs=32\n",
      "[15/50] cvF1=0.211¬±0.059 | VAL F1=0.657 acc=0.672 | (64,), Œ±=7.94e-05, lr=9.5e-04, bs=32\n",
      "[16/50] cvF1=0.247¬±0.052 | VAL F1=0.659 acc=0.672 | (256, 128, 64), Œ±=6.43e-04, lr=6.4e-04, bs=32\n",
      "[17/50] cvF1=0.242¬±0.017 | VAL F1=0.554 acc=0.582 | (256, 128), Œ±=2.35e-02, lr=1.4e-03, bs=32\n",
      "[18/50] cvF1=0.230¬±0.030 | VAL F1=0.708 acc=0.716 | (128,), Œ±=1.29e-02, lr=7.6e-04, bs=128\n",
      "[19/50] cvF1=0.235¬±0.047 | VAL F1=0.662 acc=0.672 | (256, 128, 64), Œ±=4.80e-05, lr=1.2e-04, bs=128\n",
      "[20/50] cvF1=0.229¬±0.025 | VAL F1=0.670 acc=0.687 | (256, 128), Œ±=2.25e-04, lr=2.5e-04, bs=16\n",
      "[21/50] cvF1=0.241¬±0.042 | VAL F1=0.770 acc=0.776 | (128,), Œ±=2.27e-02, lr=7.9e-04, bs=16\n",
      "[22/50] cvF1=0.219¬±0.055 | VAL F1=0.656 acc=0.672 | (64,), Œ±=1.07e-04, lr=1.8e-04, bs=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/50] cvF1=0.213¬±0.044 | VAL F1=0.656 acc=0.672 | (64,), Œ±=4.84e-03, lr=2.0e-04, bs=128\n",
      "[24/50] cvF1=0.222¬±0.059 | VAL F1=0.641 acc=0.657 | (256,), Œ±=4.91e-05, lr=1.1e-03, bs=64\n",
      "[25/50] cvF1=0.237¬±0.063 | VAL F1=0.669 acc=0.672 | (64,), Œ±=1.28e-03, lr=2.3e-03, bs=32\n",
      "[26/50] cvF1=0.230¬±0.042 | VAL F1=0.723 acc=0.731 | (64,), Œ±=1.52e-02, lr=1.8e-03, bs=128\n",
      "[27/50] cvF1=0.231¬±0.028 | VAL F1=0.642 acc=0.657 | (128,), Œ±=2.15e-05, lr=3.5e-04, bs=32\n",
      "[28/50] cvF1=0.251¬±0.019 | VAL F1=0.692 acc=0.701 | (256, 128), Œ±=3.44e-03, lr=8.7e-04, bs=64\n",
      "[29/50] cvF1=0.214¬±0.054 | VAL F1=0.641 acc=0.657 | (256,), Œ±=4.38e-04, lr=1.5e-04, bs=32\n",
      "[30/50] cvF1=0.247¬±0.049 | VAL F1=0.705 acc=0.716 | (256,), Œ±=4.42e-03, lr=6.7e-04, bs=64\n",
      "[31/50] cvF1=0.251¬±0.040 | VAL F1=0.677 acc=0.687 | (256, 128, 64), Œ±=5.21e-04, lr=5.9e-04, bs=64\n",
      "[32/50] cvF1=0.211¬±0.034 | VAL F1=0.642 acc=0.657 | (128,), Œ±=1.23e-05, lr=1.4e-04, bs=64\n",
      "[33/50] cvF1=0.199¬±0.056 | VAL F1=0.658 acc=0.672 | (64,), Œ±=1.24e-04, lr=5.6e-04, bs=32\n",
      "[34/50] cvF1=0.230¬±0.017 | VAL F1=0.643 acc=0.657 | (256, 128), Œ±=7.36e-05, lr=4.0e-04, bs=64\n",
      "[35/50] cvF1=0.220¬±0.030 | VAL F1=0.628 acc=0.642 | (256, 128), Œ±=6.25e-05, lr=1.3e-04, bs=64\n",
      "[36/50] cvF1=0.244¬±0.045 | VAL F1=0.675 acc=0.687 | (256,), Œ±=3.64e-05, lr=2.4e-03, bs=16\n",
      "[37/50] cvF1=0.232¬±0.051 | VAL F1=0.677 acc=0.687 | (256, 128, 64), Œ±=1.59e-03, lr=1.9e-03, bs=128\n",
      "[38/50] cvF1=0.229¬±0.040 | VAL F1=0.628 acc=0.642 | (128, 64), Œ±=4.45e-05, lr=2.1e-03, bs=64\n",
      "[39/50] cvF1=0.221¬±0.027 | VAL F1=0.642 acc=0.657 | (128,), Œ±=2.66e-05, lr=3.4e-04, bs=32\n",
      "[40/50] cvF1=0.239¬±0.062 | VAL F1=0.646 acc=0.657 | (64,), Œ±=8.84e-05, lr=9.1e-04, bs=16\n",
      "[41/50] cvF1=0.228¬±0.032 | VAL F1=0.612 acc=0.627 | (128, 64), Œ±=1.68e-04, lr=2.8e-04, bs=32\n",
      "[42/50] cvF1=0.219¬±0.045 | VAL F1=0.641 acc=0.657 | (256,), Œ±=2.83e-04, lr=2.1e-04, bs=64\n",
      "[43/50] cvF1=0.227¬±0.039 | VAL F1=0.656 acc=0.672 | (128,), Œ±=1.49e-04, lr=2.5e-03, bs=32\n",
      "[44/50] cvF1=0.213¬±0.046 | VAL F1=0.656 acc=0.672 | (64,), Œ±=2.54e-04, lr=1.2e-04, bs=32\n",
      "[45/50] cvF1=0.228¬±0.026 | VAL F1=0.634 acc=0.642 | (128, 64), Œ±=7.22e-05, lr=1.1e-03, bs=64\n",
      "[46/50] cvF1=0.231¬±0.059 | VAL F1=0.657 acc=0.672 | (64,), Œ±=3.27e-05, lr=3.0e-03, bs=64\n",
      "[47/50] cvF1=0.229¬±0.048 | VAL F1=0.727 acc=0.731 | (64,), Œ±=2.49e-02, lr=4.0e-04, bs=64\n",
      "[48/50] cvF1=0.252¬±0.030 | VAL F1=0.662 acc=0.672 | (256, 128), Œ±=1.58e-04, lr=8.6e-04, bs=32\n",
      "[49/50] cvF1=0.219¬±0.019 | VAL F1=0.656 acc=0.672 | (128,), Œ±=7.02e-04, lr=4.6e-04, bs=32\n",
      "[50/50] cvF1=0.230¬±0.014 | VAL F1=0.626 acc=0.642 | (128, 64), Œ±=1.91e-05, lr=3.5e-04, bs=16\n",
      "\n",
      "=== Top candidates (by VAL macro-F1) ===\n",
      "VAL F1=0.770 (acc=0.776) | cvF1=0.241¬±0.042 | params={'hidden_layer_sizes': (128,), 'alpha': 0.02271484329844196, 'learning_rate_init': 0.0007882485664694609, 'batch_size': 16, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.759 (acc=0.761) | cvF1=0.244¬±0.044 | params={'hidden_layer_sizes': (128, 64), 'alpha': 0.024314537412227038, 'learning_rate_init': 0.00022071481969101166, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.755 (acc=0.761) | cvF1=0.232¬±0.042 | params={'hidden_layer_sizes': (256,), 'alpha': 0.010275417738969424, 'learning_rate_init': 0.0007725378389307358, 'batch_size': 128, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.743 (acc=0.746) | cvF1=0.260¬±0.047 | params={'hidden_layer_sizes': (256, 128, 64), 'alpha': 0.006469870699850825, 'learning_rate_init': 0.0002818068029184724, 'batch_size': 16, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.727 (acc=0.731) | cvF1=0.223¬±0.019 | params={'hidden_layer_sizes': (128, 64), 'alpha': 0.020218499516556746, 'learning_rate_init': 0.0012057126287443765, 'batch_size': 16, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "\n",
      "üèÜ Selected params:\n",
      "{'activation': 'relu',\n",
      " 'alpha': 0.02271484329844196,\n",
      " 'batch_size': 16,\n",
      " 'early_stopping': False,\n",
      " 'hidden_layer_sizes': (128,),\n",
      " 'learning_rate_init': 0.0007882485664694609,\n",
      " 'max_iter': 800,\n",
      " 'random_state': 42,\n",
      " 'solver': 'adam',\n",
      " 'tol': 0.0001,\n",
      " 'verbose': False}\n",
      "\n",
      "===== VAL =====\n",
      "VAL: acc=0.7761  f1_macro=0.7697\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   aggregate       0.78      0.78      0.78        18\n",
      "    displace       0.78      0.88      0.82        16\n",
      "      select       0.90      0.53      0.67        17\n",
      "    simplify       0.71      0.94      0.81        16\n",
      "\n",
      "    accuracy                           0.78        67\n",
      "   macro avg       0.79      0.78      0.77        67\n",
      "weighted avg       0.79      0.78      0.77        67\n",
      "\n",
      "Confusion matrix:\n",
      " [[14  1  1  2]\n",
      " [ 1 14  0  1]\n",
      " [ 2  3  9  3]\n",
      " [ 1  0  0 15]]\n",
      "\n",
      "===== TEST =====\n",
      "TEST: acc=0.6765  f1_macro=0.6550\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   aggregate       0.71      0.56      0.62        18\n",
      "    displace       0.68      0.76      0.72        17\n",
      "      select       0.65      0.94      0.77        18\n",
      "    simplify       0.67      0.40      0.50        15\n",
      "\n",
      "    accuracy                           0.68        68\n",
      "   macro avg       0.68      0.67      0.65        68\n",
      "weighted avg       0.68      0.68      0.66        68\n",
      "\n",
      "Confusion matrix:\n",
      " [[10  2  4  2]\n",
      " [ 1 13  2  1]\n",
      " [ 1  0 17  0]\n",
      " [ 2  4  3  6]]\n",
      "\n",
      "‚úÖ Saved final MLP (trained on ALL TRAIN) to: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/best_mlp_fulltrain.joblib\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# MLP search where each model trains on ALL training data\n",
    "# =========================\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# ---- numerics: keep float64 everywhere ----\n",
    "X_train_s = X_train_s.astype(np.float64, copy=False)\n",
    "X_val_s   = X_val_s.astype(np.float64, copy=False)\n",
    "X_test_s  = X_test_s.astype(np.float64, copy=False)\n",
    "sample_w  = sample_w.astype(np.float64, copy=False)\n",
    "\n",
    "# ---- group by map_id (maps can repeat; prompts don't) ----\n",
    "assert \"map_id\" in df_train.columns, \"df_train must contain 'map_id' for grouped CV.\"\n",
    "groups_tr = df_train[\"map_id\"].astype(str).values\n",
    "\n",
    "# ---- CV splitter (for scoring only) ----\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ---- search space helpers ----\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "def draw_params(n):\n",
    "    sizes = [(64,), (128,), (256,), (128, 64), (256, 128), (256, 128, 64)]\n",
    "    batches = [16, 32, 64, 128]\n",
    "    for _ in range(n):\n",
    "        yield {\n",
    "            \"hidden_layer_sizes\": sizes[rng.randint(len(sizes))],\n",
    "            \"alpha\": 10**rng.uniform(-5, np.log10(3e-2)),          # loguniform(1e-5, 3e-2)\n",
    "            \"learning_rate_init\": 10**rng.uniform(-4, np.log10(3e-3)),  # loguniform(1e-4, 3e-3)\n",
    "            \"batch_size\": batches[rng.randint(len(batches))],\n",
    "            \"activation\": \"relu\",\n",
    "            \"solver\": \"adam\",\n",
    "            \"max_iter\": 800,            # allow convergence w/o early stopping\n",
    "            \"early_stopping\": False,    # <‚Äî IMPORTANT: use ALL training samples\n",
    "            \"random_state\": 42,\n",
    "            \"verbose\": False,\n",
    "            \"tol\": 1e-4\n",
    "        }\n",
    "\n",
    "# ---- CV scorer using grouped folds; model sees only its fold-train here (for the score only) ----\n",
    "def cv_macro_f1(params):\n",
    "    scores = []\n",
    "    for tr_idx, va_idx in cv.split(X_train_s, y_train_cls, groups_tr):\n",
    "        clf = MLPClassifier(**params)\n",
    "        clf.fit(X_train_s[tr_idx], y_train_cls[tr_idx], sample_weight=sample_w[tr_idx])\n",
    "        pred = clf.predict(X_train_s[va_idx])\n",
    "        scores.append(f1_score(y_train_cls[va_idx], pred, average=\"macro\"))\n",
    "    return float(np.mean(scores)), float(np.std(scores))\n",
    "\n",
    "@dataclass\n",
    "class Candidate:\n",
    "    params: dict\n",
    "    cv_mean: float\n",
    "    cv_std: float\n",
    "    val_f1: float\n",
    "    val_acc: float\n",
    "\n",
    "# ---- run search ----\n",
    "N_ITER = 50   # tune this for time/quality tradeoff\n",
    "candidates = []\n",
    "\n",
    "print(f\"\\nSearching {N_ITER} MLP configs...\")\n",
    "for i, params in enumerate(draw_params(N_ITER), 1):\n",
    "    cv_mean, cv_std = cv_macro_f1(params)\n",
    "\n",
    "    # IMPORTANT PART: refit SAME PARAMS on FULL TRAIN (no early_stopping) so the model sees ALL training data\n",
    "    clf_full = MLPClassifier(**params)\n",
    "    clf_full.fit(X_train_s, y_train_cls, sample_weight=sample_w)\n",
    "\n",
    "    # evaluate on external VAL (never used for training)\n",
    "    val_pred = clf_full.predict(X_val_s)\n",
    "    val_f1 = f1_score(y_val_cls, val_pred, average=\"macro\")\n",
    "    val_acc = accuracy_score(y_val_cls, val_pred)\n",
    "\n",
    "    candidates.append(Candidate(params, cv_mean, cv_std, val_f1, val_acc))\n",
    "    print(f\"[{i:02d}/{N_ITER}] cvF1={cv_mean:.3f}¬±{cv_std:.3f} | VAL F1={val_f1:.3f} acc={val_acc:.3f} | {params['hidden_layer_sizes']}, Œ±={params['alpha']:.2e}, lr={params['learning_rate_init']:.1e}, bs={params['batch_size']}\")\n",
    "\n",
    "# ---- pick winner by external VAL macro-F1 (tie-breaker: VAL acc, then CV mean) ----\n",
    "candidates.sort(key=lambda c: (c.val_f1, c.val_acc, c.cv_mean), reverse=True)\n",
    "best = candidates[0]\n",
    "print(\"\\n=== Top candidates (by VAL macro-F1) ===\")\n",
    "for c in candidates[:5]:\n",
    "    print(f\"VAL F1={c.val_f1:.3f} (acc={c.val_acc:.3f}) | cvF1={c.cv_mean:.3f}¬±{c.cv_std:.3f} | params={c.params}\")\n",
    "\n",
    "print(\"\\nüèÜ Selected params:\")\n",
    "pprint(best.params)\n",
    "\n",
    "# ---- train final model on FULL TRAIN (no early_stopping so it uses 100% of train) ----\n",
    "final_mlp = MLPClassifier(**best.params)\n",
    "final_mlp.fit(X_train_s, y_train_cls, sample_weight=sample_w)\n",
    "\n",
    "# ---- evaluate on VAL & TEST ----\n",
    "for name, Xs, ys in [(\"VAL\", X_val_s, y_val_cls), (\"TEST\", X_test_s, y_test_cls)]:\n",
    "    yhat = final_mlp.predict(Xs)\n",
    "    acc  = accuracy_score(ys, yhat)\n",
    "    f1m  = f1_score(ys, yhat, average=\"macro\")\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    print(f\"{name}: acc={acc:.4f}  f1_macro={f1m:.4f}\")\n",
    "    print(classification_report(ys, yhat, target_names=list(class_names)))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(ys, yhat))\n",
    "\n",
    "# ---- save final model ----\n",
    "out_dir = Path(PATHS.TRAIN_OUT); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "import joblib\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"model\": final_mlp,\n",
    "        \"class_names\": list(class_names),\n",
    "        \"best_params\": best.params,\n",
    "    },\n",
    "    out_dir / \"best_mlp_fulltrain.joblib\"\n",
    ")\n",
    "print(f\"\\n‚úÖ Saved final MLP (trained on ALL TRAIN) to: {out_dir / 'best_mlp_fulltrain.joblib'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea7905b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'aggregate' ===\n",
      "best CV RMSE: 1.0461720550715619\n",
      "best params:\n",
      "{'alpha': np.float64(8.530609039688927e-06),\n",
      " 'hidden_layer_sizes': (128, 64),\n",
      " 'learning_rate_init': np.float64(0.00018752209455786411)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'displace' ===\n",
      "best CV RMSE: 0.6808700586399808\n",
      "best params:\n",
      "{'alpha': np.float64(1.1465390198728487e-06),\n",
      " 'hidden_layer_sizes': (64,),\n",
      " 'learning_rate_init': np.float64(0.0026690431824362526)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'select' ===\n",
      "best CV RMSE: 1.1333539457011692\n",
      "best params:\n",
      "{'alpha': np.float64(0.003904209851777714),\n",
      " 'hidden_layer_sizes': (64,),\n",
      " 'learning_rate_init': np.float64(0.00010546221020664906)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'simplify' ===\n",
      "best CV RMSE: 1.138755790185875\n",
      "best params:\n",
      "{'alpha': np.float64(3.11927680501103e-05),\n",
      " 'hidden_layer_sizes': (256,),\n",
      " 'learning_rate_init': np.float64(0.00010725209743172001)}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 158\u001b[39m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mae, rmse\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# Classification predictions (already trained classifier)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m clf_cls = \u001b[43mclf\u001b[49m  \u001b[38;5;66;03m# <- ensure this is your trained best classifier\u001b[39;00m\n\u001b[32m    159\u001b[39m val_pred_cls = clf_cls.predict(X_val_s)\n\u001b[32m    160\u001b[39m test_pred_cls = clf_cls.predict(X_test_s)\n",
      "\u001b[31mNameError\u001b[39m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Regression branch (one MLPRegressor per operator)\n",
    "# =========================\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import joblib\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import StratifiedGroupKFold, GroupKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "# ---- 1) Prepare numeric regression targets\n",
    "def _coerce_param_to_float(s):\n",
    "    # Try robust parse; you can customize if your 'param' has units or JSON.\n",
    "    try:\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "y_train_reg = df_train[\"param\"].apply(_coerce_param_to_float).to_numpy()\n",
    "y_val_reg   = df_val[\"param\"].apply(_coerce_param_to_float).to_numpy()\n",
    "y_test_reg  = df_test[\"param\"].apply(_coerce_param_to_float).to_numpy()\n",
    "\n",
    "# Guard: drop if any NaNs (or you can filter rows; here we assert)\n",
    "assert np.isfinite(y_train_reg).all() and np.isfinite(y_val_reg).all() and np.isfinite(y_test_reg).all(), \\\n",
    "    \"Non-finite values found in regression target 'param'. Clean/parse them first.\"\n",
    "\n",
    "# Optional: log1p transform if param is positive and skewed\n",
    "USE_LOG1P = False\n",
    "if USE_LOG1P:\n",
    "    assert (y_train_reg >= 0).all() and (y_val_reg >= 0).all() and (y_test_reg >= 0).all(), \\\n",
    "        \"log1p selected but param has negatives.\"\n",
    "    ytr_reg_t = np.log1p(y_train_reg)\n",
    "    yva_reg_t = np.log1p(y_val_reg)\n",
    "    yte_reg_t = np.log1p(y_test_reg)\n",
    "    def inv_t(x): return np.expm1(x)\n",
    "else:\n",
    "    ytr_reg_t = y_train_reg.copy()\n",
    "    yva_reg_t = y_val_reg.copy()\n",
    "    yte_reg_t = y_test_reg.copy()\n",
    "    def inv_t(x): return x\n",
    "\n",
    "# ---- 2) Grouped CV by map_id for *regression* (no stratification needed on a numeric target)\n",
    "assert \"map_id\" in df_train.columns\n",
    "gk = GroupKFold(n_splits=5)\n",
    "groups_tr = df_train[\"map_id\"].astype(str).values\n",
    "\n",
    "# ---- 3) Search space for MLPRegressor (kept modest; widen n_iter to search more)\n",
    "base_reg = MLPRegressor(\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    learning_rate=\"adaptive\",   # <‚Äî helps convergence on tough subsets\n",
    "    early_stopping=False,       # keep OFF during search so it uses all class data\n",
    "    max_iter=2000,              # <‚Äî more runway\n",
    "    tol=1e-3,                   # <‚Äî slightly easier convergence threshold\n",
    "    random_state=42,\n",
    "    verbose=False,\n",
    "    batch_size=\"auto\"           # <‚Äî avoids clipping warnings\n",
    ")\n",
    "param_dist_reg = {\n",
    "    \"hidden_layer_sizes\": [(64,), (128,), (256,), (128, 64), (256, 128)],\n",
    "    \"alpha\": loguniform(1e-6, 3e-2),        # widen upper range for stronger regularization\n",
    "    \"learning_rate_init\": loguniform(1e-4, 3e-3),\n",
    "    # \"batch_size\": [\"auto\"]  # not tuning batch size anymore\n",
    "}\n",
    "\n",
    "# ---- 4) Fit one regressor per class\n",
    "class_names = list(le.classes_)\n",
    "n_classes = len(class_names)\n",
    "regressors = {}\n",
    "search_summaries = {}\n",
    "\n",
    "for cls_idx, cls_name in enumerate(class_names):\n",
    "    # mask for this class in TRAIN\n",
    "    m_tr = (y_train_cls == cls_idx)\n",
    "    Xk, yk, gk_tr = X_train_s[m_tr], ytr_reg_t[m_tr], groups_tr[m_tr]\n",
    "    if Xk.shape[0] < 10:\n",
    "        print(f\"‚ö†Ô∏è Skipping class '{cls_name}' (too few samples: {Xk.shape[0]}).\")\n",
    "        continue\n",
    "    # --- new: scale the target for this class\n",
    "    t_scaler = StandardScaler()\n",
    "    yk_s = t_scaler.fit_transform(yk.reshape(-1,1)).ravel()\n",
    "    \n",
    "    # grouped CV splits for this class only\n",
    "    splits = list(gk.split(Xk, yk_s, groups=gk_tr))\n",
    "\n",
    "    # negative RMSE is a good search objective\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base_reg,\n",
    "        param_distributions=param_dist_reg,\n",
    "        n_iter=40,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        cv=splits,\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(Xk, yk_s)\n",
    "\n",
    "    print(f\"\\n=== Regressor for class '{cls_name}' ===\")\n",
    "    print(\"best CV RMSE:\", -search.best_score_)\n",
    "    print(\"best params:\"); pprint(search.best_params_)\n",
    "    search_summaries[cls_name] = {\"neg_rmse_cv\": search.best_score_, \"params\": search.best_params_}\n",
    "\n",
    "    # Refit on the FULL class-specific TRAIN subset\n",
    "    reg_full = MLPRegressor(\n",
    "        **{**search.best_estimator_.get_params(), \"early_stopping\": False, \"max_iter\": 2000, \"random_state\": 42}\n",
    "    )\n",
    "    reg_full.fit(Xk, yk_s)\n",
    "    regressors[cls_name] = (reg_full, t_scaler)\n",
    "\n",
    "# ---- 5) Evaluate on VAL & TEST using your classifier's prediction to route to regressors\n",
    "def route_and_predict(Xs, pred_cls_idx):\n",
    "    yhat_reg = np.zeros(len(pred_cls_idx), dtype=float)\n",
    "    for i, cidx in enumerate(pred_cls_idx):\n",
    "        cname = class_names[cidx]\n",
    "        pack = regressors.get(cname)\n",
    "        if pack is None:\n",
    "            yhat_reg[i] = np.nan\n",
    "            continue\n",
    "        reg, t_scaler = pack\n",
    "        y_pred_scaled = reg.predict(Xs[i:i+1])[0]\n",
    "        # inverse target scaling\n",
    "        y_pred = t_scaler.inverse_transform([[y_pred_scaled]])[0,0]\n",
    "        yhat_reg[i] = y_pred\n",
    "    return yhat_reg\n",
    "\n",
    "\n",
    "# helper to print metrics (older sklearn: no squared=False)\n",
    "def print_reg_metrics(name, y_true, y_pred_transformed):\n",
    "    # inverse-transform predictions if you used log1p\n",
    "    y_pred = inv_t(y_pred_transformed)\n",
    "\n",
    "    # guard against NaNs (e.g., missing regressor for a class)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    if mask.sum() == 0:\n",
    "        print(f\"{name}: no finite pairs to evaluate.\")\n",
    "        return np.nan, np.nan\n",
    "    if mask.sum() < len(y_true):\n",
    "        print(f\"{name}: dropped {len(y_true) - mask.sum()} samples with NaNs.\")\n",
    "\n",
    "    y_true_m = y_true[mask]\n",
    "    y_pred_m = y_pred[mask]\n",
    "\n",
    "    mae = mean_absolute_error(y_true_m, y_pred_m)\n",
    "    mse = mean_squared_error(y_true_m, y_pred_m)   # older sklearn doesn't support squared=False\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"{name}: MAE={mae:.4f}  RMSE={rmse:.4f}\")\n",
    "    return mae, rmse\n",
    "\n",
    "\n",
    "# Classification predictions (already trained classifier)\n",
    "clf_cls = clf  # <- ensure this is your trained best classifier\n",
    "val_pred_cls = clf_cls.predict(X_val_s)\n",
    "test_pred_cls = clf_cls.predict(X_test_s)\n",
    "\n",
    "# route to per-class regressors\n",
    "yhat_val_reg_t  = route_and_predict(X_val_s,  val_pred_cls)\n",
    "yhat_test_reg_t = route_and_predict(X_test_s, test_pred_cls)\n",
    "\n",
    "print(\"\\n--- Regression with predicted classes (realistic) ---\")\n",
    "print_reg_metrics(\"VAL\",  y_val_reg,  yhat_val_reg_t)\n",
    "print_reg_metrics(\"TEST\", y_test_reg, yhat_test_reg_t)\n",
    "\n",
    "# ---- 6) Optional: 'oracle' evaluation to isolate regressor quality (use TRUE class for routing)\n",
    "yhat_val_oracle_t  = route_and_predict(X_val_s,  y_val_cls)\n",
    "yhat_test_oracle_t = route_and_predict(X_test_s, y_test_cls)\n",
    "\n",
    "print(\"\\n--- Regression with TRUE classes (oracle routing) ---\")\n",
    "print_reg_metrics(\"VAL-oracle\",  y_val_reg,  yhat_val_oracle_t)\n",
    "print_reg_metrics(\"TEST-oracle\", y_test_reg, yhat_test_oracle_t)\n",
    "\n",
    "# ---- 7) Save bundle\n",
    "bundle = {\n",
    "    \"classifier\": clf_cls,\n",
    "    \"regressors_by_class\": regressors,\n",
    "    \"label_encoder\": le,\n",
    "    \"use_log1p\": USE_LOG1P\n",
    "}\n",
    "\n",
    "out_dir = Path(PATHS.TRAIN_OUT)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(bundle, out_dir / \"cls_plus_regressors.joblib\")\n",
    "print(f\"\\n‚úÖ Saved classification+regression bundle to: {out_dir / 'cls_plus_regressors.joblib'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0ea611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
