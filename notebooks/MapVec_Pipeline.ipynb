{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f56a2c",
   "metadata": {},
   "source": [
    "üß© 0) Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "367f89f6ac45439b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:21:53.302406Z",
     "start_time": "2025-10-27T11:21:53.298709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFIG SUMMARY ===\n",
      "PROJ_ROOT  : /Users/amirdonyadide/Documents/GitHub/Thesis\n",
      "DATA_DIR   : /Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
      "INPUT_DIR  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input\n",
      "OUTPUT_DIR : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output\n",
      "MAPS_ROOT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/samples/pairs\n",
      "INPUT PAT. : *_input.geojson\n",
      "PROMPTS_CSV: /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/prompts.csv\n",
      "PAIRS_CSV  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/pairs.csv\n",
      "PROMPT_OUT : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "MAP_OUT    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n",
      "TRAIN_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out\n",
      "MODEL_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models\n",
      "SPLIT_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/splits\n",
      "PRM_NPZ    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/prompts_embeddings.npz\n",
      "--- Model ---\n",
      "USE_MODEL  : dan\n",
      "MAP_DIM    : 165\n",
      "PROMPT_DIM : 512\n",
      "FUSED_DIM  : 677\n",
      "BATCH_SIZE : 512\n",
      "VAL/TEST   : 0.15 0.15\n",
      "SEED       : 42\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models\n",
      "‚úÖ All output folders cleaned and recreated fresh.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================== PARAMETERS / IMPORTS =====================\n",
    "from pathlib import Path\n",
    "import sys, subprocess, numpy as np, pandas as pd, joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Project config\n",
    "PROJ_ROOT = Path(\"../\").resolve()\n",
    "SRC_DIR   = PROJ_ROOT / \"src\"\n",
    "if str(PROJ_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJ_ROOT))\n",
    "\n",
    "from src.config import PATHS, CFG, print_summary\n",
    "print_summary()\n",
    "\n",
    "# Dims (fallbacks if CFG unset)\n",
    "MAP_DIM     = CFG.MAP_DIM or 165\n",
    "PROMPT_DIM  = CFG.PROMPT_DIM or 512\n",
    "FUSED_DIM   = CFG.FUSED_DIM or (MAP_DIM + PROMPT_DIM)\n",
    "BATCH_SIZE  = CFG.BATCH_SIZE\n",
    "\n",
    "# Clean outputs for a fresh run\n",
    "PATHS.clean_outputs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a2350e",
   "metadata": {},
   "source": [
    "üìö 1) Build Prompt Embeddings (USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed0df45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:21:55.572701Z",
     "start_time": "2025-10-27T11:21:55.570071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMD: /opt/anaconda3/envs/thesis/bin/python -m src.mapvec.prompts.prompt_embeddings --input /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/prompts.csv --model dan --l2 --out_dir /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out -v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:44:25 | DEBUG | FILE_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/src/mapvec/prompts\n",
      "17:44:25 | DEBUG | PROJECT_ROOT=/Users/amirdonyadide/Documents/GitHub/Thesis\n",
      "17:44:25 | DEBUG | DEFAULT_DATA_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
      "17:44:25 | INFO | DATA_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
      "17:44:25 | INFO | INPUT=/Users/amirdonyadide/Documents/GitHub/Thesis/data/input/prompts.csv\n",
      "17:44:25 | INFO | OUT_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "17:44:25 | INFO | Reading CSV: /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/prompts.csv\n",
      "17:44:25 | INFO | Loaded 500 prompts (id_col=prompt_id). Sample IDs: p001, p002, p003‚Ä¶\n",
      "17:44:25 | INFO | Using local USE-dan at /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/model_dan\n",
      "17:44:25 | INFO | Loading USE-dan from local path: /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/model_dan ‚Ä¶\n",
      "17:44:28 | INFO | Fingerprint not found. Saved model loading will continue.\n",
      "17:44:28 | INFO | path_and_singleprint metric could not be logged. Saved model loading will continue.\n",
      "17:44:28 | INFO | Model loaded in 3.51s\n",
      "17:44:28 | INFO | Embedding 500 prompts (batch_size=512, l2=True)‚Ä¶\n",
      "17:44:29 | DEBUG |   embedded rows [1:500)\n",
      "17:44:29 | INFO | Done embedding in 0.14s (dim=512).\n",
      "17:44:29 | INFO | Writing outputs to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "17:44:29 | INFO |   saved prompts_embeddings.npz (shape=(500, 512))\n",
      "17:44:29 | INFO |   saved prompts.parquet (rows=500)\n",
      "17:44:29 | INFO |   saved meta.json\n",
      "17:44:29 | INFO | All done ‚úÖ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prompt embeddings completed.\n"
     ]
    }
   ],
   "source": [
    "# === PROMPT EMBEDDINGS ===\n",
    "cmd = [\n",
    "    sys.executable, \"-m\", \"src.mapvec.prompts.prompt_embeddings\",\n",
    "    \"--input\",    str(PATHS.PROMPTS_CSV),\n",
    "    \"--model\",    str(CFG.USE_MODEL),\n",
    "    \"--l2\",\n",
    "    \"--out_dir\",  str(PATHS.PROMPT_OUT),\n",
    "    \"-v\",\n",
    "]\n",
    "print(\"CMD:\", \" \".join(cmd))\n",
    "res = subprocess.run(cmd, cwd=str(PATHS.PROJ_ROOT))\n",
    "if res.returncode != 0:\n",
    "    raise SystemExit(\"Prompt embedding step failed.\")\n",
    "print(\"‚úÖ Prompt embeddings completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c0b51",
   "metadata": {},
   "source": [
    "üó∫Ô∏è 2) Build Map Embeddings (geometric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ca0c3d8b71fc70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:26:59.687350Z",
     "start_time": "2025-10-27T11:26:19.901557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMD: /opt/anaconda3/envs/thesis/bin/python -m src.mapvec.maps.map_embeddings --root /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/samples/pairs --pattern *_input.geojson --out_dir /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out --norm fixed --norm-wh 400x400 -v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:44:31 | DEBUG | PROJECT_ROOT=/Users/amirdonyadide/Documents/GitHub/Thesis\n",
      "17:44:31 | DEBUG | DATA_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
      "17:44:31 | INFO | Scanning /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/samples/pairs (pattern=*_input.geojson)‚Ä¶\n",
      "17:44:31 | INFO | First pass: counting polygons to normalize poly_count‚Ä¶\n",
      "17:44:36 | INFO | Max polygons across dataset: 789\n",
      "17:44:38 | INFO | OK  map_id=0073  -> vector[165]\n",
      "17:44:39 | INFO | OK  map_id=0080  -> vector[165]\n",
      "17:44:40 | INFO | OK  map_id=0093  -> vector[165]\n",
      "17:44:43 | INFO | OK  map_id=0122  -> vector[165]\n",
      "17:44:44 | INFO | OK  map_id=0123  -> vector[165]\n",
      "17:44:45 | INFO | OK  map_id=0127  -> vector[165]\n",
      "17:44:46 | INFO | OK  map_id=0158  -> vector[165]\n",
      "17:44:48 | INFO | OK  map_id=0159  -> vector[165]\n",
      "17:44:49 | INFO | OK  map_id=0160  -> vector[165]\n",
      "17:44:50 | INFO | OK  map_id=0165  -> vector[165]\n",
      "17:44:51 | INFO | OK  map_id=0167  -> vector[165]\n",
      "17:44:52 | INFO | OK  map_id=0168  -> vector[165]\n",
      "17:44:53 | INFO | OK  map_id=0171  -> vector[165]\n",
      "17:44:56 | INFO | OK  map_id=0208  -> vector[165]\n",
      "17:45:00 | INFO | OK  map_id=0209  -> vector[165]\n",
      "17:45:00 | INFO | OK  map_id=0215  -> vector[165]\n",
      "17:45:02 | INFO | OK  map_id=0240  -> vector[165]\n",
      "17:45:03 | INFO | OK  map_id=0256  -> vector[165]\n",
      "17:45:03 | INFO | OK  map_id=0257  -> vector[165]\n",
      "17:45:04 | INFO | OK  map_id=0262  -> vector[165]\n",
      "17:45:06 | INFO | OK  map_id=0285  -> vector[165]\n",
      "17:45:08 | INFO | OK  map_id=0286  -> vector[165]\n",
      "17:45:11 | INFO | OK  map_id=0288  -> vector[165]\n",
      "17:45:14 | INFO | OK  map_id=0289  -> vector[165]\n",
      "17:45:15 | INFO | OK  map_id=0313  -> vector[165]\n",
      "17:45:16 | INFO | OK  map_id=0341  -> vector[165]\n",
      "17:45:17 | INFO | OK  map_id=0362  -> vector[165]\n",
      "17:45:19 | INFO | OK  map_id=0363  -> vector[165]\n",
      "17:45:20 | INFO | OK  map_id=0364  -> vector[165]\n",
      "17:45:21 | INFO | OK  map_id=0379  -> vector[165]\n",
      "17:45:22 | INFO | OK  map_id=0389  -> vector[165]\n",
      "17:45:25 | INFO | OK  map_id=0390  -> vector[165]\n",
      "17:45:26 | INFO | OK  map_id=0409  -> vector[165]\n",
      "17:45:27 | INFO | OK  map_id=0410  -> vector[165]\n",
      "17:45:29 | INFO | OK  map_id=0412  -> vector[165]\n",
      "17:45:30 | INFO | OK  map_id=0413  -> vector[165]\n",
      "17:45:31 | INFO | OK  map_id=0414  -> vector[165]\n",
      "17:45:32 | INFO | OK  map_id=0417  -> vector[165]\n",
      "17:45:33 | INFO | OK  map_id=0421  -> vector[165]\n",
      "17:45:34 | INFO | OK  map_id=0426  -> vector[165]\n",
      "17:45:35 | INFO | OK  map_id=0427  -> vector[165]\n",
      "17:45:36 | INFO | OK  map_id=0432  -> vector[165]\n",
      "17:45:37 | INFO | OK  map_id=0433  -> vector[165]\n",
      "17:45:39 | INFO | OK  map_id=0437  -> vector[165]\n",
      "17:45:41 | INFO | OK  map_id=0438  -> vector[165]\n",
      "17:45:42 | INFO | OK  map_id=0439  -> vector[165]\n",
      "17:45:44 | INFO | OK  map_id=0454  -> vector[165]\n",
      "17:45:45 | INFO | OK  map_id=0458  -> vector[165]\n",
      "17:45:47 | INFO | OK  map_id=0459  -> vector[165]\n",
      "17:45:48 | INFO | OK  map_id=0460  -> vector[165]\n",
      "17:45:50 | INFO | OK  map_id=0466  -> vector[165]\n",
      "17:45:51 | INFO | OK  map_id=0469  -> vector[165]\n",
      "17:45:53 | INFO | OK  map_id=0471  -> vector[165]\n",
      "17:45:54 | INFO | OK  map_id=0472  -> vector[165]\n",
      "17:45:57 | INFO | OK  map_id=0474  -> vector[165]\n",
      "17:45:59 | INFO | OK  map_id=0475  -> vector[165]\n",
      "17:46:00 | INFO | OK  map_id=0479  -> vector[165]\n",
      "17:46:01 | INFO | OK  map_id=0480  -> vector[165]\n",
      "17:46:02 | INFO | OK  map_id=0481  -> vector[165]\n",
      "17:46:03 | INFO | OK  map_id=0482  -> vector[165]\n",
      "17:46:04 | INFO | OK  map_id=0508  -> vector[165]\n",
      "17:46:05 | INFO | OK  map_id=0509  -> vector[165]\n",
      "17:46:06 | INFO | OK  map_id=0518  -> vector[165]\n",
      "17:46:13 | INFO | OK  map_id=0520  -> vector[165]\n",
      "17:46:14 | INFO | OK  map_id=0521  -> vector[165]\n",
      "17:46:15 | INFO | OK  map_id=0523  -> vector[165]\n",
      "17:46:17 | INFO | OK  map_id=0527  -> vector[165]\n",
      "17:46:19 | INFO | OK  map_id=0528  -> vector[165]\n",
      "17:46:20 | INFO | OK  map_id=0529  -> vector[165]\n",
      "17:46:22 | INFO | OK  map_id=0530  -> vector[165]\n",
      "17:46:22 | INFO | OK  map_id=0553  -> vector[165]\n",
      "17:46:23 | INFO | OK  map_id=0557  -> vector[165]\n",
      "17:46:24 | INFO | OK  map_id=0575  -> vector[165]\n",
      "17:46:25 | INFO | OK  map_id=0576  -> vector[165]\n",
      "17:46:27 | INFO | OK  map_id=0594  -> vector[165]\n",
      "17:46:28 | INFO | OK  map_id=0595  -> vector[165]\n",
      "17:46:29 | INFO | OK  map_id=0600  -> vector[165]\n",
      "17:46:31 | INFO | OK  map_id=0605  -> vector[165]\n",
      "17:46:32 | INFO | OK  map_id=0606  -> vector[165]\n",
      "17:46:33 | INFO | OK  map_id=0608  -> vector[165]\n",
      "17:46:35 | INFO | OK  map_id=0609  -> vector[165]\n",
      "17:46:36 | INFO | OK  map_id=0611  -> vector[165]\n",
      "17:46:38 | INFO | OK  map_id=0614  -> vector[165]\n",
      "17:46:39 | INFO | OK  map_id=0615  -> vector[165]\n",
      "17:46:40 | INFO | OK  map_id=0618  -> vector[165]\n",
      "17:46:41 | INFO | OK  map_id=0623  -> vector[165]\n",
      "17:46:42 | INFO | OK  map_id=0624  -> vector[165]\n",
      "17:46:43 | INFO | OK  map_id=0645  -> vector[165]\n",
      "17:46:44 | INFO | OK  map_id=0646  -> vector[165]\n",
      "17:46:45 | INFO | OK  map_id=0655  -> vector[165]\n",
      "17:46:46 | INFO | OK  map_id=0656  -> vector[165]\n",
      "17:46:47 | INFO | OK  map_id=0657  -> vector[165]\n",
      "17:46:49 | INFO | OK  map_id=0658  -> vector[165]\n",
      "17:46:50 | INFO | OK  map_id=0659  -> vector[165]\n",
      "17:46:51 | INFO | OK  map_id=0667  -> vector[165]\n",
      "17:46:52 | INFO | OK  map_id=0672  -> vector[165]\n",
      "17:46:53 | INFO | OK  map_id=0699  -> vector[165]\n",
      "17:46:55 | INFO | OK  map_id=0700  -> vector[165]\n",
      "17:46:57 | INFO | OK  map_id=0701  -> vector[165]\n",
      "17:46:58 | INFO | OK  map_id=0706  -> vector[165]\n",
      "17:46:59 | INFO | OK  map_id=0707  -> vector[165]\n",
      "17:47:00 | INFO | OK  map_id=0715  -> vector[165]\n",
      "17:47:01 | INFO | OK  map_id=0721  -> vector[165]\n",
      "17:47:02 | INFO | OK  map_id=0747  -> vector[165]\n",
      "17:47:04 | INFO | OK  map_id=0748  -> vector[165]\n",
      "17:47:05 | INFO | OK  map_id=0749  -> vector[165]\n",
      "17:47:07 | INFO | OK  map_id=0755  -> vector[165]\n",
      "17:47:07 | INFO | OK  map_id=0758  -> vector[165]\n",
      "17:47:08 | INFO | OK  map_id=0759  -> vector[165]\n",
      "17:47:09 | INFO | OK  map_id=0762  -> vector[165]\n",
      "17:47:10 | INFO | OK  map_id=0770  -> vector[165]\n",
      "17:47:12 | INFO | OK  map_id=0804  -> vector[165]\n",
      "17:47:13 | INFO | OK  map_id=0807  -> vector[165]\n",
      "17:47:14 | INFO | OK  map_id=0808  -> vector[165]\n",
      "17:47:16 | INFO | OK  map_id=0809  -> vector[165]\n",
      "17:47:18 | INFO | OK  map_id=0819  -> vector[165]\n",
      "17:47:19 | INFO | OK  map_id=0848  -> vector[165]\n",
      "17:47:21 | INFO | OK  map_id=0853  -> vector[165]\n",
      "17:47:23 | INFO | OK  map_id=0854  -> vector[165]\n",
      "17:47:24 | INFO | OK  map_id=0856  -> vector[165]\n",
      "17:47:25 | INFO | OK  map_id=0857  -> vector[165]\n",
      "17:47:31 | INFO | OK  map_id=0858  -> vector[165]\n",
      "17:47:32 | INFO | OK  map_id=0859  -> vector[165]\n",
      "17:47:34 | INFO | OK  map_id=0867  -> vector[165]\n",
      "17:47:37 | INFO | OK  map_id=0868  -> vector[165]\n",
      "17:47:38 | INFO | OK  map_id=0869  -> vector[165]\n",
      "17:47:39 | INFO | OK  map_id=0901  -> vector[165]\n",
      "17:47:41 | INFO | OK  map_id=0903  -> vector[165]\n",
      "17:47:42 | INFO | OK  map_id=0904  -> vector[165]\n",
      "17:47:43 | INFO | OK  map_id=0905  -> vector[165]\n",
      "17:47:48 | INFO | OK  map_id=0906  -> vector[165]\n",
      "17:47:49 | INFO | OK  map_id=0907  -> vector[165]\n",
      "17:47:51 | INFO | OK  map_id=0908  -> vector[165]\n",
      "17:47:54 | INFO | OK  map_id=0917  -> vector[165]\n",
      "17:47:56 | INFO | OK  map_id=0918  -> vector[165]\n",
      "17:47:58 | INFO | OK  map_id=0926  -> vector[165]\n",
      "17:47:59 | INFO | OK  map_id=0947  -> vector[165]\n",
      "17:48:02 | INFO | OK  map_id=0948  -> vector[165]\n",
      "17:48:04 | INFO | OK  map_id=0949  -> vector[165]\n",
      "17:48:05 | INFO | OK  map_id=0950  -> vector[165]\n",
      "17:48:06 | INFO | OK  map_id=0951  -> vector[165]\n",
      "17:48:07 | INFO | OK  map_id=0952  -> vector[165]\n",
      "17:48:10 | INFO | OK  map_id=0966  -> vector[165]\n",
      "17:48:13 | INFO | OK  map_id=0967  -> vector[165]\n",
      "17:48:14 | INFO | OK  map_id=0970  -> vector[165]\n",
      "17:48:15 | INFO | OK  map_id=0971  -> vector[165]\n",
      "17:48:16 | INFO | OK  map_id=0974  -> vector[165]\n",
      "17:48:18 | INFO | OK  map_id=0975  -> vector[165]\n",
      "17:48:19 | INFO | OK  map_id=0976  -> vector[165]\n",
      "17:48:20 | INFO | OK  map_id=0994  -> vector[165]\n",
      "17:48:21 | INFO | OK  map_id=0995  -> vector[165]\n",
      "17:48:23 | INFO | OK  map_id=0997  -> vector[165]\n",
      "17:48:25 | INFO | OK  map_id=0998  -> vector[165]\n",
      "17:48:26 | INFO | OK  map_id=1019  -> vector[165]\n",
      "17:48:30 | INFO | OK  map_id=1020  -> vector[165]\n",
      "17:48:31 | INFO | OK  map_id=1052  -> vector[165]\n",
      "17:48:33 | INFO | OK  map_id=1053  -> vector[165]\n",
      "17:48:35 | INFO | OK  map_id=1054  -> vector[165]\n",
      "17:48:36 | INFO | OK  map_id=1055  -> vector[165]\n",
      "17:48:37 | INFO | OK  map_id=1056  -> vector[165]\n",
      "17:48:39 | INFO | OK  map_id=1057  -> vector[165]\n",
      "17:48:41 | INFO | OK  map_id=1069  -> vector[165]\n",
      "17:48:42 | INFO | OK  map_id=1070  -> vector[165]\n",
      "17:48:43 | INFO | OK  map_id=1090  -> vector[165]\n",
      "17:48:44 | INFO | OK  map_id=1091  -> vector[165]\n",
      "17:48:45 | INFO | OK  map_id=1092  -> vector[165]\n",
      "17:48:46 | INFO | OK  map_id=1100  -> vector[165]\n",
      "17:48:47 | INFO | OK  map_id=1103  -> vector[165]\n",
      "17:48:48 | INFO | OK  map_id=1105  -> vector[165]\n",
      "17:48:50 | INFO | OK  map_id=1106  -> vector[165]\n",
      "17:48:51 | INFO | OK  map_id=1118  -> vector[165]\n",
      "17:48:53 | INFO | OK  map_id=1119  -> vector[165]\n",
      "17:48:55 | INFO | OK  map_id=1120  -> vector[165]\n",
      "17:48:57 | INFO | OK  map_id=1139  -> vector[165]\n",
      "17:48:59 | INFO | OK  map_id=1140  -> vector[165]\n",
      "17:49:00 | INFO | OK  map_id=1148  -> vector[165]\n",
      "17:49:01 | INFO | OK  map_id=1155  -> vector[165]\n",
      "17:49:03 | INFO | OK  map_id=1157  -> vector[165]\n",
      "17:49:05 | INFO | OK  map_id=1168  -> vector[165]\n",
      "17:49:07 | INFO | OK  map_id=1169  -> vector[165]\n",
      "17:49:08 | INFO | OK  map_id=1170  -> vector[165]\n",
      "17:49:10 | INFO | OK  map_id=1197  -> vector[165]\n",
      "17:49:11 | INFO | OK  map_id=1198  -> vector[165]\n",
      "17:49:13 | INFO | OK  map_id=1202  -> vector[165]\n",
      "17:49:16 | INFO | OK  map_id=1203  -> vector[165]\n",
      "17:49:18 | INFO | OK  map_id=1204  -> vector[165]\n",
      "17:49:18 | INFO | OK  map_id=1217  -> vector[165]\n",
      "17:49:21 | INFO | OK  map_id=1218  -> vector[165]\n",
      "17:49:22 | INFO | OK  map_id=1219  -> vector[165]\n",
      "17:49:23 | INFO | OK  map_id=1221  -> vector[165]\n",
      "17:49:25 | INFO | OK  map_id=1222  -> vector[165]\n",
      "17:49:26 | INFO | OK  map_id=1231  -> vector[165]\n",
      "17:49:27 | INFO | OK  map_id=1233  -> vector[165]\n",
      "17:49:29 | INFO | OK  map_id=1234  -> vector[165]\n",
      "17:49:30 | INFO | OK  map_id=1261  -> vector[165]\n",
      "17:49:31 | INFO | OK  map_id=1269  -> vector[165]\n",
      "17:49:36 | INFO | OK  map_id=1270  -> vector[165]\n",
      "17:49:37 | INFO | OK  map_id=1271  -> vector[165]\n",
      "17:49:38 | INFO | OK  map_id=1276  -> vector[165]\n",
      "17:49:39 | INFO | OK  map_id=1277  -> vector[165]\n",
      "17:49:40 | INFO | OK  map_id=1283  -> vector[165]\n",
      "17:49:41 | INFO | OK  map_id=1284  -> vector[165]\n",
      "17:49:42 | INFO | OK  map_id=1285  -> vector[165]\n",
      "17:49:44 | INFO | OK  map_id=1295  -> vector[165]\n",
      "17:49:47 | INFO | OK  map_id=1296  -> vector[165]\n",
      "17:49:48 | INFO | OK  map_id=1297  -> vector[165]\n",
      "17:49:50 | INFO | OK  map_id=1303  -> vector[165]\n",
      "17:49:52 | INFO | OK  map_id=1304  -> vector[165]\n",
      "17:49:53 | INFO | OK  map_id=1310  -> vector[165]\n",
      "17:49:55 | INFO | OK  map_id=1319  -> vector[165]\n",
      "17:49:57 | INFO | OK  map_id=1333  -> vector[165]\n",
      "17:49:58 | INFO | OK  map_id=1334  -> vector[165]\n",
      "17:50:00 | INFO | OK  map_id=1344  -> vector[165]\n",
      "17:50:01 | INFO | OK  map_id=1349  -> vector[165]\n",
      "17:50:03 | INFO | OK  map_id=1364  -> vector[165]\n",
      "17:50:06 | INFO | OK  map_id=1365  -> vector[165]\n",
      "17:50:09 | INFO | OK  map_id=1366  -> vector[165]\n",
      "17:50:13 | INFO | OK  map_id=1367  -> vector[165]\n",
      "17:50:14 | INFO | OK  map_id=1368  -> vector[165]\n",
      "17:50:15 | INFO | OK  map_id=1369  -> vector[165]\n",
      "17:50:16 | INFO | OK  map_id=1377  -> vector[165]\n",
      "17:50:17 | INFO | OK  map_id=1378  -> vector[165]\n",
      "17:50:18 | INFO | OK  map_id=1385  -> vector[165]\n",
      "17:50:19 | INFO | OK  map_id=1386  -> vector[165]\n",
      "17:50:21 | INFO | OK  map_id=1399  -> vector[165]\n",
      "17:50:22 | INFO | OK  map_id=1401  -> vector[165]\n",
      "17:50:23 | INFO | OK  map_id=1408  -> vector[165]\n",
      "17:50:26 | INFO | OK  map_id=1409  -> vector[165]\n",
      "17:50:29 | INFO | OK  map_id=1410  -> vector[165]\n",
      "17:50:30 | INFO | OK  map_id=1413  -> vector[165]\n",
      "17:50:32 | INFO | OK  map_id=1414  -> vector[165]\n",
      "17:50:34 | INFO | OK  map_id=1415  -> vector[165]\n",
      "17:50:36 | INFO | OK  map_id=1416  -> vector[165]\n",
      "17:50:37 | INFO | OK  map_id=1417  -> vector[165]\n",
      "17:50:39 | INFO | OK  map_id=1418  -> vector[165]\n",
      "17:50:40 | INFO | OK  map_id=1434  -> vector[165]\n",
      "17:50:41 | INFO | OK  map_id=1438  -> vector[165]\n",
      "17:50:43 | INFO | OK  map_id=1439  -> vector[165]\n",
      "17:50:44 | INFO | OK  map_id=1450  -> vector[165]\n",
      "17:50:46 | INFO | OK  map_id=1451  -> vector[165]\n",
      "17:50:49 | INFO | OK  map_id=1458  -> vector[165]\n",
      "17:50:50 | INFO | OK  map_id=1459  -> vector[165]\n",
      "17:50:52 | INFO | OK  map_id=1460  -> vector[165]\n",
      "17:50:53 | INFO | OK  map_id=1465  -> vector[165]\n",
      "17:50:55 | INFO | OK  map_id=1466  -> vector[165]\n",
      "17:50:57 | INFO | OK  map_id=1467  -> vector[165]\n",
      "17:51:00 | INFO | OK  map_id=1473  -> vector[165]\n",
      "17:51:01 | INFO | OK  map_id=1474  -> vector[165]\n",
      "17:51:01 | INFO | OK  map_id=1476  -> vector[165]\n",
      "17:51:03 | INFO | OK  map_id=1479  -> vector[165]\n",
      "17:51:04 | INFO | OK  map_id=1486  -> vector[165]\n",
      "17:51:06 | INFO | OK  map_id=1487  -> vector[165]\n",
      "17:51:08 | INFO | OK  map_id=1496  -> vector[165]\n",
      "17:51:10 | INFO | OK  map_id=1500  -> vector[165]\n",
      "17:51:10 | INFO | OK  map_id=1501  -> vector[165]\n",
      "17:51:12 | INFO | OK  map_id=1507  -> vector[165]\n",
      "17:51:13 | INFO | OK  map_id=1508  -> vector[165]\n",
      "17:51:16 | INFO | OK  map_id=1509  -> vector[165]\n",
      "17:51:17 | INFO | OK  map_id=1514  -> vector[165]\n",
      "17:51:19 | INFO | OK  map_id=1515  -> vector[165]\n",
      "17:51:20 | INFO | OK  map_id=1557  -> vector[165]\n",
      "17:51:21 | INFO | OK  map_id=1563  -> vector[165]\n",
      "17:51:24 | INFO | OK  map_id=1564  -> vector[165]\n",
      "17:51:25 | INFO | OK  map_id=1565  -> vector[165]\n",
      "17:51:26 | INFO | OK  map_id=1570  -> vector[165]\n",
      "17:51:27 | INFO | OK  map_id=1579  -> vector[165]\n",
      "17:51:28 | INFO | OK  map_id=1580  -> vector[165]\n",
      "17:51:29 | INFO | OK  map_id=1583  -> vector[165]\n",
      "17:51:30 | INFO | OK  map_id=1584  -> vector[165]\n",
      "17:51:32 | INFO | OK  map_id=1598  -> vector[165]\n",
      "17:51:34 | INFO | OK  map_id=1613  -> vector[165]\n",
      "17:51:35 | INFO | OK  map_id=1614  -> vector[165]\n",
      "17:51:36 | INFO | OK  map_id=1618  -> vector[165]\n",
      "17:51:37 | INFO | OK  map_id=1619  -> vector[165]\n",
      "17:51:38 | INFO | OK  map_id=1629  -> vector[165]\n",
      "17:51:41 | INFO | OK  map_id=1630  -> vector[165]\n",
      "17:51:41 | INFO | OK  map_id=1631  -> vector[165]\n",
      "17:51:43 | INFO | OK  map_id=1647  -> vector[165]\n",
      "17:51:44 | INFO | OK  map_id=1649  -> vector[165]\n",
      "17:51:45 | INFO | OK  map_id=1650  -> vector[165]\n",
      "17:51:46 | INFO | OK  map_id=1653  -> vector[165]\n",
      "17:51:47 | INFO | OK  map_id=1666  -> vector[165]\n",
      "17:51:48 | INFO | OK  map_id=1667  -> vector[165]\n",
      "17:51:50 | INFO | OK  map_id=1672  -> vector[165]\n",
      "17:51:52 | INFO | OK  map_id=1673  -> vector[165]\n",
      "17:51:53 | INFO | OK  map_id=1679  -> vector[165]\n",
      "17:51:54 | INFO | OK  map_id=1691  -> vector[165]\n",
      "17:51:56 | INFO | OK  map_id=1696  -> vector[165]\n",
      "17:51:58 | INFO | OK  map_id=1700  -> vector[165]\n",
      "17:52:01 | INFO | OK  map_id=1702  -> vector[165]\n",
      "17:52:04 | INFO | OK  map_id=1703  -> vector[165]\n",
      "17:52:05 | INFO | OK  map_id=1709  -> vector[165]\n",
      "17:52:06 | INFO | OK  map_id=1710  -> vector[165]\n",
      "17:52:08 | INFO | OK  map_id=1748  -> vector[165]\n",
      "17:52:10 | INFO | OK  map_id=1749  -> vector[165]\n",
      "17:52:11 | INFO | OK  map_id=1750  -> vector[165]\n",
      "17:52:13 | INFO | OK  map_id=1751  -> vector[165]\n",
      "17:52:14 | INFO | OK  map_id=1752  -> vector[165]\n",
      "17:52:15 | INFO | OK  map_id=1755  -> vector[165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Map embeddings completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:52:16 | INFO | OK  map_id=1757  -> vector[165]\n",
      "17:52:16 | INFO | Saved 300 vectors (failed=0) to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n"
     ]
    }
   ],
   "source": [
    "# === MAP EMBEDDINGS ===\n",
    "cmd = [\n",
    "    sys.executable, \"-m\", \"src.mapvec.maps.map_embeddings\",\n",
    "    \"--root\", str(PATHS.MAPS_ROOT),\n",
    "    \"--pattern\", PATHS.INPUT_MAPS_PATTERN,\n",
    "    \"--out_dir\", str(PATHS.MAP_OUT),\n",
    "    \"--norm\", \"fixed\",\n",
    "    \"--norm-wh\", \"400x400\",\n",
    "    \"-v\",\n",
    "]\n",
    "print(\"CMD:\", \" \".join(cmd))\n",
    "res = subprocess.run(cmd, cwd=str(PATHS.PROJ_ROOT))\n",
    "if res.returncode != 0:\n",
    "    raise SystemExit(\"Map embedding step failed.\")\n",
    "print(\"‚úÖ Map embeddings completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd186319e89f445",
   "metadata": {},
   "source": [
    "üîó 3) Concatenate (pairs ‚Üí fused rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa2b07a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMD: /opt/anaconda3/envs/thesis/bin/python -m src.mapvec.concat.concat_embeddings --pairs /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/pairs.csv --map_npz /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/maps_embeddings.npz --prompt_npz /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/prompts_embeddings.npz --out_dir /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out --drop_dupes\n",
      "‚úÖ Concatenation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:52:23 | INFO | Map  embeddings: (300, 165) from /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/maps_embeddings.npz\n",
      "17:52:23 | INFO | Prompt embeddings: (500, 512) from /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/prompts_embeddings.npz\n",
      "17:52:23 | INFO | X shape = (450, 677)  (map_dim=165, prompt_dim=512)\n",
      "17:52:23 | INFO | Saved to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out in 0.02s\n"
     ]
    }
   ],
   "source": [
    "# === CONCATENATION ===\n",
    "cmd = [\n",
    "    sys.executable, \"-m\", \"src.mapvec.concat.concat_embeddings\",\n",
    "    \"--pairs\",      str(PATHS.PAIRS_CSV),\n",
    "    \"--map_npz\",    str(PATHS.MAP_OUT / \"maps_embeddings.npz\"),\n",
    "    \"--prompt_npz\", str(PATHS.PROMPT_OUT / \"prompts_embeddings.npz\"),\n",
    "    \"--out_dir\",    str(PATHS.TRAIN_OUT),\n",
    "    \"--drop_dupes\",\n",
    "    # \"--l2-prompt\",     # safety net if you want L2 here as well\n",
    "    # \"--fail_on_missing\"\n",
    "    # \"--save-blocks\"\n",
    "]\n",
    "print(\"CMD:\", \" \".join(cmd))\n",
    "res = subprocess.run(cmd, cwd=str(PATHS.PROJ_ROOT))\n",
    "if res.returncode != 0:\n",
    "    raise SystemExit(\"Concatenation step failed.\")\n",
    "print(\"‚úÖ Concatenation completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5142d4b68c273d37",
   "metadata": {},
   "source": [
    "üì• 4) Load & Basic Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a494fd27dfe7681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded X: (450, 677), pairs: (450, 4)\n",
      "After cleaning: X=(450, 677), df=(450, 4), ops=['aggregate', 'displace', 'select', 'simplify']\n"
     ]
    }
   ],
   "source": [
    "# === LOAD FUSED DATA ===\n",
    "X = np.load(PATHS.TRAIN_OUT / \"X_concat.npy\")\n",
    "pairs_df = pd.read_parquet(PATHS.TRAIN_OUT / \"train_pairs.parquet\")\n",
    "print(f\"Loaded X: {X.shape}, pairs: {pairs_df.shape}\")\n",
    "\n",
    "OP_COL = \"operator\"\n",
    "PARAM_COLS = [\"param\"]\n",
    "\n",
    "df = pairs_df.copy()\n",
    "df[OP_COL] = df[OP_COL].astype(str).str.strip().str.lower()\n",
    "\n",
    "mask = df[OP_COL].notna()\n",
    "for c in PARAM_COLS:\n",
    "    mask &= df[c].notna()\n",
    "\n",
    "X  = X[mask.values].astype(\"float32\", copy=False)\n",
    "df = df.loc[mask].reset_index(drop=True)\n",
    "print(f\"After cleaning: X={X.shape}, df={df.shape}, ops={sorted(df[OP_COL].unique())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997cab7",
   "metadata": {},
   "source": [
    "‚úÇÔ∏è 5) Split & Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bc0897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (315, 677), Val: (67, 677), Test: (68, 677)\n"
     ]
    }
   ],
   "source": [
    "# === SPLIT ===\n",
    "FIXED_CLASSES = [\"simplify\", \"select\", \"aggregate\", \"displace\"]\n",
    "\n",
    "X_train, X_temp, df_train, df_temp = train_test_split(\n",
    "    X, df,\n",
    "    test_size=CFG.VAL_RATIO + CFG.TEST_RATIO,\n",
    "    random_state=CFG.SEED,\n",
    "    shuffle=True,\n",
    "    stratify=df[OP_COL] if df[OP_COL].nunique() > 1 else None\n",
    ")\n",
    "rel_test = CFG.TEST_RATIO / (CFG.VAL_RATIO + CFG.TEST_RATIO)\n",
    "X_val, X_test, df_val, df_test = train_test_split(\n",
    "    X_temp, df_temp,\n",
    "    test_size=rel_test,\n",
    "    random_state=CFG.SEED,\n",
    "    shuffle=True,\n",
    "    stratify=df_temp[OP_COL] if df_temp[OP_COL].nunique() > 1 else None\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# === TARGETS ===\n",
    "le = LabelEncoder().fit(FIXED_CLASSES)\n",
    "y_train_cls = le.transform(df_train[OP_COL])\n",
    "y_val_cls   = le.transform(df_val[OP_COL])\n",
    "y_test_cls  = le.transform(df_test[OP_COL])\n",
    "\n",
    "y_train_reg = df_train[PARAM_COLS].to_numpy(dtype=\"float32\")\n",
    "y_val_reg   = df_val[PARAM_COLS].to_numpy(dtype=\"float32\")\n",
    "y_test_reg  = df_test[PARAM_COLS].to_numpy(dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187473d",
   "metadata": {},
   "source": [
    "üßº 6) Modality-Aware Preprocessing (map only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41942b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modality-aware preprocessing complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/preproc.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === MODALITY-AWARE PREPROCESSING ===\n",
    "MAP_DIM     = CFG.MAP_DIM or 165       # set to true map dim\n",
    "PROMPT_DIM  = CFG.PROMPT_DIM or 512\n",
    "\n",
    "def split_blocks(X):\n",
    "    X_map    = X[:, :MAP_DIM].astype(np.float32, copy=True)\n",
    "    X_prompt = X[:, MAP_DIM:MAP_DIM+PROMPT_DIM].astype(np.float32, copy=True)\n",
    "    return X_map, X_prompt\n",
    "\n",
    "def l2_normalize_rows(A, eps=1e-12):\n",
    "    nrm = np.sqrt((A * A).sum(axis=1, keepdims=True))\n",
    "    return A / np.maximum(nrm, eps)\n",
    "\n",
    "# split\n",
    "Xm_tr, Xp_tr = split_blocks(X_train)\n",
    "Xm_va, Xp_va = split_blocks(X_val)\n",
    "Xm_te, Xp_te = split_blocks(X_test)\n",
    "\n",
    "# prompts: L2 only\n",
    "Xp_tr = l2_normalize_rows(Xp_tr)\n",
    "Xp_va = l2_normalize_rows(Xp_va)\n",
    "Xp_te = l2_normalize_rows(Xp_te)\n",
    "\n",
    "# maps: inf‚ÜíNaN\n",
    "for A in (Xm_tr, Xm_va, Xm_te):\n",
    "    A[~np.isfinite(A)] = np.nan\n",
    "\n",
    "# impute (train)\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "Xm_tr_imp = imp.fit_transform(Xm_tr)\n",
    "Xm_va_imp = imp.transform(Xm_va)\n",
    "Xm_te_imp = imp.transform(Xm_te)\n",
    "\n",
    "# clip (5‚Äì95%) train thresholds\n",
    "q_lo = np.nanpercentile(Xm_tr_imp, 5, axis=0)\n",
    "q_hi = np.nanpercentile(Xm_tr_imp, 95, axis=0)\n",
    "def clip_to_q(A, lo, hi): return np.clip(A, lo, hi)\n",
    "\n",
    "Xm_tr_imp = clip_to_q(Xm_tr_imp, q_lo, q_hi)\n",
    "Xm_va_imp = clip_to_q(Xm_va_imp, q_lo, q_hi)\n",
    "Xm_te_imp = clip_to_q(Xm_te_imp, q_lo, q_hi)\n",
    "\n",
    "# drop zero-variance cols on train\n",
    "stds = np.nanstd(Xm_tr_imp, axis=0)\n",
    "keep_mask = stds > 1e-12\n",
    "\n",
    "# scale kept columns (train fit)\n",
    "scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5, 95))\n",
    "Xm_tr_kept = scaler.fit_transform(Xm_tr_imp[:, keep_mask])\n",
    "Xm_va_kept = scaler.transform(Xm_va_imp[:, keep_mask])\n",
    "Xm_te_kept = scaler.transform(Xm_te_imp[:, keep_mask])\n",
    "\n",
    "# rebuild full map dim (dropped cols = 0)\n",
    "Xm_tr_s = np.zeros_like(Xm_tr_imp, dtype=np.float32)\n",
    "Xm_va_s = np.zeros_like(Xm_va_imp, dtype=np.float32)\n",
    "Xm_te_s = np.zeros_like(Xm_te_imp, dtype=np.float32)\n",
    "Xm_tr_s[:, keep_mask] = Xm_tr_kept.astype(np.float32)\n",
    "Xm_va_s[:, keep_mask] = Xm_va_kept.astype(np.float32)\n",
    "Xm_te_s[:, keep_mask] = Xm_te_kept.astype(np.float32)\n",
    "\n",
    "# fuse back\n",
    "X_train_s = np.concatenate([Xm_tr_s, Xp_tr], axis=1).astype(np.float32)\n",
    "X_val_s   = np.concatenate([Xm_va_s, Xp_va], axis=1).astype(np.float32)\n",
    "X_test_s  = np.concatenate([Xm_te_s, Xp_te], axis=1).astype(np.float32)\n",
    "\n",
    "assert np.isfinite(X_train_s).all() and np.isfinite(X_val_s).all() and np.isfinite(X_test_s).all(), \"Non-finite after preprocessing.\"\n",
    "print(\"‚úÖ Modality-aware preprocessing complete.\")\n",
    "\n",
    "# save preprocessing bundle\n",
    "joblib.dump({\n",
    "    \"imp\": imp, \"q_lo\": q_lo, \"q_hi\": q_hi,\n",
    "    \"keep_mask\": keep_mask, \"scaler\": scaler,\n",
    "    \"map_dim\": MAP_DIM, \"prompt_dim\": PROMPT_DIM\n",
    "}, PATHS.TRAIN_OUT / \"preproc.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c32c2",
   "metadata": {},
   "source": [
    "‚öñÔ∏è 7) Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0df2b2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {np.str_('aggregate'): np.float64(0.9264705882352942), np.str_('displace'): np.float64(1.0361842105263157), np.str_('select'): np.float64(0.984375), np.str_('simplify'): np.float64(1.0641891891891893)}\n"
     ]
    }
   ],
   "source": [
    "classes  = list(le.classes_)\n",
    "n_classes = len(classes)\n",
    "cls_w    = compute_class_weight(class_weight=\"balanced\",\n",
    "                                classes=np.arange(n_classes),\n",
    "                                y=y_train_cls)\n",
    "sample_w = np.array([cls_w[c] for c in y_train_cls], dtype=\"float32\")\n",
    "print(\"Class weights:\", dict(zip(classes, cls_w)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988f274",
   "metadata": {},
   "source": [
    "üß† 8) Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95133825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = nan\n",
      "Validation score: 0.270091\n",
      "Iteration 2, loss = nan\n",
      "Validation score: 0.270091\n",
      "Iteration 3, loss = nan\n",
      "Validation score: 0.270091\n",
      "Iteration 4, loss = nan\n",
      "Validation score: 0.270091\n",
      "Iteration 5, loss = nan\n",
      "Validation score: 0.270091\n",
      "Iteration 6, loss = nan\n",
      "Validation score: 0.270091\n",
      "Iteration 7, loss = nan\n",
      "Validation score: 0.270091\n",
      "Iteration 8, loss = nan\n",
      "Validation score: 0.270091\n",
      "Iteration 9, loss = nan\n",
      "Validation score: 0.270091\n",
      "Iteration 10, loss = nan\n",
      "Validation score: 0.270091\n",
      "Iteration 11, loss = nan\n",
      "Validation score: 0.270091\n",
      "Iteration 12, loss = nan\n",
      "Validation score: 0.270091\n",
      "Iteration 13, loss = nan\n",
      "Validation score: 0.270091\n",
      "Iteration 14, loss = nan\n",
      "Validation score: 0.270091\n",
      "Iteration 15, loss = nan\n",
      "Validation score: 0.270091\n",
      "Iteration 16, loss = nan\n",
      "Validation score: 0.270091\n",
      "Iteration 17, loss = nan\n",
      "Validation score: 0.270091\n",
      "Validation score did not improve more than tol=0.000100 for 15 consecutive epochs. Stopping.\n",
      "‚ö†Ô∏è Adam crashed (Solver produced non-finite parameter weights. The input data may contain large values and need to be preprocessed.). Falling back to lbfgs (no sample_weight).\n",
      "‚úÖ Training done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_stochastic_optimizers.py:275: RuntimeWarning: overflow encountered in square\n",
      "  self.beta_2 * v + (1 - self.beta_2) * (grad**2)\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:173: RuntimeWarning: invalid value encountered in add\n",
      "  activations[i + 1] += self.intercepts_[i]\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:173: RuntimeWarning: invalid value encountered in add\n",
      "  activations[i + 1] += self.intercepts_[i]\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:602: ConvergenceWarning: lbfgs failed to converge after 0 iteration(s) (status=2):\n",
      "ABNORMAL: \n",
      "\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    alpha=1e-3,\n",
    "    learning_rate_init=3e-4,\n",
    "    batch_size=32,\n",
    "    max_iter=300,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=15,\n",
    "    validation_fraction=0.15,\n",
    "    tol=1e-4,\n",
    "    random_state=CFG.SEED,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    clf.fit(X_train_s, y_train_cls, sample_weight=sample_w)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Adam crashed ({e}). Falling back to lbfgs (no sample_weight).\")\n",
    "    clf = MLPClassifier(\n",
    "        hidden_layer_sizes=(128, 64),\n",
    "        activation=\"relu\",\n",
    "        solver=\"lbfgs\",\n",
    "        alpha=1e-3,\n",
    "        max_iter=500,\n",
    "        tol=1e-4,\n",
    "        random_state=CFG.SEED,\n",
    "        verbose=True\n",
    "    )\n",
    "    clf.fit(X_train_s, y_train_cls)\n",
    "print(\"‚úÖ Training done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b52314",
   "metadata": {},
   "source": [
    "üìä 9) Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be650001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Validation =====\n",
      "\n",
      "VAL:  acc=0.2090  f1_macro=0.1359\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   aggregate      0.237     0.500     0.321        18\n",
      "    displace      0.000     0.000     0.000        16\n",
      "      select      0.179     0.294     0.222        17\n",
      "    simplify      0.000     0.000     0.000        16\n",
      "\n",
      "    accuracy                          0.209        67\n",
      "   macro avg      0.104     0.199     0.136        67\n",
      "weighted avg      0.109     0.209     0.143        67\n",
      "\n",
      "\n",
      "===== Test =====\n",
      "\n",
      "TEST:  acc=0.2500  f1_macro=0.1520\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   aggregate      0.289     0.722     0.413        18\n",
      "    displace      0.000     0.000     0.000        17\n",
      "      select      0.174     0.222     0.195        18\n",
      "    simplify      0.000     0.000     0.000        15\n",
      "\n",
      "    accuracy                          0.250        68\n",
      "   macro avg      0.116     0.236     0.152        68\n",
      "weighted avg      0.123     0.250     0.161        68\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "def eval_split(name, Xs, ys_true):\n",
    "    y_pred = clf.predict(Xs)\n",
    "    acc = accuracy_score(ys_true, y_pred)\n",
    "    f1m = f1_score(ys_true, y_pred, average=\"macro\")\n",
    "    print(f\"\\n{name}:  acc={acc:.4f}  f1_macro={f1m:.4f}\")\n",
    "    print(classification_report(ys_true, y_pred, target_names=classes, digits=3))\n",
    "    return y_pred\n",
    "\n",
    "print(\"\\n===== Validation =====\")\n",
    "_ = eval_split(\"VAL\", X_val_s, y_val_cls)\n",
    "\n",
    "print(\"\\n===== Test =====\")\n",
    "y_test_pred = eval_split(\"TEST\", X_test_s, y_test_cls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ff8286",
   "metadata": {},
   "source": [
    "üîç 10) Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "490ab6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- X_train_s ---\n",
      "shape: (315, 677) dtype: float32\n",
      "finite: True\n",
      "min/max: -1.024782 1.0188221\n",
      "mean/std: 0.0036010856 0.17772073\n",
      "all-NaN cols: 0 zero-variance cols: 14\n",
      "\n",
      "--- X_val_s ---\n",
      "shape: (67, 677) dtype: float32\n",
      "finite: True\n",
      "min/max: -1.024782 1.0188221\n",
      "mean/std: 0.005705521 0.17796075\n",
      "all-NaN cols: 0 zero-variance cols: 14\n",
      "\n",
      "--- X_test_s ---\n",
      "shape: (68, 677) dtype: float32\n",
      "finite: True\n",
      "min/max: -1.024782 1.0188221\n",
      "mean/std: 0.0046883137 0.17945407\n",
      "all-NaN cols: 0 zero-variance cols: 14\n",
      "classes present in train: ['aggregate', 'displace', 'select', 'simplify']\n"
     ]
    }
   ],
   "source": [
    "def check_matrix(name, X):\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(\"shape:\", X.shape, \"dtype:\", X.dtype)\n",
    "    print(\"finite:\", np.isfinite(X).all())\n",
    "    print(\"min/max:\", np.nanmin(X), np.nanmax(X))\n",
    "    print(\"mean/std:\", np.nanmean(X), np.nanstd(X))\n",
    "    col_nan = np.isnan(X).all(axis=0).sum()\n",
    "    col_zero_var = (np.nanstd(X, axis=0) == 0).sum()\n",
    "    print(\"all-NaN cols:\", col_nan, \"zero-variance cols:\", col_zero_var)\n",
    "\n",
    "check_matrix(\"X_train_s\", X_train_s)\n",
    "check_matrix(\"X_val_s\",   X_val_s)\n",
    "check_matrix(\"X_test_s\",  X_test_s)\n",
    "print(\"classes present in train:\", sorted(set(df_train[OP_COL])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1075c4",
   "metadata": {},
   "source": [
    "üíæ 11) Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ee1eb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved model + label encoder to: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(clf, PATHS.TRAIN_OUT / \"mlp_classifier.joblib\")\n",
    "joblib.dump(le,  PATHS.TRAIN_OUT / \"label_encoder.joblib\")\n",
    "print(\"‚úÖ Saved model + label encoder to:\", PATHS.TRAIN_OUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128cea6d",
   "metadata": {},
   "source": [
    "üöÄ 12) Inference Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7811fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline ready. Use apply_preproc_and_predict(...) for inference.\n"
     ]
    }
   ],
   "source": [
    "def apply_preproc_and_predict(X_concat: np.ndarray,\n",
    "                              preproc_path=PATHS.TRAIN_OUT / \"preproc.joblib\",\n",
    "                              model_path=PATHS.TRAIN_OUT / \"mlp_classifier.joblib\",\n",
    "                              le_path=PATHS.TRAIN_OUT / \"label_encoder.joblib\"):\n",
    "    \"\"\"\n",
    "    X_concat: (N, MAP_DIM + PROMPT_DIM). Prompts can be raw; they will be L2 here.\n",
    "    Returns: (labels_str, labels_idx)\n",
    "    \"\"\"\n",
    "    bundle = joblib.load(preproc_path)\n",
    "    imp, q_lo, q_hi = bundle[\"imp\"], bundle[\"q_lo\"], bundle[\"q_hi\"]\n",
    "    keep_mask, scaler = bundle[\"keep_mask\"], bundle[\"scaler\"]\n",
    "    map_dim, prompt_dim = bundle[\"map_dim\"], bundle[\"prompt_dim\"]\n",
    "\n",
    "    def l2_rows(A, eps=1e-12):\n",
    "        n = np.sqrt((A * A).sum(axis=1, keepdims=True))\n",
    "        return A / np.maximum(n, eps)\n",
    "\n",
    "    X_map    = X_concat[:, :map_dim].astype(np.float32, copy=True)\n",
    "    X_prompt = X_concat[:, map_dim:map_dim+prompt_dim].astype(np.float32, copy=True)\n",
    "    X_prompt = l2_rows(X_prompt)\n",
    "\n",
    "    X_map[~np.isfinite(X_map)] = np.nan\n",
    "    X_map_imp = imp.transform(X_map)\n",
    "    X_map_imp = np.clip(X_map_imp, q_lo, q_hi)\n",
    "\n",
    "    X_map_std = np.zeros_like(X_map_imp, dtype=np.float32)\n",
    "    X_map_std[:, keep_mask] = scaler.transform(X_map_imp[:, keep_mask]).astype(np.float32)\n",
    "\n",
    "    X_s = np.concatenate([X_map_std, X_prompt], axis=1).astype(np.float32)\n",
    "\n",
    "    clf_ = joblib.load(model_path)\n",
    "    le_  = joblib.load(le_path)\n",
    "    y_pred_idx = clf_.predict(X_s)\n",
    "    y_pred_lbl = le_.inverse_transform(y_pred_idx)\n",
    "    return y_pred_lbl, y_pred_idx\n",
    "\n",
    "print(\"‚úÖ Pipeline ready. Use apply_preproc_and_predict(...) for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7905b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
