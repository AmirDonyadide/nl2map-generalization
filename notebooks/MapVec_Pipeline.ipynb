{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f56a2c",
   "metadata": {},
   "source": [
    "## üß© 0) Setup & Imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "367f89f6ac45439b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:21:53.302406Z",
     "start_time": "2025-10-27T11:21:53.298709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFIG SUMMARY ===\n",
      "PROJ_ROOT  : /Users/amirdonyadide/Documents/GitHub/Thesis\n",
      "DATA_DIR   : /Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
      "INPUT_DIR  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input\n",
      "OUTPUT_DIR : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output\n",
      "MAPS_ROOT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/samples/pairs\n",
      "INPUT PAT. : *_input.geojson\n",
      "--- User Study ---\n",
      "USER_STUDY_XLSX : /Users/amirdonyadide/Documents/GitHub/Thesis/data/userstudy/UserStudy.xlsx\n",
      "RESPONSES_SHEET : Responses\n",
      "TILE_ID_COL     : tile_id\n",
      "COMPLETE_COL    : complete\n",
      "REMOVE_COL      : remove\n",
      "TEXT_COL        : cleaned_text\n",
      "PARAM_VALUE_COL : param_value\n",
      "OPERATOR_COL    : operator\n",
      "INTENSITY_COL   : intensity\n",
      "--- Filters / IDs / Split ---\n",
      "ONLY_COMPLETE   : True\n",
      "EXCLUDE_REMOVED : True\n",
      "PROMPT_ID       : r{i:08d}\n",
      "SPLIT_BY        : tile\n",
      "--- Outputs ---\n",
      "PROMPT_OUT : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "MAP_OUT    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n",
      "TRAIN_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out\n",
      "MODEL_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models\n",
      "SPLIT_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/splits\n",
      "PRM_NPZ    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/prompts_embeddings.npz\n",
      "MAPS_PQ    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/maps.parquet\n",
      "PAIRS_PQ   : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/train_pairs.parquet\n",
      "--- Model ---\n",
      "PROMPT_ENCODER: openai-small\n",
      "MAP_DIM       : 165\n",
      "PROMPT_DIM    : 512\n",
      "FUSED_DIM     : 677\n",
      "BATCH_SIZE    : 512\n",
      "VAL/TEST      : 0.15 0.15\n",
      "SEED          : 42\n",
      "--- Normalization behavior ---\n",
      "USE_DYNAMIC_EXTENT_REFS : True\n",
      "ALLOW_FALLBACK_EXTENT   : True\n",
      "--- Fallback tile scale (ONLY if dynamic refs missing) ---\n",
      "DEFAULT_TILE_W/H (m) : 400.0 400.0\n",
      "DEFAULT_TILE_DIAG_M  : 565.685424949238\n",
      "DEFAULT_TILE_AREA_M2 : 160000.0\n",
      "--- Extent columns ---\n",
      "EXTENT_DIAG_COL : extent_diag_m\n",
      "EXTENT_AREA_COL : extent_area_m2\n",
      "--- Operator groups ---\n",
      "DISTANCE_OPS : ('aggregate', 'displace', 'simplify')\n",
      "AREA_OPS     : ('select',)\n",
      "--- Operator groups ---\n",
      "DISTANCE_OPS : ('aggregate', 'displace', 'simplify')\n",
      "AREA_OPS     : ('select',)\n",
      "--- Param estimation ---\n",
      "PARAM_STRATEGY : mlp\n",
      "QUAL_TO_QUANTILE: {'very_small': 0.1, 'small': 0.25, 'medium': 0.5, 'large': 0.75, 'very_large': 0.9}\n",
      "DEFAULT_PARAM_BY_OPERATOR: {'aggregate': 5.0, 'displace': 5.0, 'simplify': 5.0, 'select': 50.0}\n",
      "USE_DYNAMIC_EXTENT_REFS: True\n",
      "ALLOW_FALLBACK_EXTENT  : True\n",
      "EXTENT_DIAG_COL: extent_diag_m  EXTENT_AREA_COL: extent_area_m2\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models\n",
      "‚úÖ All output folders cleaned and recreated fresh.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================== PARAMETERS / IMPORTS =====================\n",
    "from pathlib import Path\n",
    "import sys, subprocess, numpy as np, pandas as pd, joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, make_scorer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from scipy.stats import loguniform, randint\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Project config\n",
    "PROJ_ROOT = Path(\"../\").resolve()\n",
    "SRC_DIR   = PROJ_ROOT / \"src\"\n",
    "if str(PROJ_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJ_ROOT))\n",
    "\n",
    "from src.config import (\n",
    "    PATHS, CFG, print_summary,\n",
    "    DISTANCE_OPS, AREA_OPS,\n",
    "    USE_DYNAMIC_EXTENT_REFS, ALLOW_FALLBACK_EXTENT,\n",
    "    EXTENT_DIAG_COL, EXTENT_AREA_COL\n",
    ")\n",
    "print_summary()\n",
    "print(\"USE_DYNAMIC_EXTENT_REFS:\", USE_DYNAMIC_EXTENT_REFS)\n",
    "print(\"ALLOW_FALLBACK_EXTENT  :\", ALLOW_FALLBACK_EXTENT)\n",
    "print(\"EXTENT_DIAG_COL:\", EXTENT_DIAG_COL, \" EXTENT_AREA_COL:\", EXTENT_AREA_COL)\n",
    "\n",
    "\n",
    "DEFAULT_TILE_DIAG_M = CFG.DEFAULT_TILE_DIAG_M\n",
    "DEFAULT_TILE_AREA_M2 = CFG.DEFAULT_TILE_AREA_M2\n",
    "\n",
    "# Dims (fallbacks if CFG unset)\n",
    "MAP_DIM = PROMPT_DIM = FUSED_DIM = None\n",
    "\n",
    "BATCH_SIZE  = CFG.BATCH_SIZE\n",
    "\n",
    "# Clean outputs for a fresh run\n",
    "PATHS.clean_outputs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a2350e",
   "metadata": {},
   "source": [
    "## üìö 1) Build Prompt Embeddings (USE) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ed0df45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:21:55.572701Z",
     "start_time": "2025-10-27T11:21:55.570071Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-25 00:08:30 | INFO | Reading Excel: /Users/amirdonyadide/Documents/GitHub/Thesis/data/userstudy/UserStudy.xlsx (sheet=Responses)\n",
      "2026-01-25 00:08:30 | INFO | Filtered Excel rows: 786 ‚Üí 562 (only_complete=True, exclude_removed=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 562 prompts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-25 00:08:31 | INFO | Embedding 562 prompts with OpenAI model=text-embedding-3-small (batch_size=512, l2=True)‚Ä¶\n",
      "2026-01-25 00:08:33 | INFO | HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-25 00:08:35 | INFO | HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-25 00:08:35 | INFO | Done OpenAI embedding in 3.97s (dim=1536).\n",
      "2026-01-25 00:08:35 | INFO | Writing outputs to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "2026-01-25 00:08:35 | INFO |   saved prompts_embeddings.npz (shape=(562, 1536))\n",
      "2026-01-25 00:08:35 | INFO |   saved prompts.parquet (rows=562)\n",
      "2026-01-25 00:08:35 | INFO |   saved meta.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prompt embeddings completed.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Import the module\n",
    "from src.mapvec.prompts import prompt_embeddings as pe\n",
    "\n",
    "# Choose paths\n",
    "data_dir = PATHS.DATA_DIR  # or Path(\"../data\").resolve()\n",
    "in_path  = PATHS.USER_STUDY_XLSX\n",
    "out_dir  = PATHS.PROMPT_OUT\n",
    "\n",
    "# Logging (match what CLI does)\n",
    "pe.setup_logging(verbosity=1)\n",
    "\n",
    "# Load prompts (will filter complete==True & remove==False because you updated the function)\n",
    "ids, texts, tile_ids, id_colname = pe.load_prompts_from_source(\n",
    "    input_path=Path(in_path),\n",
    "    sheet_name=PATHS.RESPONSES_SHEET,\n",
    "    tile_id_col=PATHS.TILE_ID_COL,\n",
    "    complete_col=PATHS.COMPLETE_COL,\n",
    "    remove_col=PATHS.REMOVE_COL,\n",
    "    text_col=PATHS.TEXT_COL,\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(texts)} prompts.\")\n",
    "\n",
    "# Get embedder based on CFG.PROMPT_ENCODER (dan/transformer/openai-small/openai-large)\n",
    "embed_fn, model_label = pe.get_embedder(\n",
    "    kind=CFG.PROMPT_ENCODER,\n",
    "    data_dir=Path(data_dir),\n",
    "    l2_normalize=True,\n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    ")\n",
    "\n",
    "# Embed\n",
    "E = embed_fn(texts)\n",
    "\n",
    "# Save outputs in the same format as before\n",
    "pe.save_outputs(\n",
    "    out_dir=Path(out_dir),\n",
    "    ids=ids,\n",
    "    texts=texts,\n",
    "    tile_ids=tile_ids,          \n",
    "    E=E,\n",
    "    model_name=model_label,\n",
    "    l2_normalized=True,\n",
    "    id_colname=id_colname,      \n",
    "    also_save_embeddings_csv=False,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Prompt embeddings completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c0b51",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è 2) Build Map Embeddings (geometric) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ca0c3d8b71fc70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:26:59.687350Z",
     "start_time": "2025-10-27T11:26:19.901557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed tiles from Excel: 399\n",
      "Maps to embed after filtering: 399\n",
      "‚úÖ Map embeddings completed.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.mapvec.maps import map_embeddings as me\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Load allowed tile_ids from Excel\n",
    "# ------------------------------------------------------------\n",
    "dfu = pd.read_excel(PATHS.USER_STUDY_XLSX, sheet_name=PATHS.RESPONSES_SHEET)\n",
    "\n",
    "dfu[PATHS.COMPLETE_COL] = dfu[PATHS.COMPLETE_COL].astype(bool)\n",
    "dfu[PATHS.REMOVE_COL]   = dfu[PATHS.REMOVE_COL].astype(bool)\n",
    "\n",
    "mask = pd.Series(True, index=dfu.index)\n",
    "if PATHS.ONLY_COMPLETE:\n",
    "    mask &= (dfu[PATHS.COMPLETE_COL] == True)\n",
    "if PATHS.EXCLUDE_REMOVED:\n",
    "    mask &= (dfu[PATHS.REMOVE_COL] == False)\n",
    "dfu = dfu[mask].copy()\n",
    "\n",
    "tile_raw = dfu[PATHS.TILE_ID_COL]\n",
    "tile_num = pd.to_numeric(tile_raw, errors=\"coerce\")\n",
    "if tile_num.notna().all():\n",
    "    allowed_tile_ids = set(tile_num.astype(int).astype(str).str.zfill(4).tolist())\n",
    "else:\n",
    "    allowed_tile_ids = set(tile_raw.astype(str).str.strip().str.zfill(4).tolist())\n",
    "\n",
    "print(f\"Allowed tiles from Excel: {len(allowed_tile_ids)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Discover GeoJSONs and filter by tile_id\n",
    "# ------------------------------------------------------------\n",
    "me.setup_logging(verbosity=1)\n",
    "\n",
    "pairs = list(me.find_geojsons(PATHS.MAPS_ROOT, PATHS.INPUT_MAPS_PATTERN))\n",
    "pairs = [(map_id, path) for (map_id, path) in pairs if str(map_id).strip().zfill(4) in allowed_tile_ids]\n",
    "\n",
    "if not pairs:\n",
    "    raise RuntimeError(\"No maps left after Excel filtering.\")\n",
    "\n",
    "print(f\"Maps to embed after filtering: {len(pairs)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) First pass: polygon counting\n",
    "# ------------------------------------------------------------\n",
    "counts = {}\n",
    "for map_id, path in pairs:\n",
    "    try:\n",
    "        counts[map_id] = me._count_valid_polygons(path)\n",
    "    except Exception:\n",
    "        counts[map_id] = 0\n",
    "\n",
    "max_polygons = max(max(counts.values()), 1)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Second pass: embed maps (DYNAMIC extent normalization)\n",
    "# ------------------------------------------------------------\n",
    "ids, vecs, rows = [], [], []\n",
    "feat_names = None\n",
    "first_dim = None\n",
    "\n",
    "for map_id, path in pairs:\n",
    "    vec, names = me.embed_one_map(\n",
    "        path,\n",
    "        max_polygons=max_polygons,\n",
    "        norm=\"extent\",     # ‚úÖ dynamic per-map normalization\n",
    "        norm_wh=None,\n",
    "    )\n",
    "\n",
    "    if first_dim is None:\n",
    "        first_dim = vec.shape[0]\n",
    "        feat_names = names\n",
    "    elif vec.shape[0] != first_dim:\n",
    "        print(f\"Skipping {map_id}: dim mismatch\")\n",
    "        continue\n",
    "\n",
    "    ids.append(map_id)\n",
    "    vecs.append(vec)\n",
    "\n",
    "    # ‚úÖ dynamic per-map extent refs (computed from GeoJSON)\n",
    "    extent = me.compute_extent_refs(path)\n",
    "    # optional safety: skip degenerate extents\n",
    "    if not np.isfinite(extent.get(\"extent_diag_m\", np.nan)) or extent[\"extent_diag_m\"] <= 0:\n",
    "        print(f\"Skipping {map_id}: bad extent_diag_m\")\n",
    "        continue\n",
    "    if not np.isfinite(extent.get(\"extent_area_m2\", np.nan)) or extent[\"extent_area_m2\"] <= 0:\n",
    "        print(f\"Skipping {map_id}: bad extent_area_m2\")\n",
    "        continue\n",
    "\n",
    "    rows.append({\n",
    "        \"map_id\": map_id,\n",
    "        \"geojson\": str(path),\n",
    "        \"n_polygons\": counts.get(map_id, 0),\n",
    "\n",
    "        # Save these into maps.parquet so concat can merge later\n",
    "        **extent,\n",
    "    })\n",
    "\n",
    "E = np.vstack(vecs).astype(np.float32)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Save outputs (same format as script)\n",
    "# ------------------------------------------------------------\n",
    "me.save_outputs(\n",
    "    out_dir=PATHS.MAP_OUT,\n",
    "    rows=rows,\n",
    "    E=E,\n",
    "    ids=ids,\n",
    "    feat_names=feat_names or [],\n",
    "    save_csv=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Map embeddings completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e8cdf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è CFG dims differ from inferred dims. Using inferred dims.\n",
      "   inferred: MAP_DIM=165, PROMPT_DIM=1536\n",
      "   CFG:      MAP_DIM=165, PROMPT_DIM=512\n",
      "‚úÖ Inferred dims: {'MAP_DIM': 165, 'PROMPT_DIM': 1536, 'FUSED_DIM': 1701}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def infer_dims(paths):\n",
    "    prm_npz = paths.PROMPT_OUT / \"prompts_embeddings.npz\"\n",
    "    map_npz = paths.MAP_OUT / \"maps_embeddings.npz\"\n",
    "\n",
    "    if not prm_npz.exists():\n",
    "        raise FileNotFoundError(f\"Missing {prm_npz} (run prompt embeddings first)\")\n",
    "    z = np.load(prm_npz, allow_pickle=True)\n",
    "    PROMPT_DIM = int(z[\"E\"].shape[1])\n",
    "\n",
    "    if not map_npz.exists():\n",
    "        raise FileNotFoundError(f\"Missing {map_npz} (run map embeddings first)\")\n",
    "    z2 = np.load(map_npz, allow_pickle=True)\n",
    "    MAP_DIM = int(z2[\"E\"].shape[1])\n",
    "\n",
    "    FUSED_DIM = MAP_DIM + PROMPT_DIM\n",
    "    return MAP_DIM, PROMPT_DIM, FUSED_DIM\n",
    "\n",
    "MAP_DIM, PROMPT_DIM, FUSED_DIM = infer_dims(PATHS)\n",
    "\n",
    "# Prefer inferred dims as the source of truth (CFG may be default/env)\n",
    "if MAP_DIM != CFG.MAP_DIM or PROMPT_DIM != CFG.PROMPT_DIM:\n",
    "    print(f\"‚ö†Ô∏è CFG dims differ from inferred dims. Using inferred dims.\")\n",
    "    print(f\"   inferred: MAP_DIM={MAP_DIM}, PROMPT_DIM={PROMPT_DIM}\")\n",
    "    print(f\"   CFG:      MAP_DIM={CFG.MAP_DIM}, PROMPT_DIM={CFG.PROMPT_DIM}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Inferred dims:\", {\"MAP_DIM\": MAP_DIM, \"PROMPT_DIM\": PROMPT_DIM, \"FUSED_DIM\": FUSED_DIM})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd186319e89f445",
   "metadata": {},
   "source": [
    "## üîó 3) Concatenate (pairs ‚Üí fused rows) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa2b07a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Concatenation completed.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from src.mapvec.concat import concat_embeddings as ce\n",
    "ce.setup_logging(verbosity=1)\n",
    "\n",
    "map_npz_path = Path(PATHS.MAP_OUT / \"maps_embeddings.npz\")\n",
    "maps_pq      = Path(PATHS.MAP_OUT / \"maps.parquet\")           # ‚úÖ NEW (has extent_*)\n",
    "prm_npz_path = Path(PATHS.PROMPT_OUT / \"prompts_embeddings.npz\")\n",
    "prompts_pq   = Path(PATHS.PROMPT_OUT / \"prompts.parquet\")\n",
    "out_dir      = Path(PATHS.TRAIN_OUT)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- build pairs from prompts.parquet (authoritative) ----\n",
    "pairs = pd.read_parquet(prompts_pq)\n",
    "\n",
    "if \"prompt_id\" not in pairs.columns or \"tile_id\" not in pairs.columns:\n",
    "    raise RuntimeError(\"prompts.parquet must contain columns: prompt_id, tile_id\")\n",
    "\n",
    "# keep text for hybrid/rule-based param extraction later\n",
    "need_cols = [\"tile_id\", \"prompt_id\", \"text\"]\n",
    "missing_cols = [c for c in need_cols if c not in pairs.columns]\n",
    "if missing_cols:\n",
    "    raise RuntimeError(f\"prompts.parquet missing required columns for option B: {missing_cols}\")\n",
    "\n",
    "pairs = pairs.rename(columns={\"tile_id\": \"map_id\"})[[\"map_id\", \"prompt_id\", \"text\"]].copy()\n",
    "pairs[\"map_id\"] = pairs[\"map_id\"].astype(str).str.strip().str.zfill(4)\n",
    "pairs[\"prompt_id\"] = pairs[\"prompt_id\"].astype(str).str.strip()\n",
    "pairs = pairs.dropna(subset=[\"map_id\", \"prompt_id\"])\n",
    "pairs = pairs[(pairs[\"map_id\"] != \"\") & (pairs[\"prompt_id\"] != \"\")]\n",
    "pairs = pairs.drop_duplicates(subset=[\"map_id\", \"prompt_id\"]).reset_index(drop=True)\n",
    "\n",
    "# ---- load map extent refs from maps.parquet and merge into pairs ----\n",
    "if not maps_pq.exists():\n",
    "    raise FileNotFoundError(f\"Missing {maps_pq}. Run map embedding first to create maps.parquet with extent_* columns.\")\n",
    "\n",
    "maps_df = pd.read_parquet(maps_pq)\n",
    "maps_df[\"map_id\"] = maps_df[\"map_id\"].astype(str).str.strip().str.zfill(4)\n",
    "\n",
    "required_extent_cols = [\"map_id\", \"extent_diag_m\", \"extent_area_m2\"]\n",
    "missing = [c for c in required_extent_cols if c not in maps_df.columns]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"maps.parquet is missing required extent columns: {missing}. Re-run map embedding with extent saving.\")\n",
    "\n",
    "# keep a minimal set (add more if you like)\n",
    "extent_cols = [\n",
    "    \"map_id\",\n",
    "    \"extent_diag_m\",\n",
    "    \"extent_area_m2\",\n",
    "    \"extent_width_m\",\n",
    "    \"extent_height_m\",\n",
    "    \"extent_minx\",\n",
    "    \"extent_miny\",\n",
    "    \"extent_maxx\",\n",
    "    \"extent_maxy\",\n",
    "]\n",
    "extent_cols = [c for c in extent_cols if c in maps_df.columns]\n",
    "\n",
    "pairs = pairs.merge(maps_df[extent_cols], on=\"map_id\", how=\"left\")\n",
    "\n",
    "# safety: rows missing extents means mismatch between prompts and embedded maps\n",
    "n_missing_extent = pairs[\"extent_diag_m\"].isna().sum()\n",
    "if n_missing_extent:\n",
    "    print(f\"‚ö†Ô∏è Dropping {n_missing_extent} rows with missing extent refs after merge.\")\n",
    "    pairs = pairs.dropna(subset=[\"extent_diag_m\", \"extent_area_m2\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ---- load embeddings ----\n",
    "E_map, map_ids = ce.load_npz(map_npz_path)\n",
    "E_prm, prm_ids = ce.load_npz(prm_npz_path)\n",
    "\n",
    "idx_map = {k: i for i, k in enumerate(map_ids)}\n",
    "idx_prm = {k: i for i, k in enumerate(prm_ids)}\n",
    "\n",
    "# ---- match & build X ----\n",
    "chosen_rows, im_list, ip_list = [], [], []\n",
    "missing_ids = 0\n",
    "\n",
    "for i, row in enumerate(pairs.itertuples(index=False), start=0):\n",
    "    im = idx_map.get(row.map_id)\n",
    "    ip = idx_prm.get(row.prompt_id)\n",
    "    if im is None or ip is None:\n",
    "        missing_ids += 1\n",
    "        continue\n",
    "    chosen_rows.append(i)\n",
    "    im_list.append(im)\n",
    "    ip_list.append(ip)\n",
    "\n",
    "if not im_list:\n",
    "    raise RuntimeError(\"No valid pairs after ID matching.\")\n",
    "\n",
    "if missing_ids:\n",
    "    print(f\"‚ö†Ô∏è Skipped {missing_ids} rows with missing IDs in embeddings\")\n",
    "\n",
    "X_map = E_map[np.asarray(im_list, dtype=int)].astype(np.float32, copy=False)\n",
    "X_prm = E_prm[np.asarray(ip_list, dtype=int)].astype(np.float32, copy=False)\n",
    "X = np.hstack([X_map, X_prm]).astype(np.float32, copy=False)\n",
    "\n",
    "np.save(out_dir / \"X_concat.npy\", X)\n",
    "\n",
    "join_df = pairs.iloc[chosen_rows].reset_index(drop=True)\n",
    "\n",
    "# ‚úÖ No constant tile_* columns anymore. We keep dynamic extent_* columns from maps.parquet.\n",
    "assert X.shape[0] == len(join_df), \"Row count mismatch between X and join_df.\"\n",
    "\n",
    "join_df.to_parquet(out_dir / \"train_pairs.parquet\", index=False)\n",
    "\n",
    "meta = {\n",
    "    \"shape\": [int(X.shape[0]), int(X.shape[1])],\n",
    "    \"map_dim\": int(E_map.shape[1]),\n",
    "    \"prompt_dim\": int(E_prm.shape[1]),\n",
    "    \"rows\": int(X.shape[0]),\n",
    "    \"skipped_pairs_missing_ids\": int(missing_ids),\n",
    "    \"sources\": {\n",
    "        \"prompts_parquet\": str(prompts_pq),\n",
    "        \"maps_parquet\": str(maps_pq),\n",
    "        \"map_npz\": str(map_npz_path),\n",
    "        \"prompt_npz\": str(prm_npz_path),\n",
    "    },\n",
    "    \"extent_cols_saved\": [c for c in extent_cols if c != \"map_id\"],\n",
    "}\n",
    "(out_dir / \"meta.json\").write_text(json.dumps(meta, indent=2))\n",
    "\n",
    "print(\"‚úÖ Concatenation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5142d4b68c273d37",
   "metadata": {},
   "source": [
    "## üì• 4) Load & Basic Cleaning ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a494fd27dfe7681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded X: (562, 1701), pairs: (562, 11)\n",
      "After cleaning: X=(562, 1701), df=(562, 15), ops=['aggregate', 'displace', 'select', 'simplify']\n",
      "Example rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>map_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>extent_diag_m</th>\n",
       "      <th>extent_area_m2</th>\n",
       "      <th>extent_width_m</th>\n",
       "      <th>extent_height_m</th>\n",
       "      <th>extent_minx</th>\n",
       "      <th>extent_miny</th>\n",
       "      <th>extent_maxx</th>\n",
       "      <th>extent_maxy</th>\n",
       "      <th>operator</th>\n",
       "      <th>param_value</th>\n",
       "      <th>intensity</th>\n",
       "      <th>param_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1304</td>\n",
       "      <td>r00000000</td>\n",
       "      <td>Union few of the buildings.</td>\n",
       "      <td>582.418016</td>\n",
       "      <td>169604.830164</td>\n",
       "      <td>412.352091</td>\n",
       "      <td>411.310707</td>\n",
       "      <td>369009.126498</td>\n",
       "      <td>5.624443e+06</td>\n",
       "      <td>369421.478589</td>\n",
       "      <td>5.624855e+06</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1457</td>\n",
       "      <td>r00000002</td>\n",
       "      <td>Remove small buildings and eliminate narrow an...</td>\n",
       "      <td>496.278103</td>\n",
       "      <td>116190.319796</td>\n",
       "      <td>404.903965</td>\n",
       "      <td>286.957723</td>\n",
       "      <td>370209.074685</td>\n",
       "      <td>5.626969e+06</td>\n",
       "      <td>370613.978650</td>\n",
       "      <td>5.627256e+06</td>\n",
       "      <td>select</td>\n",
       "      <td>17.805</td>\n",
       "      <td>low</td>\n",
       "      <td>0.000153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1663</td>\n",
       "      <td>r00000005</td>\n",
       "      <td>Bundle nearby buildings into larger blocks.</td>\n",
       "      <td>533.109561</td>\n",
       "      <td>138157.356917</td>\n",
       "      <td>329.923688</td>\n",
       "      <td>418.755494</td>\n",
       "      <td>371811.357509</td>\n",
       "      <td>5.630840e+06</td>\n",
       "      <td>372141.281197</td>\n",
       "      <td>5.631259e+06</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1122</td>\n",
       "      <td>r00000006</td>\n",
       "      <td>Simplify small geometric details below a speci...</td>\n",
       "      <td>597.176996</td>\n",
       "      <td>178021.909966</td>\n",
       "      <td>434.102877</td>\n",
       "      <td>410.091523</td>\n",
       "      <td>367409.757832</td>\n",
       "      <td>5.630048e+06</td>\n",
       "      <td>367843.860710</td>\n",
       "      <td>5.630458e+06</td>\n",
       "      <td>simplify</td>\n",
       "      <td>1.000</td>\n",
       "      <td>low</td>\n",
       "      <td>0.001675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1706</td>\n",
       "      <td>r00000009</td>\n",
       "      <td>Eliminate repeated blocks.</td>\n",
       "      <td>465.952069</td>\n",
       "      <td>108172.503069</td>\n",
       "      <td>315.345720</td>\n",
       "      <td>343.028290</td>\n",
       "      <td>372305.411445</td>\n",
       "      <td>5.628541e+06</td>\n",
       "      <td>372620.757165</td>\n",
       "      <td>5.628884e+06</td>\n",
       "      <td>select</td>\n",
       "      <td>18.226</td>\n",
       "      <td>low</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1174</td>\n",
       "      <td>r00000010</td>\n",
       "      <td>Make a space between the polygons.</td>\n",
       "      <td>485.597704</td>\n",
       "      <td>105023.121090</td>\n",
       "      <td>414.108392</td>\n",
       "      <td>253.612636</td>\n",
       "      <td>367806.423992</td>\n",
       "      <td>5.631230e+06</td>\n",
       "      <td>368220.532385</td>\n",
       "      <td>5.631484e+06</td>\n",
       "      <td>displace</td>\n",
       "      <td>1.975</td>\n",
       "      <td>low</td>\n",
       "      <td>0.004067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0116</td>\n",
       "      <td>r00000011</td>\n",
       "      <td>Aggregate some of the buildings.</td>\n",
       "      <td>577.475387</td>\n",
       "      <td>166688.512862</td>\n",
       "      <td>403.286009</td>\n",
       "      <td>413.325801</td>\n",
       "      <td>359417.860505</td>\n",
       "      <td>5.619642e+06</td>\n",
       "      <td>359821.146514</td>\n",
       "      <td>5.620056e+06</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0417</td>\n",
       "      <td>r00000013</td>\n",
       "      <td>Exclude shapes with an area less than 20 squar...</td>\n",
       "      <td>571.706715</td>\n",
       "      <td>163423.957488</td>\n",
       "      <td>404.661369</td>\n",
       "      <td>403.853617</td>\n",
       "      <td>361813.128457</td>\n",
       "      <td>5.622451e+06</td>\n",
       "      <td>362217.789827</td>\n",
       "      <td>5.622855e+06</td>\n",
       "      <td>select</td>\n",
       "      <td>20.000</td>\n",
       "      <td>low</td>\n",
       "      <td>0.000122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1488</td>\n",
       "      <td>r00000014</td>\n",
       "      <td>Aggregate the blocks.</td>\n",
       "      <td>500.054694</td>\n",
       "      <td>115279.716486</td>\n",
       "      <td>276.818974</td>\n",
       "      <td>416.444418</td>\n",
       "      <td>370607.033823</td>\n",
       "      <td>5.619641e+06</td>\n",
       "      <td>370883.852797</td>\n",
       "      <td>5.620058e+06</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0175</td>\n",
       "      <td>r00000015</td>\n",
       "      <td>Simplify the shapes of the small buildings by ...</td>\n",
       "      <td>503.061104</td>\n",
       "      <td>119618.905293</td>\n",
       "      <td>409.629591</td>\n",
       "      <td>292.017247</td>\n",
       "      <td>359816.877837</td>\n",
       "      <td>5.623767e+06</td>\n",
       "      <td>360226.507427</td>\n",
       "      <td>5.624059e+06</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  map_id  prompt_id                                               text  \\\n",
       "0   1304  r00000000                        Union few of the buildings.   \n",
       "1   1457  r00000002  Remove small buildings and eliminate narrow an...   \n",
       "2   1663  r00000005        Bundle nearby buildings into larger blocks.   \n",
       "3   1122  r00000006  Simplify small geometric details below a speci...   \n",
       "4   1706  r00000009                         Eliminate repeated blocks.   \n",
       "5   1174  r00000010                 Make a space between the polygons.   \n",
       "6   0116  r00000011                   Aggregate some of the buildings.   \n",
       "7   0417  r00000013  Exclude shapes with an area less than 20 squar...   \n",
       "8   1488  r00000014                              Aggregate the blocks.   \n",
       "9   0175  r00000015  Simplify the shapes of the small buildings by ...   \n",
       "\n",
       "   extent_diag_m  extent_area_m2  extent_width_m  extent_height_m  \\\n",
       "0     582.418016   169604.830164      412.352091       411.310707   \n",
       "1     496.278103   116190.319796      404.903965       286.957723   \n",
       "2     533.109561   138157.356917      329.923688       418.755494   \n",
       "3     597.176996   178021.909966      434.102877       410.091523   \n",
       "4     465.952069   108172.503069      315.345720       343.028290   \n",
       "5     485.597704   105023.121090      414.108392       253.612636   \n",
       "6     577.475387   166688.512862      403.286009       413.325801   \n",
       "7     571.706715   163423.957488      404.661369       403.853617   \n",
       "8     500.054694   115279.716486      276.818974       416.444418   \n",
       "9     503.061104   119618.905293      409.629591       292.017247   \n",
       "\n",
       "     extent_minx   extent_miny    extent_maxx   extent_maxy   operator  \\\n",
       "0  369009.126498  5.624443e+06  369421.478589  5.624855e+06  aggregate   \n",
       "1  370209.074685  5.626969e+06  370613.978650  5.627256e+06     select   \n",
       "2  371811.357509  5.630840e+06  372141.281197  5.631259e+06  aggregate   \n",
       "3  367409.757832  5.630048e+06  367843.860710  5.630458e+06   simplify   \n",
       "4  372305.411445  5.628541e+06  372620.757165  5.628884e+06     select   \n",
       "5  367806.423992  5.631230e+06  368220.532385  5.631484e+06   displace   \n",
       "6  359417.860505  5.619642e+06  359821.146514  5.620056e+06  aggregate   \n",
       "7  361813.128457  5.622451e+06  362217.789827  5.622855e+06     select   \n",
       "8  370607.033823  5.619641e+06  370883.852797  5.620058e+06  aggregate   \n",
       "9  359816.877837  5.623767e+06  360226.507427  5.624059e+06  aggregate   \n",
       "\n",
       "   param_value intensity  param_norm  \n",
       "0        0.000    medium    0.000000  \n",
       "1       17.805       low    0.000153  \n",
       "2        0.000    medium    0.000000  \n",
       "3        1.000       low    0.001675  \n",
       "4       18.226       low    0.000168  \n",
       "5        1.975       low    0.004067  \n",
       "6        0.000    medium    0.000000  \n",
       "7       20.000       low    0.000122  \n",
       "8        0.000    medium    0.000000  \n",
       "9        0.000    medium    0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === LOAD FUSED DATA (operator + param_value + DYNAMIC-EXTENT normalized target) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# IMPORTANT: make sure these are imported from src.config somewhere above in the notebook:\n",
    "# from src.config import PATHS, CFG, DISTANCE_OPS, AREA_OPS\n",
    "\n",
    "X = np.load(PATHS.TRAIN_OUT / \"X_concat.npy\")\n",
    "pairs_df = pd.read_parquet(PATHS.TRAIN_OUT / \"train_pairs.parquet\")\n",
    "print(f\"Loaded X: {X.shape}, pairs: {pairs_df.shape}\")\n",
    "\n",
    "# --- Rebuild labels from Excel and merge (train_pairs already has extent_* from maps.parquet) ---\n",
    "dfu = pd.read_excel(PATHS.USER_STUDY_XLSX, sheet_name=PATHS.RESPONSES_SHEET)\n",
    "\n",
    "dfu[PATHS.COMPLETE_COL] = dfu[PATHS.COMPLETE_COL].astype(bool)\n",
    "dfu[PATHS.REMOVE_COL]   = dfu[PATHS.REMOVE_COL].astype(bool)\n",
    "\n",
    "mask_excel = pd.Series(True, index=dfu.index)\n",
    "if PATHS.ONLY_COMPLETE:\n",
    "    mask_excel &= (dfu[PATHS.COMPLETE_COL] == True)\n",
    "if PATHS.EXCLUDE_REMOVED:\n",
    "    mask_excel &= (dfu[PATHS.REMOVE_COL] == False)\n",
    "dfu = dfu[mask_excel].copy()\n",
    "\n",
    "# prompt_id mapping must match prompt_embeddings.py\n",
    "dfu = dfu.reset_index(drop=False).rename(columns={\"index\": \"_row\"})\n",
    "prefix = PATHS.PROMPT_ID_PREFIX\n",
    "width  = PATHS.PROMPT_ID_WIDTH\n",
    "dfu[\"prompt_id\"] = dfu[\"_row\"].apply(lambda r: f\"{prefix}{int(r):0{width}d}\")\n",
    "\n",
    "# normalize tile_id -> map_id\n",
    "tile_raw = dfu[PATHS.TILE_ID_COL]\n",
    "tile_num = pd.to_numeric(tile_raw, errors=\"coerce\")\n",
    "if tile_num.notna().all():\n",
    "    dfu[\"map_id\"] = tile_num.astype(int).astype(str).str.zfill(4)\n",
    "else:\n",
    "    dfu[\"map_id\"] = tile_raw.astype(str).str.strip().str.zfill(4)\n",
    "\n",
    "labels = dfu[\n",
    "    [\"map_id\", \"prompt_id\", PATHS.OPERATOR_COL, PATHS.PARAM_VALUE_COL, PATHS.INTENSITY_COL]\n",
    "].copy()\n",
    "\n",
    "# Merge labels onto pairs_df (which already contains extent_* columns from maps.parquet)\n",
    "df = pairs_df.merge(labels, on=[\"map_id\", \"prompt_id\"], how=\"left\")\n",
    "# Option B needs prompt text for rule-based parsing\n",
    "if \"text\" not in df.columns:\n",
    "    raise RuntimeError(\n",
    "        \"train_pairs.parquet is missing 'text'. \"\n",
    "        \"Update concat step to keep 'text' from prompts.parquet.\"\n",
    "    )\n",
    "df[\"text\"] = df[\"text\"].astype(\"string\")\n",
    "\n",
    "OP_COL    = PATHS.OPERATOR_COL\n",
    "PARAM_COL = PATHS.PARAM_VALUE_COL\n",
    "\n",
    "# Clean targets\n",
    "df[OP_COL] = df[OP_COL].astype(\"string\").str.strip().str.lower()\n",
    "df.loc[df[OP_COL].isin([\"\", \"nan\"]), OP_COL] = pd.NA\n",
    "df[PARAM_COL] = pd.to_numeric(df[PARAM_COL], errors=\"coerce\")\n",
    "\n",
    "# --- Ensure dynamic extent refs exist (from concat merge of maps.parquet) ---\n",
    "REQ_EXT = [\"extent_diag_m\", \"extent_area_m2\"]\n",
    "missing = [c for c in REQ_EXT if c not in df.columns]\n",
    "assert not missing, f\"Missing {missing} in df. Check concat step merged maps.parquet extent_* columns.\"\n",
    "\n",
    "df[\"extent_diag_m\"]  = pd.to_numeric(df[\"extent_diag_m\"], errors=\"coerce\")\n",
    "df[\"extent_area_m2\"] = pd.to_numeric(df[\"extent_area_m2\"], errors=\"coerce\")\n",
    "\n",
    "# --- Keep only rows with valid targets + extents ---\n",
    "mask = (\n",
    "    df[OP_COL].notna() &\n",
    "    (df[OP_COL] != \"\") &\n",
    "    (df[OP_COL] != \"nan\") &          # ‚úÖ important\n",
    "    df[PARAM_COL].notna() &\n",
    "    df[\"extent_diag_m\"].notna() &\n",
    "    df[\"extent_area_m2\"].notna() &\n",
    "    (df[\"extent_diag_m\"] > 0) &\n",
    "    (df[\"extent_area_m2\"] > 0)\n",
    ")\n",
    "\n",
    "\n",
    "X  = X[mask.values].astype(np.float64, copy=False)\n",
    "df = df.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# --- Build normalized regression target using DYNAMIC extents ---\n",
    "DIST_OPS_SET = set(DISTANCE_OPS)  # from src.config\n",
    "AREA_OPS_SET = set(AREA_OPS)      # from src.config\n",
    "\n",
    "df[\"param_norm\"] = np.nan\n",
    "\n",
    "m_dist = df[OP_COL].isin(DIST_OPS_SET)\n",
    "m_area = df[OP_COL].isin(AREA_OPS_SET)\n",
    "\n",
    "df.loc[m_dist, \"param_norm\"] = df.loc[m_dist, PARAM_COL] / df.loc[m_dist, \"extent_diag_m\"]\n",
    "df.loc[m_area, \"param_norm\"] = df.loc[m_area, PARAM_COL] / df.loc[m_area, \"extent_area_m2\"]\n",
    "\n",
    "# Sanity: everything should be filled\n",
    "bad = df[\"param_norm\"].isna().sum()\n",
    "assert bad == 0, (\n",
    "    f\"param_norm has {bad} NaNs. \"\n",
    "    \"This usually means an operator is not in DISTANCE_OPS/AREA_OPS or extents are missing.\"\n",
    ")\n",
    "\n",
    "print(f\"After cleaning: X={X.shape}, df={df.shape}, ops={sorted(df[OP_COL].unique())}\")\n",
    "print(\"Example rows:\")\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997cab7",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è 5) Split & Targets ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bc0897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET SUMMARY ===\n",
      "Total rows (prompts): 562\n",
      "Unique maps: 399\n",
      "Multi-prompt maps (>1 prompt): 22\n",
      "Single-prompt maps (=1 prompt): 377\n",
      "\n",
      "Top 10 maps by prompt count:\n",
      "map_id\n",
      "1646    30\n",
      "1304    29\n",
      "1755    26\n",
      "1532    13\n",
      "0127    10\n",
      "0168     8\n",
      "0142     7\n",
      "0078     6\n",
      "0080     6\n",
      "0001     6\n",
      "dtype: int64\n",
      "\n",
      "=== SPLIT SUMMARY ===\n",
      "‚úÖ Split found (seed=42)\n",
      "Train maps: 285  (includes multi-prompt maps: 22)\n",
      "Val maps:   57\n",
      "Test maps:  57\n",
      "Rows -> Train: (448, 1701), Val: (57, 1701), Test: (57, 1701)\n",
      "‚úÖ Verified: no map_id leakage across splits.\n",
      "‚úÖ Verified: all multi-prompt maps are in TRAIN.\n",
      "‚úÖ Saved splits to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/splits.json\n",
      "\n",
      "TRAIN ‚Äî Operator counts\n",
      "operator\n",
      "select       144\n",
      "aggregate    134\n",
      "simplify     109\n",
      "displace      61\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "VAL ‚Äî Operator counts\n",
      "operator\n",
      "select       19\n",
      "aggregate    16\n",
      "simplify     13\n",
      "displace      9\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "TEST ‚Äî Operator counts\n",
      "operator\n",
      "select       19\n",
      "aggregate    16\n",
      "simplify     13\n",
      "displace      9\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "TRAIN ‚Äî Operator √ó Intensity table (counts)\n",
      "intensity  high  low  medium\n",
      "operator                    \n",
      "aggregate    37   38      59\n",
      "displace     13   20      28\n",
      "select       34   34      76\n",
      "simplify     25   26      58\n",
      "\n",
      "TRAIN ‚Äî Operator totals (row sums)\n",
      "operator\n",
      "aggregate    134\n",
      "displace      61\n",
      "select       144\n",
      "simplify     109\n",
      "dtype: int64\n",
      "\n",
      "TRAIN ‚Äî Intensity totals (col sums)\n",
      "intensity\n",
      "high      109\n",
      "low       118\n",
      "medium    221\n",
      "dtype: int64\n",
      "\n",
      "VAL ‚Äî Operator √ó Intensity table (counts)\n",
      "intensity  high  low  medium\n",
      "operator                    \n",
      "aggregate     5    6       5\n",
      "displace      3    3       3\n",
      "select        5    7       7\n",
      "simplify      4    4       5\n",
      "\n",
      "VAL ‚Äî Operator totals (row sums)\n",
      "operator\n",
      "aggregate    16\n",
      "displace      9\n",
      "select       19\n",
      "simplify     13\n",
      "dtype: int64\n",
      "\n",
      "VAL ‚Äî Intensity totals (col sums)\n",
      "intensity\n",
      "high      17\n",
      "low       20\n",
      "medium    20\n",
      "dtype: int64\n",
      "\n",
      "TEST ‚Äî Operator √ó Intensity table (counts)\n",
      "intensity  high  low  medium\n",
      "operator                    \n",
      "aggregate     5    6       5\n",
      "displace      3    3       3\n",
      "select        6    6       7\n",
      "simplify      4    4       5\n",
      "\n",
      "TEST ‚Äî Operator totals (row sums)\n",
      "operator\n",
      "aggregate    16\n",
      "displace      9\n",
      "select       19\n",
      "simplify     13\n",
      "dtype: int64\n",
      "\n",
      "TEST ‚Äî Intensity totals (col sums)\n",
      "intensity\n",
      "high      18\n",
      "low       19\n",
      "medium    20\n",
      "dtype: int64\n",
      "\n",
      "‚úÖ TRAIN: All operator√óintensity combos present.\n",
      "\n",
      "‚úÖ VAL: All operator√óintensity combos present.\n",
      "\n",
      "‚úÖ TEST: All operator√óintensity combos present.\n",
      "\n",
      "TRAIN ‚Äî prompts per map statistics\n",
      "TRAIN ‚Äî #maps with >1 prompt: 22\n",
      "\n",
      "VAL ‚Äî prompts per map statistics\n",
      "VAL ‚Äî #maps with >1 prompt: 0\n",
      "\n",
      "TEST ‚Äî prompts per map statistics\n",
      "TEST ‚Äî #maps with >1 prompt: 0\n",
      "\n",
      "TRAIN ‚Äî Top multi-prompt maps (forced to train):\n",
      "map_id\n",
      "1646    30\n",
      "1304    29\n",
      "1755    26\n",
      "1532    13\n",
      "0127    10\n",
      "0168     8\n",
      "0142     7\n",
      "0080     6\n",
      "0001     6\n",
      "0078     6\n",
      "0073     6\n",
      "0159     5\n",
      "0074     5\n",
      "0079     4\n",
      "0081     4\n",
      "0077     4\n",
      "0025     3\n",
      "0120     3\n",
      "0240     3\n",
      "0118     3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import json\n",
    "\n",
    "FIXED_CLASSES = [\"simplify\", \"select\", \"aggregate\", \"displace\"]\n",
    "USE_INTENSITY_FOR_STRAT = True  # will fall back to operator-only if needed\n",
    "\n",
    "OP_COL   = PATHS.OPERATOR_COL        # \"operator\"\n",
    "PARAM_COL = PATHS.PARAM_VALUE_COL    # \"param_value\"\n",
    "INT_COL  = PATHS.INTENSITY_COL       # \"intensity\"\n",
    "\n",
    "df = df.copy()\n",
    "\n",
    "# operator: keep NA properly, and drop \"nan\"/empty\n",
    "df[OP_COL] = df[OP_COL].astype(\"string\").str.strip().str.lower()\n",
    "df.loc[df[OP_COL].isin([\"\", \"nan\"]), OP_COL] = pd.NA\n",
    "\n",
    "# intensity: keep NA properly (do NOT turn NA into \"nan\" string)\n",
    "if INT_COL in df.columns:\n",
    "    df[INT_COL] = df[INT_COL].astype(\"string\").str.strip().str.lower()\n",
    "    df.loc[df[INT_COL].isin([\"\", \"nan\"]), INT_COL] = pd.NA\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Group stats: prompts per map_id\n",
    "# ------------------------------------------------------------\n",
    "prompt_counts = df.groupby(\"map_id\").size()\n",
    "multi_map_ids = prompt_counts[prompt_counts > 1].index.tolist()\n",
    "single_map_ids = prompt_counts[prompt_counts == 1].index.tolist()\n",
    "\n",
    "print(\"=== DATASET SUMMARY ===\")\n",
    "print(f\"Total rows (prompts): {len(df)}\")\n",
    "print(f\"Unique maps: {prompt_counts.shape[0]}\")\n",
    "print(f\"Multi-prompt maps (>1 prompt): {len(multi_map_ids)}\")\n",
    "print(f\"Single-prompt maps (=1 prompt): {len(single_map_ids)}\")\n",
    "print(\"\\nTop 10 maps by prompt count:\")\n",
    "print(prompt_counts.sort_values(ascending=False).head(10))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Map-level table for single maps (one row per map_id)\n",
    "# ------------------------------------------------------------\n",
    "df_single = df[df[\"map_id\"].isin(single_map_ids)].copy()\n",
    "map_level = df_single.groupby(\"map_id\").first().reset_index()\n",
    "map_level = map_level.dropna(subset=[OP_COL]).copy()\n",
    "\n",
    "# Build strat label: operator√óintensity if feasible, else operator only\n",
    "if USE_INTENSITY_FOR_STRAT and INT_COL in map_level.columns:\n",
    "    map_level[\"_strat\"] = map_level[OP_COL] + \"__\" + map_level[INT_COL]\n",
    "    vc = map_level[\"_strat\"].value_counts()\n",
    "    if (vc < 2).any():\n",
    "        print(\"\\n‚ö†Ô∏è Some operator√óintensity groups too rare (<2 single-maps). Falling back to operator-only stratification.\")\n",
    "        map_level[\"_strat\"] = map_level[OP_COL]\n",
    "else:\n",
    "    map_level[\"_strat\"] = map_level[OP_COL]\n",
    "\n",
    "def has_all_ops(dfx: pd.DataFrame) -> bool:\n",
    "    return set(dfx[OP_COL].unique()) >= set(FIXED_CLASSES)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Split single maps into train/val/test with retries\n",
    "# ------------------------------------------------------------\n",
    "test_ratio = CFG.TEST_RATIO\n",
    "val_ratio = CFG.VAL_RATIO\n",
    "val_rel = val_ratio / (1.0 - test_ratio)\n",
    "\n",
    "X_idx = np.arange(len(map_level))\n",
    "y_strat = map_level[\"_strat\"].to_numpy()\n",
    "map_ids_arr = map_level[\"map_id\"].to_numpy()\n",
    "\n",
    "best = None\n",
    "for attempt in range(500):\n",
    "    rs = CFG.SEED + attempt\n",
    "\n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=test_ratio, random_state=rs)\n",
    "    trainval_i, test_i = next(sss1.split(X_idx, y_strat))\n",
    "\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_rel, random_state=rs + 999)\n",
    "    train_i, val_i = next(sss2.split(trainval_i, y_strat[trainval_i]))\n",
    "\n",
    "    single_train_maps = set(map_ids_arr[trainval_i[train_i]])\n",
    "    single_val_maps   = set(map_ids_arr[trainval_i[val_i]])\n",
    "    single_test_maps  = set(map_ids_arr[test_i])\n",
    "\n",
    "    train_maps = set(multi_map_ids) | single_train_maps\n",
    "    val_maps   = single_val_maps\n",
    "    test_maps  = single_test_maps\n",
    "\n",
    "    # leakage check\n",
    "    if (train_maps & val_maps) or (train_maps & test_maps) or (val_maps & test_maps):\n",
    "        continue\n",
    "\n",
    "    df_train_tmp = df[df[\"map_id\"].isin(train_maps)]\n",
    "    df_val_tmp   = df[df[\"map_id\"].isin(val_maps)]\n",
    "    df_test_tmp  = df[df[\"map_id\"].isin(test_maps)]\n",
    "\n",
    "    # must contain all operators in each split\n",
    "    if not (has_all_ops(df_train_tmp) and has_all_ops(df_val_tmp) and has_all_ops(df_test_tmp)):\n",
    "        continue\n",
    "\n",
    "    best = (train_maps, val_maps, test_maps, rs)\n",
    "    break\n",
    "\n",
    "if best is None:\n",
    "    raise RuntimeError(\n",
    "        \"Could not find a leakage-safe split with operator coverage in all splits \"\n",
    "        \"and multi-prompt maps forced to TRAIN. \"\n",
    "        \"Try: USE_INTENSITY_FOR_STRAT=False, or adjust VAL/TEST ratios.\"\n",
    "    )\n",
    "\n",
    "train_maps, val_maps, test_maps, used_seed = best\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Build row-level splits (no leakage)\n",
    "# ------------------------------------------------------------\n",
    "train_idx = df.index[df[\"map_id\"].isin(train_maps)].to_numpy()\n",
    "val_idx   = df.index[df[\"map_id\"].isin(val_maps)].to_numpy()\n",
    "test_idx  = df.index[df[\"map_id\"].isin(test_maps)].to_numpy()\n",
    "\n",
    "X_train, X_val, X_test = X[train_idx], X[val_idx], X[test_idx]\n",
    "df_train = df.loc[train_idx].reset_index(drop=True)\n",
    "df_val   = df.loc[val_idx].reset_index(drop=True)\n",
    "df_test  = df.loc[test_idx].reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== SPLIT SUMMARY ===\")\n",
    "print(f\"‚úÖ Split found (seed={used_seed})\")\n",
    "print(f\"Train maps: {len(train_maps)}  (includes multi-prompt maps: {len(set(multi_map_ids))})\")\n",
    "print(f\"Val maps:   {len(val_maps)}\")\n",
    "print(f\"Test maps:  {len(test_maps)}\")\n",
    "print(f\"Rows -> Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Hard guarantees\n",
    "assert set(df_train[\"map_id\"]).isdisjoint(df_val[\"map_id\"])\n",
    "assert set(df_train[\"map_id\"]).isdisjoint(df_test[\"map_id\"])\n",
    "assert set(df_val[\"map_id\"]).isdisjoint(df_test[\"map_id\"])\n",
    "assert set(multi_map_ids).issubset(train_maps)\n",
    "print(\"‚úÖ Verified: no map_id leakage across splits.\")\n",
    "print(\"‚úÖ Verified: all multi-prompt maps are in TRAIN.\")\n",
    "\n",
    "split_path = PATHS.TRAIN_OUT / \"splits.json\"\n",
    "json.dump(\n",
    "    {\n",
    "        \"train_idx\": train_idx.tolist(),\n",
    "        \"val_idx\": val_idx.tolist(),\n",
    "        \"test_idx\": test_idx.tolist(),\n",
    "        \"seed_used\": int(used_seed),\n",
    "        \"use_intensity_for_strat\": bool(USE_INTENSITY_FOR_STRAT),\n",
    "    },\n",
    "    open(split_path, \"w\"),\n",
    "    indent=2\n",
    ")\n",
    "print(\"‚úÖ Saved splits to\", split_path)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Detailed diagnostics: operator + intensity coverage\n",
    "# ------------------------------------------------------------\n",
    "def print_operator_counts(dfx, name):\n",
    "    print(f\"\\n{name} ‚Äî Operator counts\")\n",
    "    print(dfx[OP_COL].value_counts())\n",
    "\n",
    "def print_op_intensity_table(dfx, name):\n",
    "    if INT_COL not in dfx.columns:\n",
    "        print(f\"\\n{name} ‚Äî intensity column missing; skipping op√óintensity table.\")\n",
    "        return\n",
    "    print(f\"\\n{name} ‚Äî Operator √ó Intensity table (counts)\")\n",
    "    tab = (\n",
    "        dfx.groupby([OP_COL, INT_COL]).size()\n",
    "        .unstack(fill_value=0)\n",
    "        .sort_index()\n",
    "    )\n",
    "    print(tab)\n",
    "    print(f\"\\n{name} ‚Äî Operator totals (row sums)\")\n",
    "    print(tab.sum(axis=1))\n",
    "    print(f\"\\n{name} ‚Äî Intensity totals (col sums)\")\n",
    "    print(tab.sum(axis=0))\n",
    "\n",
    "def report_missing_combos(dfx, name, all_ops, all_ints):\n",
    "    if INT_COL not in dfx.columns:\n",
    "        return\n",
    "    present = set(zip(dfx[OP_COL], dfx[INT_COL]))\n",
    "    missing = [(op, it) for op in all_ops for it in all_ints if (op, it) not in present]\n",
    "    if missing:\n",
    "        print(f\"\\n‚ö†Ô∏è {name}: Missing operator√óintensity combos ({len(missing)}):\")\n",
    "        print(missing[:40], \"...\" if len(missing) > 40 else \"\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ {name}: All operator√óintensity combos present.\")\n",
    "\n",
    "ALL_OPS = FIXED_CLASSES[:]  # enforce fixed order\n",
    "ALL_INTS = sorted(df[INT_COL].unique()) if INT_COL in df.columns else []\n",
    "\n",
    "print_operator_counts(df_train, \"TRAIN\")\n",
    "print_operator_counts(df_val,   \"VAL\")\n",
    "print_operator_counts(df_test,  \"TEST\")\n",
    "\n",
    "print_op_intensity_table(df_train, \"TRAIN\")\n",
    "print_op_intensity_table(df_val,   \"VAL\")\n",
    "print_op_intensity_table(df_test,  \"TEST\")\n",
    "\n",
    "report_missing_combos(df_train, \"TRAIN\", ALL_OPS, ALL_INTS)\n",
    "report_missing_combos(df_val,   \"VAL\",   ALL_OPS, ALL_INTS)\n",
    "report_missing_combos(df_test,  \"TEST\",  ALL_OPS, ALL_INTS)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Map-level prompt multiplicity info per split\n",
    "# ------------------------------------------------------------\n",
    "def map_prompt_stats(map_set, name):\n",
    "    sub_counts = prompt_counts.loc[list(map_set)]\n",
    "    print(f\"\\n{name} ‚Äî prompts per map statistics\")\n",
    "    print(f\"{name} ‚Äî #maps with >1 prompt:\", int((sub_counts > 1).sum()))\n",
    "\n",
    "map_prompt_stats(train_maps, \"TRAIN\")\n",
    "map_prompt_stats(val_maps,   \"VAL\")\n",
    "map_prompt_stats(test_maps,  \"TEST\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) Optional: show top multi-prompt maps in TRAIN\n",
    "# ------------------------------------------------------------\n",
    "top_multi = prompt_counts.loc[multi_map_ids].sort_values(ascending=False).head(20)\n",
    "print(\"\\nTRAIN ‚Äî Top multi-prompt maps (forced to train):\")\n",
    "print(top_multi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187473d",
   "metadata": {},
   "source": [
    "## üßº 6) Modality-Aware Preprocessing (map only) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41942b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modality-aware preprocessing complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/preproc.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === MODALITY-AWARE PREPROCESSING ===\n",
    "def split_blocks(X):\n",
    "    X_map    = X[:, :MAP_DIM].astype(np.float64, copy=True)\n",
    "    X_prompt = X[:, MAP_DIM:MAP_DIM+PROMPT_DIM].astype(np.float64, copy=True)\n",
    "    return X_map, X_prompt\n",
    "\n",
    "def l2_normalize_rows(A, eps=1e-12):\n",
    "    nrm = np.sqrt((A * A).sum(axis=1, keepdims=True))\n",
    "    return A / np.maximum(nrm, eps)\n",
    "\n",
    "# split\n",
    "Xm_tr, Xp_tr = split_blocks(X_train)\n",
    "Xm_va, Xp_va = split_blocks(X_val)\n",
    "Xm_te, Xp_te = split_blocks(X_test)\n",
    "\n",
    "# prompts: L2 only\n",
    "Xp_tr = l2_normalize_rows(Xp_tr)\n",
    "Xp_va = l2_normalize_rows(Xp_va)\n",
    "Xp_te = l2_normalize_rows(Xp_te)\n",
    "\n",
    "# maps: inf‚ÜíNaN\n",
    "for A in (Xm_tr, Xm_va, Xm_te):\n",
    "    A[~np.isfinite(A)] = np.nan\n",
    "\n",
    "# impute (train)\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "Xm_tr_imp = imp.fit_transform(Xm_tr)\n",
    "Xm_va_imp = imp.transform(Xm_va)\n",
    "Xm_te_imp = imp.transform(Xm_te)\n",
    "\n",
    "# clip (5‚Äì95%) train thresholds\n",
    "q_lo = np.nanpercentile(Xm_tr_imp, 5, axis=0)\n",
    "q_hi = np.nanpercentile(Xm_tr_imp, 95, axis=0)\n",
    "def clip_to_q(A, lo, hi): return np.clip(A, lo, hi)\n",
    "\n",
    "Xm_tr_imp = clip_to_q(Xm_tr_imp, q_lo, q_hi)\n",
    "Xm_va_imp = clip_to_q(Xm_va_imp, q_lo, q_hi)\n",
    "Xm_te_imp = clip_to_q(Xm_te_imp, q_lo, q_hi)\n",
    "\n",
    "# drop zero-variance cols on train\n",
    "stds = np.nanstd(Xm_tr_imp, axis=0)\n",
    "keep_mask = stds > 1e-12\n",
    "\n",
    "# scale kept columns (train fit)\n",
    "scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5, 95))\n",
    "Xm_tr_kept = scaler.fit_transform(Xm_tr_imp[:, keep_mask])\n",
    "Xm_va_kept = scaler.transform(Xm_va_imp[:, keep_mask])\n",
    "Xm_te_kept = scaler.transform(Xm_te_imp[:, keep_mask])\n",
    "\n",
    "# rebuild full map dim (dropped cols = 0)\n",
    "Xm_tr_s = np.zeros_like(Xm_tr_imp, dtype=np.float64)\n",
    "Xm_va_s = np.zeros_like(Xm_va_imp, dtype=np.float64)\n",
    "Xm_te_s = np.zeros_like(Xm_te_imp, dtype=np.float64)\n",
    "Xm_tr_s[:, keep_mask] = Xm_tr_kept.astype(np.float64)\n",
    "Xm_va_s[:, keep_mask] = Xm_va_kept.astype(np.float64)\n",
    "Xm_te_s[:, keep_mask] = Xm_te_kept.astype(np.float64)\n",
    "\n",
    "# fuse back\n",
    "X_train_s = np.concatenate([Xm_tr_s, Xp_tr], axis=1).astype(np.float64)\n",
    "X_val_s   = np.concatenate([Xm_va_s, Xp_va], axis=1).astype(np.float64)\n",
    "X_test_s  = np.concatenate([Xm_te_s, Xp_te], axis=1).astype(np.float64)\n",
    "\n",
    "assert np.isfinite(X_train_s).all() and np.isfinite(X_val_s).all() and np.isfinite(X_test_s).all(), \"Non-finite after preprocessing.\"\n",
    "print(\"‚úÖ Modality-aware preprocessing complete.\")\n",
    "\n",
    "# save preprocessing bundle\n",
    "joblib.dump({\n",
    "    \"imp\": imp,\n",
    "    \"q_lo\": q_lo,\n",
    "    \"q_hi\": q_hi,\n",
    "    \"clip_quantiles\": (5, 95),\n",
    "    \"keep_mask\": keep_mask,\n",
    "    \"scaler\": scaler,\n",
    "    \"map_dim\": MAP_DIM,\n",
    "    \"prompt_dim\": PROMPT_DIM,\n",
    "    \"prompt_l2_eps\": 1e-12,\n",
    "}, PATHS.TRAIN_OUT / \"preproc.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c32c2",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è 7) Class Weights ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0df2b2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator classes: [np.str_('simplify'), np.str_('select'), np.str_('aggregate'), np.str_('displace')]\n",
      "Class weights: {np.str_('simplify'): np.float64(1.0275229357798166), np.str_('select'): np.float64(0.7777777777777778), np.str_('aggregate'): np.float64(0.835820895522388), np.str_('displace'): np.float64(1.8360655737704918)}\n",
      "Sample weight summary: {'min': 0.025925925925925925, 'max': 1.8360655737704918, 'mean': 0.6487687942076353}\n"
     ]
    }
   ],
   "source": [
    "# === BUILD y_train_cls + CLASS + MAP-AWARE SAMPLE WEIGHTS ===\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "OP_COL = PATHS.OPERATOR_COL  # \"operator\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Build y_train_cls directly from df_train\n",
    "# ------------------------------------------------------------\n",
    "# factorize gives stable integer labels starting at 0\n",
    "# Use a fixed, stable class order\n",
    "class_names = np.array(FIXED_CLASSES)  # [\"simplify\",\"select\",\"aggregate\",\"displace\"]\n",
    "y_train_cls = pd.Categorical(df_train[OP_COL], categories=class_names).codes\n",
    "assert (y_train_cls >= 0).all(), \"Found an operator not in FIXED_CLASSES\"\n",
    "n_classes = len(class_names)\n",
    "\n",
    "\n",
    "print(\"Operator classes:\", list(class_names))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Class weights (operator imbalance)\n",
    "# ------------------------------------------------------------\n",
    "classes = np.arange(n_classes)\n",
    "\n",
    "cls_w = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=y_train_cls\n",
    ")\n",
    "cls_w = np.asarray(cls_w, dtype=\"float64\")\n",
    "\n",
    "class_weight_map = dict(zip(class_names, cls_w))\n",
    "print(\"Class weights:\", class_weight_map)\n",
    "\n",
    "# per-sample class weight\n",
    "w_class = np.array([cls_w[c] for c in y_train_cls], dtype=\"float64\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Map-level weighting (prompt multiplicity correction)\n",
    "# ------------------------------------------------------------\n",
    "map_counts = df_train[\"map_id\"].value_counts()\n",
    "\n",
    "# each map contributes ~1 total weight\n",
    "w_map = df_train[\"map_id\"].map(lambda m: 1.0 / map_counts[m]).to_numpy(dtype=\"float64\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Final sample weights\n",
    "# ------------------------------------------------------------\n",
    "sample_w = w_class * w_map\n",
    "\n",
    "print(\n",
    "    \"Sample weight summary:\",\n",
    "    {\n",
    "        \"min\": float(sample_w.min()),\n",
    "        \"max\": float(sample_w.max()),\n",
    "        \"mean\": float(sample_w.mean()),\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf3c9cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes (fixed order): [np.str_('simplify'), np.str_('select'), np.str_('aggregate'), np.str_('displace')]\n",
      "y_train_cls shape: (448,)\n",
      "y_val_cls shape: (57,)\n",
      "y_test_cls shape: (57,)\n"
     ]
    }
   ],
   "source": [
    "# === BUILD CLASS LABELS (train / val / test) ===\n",
    "\n",
    "OP_COL = PATHS.OPERATOR_COL  # \"operator\"\n",
    "\n",
    "# Fixed, global class order (MUST match training + bundle)\n",
    "class_names = np.array(FIXED_CLASSES)\n",
    "\n",
    "y_train_cls = pd.Categorical(\n",
    "    df_train[OP_COL],\n",
    "    categories=class_names\n",
    ").codes\n",
    "\n",
    "y_val_cls = pd.Categorical(\n",
    "    df_val[OP_COL],\n",
    "    categories=class_names\n",
    ").codes\n",
    "\n",
    "y_test_cls = pd.Categorical(\n",
    "    df_test[OP_COL],\n",
    "    categories=class_names\n",
    ").codes\n",
    "\n",
    "# Safety checks\n",
    "assert (y_train_cls >= 0).all(), \"TRAIN contains unseen operator labels\"\n",
    "assert (y_val_cls >= 0).all(), \"VAL contains unseen operator labels\"\n",
    "assert (y_test_cls >= 0).all(), \"TEST contains unseen operator labels\"\n",
    "\n",
    "print(\"Classes (fixed order):\", list(class_names))\n",
    "print(\"y_train_cls shape:\", y_train_cls.shape)\n",
    "print(\"y_val_cls shape:\", y_val_cls.shape)\n",
    "print(\"y_test_cls shape:\", y_test_cls.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988f274",
   "metadata": {},
   "source": [
    "## üß† 8) Train MLP ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95133825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching 50 MLP configs...\n",
      "[01/50] cvF1=0.717¬±0.087 | VAL F1=0.830 acc=0.842 | (128, 64), Œ±=2.02e-02, lr=1.2e-03, bs=16\n",
      "[02/50] cvF1=0.794¬±0.093 | VAL F1=0.944 acc=0.947 | (256, 128), Œ±=3.49e-05, lr=1.7e-04, bs=64\n",
      "[03/50] cvF1=0.710¬±0.113 | VAL F1=0.830 acc=0.842 | (256,), Œ±=1.03e-02, lr=7.7e-04, bs=128\n",
      "[04/50] cvF1=0.784¬±0.086 | VAL F1=0.920 acc=0.930 | (256,), Œ±=1.18e-05, lr=2.7e-03, bs=128\n",
      "[05/50] cvF1=0.798¬±0.077 | VAL F1=0.903 acc=0.912 | (256, 128, 64), Œ±=5.47e-05, lr=1.9e-04, bs=16\n",
      "[06/50] cvF1=0.814¬±0.064 | VAL F1=0.944 acc=0.947 | (64,), Œ±=1.14e-04, lr=6.0e-04, bs=128\n",
      "[07/50] cvF1=0.818¬±0.074 | VAL F1=0.944 acc=0.947 | (64,), Œ±=1.03e-04, lr=8.0e-04, bs=32\n",
      "[08/50] cvF1=0.662¬±0.122 | VAL F1=0.808 acc=0.825 | (128, 64), Œ±=2.43e-02, lr=2.2e-04, bs=32\n",
      "[09/50] cvF1=0.782¬±0.083 | VAL F1=0.940 acc=0.947 | (256, 128, 64), Œ±=4.95e-05, lr=5.7e-04, bs=128\n",
      "[10/50] cvF1=0.834¬±0.069 | VAL F1=0.944 acc=0.947 | (64,), Œ±=1.45e-05, lr=7.9e-04, bs=16\n",
      "[11/50] cvF1=0.821¬±0.063 | VAL F1=0.927 acc=0.930 | (64,), Œ±=1.68e-05, lr=2.5e-03, bs=128\n",
      "[12/50] cvF1=0.729¬±0.095 | VAL F1=0.827 acc=0.842 | (256, 128, 64), Œ±=6.47e-03, lr=2.8e-04, bs=16\n",
      "[13/50] cvF1=0.789¬±0.078 | VAL F1=0.903 acc=0.912 | (128,), Œ±=2.39e-03, lr=4.5e-04, bs=64\n",
      "[14/50] cvF1=0.812¬±0.065 | VAL F1=0.927 acc=0.930 | (128, 64), Œ±=5.27e-04, lr=1.1e-04, bs=32\n",
      "[15/50] cvF1=0.813¬±0.067 | VAL F1=0.944 acc=0.947 | (64,), Œ±=7.94e-05, lr=9.5e-04, bs=32\n",
      "[16/50] cvF1=0.735¬±0.087 | VAL F1=0.903 acc=0.912 | (256, 128, 64), Œ±=6.43e-04, lr=6.4e-04, bs=32\n",
      "[17/50] cvF1=0.643¬±0.097 | VAL F1=0.852 acc=0.860 | (256, 128), Œ±=2.35e-02, lr=1.4e-03, bs=32\n",
      "[18/50] cvF1=0.714¬±0.117 | VAL F1=0.829 acc=0.842 | (128,), Œ±=1.29e-02, lr=7.6e-04, bs=128\n",
      "[19/50] cvF1=0.790¬±0.084 | VAL F1=0.917 acc=0.930 | (256, 128, 64), Œ±=4.80e-05, lr=1.2e-04, bs=128\n",
      "[20/50] cvF1=0.786¬±0.080 | VAL F1=0.920 acc=0.930 | (256, 128), Œ±=2.25e-04, lr=2.5e-04, bs=16\n",
      "[21/50] cvF1=0.624¬±0.104 | VAL F1=0.755 acc=0.772 | (128,), Œ±=2.27e-02, lr=7.9e-04, bs=16\n",
      "[22/50] cvF1=0.827¬±0.062 | VAL F1=0.920 acc=0.930 | (64,), Œ±=1.07e-04, lr=1.8e-04, bs=16\n",
      "[23/50] cvF1=0.804¬±0.070 | VAL F1=0.944 acc=0.947 | (64,), Œ±=4.84e-03, lr=2.0e-04, bs=128\n",
      "[24/50] cvF1=0.822¬±0.072 | VAL F1=0.920 acc=0.930 | (256,), Œ±=4.91e-05, lr=1.1e-03, bs=64\n",
      "[25/50] cvF1=0.656¬±0.103 | VAL F1=0.773 acc=0.789 | (64,), Œ±=1.28e-03, lr=2.3e-03, bs=32\n",
      "[26/50] cvF1=0.679¬±0.121 | VAL F1=0.793 acc=0.807 | (64,), Œ±=1.52e-02, lr=1.8e-03, bs=128\n",
      "[27/50] cvF1=0.817¬±0.053 | VAL F1=0.944 acc=0.947 | (128,), Œ±=2.15e-05, lr=3.5e-04, bs=32\n",
      "[28/50] cvF1=0.683¬±0.122 | VAL F1=0.863 acc=0.877 | (256, 128), Œ±=3.44e-03, lr=8.7e-04, bs=64\n",
      "[29/50] cvF1=0.817¬±0.084 | VAL F1=0.920 acc=0.930 | (256,), Œ±=4.38e-04, lr=1.5e-04, bs=32\n",
      "[30/50] cvF1=0.699¬±0.116 | VAL F1=0.812 acc=0.825 | (256,), Œ±=4.42e-03, lr=6.7e-04, bs=64\n",
      "[31/50] cvF1=0.793¬±0.081 | VAL F1=0.940 acc=0.947 | (256, 128, 64), Œ±=5.21e-04, lr=5.9e-04, bs=64\n",
      "[32/50] cvF1=0.822¬±0.045 | VAL F1=0.927 acc=0.930 | (128,), Œ±=1.23e-05, lr=1.4e-04, bs=64\n",
      "[33/50] cvF1=0.832¬±0.064 | VAL F1=0.944 acc=0.947 | (64,), Œ±=1.24e-04, lr=5.6e-04, bs=32\n",
      "[34/50] cvF1=0.786¬±0.094 | VAL F1=0.962 acc=0.965 | (256, 128), Œ±=7.36e-05, lr=4.0e-04, bs=64\n",
      "[35/50] cvF1=0.793¬±0.096 | VAL F1=0.944 acc=0.947 | (256, 128), Œ±=6.25e-05, lr=1.3e-04, bs=64\n",
      "[36/50] cvF1=0.791¬±0.082 | VAL F1=0.920 acc=0.930 | (256,), Œ±=3.64e-05, lr=2.4e-03, bs=16\n",
      "[37/50] cvF1=0.752¬±0.091 | VAL F1=0.866 acc=0.877 | (256, 128, 64), Œ±=1.59e-03, lr=1.9e-03, bs=128\n",
      "[38/50] cvF1=0.801¬±0.055 | VAL F1=0.927 acc=0.930 | (128, 64), Œ±=4.45e-05, lr=2.1e-03, bs=64\n",
      "[39/50] cvF1=0.817¬±0.053 | VAL F1=0.944 acc=0.947 | (128,), Œ±=2.66e-05, lr=3.4e-04, bs=32\n",
      "[40/50] cvF1=0.832¬±0.070 | VAL F1=0.920 acc=0.930 | (64,), Œ±=8.84e-05, lr=9.1e-04, bs=16\n",
      "[41/50] cvF1=0.803¬±0.063 | VAL F1=0.927 acc=0.930 | (128, 64), Œ±=1.68e-04, lr=2.8e-04, bs=32\n",
      "[42/50] cvF1=0.818¬±0.064 | VAL F1=0.944 acc=0.947 | (256,), Œ±=2.83e-04, lr=2.1e-04, bs=64\n",
      "[43/50] cvF1=0.813¬±0.063 | VAL F1=0.920 acc=0.930 | (128,), Œ±=1.49e-04, lr=2.5e-03, bs=32\n",
      "[44/50] cvF1=0.819¬±0.065 | VAL F1=0.944 acc=0.947 | (64,), Œ±=2.54e-04, lr=1.2e-04, bs=32\n",
      "[45/50] cvF1=0.807¬±0.067 | VAL F1=0.927 acc=0.930 | (128, 64), Œ±=7.22e-05, lr=1.1e-03, bs=64\n",
      "[46/50] cvF1=0.817¬±0.062 | VAL F1=0.944 acc=0.947 | (64,), Œ±=3.27e-05, lr=3.0e-03, bs=64\n",
      "[47/50] cvF1=0.697¬±0.088 | VAL F1=0.810 acc=0.825 | (64,), Œ±=2.49e-02, lr=4.0e-04, bs=64\n",
      "[48/50] cvF1=0.789¬±0.093 | VAL F1=0.962 acc=0.965 | (256, 128), Œ±=1.58e-04, lr=8.6e-04, bs=32\n",
      "[49/50] cvF1=0.789¬±0.080 | VAL F1=0.920 acc=0.930 | (128,), Œ±=7.02e-04, lr=4.6e-04, bs=32\n",
      "[50/50] cvF1=0.808¬±0.080 | VAL F1=0.944 acc=0.947 | (128, 64), Œ±=1.91e-05, lr=3.5e-04, bs=16\n",
      "\n",
      "=== Top candidates (by VAL macro-F1) ===\n",
      "VAL F1=0.962 (acc=0.965) | cvF1=0.789¬±0.093 | params={'hidden_layer_sizes': (256, 128), 'alpha': 0.0001584362559438067, 'learning_rate_init': 0.0008649955121393716, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.962 (acc=0.965) | cvF1=0.786¬±0.094 | params={'hidden_layer_sizes': (256, 128), 'alpha': 7.35900856867979e-05, 'learning_rate_init': 0.0004038176882071837, 'batch_size': 64, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.944 (acc=0.947) | cvF1=0.834¬±0.069 | params={'hidden_layer_sizes': (64,), 'alpha': 1.4504865877614242e-05, 'learning_rate_init': 0.0007896186801026691, 'batch_size': 16, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.944 (acc=0.947) | cvF1=0.832¬±0.064 | params={'hidden_layer_sizes': (64,), 'alpha': 0.00012389502377355922, 'learning_rate_init': 0.0005639239991667559, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.944 (acc=0.947) | cvF1=0.819¬±0.065 | params={'hidden_layer_sizes': (64,), 'alpha': 0.0002536222399285845, 'learning_rate_init': 0.00012469634313376035, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "\n",
      "üèÜ Selected params:\n",
      "{'activation': 'relu',\n",
      " 'alpha': 0.0001584362559438067,\n",
      " 'batch_size': 32,\n",
      " 'early_stopping': False,\n",
      " 'hidden_layer_sizes': (256, 128),\n",
      " 'learning_rate_init': 0.0008649955121393716,\n",
      " 'max_iter': 800,\n",
      " 'random_state': 42,\n",
      " 'solver': 'adam',\n",
      " 'tol': 0.0001,\n",
      " 'verbose': False}\n",
      "\n",
      "===== VAL =====\n",
      "VAL: acc=0.9649  f1_macro=0.9616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    simplify       0.93      1.00      0.96        13\n",
      "      select       0.95      1.00      0.97        19\n",
      "   aggregate       1.00      0.94      0.97        16\n",
      "    displace       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.96        57\n",
      "   macro avg       0.97      0.96      0.96        57\n",
      "weighted avg       0.97      0.96      0.96        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[13  0  0  0]\n",
      " [ 0 19  0  0]\n",
      " [ 1  0 15  0]\n",
      " [ 0  1  0  8]]\n",
      "\n",
      "===== TEST =====\n",
      "TEST: acc=0.8421  f1_macro=0.8519\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    simplify       0.73      0.85      0.79        13\n",
      "      select       0.94      0.79      0.86        19\n",
      "   aggregate       0.78      0.88      0.82        16\n",
      "    displace       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.84        57\n",
      "   macro avg       0.86      0.85      0.85        57\n",
      "weighted avg       0.86      0.84      0.84        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[11  0  2  0]\n",
      " [ 2 15  2  0]\n",
      " [ 1  1 14  0]\n",
      " [ 1  0  0  8]]\n",
      "\n",
      "‚úÖ Saved final MLP (trained on ALL TRAIN) to: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/best_mlp_fulltrain.joblib\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# MLP search where each model trains on ALL training data\n",
    "# =========================\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# ---- numerics: keep float64 everywhere ----\n",
    "X_train_s = X_train_s.astype(np.float64, copy=False)\n",
    "X_val_s   = X_val_s.astype(np.float64, copy=False)\n",
    "X_test_s  = X_test_s.astype(np.float64, copy=False)\n",
    "sample_w  = sample_w.astype(np.float64, copy=False)\n",
    "\n",
    "# ---- group by map_id (maps can repeat; prompts don't) ----\n",
    "assert \"map_id\" in df_train.columns, \"df_train must contain 'map_id' for grouped CV.\"\n",
    "groups_tr = df_train[\"map_id\"].astype(str).values\n",
    "# ---- fixed class order (must match how y_*_cls were created) ----\n",
    "class_names = np.array(FIXED_CLASSES)\n",
    "\n",
    "\n",
    "# ---- CV splitter (for scoring only) ----\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ---- search space helpers ----\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "def draw_params(n):\n",
    "    sizes = [(64,), (128,), (256,), (128, 64), (256, 128), (256, 128, 64)]\n",
    "    batches = [16, 32, 64, 128]\n",
    "    for _ in range(n):\n",
    "        yield {\n",
    "            \"hidden_layer_sizes\": sizes[rng.randint(len(sizes))],\n",
    "            \"alpha\": 10**rng.uniform(-5, np.log10(3e-2)),          # loguniform(1e-5, 3e-2)\n",
    "            \"learning_rate_init\": 10**rng.uniform(-4, np.log10(3e-3)),  # loguniform(1e-4, 3e-3)\n",
    "            \"batch_size\": batches[rng.randint(len(batches))],\n",
    "            \"activation\": \"relu\",\n",
    "            \"solver\": \"adam\",\n",
    "            \"max_iter\": 800,            # allow convergence w/o early stopping\n",
    "            \"early_stopping\": False,    # <‚Äî IMPORTANT: use ALL training samples\n",
    "            \"random_state\": 42,\n",
    "            \"verbose\": False,\n",
    "            \"tol\": 1e-4\n",
    "        }\n",
    "\n",
    "# ---- CV scorer using grouped folds; model sees only its fold-train here (for the score only) ----\n",
    "def cv_macro_f1(params):\n",
    "    scores = []\n",
    "    for tr_idx, va_idx in cv.split(X_train_s, y_train_cls, groups_tr):\n",
    "        clf = MLPClassifier(**params)\n",
    "        clf.fit(X_train_s[tr_idx], y_train_cls[tr_idx], sample_weight=sample_w[tr_idx])\n",
    "        pred = clf.predict(X_train_s[va_idx])\n",
    "        scores.append(f1_score(y_train_cls[va_idx], pred, average=\"macro\"))\n",
    "    return float(np.mean(scores)), float(np.std(scores))\n",
    "\n",
    "@dataclass\n",
    "class Candidate:\n",
    "    params: dict\n",
    "    cv_mean: float\n",
    "    cv_std: float\n",
    "    val_f1: float\n",
    "    val_acc: float\n",
    "\n",
    "# ---- run search ----\n",
    "N_ITER = 50   # tune this for time/quality tradeoff\n",
    "candidates = []\n",
    "\n",
    "print(f\"\\nSearching {N_ITER} MLP configs...\")\n",
    "for i, params in enumerate(draw_params(N_ITER), 1):\n",
    "    cv_mean, cv_std = cv_macro_f1(params)\n",
    "\n",
    "    # IMPORTANT PART: refit SAME PARAMS on FULL TRAIN (no early_stopping) so the model sees ALL training data\n",
    "    clf_full = MLPClassifier(**params)\n",
    "    clf_full.fit(X_train_s, y_train_cls, sample_weight=sample_w)\n",
    "\n",
    "    # evaluate on external VAL (never used for training)\n",
    "    val_pred = clf_full.predict(X_val_s)\n",
    "    val_f1 = f1_score(y_val_cls, val_pred, average=\"macro\")\n",
    "    val_acc = accuracy_score(y_val_cls, val_pred)\n",
    "\n",
    "    candidates.append(Candidate(params, cv_mean, cv_std, val_f1, val_acc))\n",
    "    print(f\"[{i:02d}/{N_ITER}] cvF1={cv_mean:.3f}¬±{cv_std:.3f} | VAL F1={val_f1:.3f} acc={val_acc:.3f} | {params['hidden_layer_sizes']}, Œ±={params['alpha']:.2e}, lr={params['learning_rate_init']:.1e}, bs={params['batch_size']}\")\n",
    "\n",
    "# ---- pick winner by external VAL macro-F1 (tie-breaker: VAL acc, then CV mean) ----\n",
    "candidates.sort(key=lambda c: (c.val_f1, c.val_acc, c.cv_mean), reverse=True)\n",
    "best = candidates[0]\n",
    "print(\"\\n=== Top candidates (by VAL macro-F1) ===\")\n",
    "for c in candidates[:5]:\n",
    "    print(f\"VAL F1={c.val_f1:.3f} (acc={c.val_acc:.3f}) | cvF1={c.cv_mean:.3f}¬±{c.cv_std:.3f} | params={c.params}\")\n",
    "\n",
    "print(\"\\nüèÜ Selected params:\")\n",
    "pprint(best.params)\n",
    "\n",
    "# ---- train final model on FULL TRAIN (no early_stopping so it uses 100% of train) ----\n",
    "final_mlp = MLPClassifier(**best.params)\n",
    "final_mlp.fit(X_train_s, y_train_cls, sample_weight=sample_w)\n",
    "\n",
    "# ---- evaluate on VAL & TEST ----\n",
    "for name, Xs, ys in [(\"VAL\", X_val_s, y_val_cls), (\"TEST\", X_test_s, y_test_cls)]:\n",
    "    yhat = final_mlp.predict(Xs)\n",
    "    acc  = accuracy_score(ys, yhat)\n",
    "    f1m  = f1_score(ys, yhat, average=\"macro\")\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    print(f\"{name}: acc={acc:.4f}  f1_macro={f1m:.4f}\")\n",
    "    print(classification_report(ys, yhat, labels=np.arange(len(class_names)), target_names=list(class_names)))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(ys, yhat))\n",
    "\n",
    "# ---- save final model ----\n",
    "out_dir = Path(PATHS.TRAIN_OUT); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "import joblib\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"model\": final_mlp,\n",
    "        \"class_names\": list(class_names),\n",
    "        \"best_params\": best.params,\n",
    "    },\n",
    "    out_dir / \"best_mlp_fulltrain.joblib\"\n",
    ")\n",
    "print(f\"\\n‚úÖ Saved final MLP (trained on ALL TRAIN) to: {out_dir / 'best_mlp_fulltrain.joblib'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea7905b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'simplify' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 1.2821859869441417\n",
      "best CV RMSE (param_norm units): 0.0051663700097502215\n",
      "best params:\n",
      "{'alpha': np.float64(0.0041619125396912095),\n",
      " 'hidden_layer_sizes': (64,),\n",
      " 'learning_rate_init': np.float64(0.00010558059144381523)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'select' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 0.679893022270926\n",
      "best CV RMSE (param_norm units): 0.00026332285975812415\n",
      "best params:\n",
      "{'alpha': np.float64(2.1453931225439485e-06),\n",
      " 'hidden_layer_sizes': (128,),\n",
      " 'learning_rate_init': np.float64(0.0001483039268456802)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'aggregate' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 1.0448835330852817\n",
      "best CV RMSE (param_norm units): 0.003430202896463292\n",
      "best params:\n",
      "{'alpha': np.float64(0.0041619125396912095),\n",
      " 'hidden_layer_sizes': (64,),\n",
      " 'learning_rate_init': np.float64(0.00010558059144381523)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'displace' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 0.8405509489016343\n",
      "best CV RMSE (param_norm units): 0.003117596338277\n",
      "best params:\n",
      "{'alpha': np.float64(2.1453931225439485e-06),\n",
      " 'hidden_layer_sizes': (128,),\n",
      " 'learning_rate_init': np.float64(0.0001483039268456802)}\n",
      "\n",
      "--- Regression with predicted classes (realistic) ---\n",
      "VAL: MAE=10.9643  RMSE=22.6260\n",
      "TEST: MAE=12.3290  RMSE=24.9179\n",
      "\n",
      "--- Regression with TRUE classes (oracle routing) ---\n",
      "VAL-oracle: MAE=11.0287  RMSE=22.6312\n",
      "TEST-oracle: MAE=11.3786  RMSE=22.5569\n",
      "\n",
      "‚úÖ Saved classification+regression bundle to: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/cls_plus_regressors.joblib\n",
      "\n",
      "--- Regression with TRUE classes (oracle routing) ---\n",
      "VAL-oracle: MAE=11.0287  RMSE=22.6312\n",
      "TEST-oracle: MAE=11.3786  RMSE=22.5569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11.37863762165307, 22.556855347571947)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Regression branch (one MLPRegressor per operator) ‚Äî DYNAMIC EXTENT VERSION\n",
    "# =========================\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import joblib\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "# -------------------------\n",
    "# 0) Targets: use param_norm (created earlier) + keep raw param_value for reporting\n",
    "# -------------------------\n",
    "OP_COL    = PATHS.OPERATOR_COL        # \"operator\"\n",
    "PARAM_COL = PATHS.PARAM_VALUE_COL     # \"param_value\"\n",
    "\n",
    "# FIX: enforce the same class order everywhere\n",
    "class_names = np.array(FIXED_CLASSES)\n",
    "\n",
    "# FIX: ensure labels use SAME class order\n",
    "y_train_cls = pd.Categorical(df_train[OP_COL], categories=class_names).codes\n",
    "y_val_cls   = pd.Categorical(df_val[OP_COL], categories=class_names).codes\n",
    "y_test_cls  = pd.Categorical(df_test[OP_COL], categories=class_names).codes\n",
    "\n",
    "# Safety checks (no unseen labels)\n",
    "assert (y_train_cls >= 0).all(), \"TRAIN contains unseen operator labels (check FIXED_CLASSES)\"\n",
    "assert (y_val_cls   >= 0).all(), \"VAL contains unseen operator labels (check FIXED_CLASSES)\"\n",
    "assert (y_test_cls  >= 0).all(), \"TEST contains unseen operator labels (check FIXED_CLASSES)\"\n",
    "\n",
    "# param_norm must exist from the edited LOAD-FUSED cell\n",
    "assert \"param_norm\" in df_train.columns and \"param_norm\" in df_val.columns and \"param_norm\" in df_test.columns, \\\n",
    "    \"Missing param_norm in df_* (make sure you computed it in the earlier cell).\"\n",
    "\n",
    "# Dynamic per-map refs must exist (merged from maps.parquet into train_pairs.parquet)\n",
    "REQ_EXT = [\"extent_diag_m\", \"extent_area_m2\"]\n",
    "for split_name, dfx in [(\"df_train\", df_train), (\"df_val\", df_val), (\"df_test\", df_test)]:\n",
    "    missing = [c for c in REQ_EXT if c not in dfx.columns]\n",
    "    assert not missing, f\"Missing {missing} in {split_name}. Check concat merged maps.parquet extent_* columns.\"\n",
    "    dfx[\"extent_diag_m\"]  = pd.to_numeric(dfx[\"extent_diag_m\"], errors=\"coerce\")\n",
    "    dfx[\"extent_area_m2\"] = pd.to_numeric(dfx[\"extent_area_m2\"], errors=\"coerce\")\n",
    "\n",
    "y_train_norm = pd.to_numeric(df_train[\"param_norm\"], errors=\"coerce\").to_numpy()\n",
    "y_val_norm   = pd.to_numeric(df_val[\"param_norm\"], errors=\"coerce\").to_numpy()\n",
    "y_test_norm  = pd.to_numeric(df_test[\"param_norm\"], errors=\"coerce\").to_numpy()\n",
    "\n",
    "y_val_raw  = pd.to_numeric(df_val[PARAM_COL], errors=\"coerce\").to_numpy()\n",
    "y_test_raw = pd.to_numeric(df_test[PARAM_COL], errors=\"coerce\").to_numpy()\n",
    "\n",
    "assert np.isfinite(y_train_norm).all() and np.isfinite(y_val_norm).all() and np.isfinite(y_test_norm).all(), \\\n",
    "    \"Non-finite values found in param_norm. Check your normalization step.\"\n",
    "assert np.isfinite(y_val_raw).all() and np.isfinite(y_test_raw).all(), \\\n",
    "    \"Non-finite values found in param_value in val/test. Check Excel parsing.\"\n",
    "\n",
    "# Optional: log1p on normalized values (usually not needed after normalization)\n",
    "USE_LOG1P = False\n",
    "if USE_LOG1P:\n",
    "    assert (y_train_norm >= 0).all() and (y_val_norm >= 0).all() and (y_test_norm >= 0).all(), \\\n",
    "        \"log1p selected but param_norm has negatives.\"\n",
    "    ytr_t = np.log1p(y_train_norm)\n",
    "    yva_t = np.log1p(y_val_norm)\n",
    "    yte_t = np.log1p(y_test_norm)\n",
    "    def inv_t(x): return np.expm1(x)\n",
    "else:\n",
    "    ytr_t = y_train_norm.copy()\n",
    "    yva_t = y_val_norm.copy()\n",
    "    yte_t = y_test_norm.copy()\n",
    "    def inv_t(x): return x\n",
    "\n",
    "# -------------------------\n",
    "# 1) Grouped CV by map_id (no leakage)\n",
    "# -------------------------\n",
    "assert \"map_id\" in df_train.columns\n",
    "gk = GroupKFold(n_splits=5)\n",
    "groups_tr = df_train[\"map_id\"].astype(str).values\n",
    "\n",
    "# -------------------------\n",
    "# 2) Search space for MLPRegressor\n",
    "# -------------------------\n",
    "base_reg = MLPRegressor(\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    learning_rate=\"adaptive\",\n",
    "    early_stopping=False,\n",
    "    max_iter=2000,\n",
    "    tol=1e-3,\n",
    "    random_state=42,\n",
    "    verbose=False,\n",
    "    batch_size=\"auto\"\n",
    ")\n",
    "\n",
    "param_dist_reg = {\n",
    "    \"hidden_layer_sizes\": [(64,), (128,), (256,), (128, 64), (256, 128)],\n",
    "    \"alpha\": loguniform(1e-6, 3e-2),\n",
    "    \"learning_rate_init\": loguniform(1e-4, 3e-3),\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# 3) Fit one regressor per operator (on normalized target)\n",
    "# -------------------------\n",
    "regressors = {}\n",
    "search_summaries = {}\n",
    "\n",
    "for cls_idx, cls_name in enumerate(class_names):\n",
    "    cls_name = str(cls_name)  # ‚úÖ REQUIRED FIX: ensure dict keys are normal Python strings\n",
    "\n",
    "    m_tr = (y_train_cls == cls_idx)\n",
    "\n",
    "    Xk = X_train_s[m_tr]\n",
    "    yk = ytr_t[m_tr]\n",
    "    gk_tr = groups_tr[m_tr]\n",
    "    wk = sample_w[m_tr]\n",
    "\n",
    "    if Xk.shape[0] < 10:\n",
    "        print(f\"‚ö†Ô∏è Skipping class '{cls_name}' (too few samples: {Xk.shape[0]}).\")\n",
    "        continue\n",
    "\n",
    "    t_scaler = StandardScaler()\n",
    "    yk_s = t_scaler.fit_transform(yk.reshape(-1, 1)).ravel()\n",
    "\n",
    "    splits = list(gk.split(Xk, yk_s, groups=gk_tr))\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base_reg,\n",
    "        param_distributions=param_dist_reg,\n",
    "        n_iter=40,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        cv=splits,\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    search.fit(Xk, yk_s, sample_weight=wk)\n",
    "\n",
    "    rmse_scaled = -search.best_score_\n",
    "    rmse_norm_units = rmse_scaled * float(t_scaler.scale_[0])\n",
    "\n",
    "    print(f\"\\n=== Regressor for class '{cls_name}' (predicting param_norm) ===\")\n",
    "    print(\"best CV RMSE (scaled):\", rmse_scaled)\n",
    "    print(\"best CV RMSE (param_norm units):\", rmse_norm_units)\n",
    "    print(\"best params:\"); pprint(search.best_params_)\n",
    "\n",
    "    search_summaries[cls_name] = {\n",
    "        \"rmse_scaled\": float(rmse_scaled),\n",
    "        \"rmse_param_norm\": float(rmse_norm_units),\n",
    "        \"params\": search.best_params_\n",
    "    }\n",
    "\n",
    "    reg_full = MLPRegressor(\n",
    "        **{**search.best_estimator_.get_params(), \"early_stopping\": False, \"max_iter\": 2000, \"random_state\": 42}\n",
    "    )\n",
    "    reg_full.fit(Xk, yk_s, sample_weight=wk)\n",
    "\n",
    "    regressors[cls_name] = (reg_full, t_scaler)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Routing + prediction: output REAL param_value using DYNAMIC extents\n",
    "# -------------------------\n",
    "DIST_OPS_SET = set(DISTANCE_OPS)\n",
    "AREA_OPS_SET = set(AREA_OPS)\n",
    "\n",
    "def route_and_predict_param_value(Xs, df_s, pred_cls_idx):\n",
    "    \"\"\"\n",
    "    Predict param_value in original units (meters or m¬≤).\n",
    "    Regressors predict param_norm; then:\n",
    "      - inverse StandardScaler\n",
    "      - inverse log1p (if used)\n",
    "      - unnormalize with per-row dynamic extent: diag for distance ops, area for select\n",
    "    \"\"\"\n",
    "    yhat = np.full(len(pred_cls_idx), np.nan, dtype=float)\n",
    "\n",
    "    extent_diag = pd.to_numeric(df_s[\"extent_diag_m\"], errors=\"coerce\").to_numpy(dtype=float)\n",
    "    extent_area = pd.to_numeric(df_s[\"extent_area_m2\"], errors=\"coerce\").to_numpy(dtype=float)\n",
    "\n",
    "    for i, cidx in enumerate(pred_cls_idx):\n",
    "        cname = str(class_names[int(cidx)])\n",
    "        pack = regressors.get(cname)\n",
    "        if pack is None:\n",
    "            continue\n",
    "\n",
    "        reg, t_scaler = pack\n",
    "        pred_scaled = reg.predict(Xs[i:i+1])[0]\n",
    "        pred_t = t_scaler.inverse_transform([[pred_scaled]])[0, 0]  # back to ytr_t units\n",
    "        pred_norm = inv_t(pred_t)\n",
    "\n",
    "        # OPTIONAL safety: clamp negatives\n",
    "        pred_norm = max(0.0, float(pred_norm))\n",
    "\n",
    "        if cname in DIST_OPS_SET:\n",
    "            if np.isfinite(extent_diag[i]) and extent_diag[i] > 0:\n",
    "                yhat[i] = pred_norm * extent_diag[i]\n",
    "        elif cname in AREA_OPS_SET:\n",
    "            if np.isfinite(extent_area[i]) and extent_area[i] > 0:\n",
    "                yhat[i] = pred_norm * extent_area[i]\n",
    "        else:\n",
    "            yhat[i] = np.nan\n",
    "\n",
    "    return yhat\n",
    "\n",
    "def print_reg_metrics(name, y_true_raw, y_pred_raw):\n",
    "    mask = np.isfinite(y_true_raw) & np.isfinite(y_pred_raw)\n",
    "    if mask.sum() == 0:\n",
    "        print(f\"{name}: no finite pairs to evaluate.\")\n",
    "        return np.nan, np.nan\n",
    "    if mask.sum() < len(y_true_raw):\n",
    "        print(f\"{name}: dropped {len(y_true_raw) - mask.sum()} samples with NaNs.\")\n",
    "\n",
    "    yt = y_true_raw[mask]\n",
    "    yp = y_pred_raw[mask]\n",
    "\n",
    "    mae = mean_absolute_error(yt, yp)\n",
    "    rmse = float(np.sqrt(mean_squared_error(yt, yp)))\n",
    "    print(f\"{name}: MAE={mae:.4f}  RMSE={rmse:.4f}\")\n",
    "    return mae, rmse\n",
    "\n",
    "# Classification predictions (already trained classifier)\n",
    "clf_cls = final_mlp\n",
    "val_pred_cls  = clf_cls.predict(X_val_s)\n",
    "test_pred_cls = clf_cls.predict(X_test_s)\n",
    "\n",
    "# Predict param_value (real units)\n",
    "yhat_val  = route_and_predict_param_value(X_val_s,  df_val,  val_pred_cls)\n",
    "yhat_test = route_and_predict_param_value(X_test_s, df_test, test_pred_cls)\n",
    "\n",
    "print(\"\\n--- Regression with predicted classes (realistic) ---\")\n",
    "print_reg_metrics(\"VAL\",  y_val_raw,  yhat_val)\n",
    "print_reg_metrics(\"TEST\", y_test_raw, yhat_test)\n",
    "\n",
    "# Oracle routing (true operator)\n",
    "yhat_val_or  = route_and_predict_param_value(X_val_s,  df_val,  y_val_cls)\n",
    "yhat_test_or = route_and_predict_param_value(X_test_s, df_test, y_test_cls)\n",
    "\n",
    "print(\"\\n--- Regression with TRUE classes (oracle routing) ---\")\n",
    "print_reg_metrics(\"VAL-oracle\",  y_val_raw,  yhat_val_or)\n",
    "print_reg_metrics(\"TEST-oracle\", y_test_raw, yhat_test_or)\n",
    "\n",
    "# -------------------------\n",
    "# 5) Save bundle (include normalization metadata)\n",
    "# -------------------------\n",
    "bundle = {\n",
    "    \"classifier\": clf_cls,\n",
    "    \"regressors_by_class\": regressors,\n",
    "    \"class_names\": list(map(str, class_names)),\n",
    "    \"use_log1p\": USE_LOG1P,\n",
    "    \"target\": \"param_norm\",\n",
    "    \"normalization\": {\n",
    "        \"type\": \"dynamic_extent\",\n",
    "        \"distance_ops\": list(DISTANCE_OPS),\n",
    "        \"area_ops\": list(AREA_OPS),\n",
    "        \"distance_ref_col\": \"extent_diag_m\",\n",
    "        \"area_ref_col\": \"extent_area_m2\",\n",
    "    },\n",
    "    \"cv_summary\": search_summaries,\n",
    "}\n",
    "\n",
    "out_dir = Path(PATHS.TRAIN_OUT)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(bundle, out_dir / \"cls_plus_regressors.joblib\")\n",
    "print(f\"\\n‚úÖ Saved classification+regression bundle to: {out_dir / 'cls_plus_regressors.joblib'}\")\n",
    "print(\"\\n--- Regression with TRUE classes (oracle routing) ---\")\n",
    "print_reg_metrics(\"VAL-oracle\",  y_val_raw,  yhat_val_or)\n",
    "print_reg_metrics(\"TEST-oracle\", y_test_raw, yhat_test_or)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d27e3ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>operator</th>\n",
       "      <th>n</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_oracle_by_true_op</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>22</td>\n",
       "      <td>1.647941</td>\n",
       "      <td>2.411753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_oracle_by_true_op</td>\n",
       "      <td>displace</td>\n",
       "      <td>12</td>\n",
       "      <td>2.026431</td>\n",
       "      <td>2.185604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_oracle_by_true_op</td>\n",
       "      <td>select</td>\n",
       "      <td>25</td>\n",
       "      <td>30.494501</td>\n",
       "      <td>39.160429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_oracle_by_true_op</td>\n",
       "      <td>simplify</td>\n",
       "      <td>17</td>\n",
       "      <td>2.461298</td>\n",
       "      <td>2.929820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_realistic_by_pred_op</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>21</td>\n",
       "      <td>1.691911</td>\n",
       "      <td>2.463439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TEST_realistic_by_pred_op</td>\n",
       "      <td>displace</td>\n",
       "      <td>12</td>\n",
       "      <td>2.026431</td>\n",
       "      <td>2.185604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TEST_realistic_by_pred_op</td>\n",
       "      <td>select</td>\n",
       "      <td>24</td>\n",
       "      <td>30.553488</td>\n",
       "      <td>39.524730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TEST_realistic_by_pred_op</td>\n",
       "      <td>simplify</td>\n",
       "      <td>19</td>\n",
       "      <td>7.572395</td>\n",
       "      <td>22.373621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VAL_oracle_by_true_op</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>11</td>\n",
       "      <td>1.706704</td>\n",
       "      <td>2.266044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>VAL_oracle_by_true_op</td>\n",
       "      <td>displace</td>\n",
       "      <td>6</td>\n",
       "      <td>1.396595</td>\n",
       "      <td>1.551651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>VAL_oracle_by_true_op</td>\n",
       "      <td>select</td>\n",
       "      <td>12</td>\n",
       "      <td>31.485423</td>\n",
       "      <td>40.160687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>VAL_oracle_by_true_op</td>\n",
       "      <td>simplify</td>\n",
       "      <td>9</td>\n",
       "      <td>1.567911</td>\n",
       "      <td>2.025539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>VAL_realistic_by_pred_op</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>10</td>\n",
       "      <td>1.573735</td>\n",
       "      <td>2.174047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>VAL_realistic_by_pred_op</td>\n",
       "      <td>displace</td>\n",
       "      <td>6</td>\n",
       "      <td>1.396595</td>\n",
       "      <td>1.551651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>VAL_realistic_by_pred_op</td>\n",
       "      <td>select</td>\n",
       "      <td>12</td>\n",
       "      <td>31.485423</td>\n",
       "      <td>40.160687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>VAL_realistic_by_pred_op</td>\n",
       "      <td>simplify</td>\n",
       "      <td>10</td>\n",
       "      <td>1.469999</td>\n",
       "      <td>1.930595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        split   operator   n        MAE       RMSE\n",
       "0      TEST_oracle_by_true_op  aggregate  22   1.647941   2.411753\n",
       "1      TEST_oracle_by_true_op   displace  12   2.026431   2.185604\n",
       "2      TEST_oracle_by_true_op     select  25  30.494501  39.160429\n",
       "3      TEST_oracle_by_true_op   simplify  17   2.461298   2.929820\n",
       "4   TEST_realistic_by_pred_op  aggregate  21   1.691911   2.463439\n",
       "5   TEST_realistic_by_pred_op   displace  12   2.026431   2.185604\n",
       "6   TEST_realistic_by_pred_op     select  24  30.553488  39.524730\n",
       "7   TEST_realistic_by_pred_op   simplify  19   7.572395  22.373621\n",
       "8       VAL_oracle_by_true_op  aggregate  11   1.706704   2.266044\n",
       "9       VAL_oracle_by_true_op   displace   6   1.396595   1.551651\n",
       "10      VAL_oracle_by_true_op     select  12  31.485423  40.160687\n",
       "11      VAL_oracle_by_true_op   simplify   9   1.567911   2.025539\n",
       "12   VAL_realistic_by_pred_op  aggregate  10   1.573735   2.174047\n",
       "13   VAL_realistic_by_pred_op   displace   6   1.396595   1.551651\n",
       "14   VAL_realistic_by_pred_op     select  12  31.485423  40.160687\n",
       "15   VAL_realistic_by_pred_op   simplify  10   1.469999   1.930595"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def per_operator_reg_report(split_name, df_s, y_true_raw, y_pred_raw, op_idx, class_names):\n",
    "    rows = []\n",
    "    for k, op in enumerate(class_names):\n",
    "        m = (op_idx == k)\n",
    "        # keep only finite pairs\n",
    "        m = m & np.isfinite(y_true_raw) & np.isfinite(y_pred_raw)\n",
    "        n = int(m.sum())\n",
    "        if n == 0:\n",
    "            continue\n",
    "        yt = y_true_raw[m]\n",
    "        yp = y_pred_raw[m]\n",
    "        mae = mean_absolute_error(yt, yp)\n",
    "        rmse = float(np.sqrt(mean_squared_error(yt, yp)))\n",
    "        rows.append({\n",
    "            \"split\": split_name,\n",
    "            \"operator\": str(op),\n",
    "            \"n\": n,\n",
    "            \"MAE\": mae,\n",
    "            \"RMSE\": rmse,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# -------- realistic: grouped by PREDICTED operator --------\n",
    "df_val_perop_real = per_operator_reg_report(\n",
    "    \"VAL_realistic_by_pred_op\", df_val, y_val_raw, yhat_val, val_pred_cls, class_names\n",
    ")\n",
    "df_test_perop_real = per_operator_reg_report(\n",
    "    \"TEST_realistic_by_pred_op\", df_test, y_test_raw, yhat_test, test_pred_cls, class_names\n",
    ")\n",
    "\n",
    "# -------- oracle: grouped by TRUE operator --------\n",
    "df_val_perop_or = per_operator_reg_report(\n",
    "    \"VAL_oracle_by_true_op\", df_val, y_val_raw, yhat_val_or, y_val_cls, class_names\n",
    ")\n",
    "df_test_perop_or = per_operator_reg_report(\n",
    "    \"TEST_oracle_by_true_op\", df_test, y_test_raw, yhat_test_or, y_test_cls, class_names\n",
    ")\n",
    "\n",
    "df_perop = pd.concat(\n",
    "    [df_val_perop_real, df_test_perop_real, df_val_perop_or, df_test_perop_or],\n",
    "    ignore_index=True\n",
    ").sort_values([\"split\", \"operator\"]).reset_index(drop=True)\n",
    "\n",
    "display(df_perop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d98fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
