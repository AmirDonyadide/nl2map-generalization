{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f56a2c",
   "metadata": {},
   "source": [
    "# MapVec end-to-end pipeline üìí\n",
    "\n",
    "This notebook runs the **entire pipeline**:\n",
    "1. Prompt embeddings (Universal Sentence Encoder)\n",
    "2. Map embeddings (handcrafted polygon features)\n",
    "3. Concatenation into a training matrix\n",
    "4. Helper cells to inspect vectors by `prompt_id` or `map_id`\n",
    "\n",
    "**Edit the Parameters** in the next cell to match your project layout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "367f89f6ac45439b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:21:53.302406Z",
     "start_time": "2025-10-27T11:21:53.298709Z"
    }
   },
   "outputs": [],
   "source": [
    "# ===================== PARAMETERS =====================\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shlex\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, mean_squared_error, mean_absolute_error\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "PROJ_ROOT = Path(\"../\").resolve()       # adjust if your notebook sits elsewhere\n",
    "SRC_DIR   = PROJ_ROOT / \"src\"\n",
    "# Put project root on sys.path\n",
    "if str(PROJ_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJ_ROOT))\n",
    "\n",
    "from src.config import PATHS, CFG, print_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ed0df45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:21:55.572701Z",
     "start_time": "2025-10-27T11:21:55.570071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFIG SUMMARY ===\n",
      "PROJ_ROOT  : /Users/amirdonyadide/Documents/GitHub/Thesis\n",
      "DATA_DIR   : /Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
      "INPUT_DIR  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input\n",
      "OUTPUT_DIR : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output\n",
      "MAPS_ROOT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/samples/pairs\n",
      "INPUT PAT. : *_input.geojson\n",
      "PROMPTS_CSV: /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/prompts.csv\n",
      "PAIRS_CSV  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/pairs.csv\n",
      "PROMPT_OUT : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "MAP_OUT    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n",
      "TRAIN_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out\n",
      "MODEL_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models\n",
      "SPLIT_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/splits\n",
      "PRM_NPZ    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/prompts_embeddings.npz\n",
      "--- Model ---\n",
      "USE_MODEL  : dan\n",
      "MAP_DIM    : 249\n",
      "PROMPT_DIM : 512\n",
      "FUSED_DIM  : 761\n",
      "BATCH_SIZE : 512\n",
      "VAL/TEST   : 0.15 0.15\n",
      "SEED       : 42\n"
     ]
    }
   ],
   "source": [
    "print_summary()  # optional\n",
    "\n",
    "# Access paths like:\n",
    "MAPS_ROOT = PATHS.MAPS_ROOT\n",
    "INPUT_MAPS_PATTERN = PATHS.INPUT_MAPS_PATTERN\n",
    "PROMPTS_CSV = PATHS.PROMPTS_CSV\n",
    "PRM_NPZ = PATHS.PRM_NPZ\n",
    "\n",
    "# Dims (auto-inferred if available; else None until you set them)\n",
    "MAP_DIM = CFG.MAP_DIM or 996         # fallback\n",
    "PROMPT_DIM = CFG.PROMPT_DIM or 512   # fallback\n",
    "FUSED_DIM = CFG.FUSED_DIM or (MAP_DIM + PROMPT_DIM)\n",
    "BATCH_SIZE = CFG.BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e8f95c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:22:00.422846Z",
     "start_time": "2025-10-27T11:22:00.417135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models\n",
      "‚úÖ All output folders cleaned and recreated fresh.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATHS.clean_outputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bccc1aced708b2",
   "metadata": {},
   "source": [
    "## 1) Prompt embeddings\n",
    "Runs `src/mapvec/prompts/prompt_embeddings.py` using your chosen USE model and saves artifacts to `PROMPT_OUT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9ca0c3d8b71fc70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:26:59.687350Z",
     "start_time": "2025-10-27T11:26:19.901557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMD: /opt/anaconda3/envs/thesis/bin/python -m src.mapvec.prompts.prompt_embeddings --input /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/prompts.csv --model dan --l2 --out_dir /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out -v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:51:08 | DEBUG | FILE_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/src/mapvec/prompts\n",
      "16:51:08 | DEBUG | PROJECT_ROOT=/Users/amirdonyadide/Documents/GitHub/Thesis\n",
      "16:51:08 | DEBUG | DEFAULT_DATA_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
      "16:51:08 | INFO | DATA_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
      "16:51:08 | INFO | INPUT=/Users/amirdonyadide/Documents/GitHub/Thesis/data/input/prompts.csv\n",
      "16:51:08 | INFO | OUT_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "16:51:08 | INFO | Reading CSV: /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/prompts.csv\n",
      "16:51:08 | INFO | Loaded 500 prompts (id_col=prompt_id). Sample IDs: p001, p002, p003‚Ä¶\n",
      "16:51:08 | INFO | Using local USE-dan at /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/model_dan\n",
      "16:51:08 | INFO | Loading USE-dan from local path: /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/model_dan ‚Ä¶\n",
      "16:51:11 | INFO | Fingerprint not found. Saved model loading will continue.\n",
      "16:51:11 | INFO | path_and_singleprint metric could not be logged. Saved model loading will continue.\n",
      "16:51:11 | INFO | Model loaded in 3.23s\n",
      "16:51:11 | INFO | Embedding 500 prompts (batch_size=512, l2=True)‚Ä¶\n",
      "16:51:11 | DEBUG |   embedded rows [1:500)\n",
      "16:51:11 | INFO | Done embedding in 0.14s (dim=512).\n",
      "16:51:11 | INFO | Writing outputs to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "16:51:11 | INFO |   saved prompts_embeddings.npz (shape=(500, 512))\n",
      "16:51:11 | INFO |   saved prompts.parquet (rows=500)\n",
      "16:51:11 | INFO |   saved meta.json\n",
      "16:51:11 | INFO | All done ‚úÖ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt embeddings completed.\n"
     ]
    }
   ],
   "source": [
    "cmd = [\n",
    "    sys.executable, \"-m\", \"src.mapvec.prompts.prompt_embeddings\",\n",
    "    \"--input\",    str(PATHS.PROMPTS_CSV),\n",
    "    \"--model\",    str(CFG.USE_MODEL),\n",
    "    \"--l2\",       \n",
    "    \"--out_dir\",  str(PATHS.PROMPT_OUT),\n",
    "    \"-v\"\n",
    "]\n",
    "print(\"CMD:\", \" \".join(cmd))\n",
    "res = subprocess.run(cmd, cwd=str(PATHS.PROJ_ROOT))  # shell=False by default\n",
    "if res.returncode != 0:\n",
    "    raise SystemExit(\"Prompt embedding step failed.\")\n",
    "print(\"Prompt embeddings completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd186319e89f445",
   "metadata": {},
   "source": [
    "## 2) Map embeddings\n",
    "Runs the map embedding module on the GeoJSON inputs. Skips problematic features, logs warnings, and writes `embeddings.npz` to `PAIR_MAP_OUT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa2b07a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMD: /opt/anaconda3/envs/thesis/bin/python -m src.mapvec.maps.map_embeddings --root /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/samples/pairs --pattern *_input.geojson --out_dir /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out --norm fixed --norm-wh 400x400 -v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:57:15 | DEBUG | PROJECT_ROOT=/Users/amirdonyadide/Documents/GitHub/Thesis\n",
      "16:57:15 | DEBUG | DATA_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
      "16:57:15 | INFO | Scanning /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/samples/pairs (pattern=*_input.geojson)‚Ä¶\n",
      "16:57:15 | INFO | First pass: counting polygons to normalize poly_count‚Ä¶\n",
      "16:57:19 | INFO | Max polygons across dataset: 789\n",
      "16:57:21 | INFO | OK  map_id=0073  -> vector[158]\n",
      "16:57:22 | INFO | OK  map_id=0080  -> vector[158]\n",
      "16:57:23 | INFO | OK  map_id=0093  -> vector[158]\n",
      "16:57:26 | INFO | OK  map_id=0122  -> vector[158]\n",
      "16:57:27 | INFO | OK  map_id=0123  -> vector[158]\n",
      "16:57:28 | INFO | OK  map_id=0127  -> vector[158]\n",
      "16:57:29 | INFO | OK  map_id=0158  -> vector[158]\n",
      "16:57:31 | INFO | OK  map_id=0159  -> vector[158]\n",
      "16:57:32 | INFO | OK  map_id=0160  -> vector[158]\n",
      "16:57:33 | INFO | OK  map_id=0165  -> vector[158]\n",
      "16:57:34 | INFO | OK  map_id=0167  -> vector[158]\n",
      "16:57:35 | INFO | OK  map_id=0168  -> vector[158]\n",
      "16:57:36 | INFO | OK  map_id=0171  -> vector[158]\n",
      "16:57:39 | INFO | OK  map_id=0208  -> vector[158]\n",
      "16:57:43 | INFO | OK  map_id=0209  -> vector[158]\n",
      "16:57:43 | INFO | OK  map_id=0215  -> vector[158]\n",
      "16:57:45 | INFO | OK  map_id=0240  -> vector[158]\n",
      "16:57:46 | INFO | OK  map_id=0256  -> vector[158]\n",
      "16:57:46 | INFO | OK  map_id=0257  -> vector[158]\n",
      "16:57:47 | INFO | OK  map_id=0262  -> vector[158]\n",
      "16:57:49 | INFO | OK  map_id=0285  -> vector[158]\n",
      "16:57:51 | INFO | OK  map_id=0286  -> vector[158]\n",
      "16:57:54 | INFO | OK  map_id=0288  -> vector[158]\n",
      "16:57:57 | INFO | OK  map_id=0289  -> vector[158]\n",
      "16:57:58 | INFO | OK  map_id=0313  -> vector[158]\n",
      "16:57:58 | INFO | OK  map_id=0341  -> vector[158]\n",
      "16:57:59 | INFO | OK  map_id=0362  -> vector[158]\n",
      "16:58:02 | INFO | OK  map_id=0363  -> vector[158]\n",
      "16:58:03 | INFO | OK  map_id=0364  -> vector[158]\n",
      "16:58:04 | INFO | OK  map_id=0379  -> vector[158]\n",
      "16:58:05 | INFO | OK  map_id=0389  -> vector[158]\n",
      "16:58:07 | INFO | OK  map_id=0390  -> vector[158]\n",
      "16:58:09 | INFO | OK  map_id=0409  -> vector[158]\n",
      "16:58:10 | INFO | OK  map_id=0410  -> vector[158]\n",
      "16:58:12 | INFO | OK  map_id=0412  -> vector[158]\n",
      "16:58:13 | INFO | OK  map_id=0413  -> vector[158]\n",
      "16:58:14 | INFO | OK  map_id=0414  -> vector[158]\n",
      "16:58:15 | INFO | OK  map_id=0417  -> vector[158]\n",
      "16:58:16 | INFO | OK  map_id=0421  -> vector[158]\n",
      "16:58:17 | INFO | OK  map_id=0426  -> vector[158]\n",
      "16:58:18 | INFO | OK  map_id=0427  -> vector[158]\n",
      "16:58:19 | INFO | OK  map_id=0432  -> vector[158]\n",
      "16:58:20 | INFO | OK  map_id=0433  -> vector[158]\n",
      "16:58:22 | INFO | OK  map_id=0437  -> vector[158]\n",
      "16:58:24 | INFO | OK  map_id=0438  -> vector[158]\n",
      "16:58:26 | INFO | OK  map_id=0439  -> vector[158]\n",
      "16:58:27 | INFO | OK  map_id=0454  -> vector[158]\n",
      "16:58:28 | INFO | OK  map_id=0458  -> vector[158]\n",
      "16:58:30 | INFO | OK  map_id=0459  -> vector[158]\n",
      "16:58:32 | INFO | OK  map_id=0460  -> vector[158]\n",
      "16:58:33 | INFO | OK  map_id=0466  -> vector[158]\n",
      "16:58:34 | INFO | OK  map_id=0469  -> vector[158]\n",
      "16:58:36 | INFO | OK  map_id=0471  -> vector[158]\n",
      "16:58:38 | INFO | OK  map_id=0472  -> vector[158]\n",
      "16:58:41 | INFO | OK  map_id=0474  -> vector[158]\n",
      "16:58:42 | INFO | OK  map_id=0475  -> vector[158]\n",
      "16:58:44 | INFO | OK  map_id=0479  -> vector[158]\n",
      "16:58:45 | INFO | OK  map_id=0480  -> vector[158]\n",
      "16:58:46 | INFO | OK  map_id=0481  -> vector[158]\n",
      "16:58:47 | INFO | OK  map_id=0482  -> vector[158]\n",
      "16:58:47 | INFO | OK  map_id=0508  -> vector[158]\n",
      "16:58:49 | INFO | OK  map_id=0509  -> vector[158]\n",
      "16:58:50 | INFO | OK  map_id=0518  -> vector[158]\n",
      "16:58:57 | INFO | OK  map_id=0520  -> vector[158]\n",
      "16:58:58 | INFO | OK  map_id=0521  -> vector[158]\n",
      "16:59:00 | INFO | OK  map_id=0523  -> vector[158]\n",
      "16:59:02 | INFO | OK  map_id=0527  -> vector[158]\n",
      "16:59:03 | INFO | OK  map_id=0528  -> vector[158]\n",
      "16:59:05 | INFO | OK  map_id=0529  -> vector[158]\n",
      "16:59:06 | INFO | OK  map_id=0530  -> vector[158]\n",
      "16:59:07 | INFO | OK  map_id=0553  -> vector[158]\n",
      "16:59:08 | INFO | OK  map_id=0557  -> vector[158]\n",
      "16:59:09 | INFO | OK  map_id=0575  -> vector[158]\n",
      "16:59:10 | INFO | OK  map_id=0576  -> vector[158]\n",
      "16:59:11 | INFO | OK  map_id=0594  -> vector[158]\n",
      "16:59:13 | INFO | OK  map_id=0595  -> vector[158]\n",
      "16:59:14 | INFO | OK  map_id=0600  -> vector[158]\n",
      "16:59:16 | INFO | OK  map_id=0605  -> vector[158]\n",
      "16:59:17 | INFO | OK  map_id=0606  -> vector[158]\n",
      "16:59:18 | INFO | OK  map_id=0608  -> vector[158]\n",
      "16:59:19 | INFO | OK  map_id=0609  -> vector[158]\n",
      "16:59:21 | INFO | OK  map_id=0611  -> vector[158]\n",
      "16:59:23 | INFO | OK  map_id=0614  -> vector[158]\n",
      "16:59:24 | INFO | OK  map_id=0615  -> vector[158]\n",
      "16:59:25 | INFO | OK  map_id=0618  -> vector[158]\n",
      "16:59:26 | INFO | OK  map_id=0623  -> vector[158]\n",
      "16:59:27 | INFO | OK  map_id=0624  -> vector[158]\n",
      "16:59:28 | INFO | OK  map_id=0645  -> vector[158]\n",
      "16:59:29 | INFO | OK  map_id=0646  -> vector[158]\n",
      "16:59:30 | INFO | OK  map_id=0655  -> vector[158]\n",
      "16:59:31 | INFO | OK  map_id=0656  -> vector[158]\n",
      "16:59:32 | INFO | OK  map_id=0657  -> vector[158]\n",
      "16:59:34 | INFO | OK  map_id=0658  -> vector[158]\n",
      "16:59:35 | INFO | OK  map_id=0659  -> vector[158]\n",
      "16:59:37 | INFO | OK  map_id=0667  -> vector[158]\n",
      "16:59:38 | INFO | OK  map_id=0672  -> vector[158]\n",
      "16:59:39 | INFO | OK  map_id=0699  -> vector[158]\n",
      "16:59:41 | INFO | OK  map_id=0700  -> vector[158]\n",
      "16:59:42 | INFO | OK  map_id=0701  -> vector[158]\n",
      "16:59:44 | INFO | OK  map_id=0706  -> vector[158]\n",
      "16:59:45 | INFO | OK  map_id=0707  -> vector[158]\n",
      "16:59:46 | INFO | OK  map_id=0715  -> vector[158]\n",
      "16:59:47 | INFO | OK  map_id=0721  -> vector[158]\n",
      "16:59:49 | INFO | OK  map_id=0747  -> vector[158]\n",
      "16:59:50 | INFO | OK  map_id=0748  -> vector[158]\n",
      "16:59:51 | INFO | OK  map_id=0749  -> vector[158]\n",
      "16:59:53 | INFO | OK  map_id=0755  -> vector[158]\n",
      "16:59:54 | INFO | OK  map_id=0758  -> vector[158]\n",
      "16:59:55 | INFO | OK  map_id=0759  -> vector[158]\n",
      "16:59:55 | INFO | OK  map_id=0762  -> vector[158]\n",
      "16:59:56 | INFO | OK  map_id=0770  -> vector[158]\n",
      "16:59:58 | INFO | OK  map_id=0804  -> vector[158]\n",
      "16:59:59 | INFO | OK  map_id=0807  -> vector[158]\n",
      "17:00:00 | INFO | OK  map_id=0808  -> vector[158]\n",
      "17:00:02 | INFO | OK  map_id=0809  -> vector[158]\n",
      "17:00:05 | INFO | OK  map_id=0819  -> vector[158]\n",
      "17:00:06 | INFO | OK  map_id=0848  -> vector[158]\n",
      "17:00:08 | INFO | OK  map_id=0853  -> vector[158]\n",
      "17:00:10 | INFO | OK  map_id=0854  -> vector[158]\n",
      "17:00:11 | INFO | OK  map_id=0856  -> vector[158]\n",
      "17:00:12 | INFO | OK  map_id=0857  -> vector[158]\n",
      "17:00:18 | INFO | OK  map_id=0858  -> vector[158]\n",
      "17:00:20 | INFO | OK  map_id=0859  -> vector[158]\n",
      "17:00:21 | INFO | OK  map_id=0867  -> vector[158]\n",
      "17:00:24 | INFO | OK  map_id=0868  -> vector[158]\n",
      "17:00:25 | INFO | OK  map_id=0869  -> vector[158]\n",
      "17:00:26 | INFO | OK  map_id=0901  -> vector[158]\n",
      "17:00:28 | INFO | OK  map_id=0903  -> vector[158]\n",
      "17:00:29 | INFO | OK  map_id=0904  -> vector[158]\n",
      "17:00:30 | INFO | OK  map_id=0905  -> vector[158]\n",
      "17:00:36 | INFO | OK  map_id=0906  -> vector[158]\n",
      "17:00:37 | INFO | OK  map_id=0907  -> vector[158]\n",
      "17:00:39 | INFO | OK  map_id=0908  -> vector[158]\n",
      "17:00:42 | INFO | OK  map_id=0917  -> vector[158]\n",
      "17:00:43 | INFO | OK  map_id=0918  -> vector[158]\n",
      "17:00:46 | INFO | OK  map_id=0926  -> vector[158]\n",
      "17:00:47 | INFO | OK  map_id=0947  -> vector[158]\n",
      "17:00:50 | INFO | OK  map_id=0948  -> vector[158]\n",
      "17:00:52 | INFO | OK  map_id=0949  -> vector[158]\n",
      "17:00:53 | INFO | OK  map_id=0950  -> vector[158]\n",
      "17:00:54 | INFO | OK  map_id=0951  -> vector[158]\n",
      "17:00:55 | INFO | OK  map_id=0952  -> vector[158]\n",
      "17:00:58 | INFO | OK  map_id=0966  -> vector[158]\n",
      "17:01:01 | INFO | OK  map_id=0967  -> vector[158]\n",
      "17:01:02 | INFO | OK  map_id=0970  -> vector[158]\n",
      "17:01:03 | INFO | OK  map_id=0971  -> vector[158]\n",
      "17:01:05 | INFO | OK  map_id=0974  -> vector[158]\n",
      "17:01:06 | INFO | OK  map_id=0975  -> vector[158]\n",
      "17:01:07 | INFO | OK  map_id=0976  -> vector[158]\n",
      "17:01:08 | INFO | OK  map_id=0994  -> vector[158]\n",
      "17:01:10 | INFO | OK  map_id=0995  -> vector[158]\n",
      "17:01:11 | INFO | OK  map_id=0997  -> vector[158]\n",
      "17:01:13 | INFO | OK  map_id=0998  -> vector[158]\n",
      "17:01:15 | INFO | OK  map_id=1019  -> vector[158]\n",
      "17:01:18 | INFO | OK  map_id=1020  -> vector[158]\n",
      "17:01:20 | INFO | OK  map_id=1052  -> vector[158]\n",
      "17:01:22 | INFO | OK  map_id=1053  -> vector[158]\n",
      "17:01:23 | INFO | OK  map_id=1054  -> vector[158]\n",
      "17:01:25 | INFO | OK  map_id=1055  -> vector[158]\n",
      "17:01:26 | INFO | OK  map_id=1056  -> vector[158]\n",
      "17:01:27 | INFO | OK  map_id=1057  -> vector[158]\n",
      "17:01:29 | INFO | OK  map_id=1069  -> vector[158]\n",
      "17:01:30 | INFO | OK  map_id=1070  -> vector[158]\n",
      "17:01:32 | INFO | OK  map_id=1090  -> vector[158]\n",
      "17:01:33 | INFO | OK  map_id=1091  -> vector[158]\n",
      "17:01:34 | INFO | OK  map_id=1092  -> vector[158]\n",
      "17:01:34 | INFO | OK  map_id=1100  -> vector[158]\n",
      "17:01:36 | INFO | OK  map_id=1103  -> vector[158]\n",
      "17:01:37 | INFO | OK  map_id=1105  -> vector[158]\n",
      "17:01:39 | INFO | OK  map_id=1106  -> vector[158]\n",
      "17:01:40 | INFO | OK  map_id=1118  -> vector[158]\n",
      "17:01:42 | INFO | OK  map_id=1119  -> vector[158]\n",
      "17:01:44 | INFO | OK  map_id=1120  -> vector[158]\n",
      "17:01:46 | INFO | OK  map_id=1139  -> vector[158]\n",
      "17:01:47 | INFO | OK  map_id=1140  -> vector[158]\n",
      "17:01:48 | INFO | OK  map_id=1148  -> vector[158]\n",
      "17:01:49 | INFO | OK  map_id=1155  -> vector[158]\n",
      "17:01:51 | INFO | OK  map_id=1157  -> vector[158]\n",
      "17:01:53 | INFO | OK  map_id=1168  -> vector[158]\n",
      "17:01:55 | INFO | OK  map_id=1169  -> vector[158]\n",
      "17:01:56 | INFO | OK  map_id=1170  -> vector[158]\n",
      "17:01:58 | INFO | OK  map_id=1197  -> vector[158]\n",
      "17:02:00 | INFO | OK  map_id=1198  -> vector[158]\n",
      "17:02:01 | INFO | OK  map_id=1202  -> vector[158]\n",
      "17:02:04 | INFO | OK  map_id=1203  -> vector[158]\n",
      "17:02:06 | INFO | OK  map_id=1204  -> vector[158]\n",
      "17:02:07 | INFO | OK  map_id=1217  -> vector[158]\n",
      "17:02:09 | INFO | OK  map_id=1218  -> vector[158]\n",
      "17:02:10 | INFO | OK  map_id=1219  -> vector[158]\n",
      "17:02:12 | INFO | OK  map_id=1221  -> vector[158]\n",
      "17:02:13 | INFO | OK  map_id=1222  -> vector[158]\n",
      "17:02:14 | INFO | OK  map_id=1231  -> vector[158]\n",
      "17:02:15 | INFO | OK  map_id=1233  -> vector[158]\n",
      "17:02:17 | INFO | OK  map_id=1234  -> vector[158]\n",
      "17:02:18 | INFO | OK  map_id=1261  -> vector[158]\n",
      "17:02:19 | INFO | OK  map_id=1269  -> vector[158]\n",
      "17:02:24 | INFO | OK  map_id=1270  -> vector[158]\n",
      "17:02:25 | INFO | OK  map_id=1271  -> vector[158]\n",
      "17:02:26 | INFO | OK  map_id=1276  -> vector[158]\n",
      "17:02:27 | INFO | OK  map_id=1277  -> vector[158]\n",
      "17:02:28 | INFO | OK  map_id=1283  -> vector[158]\n",
      "17:02:29 | INFO | OK  map_id=1284  -> vector[158]\n",
      "17:02:30 | INFO | OK  map_id=1285  -> vector[158]\n",
      "17:02:33 | INFO | OK  map_id=1295  -> vector[158]\n",
      "17:02:35 | INFO | OK  map_id=1296  -> vector[158]\n",
      "17:02:36 | INFO | OK  map_id=1297  -> vector[158]\n",
      "17:02:38 | INFO | OK  map_id=1303  -> vector[158]\n",
      "17:02:40 | INFO | OK  map_id=1304  -> vector[158]\n",
      "17:02:41 | INFO | OK  map_id=1310  -> vector[158]\n",
      "17:02:43 | INFO | OK  map_id=1319  -> vector[158]\n",
      "17:02:45 | INFO | OK  map_id=1333  -> vector[158]\n",
      "17:02:46 | INFO | OK  map_id=1334  -> vector[158]\n",
      "17:02:48 | INFO | OK  map_id=1344  -> vector[158]\n",
      "17:02:49 | INFO | OK  map_id=1349  -> vector[158]\n",
      "17:02:51 | INFO | OK  map_id=1364  -> vector[158]\n",
      "17:02:54 | INFO | OK  map_id=1365  -> vector[158]\n",
      "17:02:57 | INFO | OK  map_id=1366  -> vector[158]\n",
      "17:03:01 | INFO | OK  map_id=1367  -> vector[158]\n",
      "17:03:02 | INFO | OK  map_id=1368  -> vector[158]\n",
      "17:03:03 | INFO | OK  map_id=1369  -> vector[158]\n",
      "17:03:04 | INFO | OK  map_id=1377  -> vector[158]\n",
      "17:03:05 | INFO | OK  map_id=1378  -> vector[158]\n",
      "17:03:06 | INFO | OK  map_id=1385  -> vector[158]\n",
      "17:03:08 | INFO | OK  map_id=1386  -> vector[158]\n",
      "17:03:09 | INFO | OK  map_id=1399  -> vector[158]\n",
      "17:03:10 | INFO | OK  map_id=1401  -> vector[158]\n",
      "17:03:11 | INFO | OK  map_id=1408  -> vector[158]\n",
      "17:03:14 | INFO | OK  map_id=1409  -> vector[158]\n",
      "17:03:17 | INFO | OK  map_id=1410  -> vector[158]\n",
      "17:03:18 | INFO | OK  map_id=1413  -> vector[158]\n",
      "17:03:20 | INFO | OK  map_id=1414  -> vector[158]\n",
      "17:03:22 | INFO | OK  map_id=1415  -> vector[158]\n",
      "17:03:24 | INFO | OK  map_id=1416  -> vector[158]\n",
      "17:03:25 | INFO | OK  map_id=1417  -> vector[158]\n",
      "17:03:27 | INFO | OK  map_id=1418  -> vector[158]\n",
      "17:03:28 | INFO | OK  map_id=1434  -> vector[158]\n",
      "17:03:29 | INFO | OK  map_id=1438  -> vector[158]\n",
      "17:03:31 | INFO | OK  map_id=1439  -> vector[158]\n",
      "17:03:32 | INFO | OK  map_id=1450  -> vector[158]\n",
      "17:03:34 | INFO | OK  map_id=1451  -> vector[158]\n",
      "17:03:37 | INFO | OK  map_id=1458  -> vector[158]\n",
      "17:03:38 | INFO | OK  map_id=1459  -> vector[158]\n",
      "17:03:40 | INFO | OK  map_id=1460  -> vector[158]\n",
      "17:03:41 | INFO | OK  map_id=1465  -> vector[158]\n",
      "17:03:43 | INFO | OK  map_id=1466  -> vector[158]\n",
      "17:03:45 | INFO | OK  map_id=1467  -> vector[158]\n",
      "17:03:48 | INFO | OK  map_id=1473  -> vector[158]\n",
      "17:03:49 | INFO | OK  map_id=1474  -> vector[158]\n",
      "17:03:49 | INFO | OK  map_id=1476  -> vector[158]\n",
      "17:03:51 | INFO | OK  map_id=1479  -> vector[158]\n",
      "17:03:52 | INFO | OK  map_id=1486  -> vector[158]\n",
      "17:03:54 | INFO | OK  map_id=1487  -> vector[158]\n",
      "17:03:55 | INFO | OK  map_id=1496  -> vector[158]\n",
      "17:03:58 | INFO | OK  map_id=1500  -> vector[158]\n",
      "17:03:58 | INFO | OK  map_id=1501  -> vector[158]\n",
      "17:04:00 | INFO | OK  map_id=1507  -> vector[158]\n",
      "17:04:01 | INFO | OK  map_id=1508  -> vector[158]\n",
      "17:04:04 | INFO | OK  map_id=1509  -> vector[158]\n",
      "17:04:05 | INFO | OK  map_id=1514  -> vector[158]\n",
      "17:04:07 | INFO | OK  map_id=1515  -> vector[158]\n",
      "17:04:08 | INFO | OK  map_id=1557  -> vector[158]\n",
      "17:04:09 | INFO | OK  map_id=1563  -> vector[158]\n",
      "17:04:12 | INFO | OK  map_id=1564  -> vector[158]\n",
      "17:04:13 | INFO | OK  map_id=1565  -> vector[158]\n",
      "17:04:14 | INFO | OK  map_id=1570  -> vector[158]\n",
      "17:04:15 | INFO | OK  map_id=1579  -> vector[158]\n",
      "17:04:16 | INFO | OK  map_id=1580  -> vector[158]\n",
      "17:04:17 | INFO | OK  map_id=1583  -> vector[158]\n",
      "17:04:18 | INFO | OK  map_id=1584  -> vector[158]\n",
      "17:04:20 | INFO | OK  map_id=1598  -> vector[158]\n",
      "17:04:21 | INFO | OK  map_id=1613  -> vector[158]\n",
      "17:04:22 | INFO | OK  map_id=1614  -> vector[158]\n",
      "17:04:23 | INFO | OK  map_id=1618  -> vector[158]\n",
      "17:04:25 | INFO | OK  map_id=1619  -> vector[158]\n",
      "17:04:25 | INFO | OK  map_id=1629  -> vector[158]\n",
      "17:04:28 | INFO | OK  map_id=1630  -> vector[158]\n",
      "17:04:29 | INFO | OK  map_id=1631  -> vector[158]\n",
      "17:04:30 | INFO | OK  map_id=1647  -> vector[158]\n",
      "17:04:31 | INFO | OK  map_id=1649  -> vector[158]\n",
      "17:04:32 | INFO | OK  map_id=1650  -> vector[158]\n",
      "17:04:33 | INFO | OK  map_id=1653  -> vector[158]\n",
      "17:04:34 | INFO | OK  map_id=1666  -> vector[158]\n",
      "17:04:35 | INFO | OK  map_id=1667  -> vector[158]\n",
      "17:04:38 | INFO | OK  map_id=1672  -> vector[158]\n",
      "17:04:40 | INFO | OK  map_id=1673  -> vector[158]\n",
      "17:04:41 | INFO | OK  map_id=1679  -> vector[158]\n",
      "17:04:41 | INFO | OK  map_id=1691  -> vector[158]\n",
      "17:04:43 | INFO | OK  map_id=1696  -> vector[158]\n",
      "17:04:46 | INFO | OK  map_id=1700  -> vector[158]\n",
      "17:04:49 | INFO | OK  map_id=1702  -> vector[158]\n",
      "17:04:51 | INFO | OK  map_id=1703  -> vector[158]\n",
      "17:04:53 | INFO | OK  map_id=1709  -> vector[158]\n",
      "17:04:54 | INFO | OK  map_id=1710  -> vector[158]\n",
      "17:04:56 | INFO | OK  map_id=1748  -> vector[158]\n",
      "17:04:57 | INFO | OK  map_id=1749  -> vector[158]\n",
      "17:04:59 | INFO | OK  map_id=1750  -> vector[158]\n",
      "17:05:00 | INFO | OK  map_id=1751  -> vector[158]\n",
      "17:05:01 | INFO | OK  map_id=1752  -> vector[158]\n",
      "17:05:02 | INFO | OK  map_id=1755  -> vector[158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Input map embeddings completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:05:03 | INFO | OK  map_id=1757  -> vector[158]\n",
      "17:05:03 | INFO | Saved 300 vectors (failed=0) to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n"
     ]
    }
   ],
   "source": [
    "cmd = [\n",
    "    sys.executable, \"-m\", \"src.mapvec.maps.map_embeddings\",\n",
    "    \"--root\", str(PATHS.MAPS_ROOT),\n",
    "    \"--pattern\", PATHS.INPUT_MAPS_PATTERN,\n",
    "    \"--out_dir\", str(PATHS.MAP_OUT),\n",
    "    \"--norm\", \"fixed\",\n",
    "    \"--norm-wh\", \"400x400\",\n",
    "    \"-v\",\n",
    "]\n",
    "print(\"CMD:\", \" \".join(cmd))\n",
    "res = subprocess.run(cmd, cwd=str(PATHS.PROJ_ROOT)) \n",
    "if res.returncode != 0:\n",
    "    raise SystemExit(\"Map embedding step failed.\")\n",
    "print(\"‚úÖ Input map embeddings completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5142d4b68c273d37",
   "metadata": {},
   "source": [
    "## 3) Concatenate\n",
    "Joins map & prompt vectors using `pairs.csv` and writes `X_concat.npy` and `train_pairs.parquet` to `TRAIN_OUT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a494fd27dfe7681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMD: /opt/anaconda3/envs/thesis/bin/python -m src.mapvec.concat.concat_embeddings --pairs /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/pairs.csv --map_npz /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/maps_embeddings.npz --prompt_npz /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/prompts_embeddings.npz --out_dir /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out --drop_dupes\n",
      "‚úÖ Concatenation completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:36:23 | INFO | Map  embeddings: (300, 249) from /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/maps_embeddings.npz\n",
      "12:36:23 | INFO | Prompt embeddings: (500, 512) from /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/prompts_embeddings.npz\n",
      "12:36:23 | INFO | X shape = (450, 761)  (map_dim=249, prompt_dim=512)\n",
      "12:36:23 | INFO | Saved to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out in 0.02s\n"
     ]
    }
   ],
   "source": [
    "cmd = [\n",
    "    sys.executable, \"-m\", \"src.mapvec.concat.concat_embeddings\",\n",
    "    \"--pairs\",      str(PATHS.PAIRS_CSV),\n",
    "    \"--map_npz\",    str(PATHS.MAP_OUT / \"maps_embeddings.npz\"),   # single-map embeddings\n",
    "    \"--prompt_npz\", str(PATHS.PROMPT_OUT / \"prompts_embeddings.npz\"),\n",
    "    \"--out_dir\",    str(PATHS.TRAIN_OUT),\n",
    "    \"--drop_dupes\",          # optional flag (keep if you want)\n",
    "    # \"--fail_on_missing\",   # optional: uncomment if you prefer hard failure on missing IDs\n",
    "]\n",
    "\n",
    "print(\"CMD:\", \" \".join(cmd))\n",
    "res = subprocess.run(cmd, cwd=str(PATHS.PROJ_ROOT))  # run from project root\n",
    "if res.returncode != 0:\n",
    "    raise SystemExit(\"Concatenation step failed.\")\n",
    "print(\"‚úÖ Concatenation completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997cab7",
   "metadata": {},
   "source": [
    "## 4) Load + basic cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bc0897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded X: (450, 761), pairs: (450, 4)\n",
      "After cleaning: (450, 761), (450, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X = np.load(PATHS.TRAIN_OUT / \"X_concat.npy\")\n",
    "pairs_df = pd.read_parquet(PATHS.TRAIN_OUT / \"train_pairs.parquet\")\n",
    "print(f\"Loaded X: {X.shape}, pairs: {pairs_df.shape}\")\n",
    "\n",
    "# Basic cleaning (lowercase + remove missing operator/param)\n",
    "OP_COL = \"operator\"\n",
    "PARAM_COLS = [\"param\"]\n",
    "\n",
    "df = pairs_df.copy()\n",
    "df[OP_COL] = df[OP_COL].astype(str).str.strip().str.lower()\n",
    "mask = df[OP_COL].notna()\n",
    "for c in PARAM_COLS:\n",
    "    mask &= df[c].notna()\n",
    "\n",
    "X = X[mask.values].astype(\"float32\", copy=False)\n",
    "df = df.loc[mask].reset_index(drop=True)\n",
    "print(f\"After cleaning: {X.shape}, {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187473d",
   "metadata": {},
   "source": [
    "## 5) Split + target encoding + Feature preprocessing (fit only on train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41942b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (315, 761), Val: (67, 761), Test: (68, 761)\n",
      "‚úÖ Preprocessing complete ‚Äî data ready for model training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Preprocessing complete ‚Äî data ready for model training\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Class weights for imbalance\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m cls_w    = compute_class_weight(\u001b[33m\"\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m\"\u001b[39m, classes=np.arange(\u001b[38;5;28mlen\u001b[39m(\u001b[43mclasses\u001b[49m)), y=y_train_cls)\n\u001b[32m     53\u001b[39m sample_w = np.array([cls_w[c] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m y_train_cls], dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mClass weights:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(classes, cls_w)))\n",
      "\u001b[31mNameError\u001b[39m: name 'classes' is not defined"
     ]
    }
   ],
   "source": [
    "# ========== SPLIT + TARGETS ==========\n",
    "FIXED_CLASSES = [\"simplify\", \"select\", \"aggregate\", \"displace\"]\n",
    "\n",
    "# --- Split into train / val / test (stratify by operator if balanced)\n",
    "X_train, X_temp, df_train, df_temp = train_test_split(\n",
    "    X, df,\n",
    "    test_size=CFG.VAL_RATIO + CFG.TEST_RATIO,\n",
    "    random_state=CFG.SEED,\n",
    "    shuffle=True,\n",
    "    stratify=df[OP_COL] if df[OP_COL].nunique() > 1 else None\n",
    ")\n",
    "rel_test = CFG.TEST_RATIO / (CFG.VAL_RATIO + CFG.TEST_RATIO)\n",
    "X_val, X_test, df_val, df_test = train_test_split(\n",
    "    X_temp, df_temp,\n",
    "    test_size=rel_test,\n",
    "    random_state=CFG.SEED,\n",
    "    shuffle=True,\n",
    "    stratify=df_temp[OP_COL] if df_temp[OP_COL].nunique() > 1 else None\n",
    ")\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# --- Encode targets\n",
    "le = LabelEncoder().fit(FIXED_CLASSES)\n",
    "y_train_cls = le.transform(df_train[OP_COL])\n",
    "y_val_cls   = le.transform(df_val[OP_COL])\n",
    "y_test_cls  = le.transform(df_test[OP_COL])\n",
    "\n",
    "y_train_reg = df_train[PARAM_COLS].to_numpy(dtype=\"float32\")\n",
    "y_val_reg   = df_val[PARAM_COLS].to_numpy(dtype=\"float32\")\n",
    "y_test_reg  = df_test[PARAM_COLS].to_numpy(dtype=\"float32\")\n",
    "\n",
    "# ========== IMPUTE + SCALE (TRAIN-ONLY FIT) ==========\n",
    "for X_ in (X_train, X_val, X_test):\n",
    "    X_[~np.isfinite(X_)] = np.nan  # replace inf ‚Üí NaN\n",
    "\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_train_imp = imp.fit_transform(X_train)\n",
    "X_val_imp   = imp.transform(X_val)\n",
    "X_test_imp  = imp.transform(X_test)\n",
    "\n",
    "scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5, 95))\n",
    "X_train_s = scaler.fit_transform(X_train_imp)\n",
    "X_val_s   = scaler.transform(X_val_imp)\n",
    "X_test_s  = scaler.transform(X_test_imp)\n",
    "\n",
    "assert np.isfinite(X_train_s).all() and np.isfinite(X_val_s).all() and np.isfinite(X_test_s).all(), \\\n",
    "    \"Non-finite values after scaling.\"\n",
    "\n",
    "print(\"‚úÖ Preprocessing complete ‚Äî data ready for model training\")\n",
    "\n",
    "# Class weights for imbalance\n",
    "cls_w    = compute_class_weight(\"balanced\", classes=np.arange(len(classes)), y=y_train_cls)\n",
    "sample_w = np.array([cls_w[c] for c in y_train_cls], dtype=\"float32\")\n",
    "print(\"Class weights:\", dict(zip(classes, cls_w)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b13ebb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (315, 249) dtype: float32\n",
      "finite %: 100.0\n",
      "cols having any NaN: 0\n",
      "zero-variance cols: 24\n",
      "area (log1p of [0,1]): max=0.122  (cap=0.7)\n",
      "length/distance families (log1p normalized): max=222.054  (cap=4.0)\n",
      "‚ö†Ô∏è length/distance families (log1p normalized) exceeds expected cap 4.00. Inspect tails/features.\n",
      "count families (log1p): max=98.000  (cap=6.0)\n",
      "‚ö†Ô∏è count families (log1p) exceeds expected cap 6.00. Inspect tails/features.\n",
      "ratios: abs max=22876.488  p99=19.147\n",
      "centroid coords: abs max=0.998  p99=0.992\n",
      "\n",
      "Top-10 |value| columns:\n",
      " 22876.488  eccentricity__max\n",
      "  1411.121  eccentricity__std\n",
      "   222.054  knn3__max\n",
      "   221.796  knn2__max\n",
      "   179.998  orientation__max\n",
      "   178.490  orientation__q75\n",
      "   170.792  orientation__q50\n",
      "   150.490  nn_dist_max__max\n",
      "   150.490  nn_dist_median__max\n",
      "   149.664  mean_neighbor_distance_touches__max\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "\n",
    "# --- inputs you already have ---\n",
    "# X_train_s (fused) is after downstream scaling; for sanity of the raw map embeddings,\n",
    "# run this on the *pre-scaler embeddings* right after pooling (i.e., before StandardScaler/RobustScaler).\n",
    "# If you only have fused now, also keep the unfused map block around as X_map (N, 249).\n",
    "X_map = X_train[:, :249]  # adapt if you store the map block separately\n",
    "# load your real feature names (you already save this in map_out/feature_names.json)\n",
    "feat_path = Path(PATHS.MAP_OUT) / \"feature_names.json\"\n",
    "map_feat_names = np.array(json.loads(feat_path.read_text()))\n",
    "\n",
    "def family_of(name: str):\n",
    "    m = re.match(r\"^([^_]+(?:_[^_]+)*)__(mean|min|max|std|q25|q50|q75)$\", name)\n",
    "    return (m.group(1), m.group(2)) if m else (name, \"\")\n",
    "\n",
    "# 1) Basic integrity\n",
    "print(\"shape:\", X_map.shape, \"dtype:\", X_map.dtype)\n",
    "print(\"finite %:\", 100*np.isfinite(X_map).mean())\n",
    "nan_cols = np.isnan(X_map).any(axis=0).sum()\n",
    "print(\"cols having any NaN:\", nan_cols)\n",
    "\n",
    "# 2) Zero/near-zero variance (bad / uninformative)\n",
    "stds = np.nanstd(X_map, axis=0)\n",
    "zz = (stds < 1e-12).sum()\n",
    "print(\"zero-variance cols:\", zz)\n",
    "\n",
    "# 3) Per-family expected ranges after stabilization\n",
    "#    area was normalized to [0,1] then log1p -> [0, log(2)~=0.693]\n",
    "#    lengths/dists normalized by diag then log1p -> typically [0, ~2.5] (depending on geometry)\n",
    "families_log_cap = {\n",
    "    \"area\": 0.70,  # safety margin over log(2)=0.693\n",
    "}\n",
    "len_fams = {\n",
    "    \"perimeter\",\"eq_diameter\",\"bbox_width\",\"bbox_height\",\n",
    "    \"nn_dist_min\",\"nn_dist_median\",\"nn_dist_max\",\"knn1\",\"knn2\",\"knn3\",\n",
    "    \"mean_neighbor_distance_touches\",\"mean_neighbor_distance_intersects\",\n",
    "}\n",
    "count_fams = {\n",
    "    \"vertex_count\",\"neighbor_count_touches\",\"neighbor_count_intersects\",\n",
    "    \"hole_count\",\"density_r05\",\"density_r10\",\"reflex_count\",\n",
    "}\n",
    "ratio_fams = {\n",
    "    \"compactness\",\"circularity\",\"elongation\",\"convexity\",\"rectangularity\",\n",
    "    \"straightness\",\"bbox_aspect\",\"extent\",\"eccentricity\",\"hole_area_ratio\",\"reflex_ratio\",\n",
    "}\n",
    "coord_fams = {\"centroid_x\",\"centroid_y\"}  # bbox-normalized (‚âà[0,1])\n",
    "\n",
    "def colmask(families):\n",
    "    return np.array([family_of(n)[0] in families for n in map_feat_names])\n",
    "\n",
    "# Hard checks\n",
    "def assert_max(mask, cap, label):\n",
    "    if mask.any():\n",
    "        vmax = np.nanmax(X_map[:, mask])\n",
    "        print(f\"{label}: max={vmax:.3f}  (cap={cap})\")\n",
    "        assert np.isfinite(vmax)\n",
    "        # soft warn instead of hard assert to avoid stopping your flow\n",
    "        if vmax > cap:\n",
    "            print(f\"‚ö†Ô∏è {label} exceeds expected cap {cap:.2f}. Inspect tails/features.\")\n",
    "\n",
    "# Area cap\n",
    "assert_max(colmask({\"area\"}), families_log_cap[\"area\"], \"area (log1p of [0,1])\")\n",
    "\n",
    "# Length distance families: log1p(perimeter/diag) etc. Usually < ~2.5; warn if > 4.\n",
    "assert_max(colmask(len_fams), 4.0, \"length/distance families (log1p normalized)\")\n",
    "\n",
    "# Counts: log1p(count). Usually small; warn if > 6.\n",
    "assert_max(colmask(count_fams), 6.0, \"count families (log1p)\")\n",
    "\n",
    "# Ratios: scale-free (not logged). Should generally be bounded: extent‚àà[0,1], bbox_aspect>=0, etc.\n",
    "def describe_family(fams, name):\n",
    "    m = colmask(fams)\n",
    "    if m.any():\n",
    "        fam_vals = X_map[:, m]\n",
    "        print(f\"{name}: abs max={np.nanmax(np.abs(fam_vals)):.3f}  p99={np.nanpercentile(np.abs(fam_vals),99):.3f}\")\n",
    "describe_family(ratio_fams, \"ratios\")\n",
    "describe_family(coord_fams, \"centroid coords\")\n",
    "\n",
    "# 4) Top offenders by absolute value (to find any remaining blow-ups)\n",
    "absmax = np.nanmax(np.abs(X_map), axis=0)\n",
    "idx = np.argsort(-absmax)[:10]\n",
    "print(\"\\nTop-10 |value| columns:\")\n",
    "for j in idx:\n",
    "    print(f\"{absmax[j]:10.3f}  {map_feat_names[j]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb3b670c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-10 median shifts (val) normalized by train IQR:\n",
      "  0.368  nn_dist_min__q50  med_tr=3.405  med_val=4.306  IQR_tr=2.449\n",
      "  0.333  density_r10__min  med_tr=2.000  med_val=1.000  IQR_tr=3.000\n",
      "  0.333  density_r05__q50  med_tr=8.000  med_val=7.000  IQR_tr=3.000\n",
      "  0.294  density_r10__max  med_tr=54.000  med_val=49.000  IQR_tr=17.000\n",
      "  0.251  angle_std__min  med_tr=0.000  med_val=0.000  IQR_tr=0.001\n",
      "  0.250  density_r05__max  med_tr=21.000  med_val=19.000  IQR_tr=8.000\n",
      "  0.250  density_r05__q75  med_tr=11.000  med_val=10.000  IQR_tr=4.000\n",
      "  0.244  convexity__q25  med_tr=0.975  med_val=0.983  IQR_tr=0.031\n",
      "  0.232  eq_diameter__q25  med_tr=0.010  med_val=0.011  IQR_tr=0.003\n",
      "  0.228  centroid_y__min  med_tr=0.018  med_val=0.020  IQR_tr=0.009\n",
      "\n",
      "Top-10 median shifts (test) normalized by train IQR:\n",
      "  0.345  elongation__q25  med_tr=1.054  med_test=1.002  IQR_tr=0.151\n",
      "  0.277  map_bbox_h  med_tr=0.964  med_test=0.958  IQR_tr=0.019\n",
      "  0.255  nn_dist_median__q50  med_tr=4.508  med_test=3.761  IQR_tr=2.926\n",
      "  0.251  orientation__min  med_tr=2.071  med_test=4.319  IQR_tr=8.975\n",
      "  0.250  density_r05__q75  med_tr=11.000  med_test=10.000  IQR_tr=4.000\n",
      "  0.250  neighbor_count_touches__max  med_tr=4.000  med_test=3.500  IQR_tr=2.000\n",
      "  0.249  knn2__std  med_tr=7.335  med_test=7.908  IQR_tr=2.301\n",
      "  0.245  compactness__q50  med_tr=1.406  med_test=1.414  IQR_tr=0.031\n",
      "  0.245  circularity__q50  med_tr=0.711  med_test=0.707  IQR_tr=0.016\n",
      "  0.238  extent__mean  med_tr=0.557  med_test=0.520  IQR_tr=0.156\n"
     ]
    }
   ],
   "source": [
    "def split_map(X, n_map=249): return X[:, :n_map]\n",
    "\n",
    "def summarize(X):\n",
    "    med = np.nanmedian(X, axis=0)\n",
    "    iqr = np.nanpercentile(X, 75, axis=0) - np.nanpercentile(X, 25, axis=0)\n",
    "    return med, iqr\n",
    "\n",
    "Xtr_map = split_map(X_train)\n",
    "Xva_map = split_map(X_val)\n",
    "Xte_map = split_map(X_test)\n",
    "\n",
    "med_tr, iqr_tr = summarize(Xtr_map)\n",
    "med_va, iqr_va = summarize(Xva_map)\n",
    "med_te, iqr_te = summarize(Xte_map)\n",
    "\n",
    "def report_drift(med_a, iqr_a, med_b, iqr_b, label):\n",
    "    # relative median shift normalized by train IQR\n",
    "    eps = 1e-6\n",
    "    rel = np.abs(med_b - med_a) / np.maximum(iqr_a, eps)\n",
    "    worst = np.argsort(-rel)[:10]\n",
    "    print(f\"\\nTop-10 median shifts ({label}) normalized by train IQR:\")\n",
    "    for j in worst:\n",
    "        print(f\"{rel[j]:7.3f}  {map_feat_names[j]}  med_tr={med_a[j]:.3f}  med_{label}={med_b[j]:.3f}  IQR_tr={iqr_a[j]:.3f}\")\n",
    "\n",
    "report_drift(med_tr, iqr_tr, med_va, iqr_va, \"val\")\n",
    "report_drift(med_tr, iqr_tr, med_te, iqr_te, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc8d42d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-20 features by mutual information with class:\n",
      "MI=0.4162  nn_dist_median__q75\n",
      "MI=0.4147  nn_dist_min__mean\n",
      "MI=0.4080  orientation__max\n",
      "MI=0.3770  knn3__q50\n",
      "MI=0.3726  neighbor_count_touches__std\n",
      "MI=0.3694  rectangularity__q50\n",
      "MI=0.3686  knn1__q50\n",
      "MI=0.3679  knn1__max\n",
      "MI=0.3654  bbox_height__q25\n",
      "MI=0.3606  eq_diameter__mean\n",
      "MI=0.3599  eq_diameter__q75\n",
      "MI=0.3591  elongation__q50\n",
      "MI=0.3577  angle_std__q75\n",
      "MI=0.3544  eccentricity__min\n",
      "MI=0.3485  centroid_x__q50\n",
      "MI=0.3481  nn_dist_max__std\n",
      "MI=0.3478  bbox_aspect__q50\n",
      "MI=0.3476  map_bbox_w\n",
      "MI=0.3402  centroid_y__min\n",
      "MI=0.3391  area__std\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# y_train_cls already encoded with LabelEncoder\n",
    "mi = mutual_info_classif(Xtr_map, y_train_cls, discrete_features=False, random_state=0)\n",
    "top = np.argsort(-mi)[:20]\n",
    "print(\"\\nTop-20 features by mutual information with class:\")\n",
    "for j in top:\n",
    "    print(f\"MI={mi[j]:.4f}  {map_feat_names[j]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "158f6a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 24 zero-variance map columns\n",
      "After sanitize: max|train| = 272.45865\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# 1) Split fused features -> map (249) + text (rest)\n",
    "MAP_DIM = 249\n",
    "Xtr_map = X_train[:, :MAP_DIM].astype(np.float32, copy=True)\n",
    "Xva_map = X_val[:,   :MAP_DIM].astype(np.float32, copy=True)\n",
    "Xte_map = X_test[:,  :MAP_DIM].astype(np.float32, copy=True)\n",
    "\n",
    "# names[] must be the exact 249 names in order\n",
    "feat_path = Path(PATHS.MAP_OUT) / \"feature_names.json\"\n",
    "map_feat_names = np.array(json.loads(feat_path.read_text()))\n",
    "\n",
    "def family_of(name: str):\n",
    "    m = re.match(r\"^([^_]+(?:_[^_]+)*)__(mean|min|max|std|q25|q50|q75)$\", name)\n",
    "    return (m.group(1), m.group(2)) if m else (name, \"\")\n",
    "\n",
    "bases = np.array([family_of(n)[0] for n in map_feat_names])\n",
    "stats = np.array([family_of(n)[1] for n in map_feat_names])\n",
    "\n",
    "# 2) locate globals for per-sample scale\n",
    "ix_w  = int(np.where(map_feat_names == \"map_bbox_w\")[0][0])\n",
    "ix_h  = int(np.where(map_feat_names == \"map_bbox_h\")[0][0])\n",
    "\n",
    "def diag_from_row(row):\n",
    "    w = max(row[ix_w], 1e-12)\n",
    "    h = max(row[ix_h], 1e-12)\n",
    "    return float((w*w + h*h)**0.5)\n",
    "\n",
    "def area_from_row(row):\n",
    "    w = max(row[ix_w], 1e-12)\n",
    "    h = max(row[ix_h], 1e-12)\n",
    "    return float(w*h)\n",
    "\n",
    "len_fams = {\n",
    "    \"perimeter\",\"eq_diameter\",\"bbox_width\",\"bbox_height\",\n",
    "    \"nn_dist_min\",\"nn_dist_median\",\"nn_dist_max\",\n",
    "    \"knn1\",\"knn2\",\"knn3\",\n",
    "    \"mean_neighbor_distance_touches\",\"mean_neighbor_distance_intersects\",\n",
    "}\n",
    "count_fams = {\n",
    "    \"vertex_count\",\"neighbor_count_touches\",\"neighbor_count_intersects\",\n",
    "    \"hole_count\",\"density_r05\",\"density_r10\",\"reflex_count\",\n",
    "}\n",
    "area_fams = {\"area\"}  # extent is already normalized\n",
    "\n",
    "len_mask   = np.isin(bases, list(len_fams))\n",
    "count_mask = np.isin(bases, list(count_fams))\n",
    "area_mask  = np.isin(bases, list(area_fams))\n",
    "\n",
    "# Don‚Äôt touch std columns when logging (std can be 0 and negative not applicable)\n",
    "is_std = (stats == \"__std\")\n",
    "\n",
    "def normalize_and_log(X):\n",
    "    X = X.copy()\n",
    "    # per-row diag and area\n",
    "    diags = np.array([diag_from_row(r)  for r in X], dtype=np.float32)[:, None]\n",
    "    areas = np.array([area_from_row(r)  for r in X], dtype=np.float32)[:, None]\n",
    "\n",
    "    # lengths/distances: divide by diag\n",
    "    if len_mask.any():\n",
    "        X[:, len_mask] = X[:, len_mask] / np.clip(diags, 1e-12, None)\n",
    "\n",
    "    # areas: divide by bbox area\n",
    "    if area_mask.any():\n",
    "        X[:, area_mask] = X[:, area_mask] / np.clip(areas, 1e-12, None)\n",
    "\n",
    "    # log1p: counts + (now normalized) lengths/distances; skip std columns\n",
    "    log_mask = (count_mask | len_mask) & (~is_std)\n",
    "    if log_mask.any():\n",
    "        A = X[:, log_mask]\n",
    "        A = np.where(np.isfinite(A) & (A >= 0), np.log1p(A), 0.0)\n",
    "        X[:, log_mask] = A\n",
    "\n",
    "    # winsorize EVERY map column by train percentiles (1‚Äì99) computed once on Xtr_map\n",
    "    return X\n",
    "\n",
    "# compute train caps, then apply to all\n",
    "Xtr_tmp = normalize_and_log(Xtr_map)\n",
    "lo = np.percentile(Xtr_tmp, 1, axis=0)\n",
    "hi = np.percentile(Xtr_tmp, 99, axis=0)\n",
    "def win(A): return np.clip(A, lo, hi)\n",
    "\n",
    "Xtr_map_sane = win(normalize_and_log(Xtr_map))\n",
    "Xva_map_sane = win(normalize_and_log(Xva_map))\n",
    "Xte_map_sane = win(normalize_and_log(Xte_map))\n",
    "\n",
    "# drop zero-variance cols (on train only)\n",
    "stds = np.std(Xtr_map_sane, axis=0)\n",
    "keep = stds > 1e-12\n",
    "print(\"Dropping\", (~keep).sum(), \"zero-variance map columns\")\n",
    "Xtr_map_sane = Xtr_map_sane[:, keep]\n",
    "Xva_map_sane = Xva_map_sane[:, keep]\n",
    "Xte_map_sane = Xte_map_sane[:, keep]\n",
    "\n",
    "# sanity peek\n",
    "print(\"After sanitize: max|train| =\", np.max(np.abs(Xtr_map_sane)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "229a52d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Adam crashed (name 'sample_w' is not defined). Falling back to lbfgs (no sample_weight).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:173: RuntimeWarning: invalid value encountered in add\n",
      "  activations[i + 1] += self.intercepts_[i]\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:602: ConvergenceWarning: lbfgs failed to converge after 0 iteration(s) (status=2):\n",
      "ABNORMAL: \n",
      "\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64),   # smaller net for 450 samples\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    alpha=1e-3,                     # L2 regularization\n",
    "    learning_rate_init=3e-4,        # works well with RobustScaler outputs (~O(1))\n",
    "    batch_size=32,                  # smaller batches stabilize updates\n",
    "    max_iter=300,                   # early stopping will cut it shorter anyway\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=15,\n",
    "    validation_fraction=0.15,       # matches your val split ratio\n",
    "    tol=1e-4,\n",
    "    random_state=CFG.SEED,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    clf.fit(X_train_s, y_train_cls, sample_weight=sample_w)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Adam crashed ({e}). Falling back to lbfgs (no sample_weight).\")\n",
    "    clf = MLPClassifier(\n",
    "        hidden_layer_sizes=(128, 64),\n",
    "        activation=\"relu\",\n",
    "        solver=\"lbfgs\",\n",
    "        alpha=1e-3,       # slightly stronger L2 for stability\n",
    "        max_iter=500,     # usually converges earlier\n",
    "        tol=1e-4,\n",
    "        random_state=CFG.SEED,\n",
    "        verbose=True\n",
    "    )\n",
    "    clf.fit(X_train_s, y_train_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44b82400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- X_train ---\n",
      "shape: (315, 761) dtype: float32\n",
      "finite: True\n",
      "min/max: -0.1059115 22876.488\n",
      "mean/std: 2.283548 67.136475\n",
      "all-NaN cols: 0 zero-variance cols: 24\n",
      "--- X_val ---\n",
      "shape: (67, 761) dtype: float32\n",
      "finite: True\n",
      "min/max: -0.10371275 215.96675\n",
      "mean/std: 2.0767243 11.157472\n",
      "all-NaN cols: 0 zero-variance cols: 24\n",
      "--- X_test ---\n",
      "shape: (68, 761) dtype: float32\n",
      "finite: True\n",
      "min/max: -0.10311565 205.76747\n",
      "mean/std: 2.0587165 11.092547\n",
      "all-NaN cols: 0 zero-variance cols: 24\n",
      "--- X_train_imp ---\n",
      "shape: (315, 761) dtype: float32\n",
      "finite: True\n",
      "min/max: -0.1059115 22876.488\n",
      "mean/std: 2.283548 67.136475\n",
      "all-NaN cols: 0 zero-variance cols: 24\n",
      "--- X_train_s ---\n",
      "shape: (315, 761) dtype: float32\n",
      "finite: True\n",
      "min/max: -6.580961 226.34361\n",
      "mean/std: 0.014436657 0.8922743\n",
      "all-NaN cols: 0 zero-variance cols: 25\n",
      "y_train_cls has NaN? False\n",
      "classes present in train: ['aggregate', 'displace', 'select', 'simplify']\n"
     ]
    }
   ],
   "source": [
    "def check_matrix(name, X):\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(\"shape:\", X.shape, \"dtype:\", X.dtype)\n",
    "    print(\"finite:\", np.isfinite(X).all())\n",
    "    print(\"min/max:\", np.nanmin(X), np.nanmax(X))\n",
    "    print(\"mean/std:\", np.nanmean(X), np.nanstd(X))\n",
    "    # any all-NaN or all-constant columns before scaling?\n",
    "    col_nan = np.isnan(X).all(axis=0).sum()\n",
    "    col_zero_var = (np.nanstd(X, axis=0) == 0).sum()\n",
    "    print(\"all-NaN cols:\", col_nan, \"zero-variance cols:\", col_zero_var)\n",
    "\n",
    "check_matrix(\"X_train\", X_train)\n",
    "check_matrix(\"X_val\",   X_val)\n",
    "check_matrix(\"X_test\",  X_test)\n",
    "\n",
    "check_matrix(\"X_train_imp\", X_train_imp)\n",
    "check_matrix(\"X_train_s\",   X_train_s)\n",
    "\n",
    "print(\"y_train_cls has NaN?\", np.isnan(y_train_cls).any() if hasattr(y_train_cls, \"__len__\") else False)\n",
    "print(\"classes present in train:\", sorted(set(df_train[OP_COL])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13fca12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] acc=0.254  f1_macro=0.221\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m val_f1m     = f1_score(y_val_cls, y_val_pred, average=\u001b[33m\"\u001b[39m\u001b[33mmacro\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[VAL] acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  f1_macro=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_f1m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28mprint\u001b[39m(classification_report(y_val_cls, y_val_pred, target_names=\u001b[43mclasses\u001b[49m))\n",
      "\u001b[31mNameError\u001b[39m: name 'classes' is not defined"
     ]
    }
   ],
   "source": [
    "y_val_pred  = clf.predict(X_val_s)\n",
    "val_acc     = accuracy_score(y_val_cls, y_val_pred)\n",
    "val_f1m     = f1_score(y_val_cls, y_val_pred, average=\"macro\")\n",
    "print(f\"[VAL] acc={val_acc:.3f}  f1_macro={val_f1m:.3f}\")\n",
    "print(classification_report(y_val_cls, y_val_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1980b8b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m regressors = {}\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mclasses\u001b[49m):\n\u001b[32m      3\u001b[39m     sel = (y_train_cls == idx)\n\u001b[32m      4\u001b[39m     n = \u001b[38;5;28mint\u001b[39m(sel.sum())\n",
      "\u001b[31mNameError\u001b[39m: name 'classes' is not defined"
     ]
    }
   ],
   "source": [
    "regressors = {}\n",
    "for idx, name in enumerate(classes):\n",
    "    sel = (y_train_cls == idx)\n",
    "    n = int(sel.sum())\n",
    "    if n < 5:\n",
    "        print(f\"‚ö†Ô∏è  Skipping regressor for '{name}' (only {n} samples).\")\n",
    "        continue\n",
    "\n",
    "    reg = MLPRegressor(\n",
    "        hidden_layer_sizes=(128, 64),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=1e-3,\n",
    "        learning_rate_init=5e-4,\n",
    "        max_iter=600,\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=30,\n",
    "        random_state=SEED,\n",
    "        verbose=False\n",
    "    )\n",
    "    reg.fit(X_train_s[sel], y_train_reg[sel])  # (n,d) -> (n,1)\n",
    "    regressors[name] = reg\n",
    "    print(f\"‚úÖ Regressor trained for '{name}' on {n} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5603571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] acc=0.235  f1_macro=0.202\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m test_f1m    = f1_score(y_test_cls, y_test_pred, average=\u001b[33m\"\u001b[39m\u001b[33mmacro\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[TEST] acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  f1_macro=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_f1m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test_cls, y_test_pred, target_names=\u001b[43mclasses\u001b[49m))\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Parameter regression (conditioned on correct operator)\u001b[39;00m\n\u001b[32m      9\u001b[39m mask = (y_test_pred == y_test_cls)\n",
      "\u001b[31mNameError\u001b[39m: name 'classes' is not defined"
     ]
    }
   ],
   "source": [
    "# Classification\n",
    "y_test_pred = clf.predict(X_test_s)\n",
    "test_acc    = accuracy_score(y_test_cls, y_test_pred)\n",
    "test_f1m    = f1_score(y_test_cls, y_test_pred, average=\"macro\")\n",
    "print(f\"[TEST] acc={test_acc:.3f}  f1_macro={test_f1m:.3f}\")\n",
    "print(classification_report(y_test_cls, y_test_pred, target_names=classes))\n",
    "\n",
    "# Parameter regression (conditioned on correct operator)\n",
    "mask = (y_test_pred == y_test_cls)\n",
    "print(f\"Parameter evaluation on {int(mask.sum())}/{len(mask)} samples with correct operator prediction.\")\n",
    "y_pred_params = np.full_like(y_test_reg, np.nan, dtype=\"float32\")  # (n,1)\n",
    "\n",
    "for i, ok in enumerate(mask):\n",
    "    if not ok:\n",
    "        continue\n",
    "    cls_name = classes[y_test_pred[i]]\n",
    "    reg = regressors.get(cls_name)\n",
    "    if reg is None:\n",
    "        continue\n",
    "    pred = reg.predict(X_test_s[i:i+1])[0]\n",
    "    y_pred_params[i] = pred if hasattr(pred, \"__len__\") else [float(pred)]\n",
    "\n",
    "valid = np.isfinite(y_pred_params).all(axis=1) & mask\n",
    "if valid.any():\n",
    "    mse = mean_squared_error(y_test_reg[valid], y_pred_params[valid])\n",
    "    mae = mean_absolute_error(y_test_reg[valid], y_pred_params[valid])\n",
    "    print(f\"[TEST Param | correct-ops] MSE={mse:.4f}  MAE={mae:.4f}\")\n",
    "else:\n",
    "    print(\"No valid parameter predictions to score.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50f905d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_OUT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m joblib.dump(imp,        \u001b[43mMODEL_OUT\u001b[49m / \u001b[33m\"\u001b[39m\u001b[33mimputer.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m joblib.dump(scaler,     MODEL_OUT / \u001b[33m\"\u001b[39m\u001b[33mscaler.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m joblib.dump(le,         MODEL_OUT / \u001b[33m\"\u001b[39m\u001b[33mlabel_encoder.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'MODEL_OUT' is not defined"
     ]
    }
   ],
   "source": [
    "joblib.dump(imp,        MODEL_OUT / \"imputer.joblib\")\n",
    "joblib.dump(scaler,     MODEL_OUT / \"scaler.joblib\")\n",
    "joblib.dump(le,         MODEL_OUT / \"label_encoder.joblib\")\n",
    "joblib.dump(clf,        MODEL_OUT / \"mlp_classifier.joblib\")\n",
    "joblib.dump(regressors, MODEL_OUT / \"per_class_regressors.joblib\")\n",
    "print(f\"Models saved to {MODEL_OUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10fd82f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pipeline(X_batch):\n",
    "    \"\"\"\n",
    "    X_batch: (n, 1508) raw concatenated features\n",
    "    Returns: dict with operator_probs, operator_label, params_pred\n",
    "    \"\"\"\n",
    "    Xb = np.asarray(X_batch, dtype=\"float32\")\n",
    "    Xb[~np.isfinite(Xb)] = np.nan\n",
    "    Xb = imp.transform(Xb)\n",
    "    Xb = scaler.transform(Xb)\n",
    "    Xb = np.clip(Xb, -CLIP_ABS, CLIP_ABS)\n",
    "\n",
    "    proba = clf.predict_proba(Xb)                    # (n, 4)\n",
    "    pred_idx = np.argmax(proba, axis=1)\n",
    "    pred_labels = le.inverse_transform(pred_idx).tolist()\n",
    "\n",
    "    params = []\n",
    "    for i, lbl in enumerate(pred_labels):\n",
    "        reg = regressors.get(lbl)\n",
    "        if reg is None:\n",
    "            params.append([np.nan])                 # single param\n",
    "        else:\n",
    "            pred = reg.predict(Xb[i:i+1])[0]\n",
    "            params.append(pred.tolist() if hasattr(pred, \"tolist\") else [float(pred)])\n",
    "\n",
    "    return {\"operator_probs\": proba, \"operator_label\": pred_labels, \"params_pred\": np.array(params)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d21771b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
