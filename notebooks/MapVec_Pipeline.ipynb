{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f56a2c",
   "metadata": {},
   "source": [
    "## üß© 0) Setup & Imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "367f89f6ac45439b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:21:53.302406Z",
     "start_time": "2025-10-27T11:21:53.298709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFIG SUMMARY ===\n",
      "PROJ_ROOT  : /Users/amirdonyadide/Documents/GitHub/Thesis\n",
      "DATA_DIR   : /Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
      "INPUT_DIR  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input\n",
      "OUTPUT_DIR : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output\n",
      "MAPS_ROOT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/samples/pairs\n",
      "INPUT PAT. : *_input.geojson\n",
      "--- User Study ---\n",
      "USER_STUDY_XLSX : /Users/amirdonyadide/Documents/GitHub/Thesis/data/userstudy/UserStudy.xlsx\n",
      "RESPONSES_SHEET : Responses\n",
      "TILE_ID_COL     : tile_id\n",
      "COMPLETE_COL    : complete\n",
      "REMOVE_COL      : remove\n",
      "TEXT_COL        : cleaned_text\n",
      "PARAM_VALUE_COL : param_value\n",
      "OPERATOR_COL    : operator\n",
      "INTENSITY_COL   : intensity\n",
      "--- Filters / IDs / Split ---\n",
      "ONLY_COMPLETE   : True\n",
      "EXCLUDE_REMOVED : True\n",
      "PROMPT_ID       : r{i:08d}\n",
      "SPLIT_BY        : tile\n",
      "--- Outputs ---\n",
      "PROMPT_OUT : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "MAP_OUT    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n",
      "TRAIN_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out\n",
      "MODEL_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models\n",
      "SPLIT_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/splits\n",
      "PRM_NPZ    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/prompts_embeddings.npz\n",
      "--- Model ---\n",
      "PROMPT_ENCODER: openai-small\n",
      "MAP_DIM       : 165\n",
      "PROMPT_DIM    : 1536\n",
      "FUSED_DIM     : 1701\n",
      "BATCH_SIZE    : 512\n",
      "VAL/TEST      : 0.15 0.15\n",
      "SEED          : 42\n",
      "--- Tile scale (Solution 1) ---\n",
      "TILE_W/H (m)  : 400.0 400.0\n",
      "TILE_DIAG_M   : 565.685424949238\n",
      "TILE_AREA_M2  : 160000.0\n",
      "--- Operator groups (Solution 1) ---\n",
      "DISTANCE_OPS  : ('aggregate', 'displace', 'simplify')\n",
      "AREA_OPS      : ('select',)\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models\n",
      "‚úÖ All output folders cleaned and recreated fresh.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================== PARAMETERS / IMPORTS =====================\n",
    "from pathlib import Path\n",
    "import sys, subprocess, numpy as np, pandas as pd, joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, make_scorer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from scipy.stats import loguniform, randint\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Project config\n",
    "PROJ_ROOT = Path(\"../\").resolve()\n",
    "SRC_DIR   = PROJ_ROOT / \"src\"\n",
    "if str(PROJ_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJ_ROOT))\n",
    "\n",
    "from src.config import PATHS, CFG, print_summary, DISTANCE_OPS, AREA_OPS\n",
    "print_summary()\n",
    "\n",
    "TILE_DIAG_M = CFG.TILE_DIAG_M\n",
    "TILE_AREA_M2 = CFG.TILE_AREA_M2\n",
    "\n",
    "# Dims (fallbacks if CFG unset)\n",
    "MAP_DIM = PROMPT_DIM = FUSED_DIM = None\n",
    "\n",
    "BATCH_SIZE  = CFG.BATCH_SIZE\n",
    "\n",
    "# Clean outputs for a fresh run\n",
    "PATHS.clean_outputs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a2350e",
   "metadata": {},
   "source": [
    "## üìö 1) Build Prompt Embeddings (USE) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ed0df45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:21:55.572701Z",
     "start_time": "2025-10-27T11:21:55.570071Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 20:00:43 | INFO | Reading Excel: /Users/amirdonyadide/Documents/GitHub/Thesis/data/userstudy/UserStudy.xlsx (sheet=Responses)\n",
      "2026-01-22 20:00:43 | INFO | Filtered Excel rows: 786 ‚Üí 564 (only_complete=True, exclude_removed=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 564 prompts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 20:00:44 | INFO | Embedding 564 prompts with OpenAI model=text-embedding-3-small (batch_size=512, l2=True)‚Ä¶\n",
      "2026-01-22 20:00:46 | INFO | HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-22 20:00:47 | INFO | HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-01-22 20:00:47 | INFO | Done OpenAI embedding in 2.86s (dim=1536).\n",
      "2026-01-22 20:00:47 | INFO | Writing outputs to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "2026-01-22 20:00:47 | INFO |   saved prompts_embeddings.npz (shape=(564, 1536))\n",
      "2026-01-22 20:00:47 | INFO |   saved prompts.parquet (rows=564)\n",
      "2026-01-22 20:00:47 | INFO |   saved meta.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prompt embeddings completed.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Import the module\n",
    "from src.mapvec.prompts import prompt_embeddings as pe\n",
    "\n",
    "# Choose paths\n",
    "data_dir = PATHS.DATA_DIR  # or Path(\"../data\").resolve()\n",
    "in_path  = PATHS.USER_STUDY_XLSX\n",
    "out_dir  = PATHS.PROMPT_OUT\n",
    "\n",
    "# Logging (match what CLI does)\n",
    "pe.setup_logging(verbosity=1)\n",
    "\n",
    "# Load prompts (will filter complete==True & remove==False because you updated the function)\n",
    "ids, texts, id_colname = pe.load_prompts_from_source(\n",
    "    input_path=Path(in_path),\n",
    "    sheet_name=PATHS.RESPONSES_SHEET,\n",
    "    tile_id_col=PATHS.TILE_ID_COL,\n",
    "    complete_col=PATHS.COMPLETE_COL,\n",
    "    remove_col=PATHS.REMOVE_COL,\n",
    "    text_col=PATHS.TEXT_COL,\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(texts)} prompts.\")\n",
    "\n",
    "# Get embedder based on CFG.PROMPT_ENCODER (dan/transformer/openai-small/openai-large)\n",
    "embed_fn, model_label = pe.get_embedder(\n",
    "    kind=CFG.PROMPT_ENCODER,\n",
    "    data_dir=Path(data_dir),\n",
    "    l2_normalize=True,\n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    ")\n",
    "\n",
    "# Embed\n",
    "E = embed_fn(texts)\n",
    "\n",
    "# Save outputs in the same format as before\n",
    "pe.save_outputs(\n",
    "    out_dir=Path(out_dir),\n",
    "    ids=ids,\n",
    "    texts=texts,\n",
    "    E=E,\n",
    "    model_name=model_label,\n",
    "    l2_normalized=True,\n",
    "    id_colname=\"prompt_id\",\n",
    "    also_save_embeddings_csv=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Prompt embeddings completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c0b51",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è 2) Build Map Embeddings (geometric) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ca0c3d8b71fc70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:26:59.687350Z",
     "start_time": "2025-10-27T11:26:19.901557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed tiles from Excel: 401\n",
      "Maps to embed after filtering: 401\n",
      "‚úÖ Map embeddings completed.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.mapvec.maps import map_embeddings as me\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Load allowed tile_ids from Excel\n",
    "# ------------------------------------------------------------\n",
    "dfu = pd.read_excel(PATHS.USER_STUDY_XLSX, sheet_name=PATHS.RESPONSES_SHEET)\n",
    "\n",
    "dfu[PATHS.COMPLETE_COL] = dfu[PATHS.COMPLETE_COL].astype(bool)\n",
    "dfu[PATHS.REMOVE_COL]   = dfu[PATHS.REMOVE_COL].astype(bool)\n",
    "\n",
    "mask = pd.Series(True, index=dfu.index)\n",
    "if PATHS.ONLY_COMPLETE:\n",
    "    mask &= (dfu[PATHS.COMPLETE_COL] == True)\n",
    "if PATHS.EXCLUDE_REMOVED:\n",
    "    mask &= (dfu[PATHS.REMOVE_COL] == False)\n",
    "dfu = dfu[mask].copy()\n",
    "\n",
    "\n",
    "tile_raw = dfu[PATHS.TILE_ID_COL]\n",
    "tile_num = pd.to_numeric(tile_raw, errors=\"coerce\")\n",
    "if tile_num.notna().all():\n",
    "    allowed_tile_ids = set(tile_num.astype(int).astype(str).str.zfill(4).tolist())\n",
    "else:\n",
    "    allowed_tile_ids = set(tile_raw.astype(str).str.strip().str.zfill(4).tolist())\n",
    "\n",
    "print(f\"Allowed tiles from Excel: {len(allowed_tile_ids)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Discover GeoJSONs and filter by tile_id\n",
    "# ------------------------------------------------------------\n",
    "me.setup_logging(verbosity=1)\n",
    "\n",
    "pairs = list(me.find_geojsons(PATHS.MAPS_ROOT, PATHS.INPUT_MAPS_PATTERN))\n",
    "\n",
    "pairs = [\n",
    "    (map_id, path)\n",
    "    for (map_id, path) in pairs\n",
    "    if str(map_id) in allowed_tile_ids\n",
    "]\n",
    "\n",
    "if not pairs:\n",
    "    raise RuntimeError(\"No maps left after Excel filtering.\")\n",
    "\n",
    "print(f\"Maps to embed after filtering: {len(pairs)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) First pass: polygon counting\n",
    "# ------------------------------------------------------------\n",
    "counts = {}\n",
    "for map_id, path in pairs:\n",
    "    try:\n",
    "        counts[map_id] = me._count_valid_polygons(path)\n",
    "    except Exception:\n",
    "        counts[map_id] = 0\n",
    "\n",
    "max_polygons = max(max(counts.values()), 1)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Second pass: embed maps\n",
    "# ------------------------------------------------------------\n",
    "ids, vecs, rows = [], [], []\n",
    "feat_names = None\n",
    "first_dim = None\n",
    "\n",
    "for map_id, path in pairs:\n",
    "    vec, names = me.embed_one_map(\n",
    "        path,\n",
    "        max_polygons=max_polygons,\n",
    "        norm=\"fixed\",\n",
    "        norm_wh=f\"{CFG.TILE_WIDTH_M}x{CFG.TILE_HEIGHT_M}\",\n",
    "    )\n",
    "\n",
    "    if first_dim is None:\n",
    "        first_dim = vec.shape[0]\n",
    "        feat_names = names\n",
    "    elif vec.shape[0] != first_dim:\n",
    "        print(f\"Skipping {map_id}: dim mismatch\")\n",
    "        continue\n",
    "\n",
    "    ids.append(map_id)\n",
    "    vecs.append(vec)\n",
    "\n",
    "    rows.append({\n",
    "        \"map_id\": map_id,\n",
    "        \"geojson\": str(path),\n",
    "        \"n_polygons\": counts.get(map_id, 0),\n",
    "    })\n",
    "\n",
    "E = np.vstack(vecs).astype(np.float32)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Save outputs (same format as script)\n",
    "# ------------------------------------------------------------\n",
    "me.save_outputs(\n",
    "    out_dir=PATHS.MAP_OUT,\n",
    "    rows=rows,\n",
    "    E=E,\n",
    "    ids=ids,\n",
    "    feat_names=feat_names or [],\n",
    "    save_csv=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Map embeddings completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e8cdf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inferred dims: {'MAP_DIM': 165, 'PROMPT_DIM': 1536, 'FUSED_DIM': 1701}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def infer_dims(paths):\n",
    "    prm_npz = paths.PROMPT_OUT / \"prompts_embeddings.npz\"\n",
    "    map_npz = paths.MAP_OUT / \"maps_embeddings.npz\"\n",
    "\n",
    "    if not prm_npz.exists():\n",
    "        raise FileNotFoundError(f\"Missing {prm_npz} (run prompt embeddings first)\")\n",
    "    z = np.load(prm_npz, allow_pickle=True)\n",
    "    PROMPT_DIM = int(z[\"E\"].shape[1])\n",
    "\n",
    "    if not map_npz.exists():\n",
    "        raise FileNotFoundError(f\"Missing {map_npz} (run map embeddings first)\")\n",
    "    z2 = np.load(map_npz, allow_pickle=True)\n",
    "    MAP_DIM = int(z2[\"E\"].shape[1])\n",
    "\n",
    "    FUSED_DIM = MAP_DIM + PROMPT_DIM\n",
    "    return MAP_DIM, PROMPT_DIM, FUSED_DIM\n",
    "\n",
    "MAP_DIM, PROMPT_DIM, FUSED_DIM = infer_dims(PATHS)\n",
    "\n",
    "assert MAP_DIM == CFG.MAP_DIM, f\"MAP_DIM mismatch: inferred {MAP_DIM} vs CFG {CFG.MAP_DIM}\"\n",
    "assert PROMPT_DIM == CFG.PROMPT_DIM, f\"PROMPT_DIM mismatch: inferred {PROMPT_DIM} vs CFG {CFG.PROMPT_DIM}\"\n",
    "\n",
    "print(\"‚úÖ Inferred dims:\", {\"MAP_DIM\": MAP_DIM, \"PROMPT_DIM\": PROMPT_DIM, \"FUSED_DIM\": FUSED_DIM})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd186319e89f445",
   "metadata": {},
   "source": [
    "## üîó 3) Concatenate (pairs ‚Üí fused rows) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa2b07a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Concatenation completed.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from src.mapvec.concat import concat_embeddings as ce\n",
    "ce.setup_logging(verbosity=1)\n",
    "\n",
    "map_npz_path = Path(PATHS.MAP_OUT / \"maps_embeddings.npz\")\n",
    "prm_npz_path = Path(PATHS.PROMPT_OUT / \"prompts_embeddings.npz\")\n",
    "prompts_pq   = Path(PATHS.PROMPT_OUT / \"prompts.parquet\")\n",
    "out_dir      = Path(PATHS.TRAIN_OUT)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- build pairs from prompts.parquet (authoritative) ----\n",
    "pairs = pd.read_parquet(prompts_pq)\n",
    "\n",
    "if \"prompt_id\" not in pairs.columns or \"tile_id\" not in pairs.columns:\n",
    "    raise RuntimeError(\"prompts.parquet must contain columns: prompt_id, tile_id\")\n",
    "\n",
    "pairs = pairs.rename(columns={\"tile_id\": \"map_id\"})[[\"map_id\", \"prompt_id\"]].copy()\n",
    "pairs[\"map_id\"] = pairs[\"map_id\"].astype(str).str.strip()\n",
    "pairs[\"prompt_id\"] = pairs[\"prompt_id\"].astype(str).str.strip()\n",
    "pairs = pairs.dropna(subset=[\"map_id\", \"prompt_id\"])\n",
    "pairs = pairs[(pairs[\"map_id\"] != \"\") & (pairs[\"prompt_id\"] != \"\")]\n",
    "pairs = pairs.drop_duplicates(subset=[\"map_id\", \"prompt_id\"])\n",
    "\n",
    "# ---- load embeddings ----\n",
    "E_map, map_ids = ce.load_npz(map_npz_path)\n",
    "E_prm, prm_ids = ce.load_npz(prm_npz_path)\n",
    "\n",
    "idx_map = {k: i for i, k in enumerate(map_ids)}\n",
    "idx_prm = {k: i for i, k in enumerate(prm_ids)}\n",
    "\n",
    "# ---- match & build X ----\n",
    "chosen_rows, im_list, ip_list = [], [], []\n",
    "missing = 0\n",
    "\n",
    "for i, row in enumerate(pairs.itertuples(index=False), start=0):\n",
    "    im = idx_map.get(row.map_id)\n",
    "    ip = idx_prm.get(row.prompt_id)\n",
    "    if im is None or ip is None:\n",
    "        missing += 1\n",
    "        continue\n",
    "    chosen_rows.append(i)\n",
    "    im_list.append(im)\n",
    "    ip_list.append(ip)\n",
    "\n",
    "if not im_list:\n",
    "    raise RuntimeError(\"No valid pairs after ID matching.\")\n",
    "\n",
    "if missing:\n",
    "    print(f\"‚ö†Ô∏è Skipped {missing} rows with missing IDs\")\n",
    "\n",
    "X_map = E_map[np.asarray(im_list, dtype=int)].astype(np.float32, copy=False)\n",
    "X_prm = E_prm[np.asarray(ip_list, dtype=int)].astype(np.float32, copy=False)\n",
    "X = np.hstack([X_map, X_prm]).astype(np.float32, copy=False)\n",
    "\n",
    "np.save(out_dir / \"X_concat.npy\", X)\n",
    "join_df = pairs.iloc[chosen_rows].reset_index(drop=True)\n",
    "# --- Solution 1: tile reference scales (meters / m¬≤) ---\n",
    "tile_w = float(CFG.TILE_WIDTH_M)\n",
    "tile_h = float(CFG.TILE_HEIGHT_M)\n",
    "join_df[\"tile_w_m\"] = tile_w\n",
    "join_df[\"tile_h_m\"] = tile_h\n",
    "join_df[\"tile_diag_m\"] = float((tile_w**2 + tile_h**2) ** 0.5)\n",
    "join_df[\"tile_area_m2\"] = float(tile_w * tile_h)\n",
    "\n",
    "join_df.to_parquet(out_dir / \"train_pairs.parquet\", index=False)\n",
    "\n",
    "meta = {\n",
    "    \"shape\": [int(X.shape[0]), int(X.shape[1])],\n",
    "    \"map_dim\": int(E_map.shape[1]),\n",
    "    \"prompt_dim\": int(E_prm.shape[1]),\n",
    "    \"rows\": int(X.shape[0]),\n",
    "    \"skipped_pairs\": int(missing),\n",
    "    \"sources\": {\n",
    "        \"prompts_parquet\": str(prompts_pq),\n",
    "        \"map_npz\": str(map_npz_path),\n",
    "        \"prompt_npz\": str(prm_npz_path),\n",
    "    },\n",
    "    \"tile_ref\": {\n",
    "        \"tile_w_m\": tile_w,\n",
    "        \"tile_h_m\": tile_h,\n",
    "        \"tile_diag_m\": float((tile_w**2 + tile_h**2) ** 0.5),\n",
    "        \"tile_area_m2\": float(tile_w * tile_h),\n",
    "    },\n",
    "}\n",
    "(out_dir / \"meta.json\").write_text(json.dumps(meta, indent=2))\n",
    "\n",
    "print(\"‚úÖ Concatenation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5142d4b68c273d37",
   "metadata": {},
   "source": [
    "## üì• 4) Load & Basic Cleaning ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a494fd27dfe7681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded X: (564, 1701), pairs: (564, 6)\n",
      "After cleaning: X=(564, 1701), df=(564, 10), ops=['aggregate', 'displace', 'select', 'simplify']\n",
      "Example rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>map_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>tile_w_m</th>\n",
       "      <th>tile_h_m</th>\n",
       "      <th>tile_diag_m</th>\n",
       "      <th>tile_area_m2</th>\n",
       "      <th>operator</th>\n",
       "      <th>param_value</th>\n",
       "      <th>intensity</th>\n",
       "      <th>param_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1304</td>\n",
       "      <td>r00000000</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>565.685425</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1690</td>\n",
       "      <td>r00000001</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>565.685425</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>select</td>\n",
       "      <td>47.584</td>\n",
       "      <td>low</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1646</td>\n",
       "      <td>r00000002</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>565.685425</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>select</td>\n",
       "      <td>129.722</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.000811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1663</td>\n",
       "      <td>r00000005</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>565.685425</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>0.000</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0856</td>\n",
       "      <td>r00000006</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>565.685425</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>simplify</td>\n",
       "      <td>16.917</td>\n",
       "      <td>high</td>\n",
       "      <td>0.029905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0080</td>\n",
       "      <td>r00000007</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>565.685425</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>select</td>\n",
       "      <td>65.252</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.000408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1649</td>\n",
       "      <td>r00000009</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>565.685425</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>select</td>\n",
       "      <td>65.912</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.000412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0421</td>\n",
       "      <td>r00000010</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>565.685425</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>displace</td>\n",
       "      <td>5.634</td>\n",
       "      <td>high</td>\n",
       "      <td>0.009960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1223</td>\n",
       "      <td>r00000011</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>565.685425</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>aggregate</td>\n",
       "      <td>4.315</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.007628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1380</td>\n",
       "      <td>r00000013</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>565.685425</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>select</td>\n",
       "      <td>94.469</td>\n",
       "      <td>low</td>\n",
       "      <td>0.000590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  map_id  prompt_id  tile_w_m  tile_h_m  tile_diag_m  tile_area_m2   operator  \\\n",
       "0   1304  r00000000     400.0     400.0   565.685425      160000.0  aggregate   \n",
       "1   1690  r00000001     400.0     400.0   565.685425      160000.0     select   \n",
       "2   1646  r00000002     400.0     400.0   565.685425      160000.0     select   \n",
       "3   1663  r00000005     400.0     400.0   565.685425      160000.0  aggregate   \n",
       "4   0856  r00000006     400.0     400.0   565.685425      160000.0   simplify   \n",
       "5   0080  r00000007     400.0     400.0   565.685425      160000.0     select   \n",
       "6   1649  r00000009     400.0     400.0   565.685425      160000.0     select   \n",
       "7   0421  r00000010     400.0     400.0   565.685425      160000.0   displace   \n",
       "8   1223  r00000011     400.0     400.0   565.685425      160000.0  aggregate   \n",
       "9   1380  r00000013     400.0     400.0   565.685425      160000.0     select   \n",
       "\n",
       "   param_value intensity  param_norm  \n",
       "0        0.000    medium    0.000000  \n",
       "1       47.584       low    0.000297  \n",
       "2      129.722    medium    0.000811  \n",
       "3        0.000    medium    0.000000  \n",
       "4       16.917      high    0.029905  \n",
       "5       65.252    medium    0.000408  \n",
       "6       65.912    medium    0.000412  \n",
       "7        5.634      high    0.009960  \n",
       "8        4.315    medium    0.007628  \n",
       "9       94.469       low    0.000590  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === LOAD FUSED DATA (operator + param_value + Solution-1 normalized target) ===\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = np.load(PATHS.TRAIN_OUT / \"X_concat.npy\")\n",
    "pairs_df = pd.read_parquet(PATHS.TRAIN_OUT / \"train_pairs.parquet\")\n",
    "print(f\"Loaded X: {X.shape}, pairs: {pairs_df.shape}\")\n",
    "\n",
    "# --- Rebuild labels from Excel and merge (since train_pairs.parquet has only ids + tile refs) ---\n",
    "dfu = pd.read_excel(PATHS.USER_STUDY_XLSX, sheet_name=PATHS.RESPONSES_SHEET)\n",
    "\n",
    "dfu[PATHS.COMPLETE_COL] = dfu[PATHS.COMPLETE_COL].astype(bool)\n",
    "dfu[PATHS.REMOVE_COL]   = dfu[PATHS.REMOVE_COL].astype(bool)\n",
    "\n",
    "mask_excel = pd.Series(True, index=dfu.index)\n",
    "if PATHS.ONLY_COMPLETE:\n",
    "    mask_excel &= (dfu[PATHS.COMPLETE_COL] == True)\n",
    "if PATHS.EXCLUDE_REMOVED:\n",
    "    mask_excel &= (dfu[PATHS.REMOVE_COL] == False)\n",
    "dfu = dfu[mask_excel].copy()\n",
    "\n",
    "# match prompt_embeddings.py: prompt_id uses original Excel row index\n",
    "dfu = dfu.reset_index(drop=False).rename(columns={\"index\": \"_row\"})\n",
    "prefix = PATHS.PROMPT_ID_PREFIX\n",
    "width  = PATHS.PROMPT_ID_WIDTH\n",
    "dfu[\"prompt_id\"] = dfu[\"_row\"].apply(lambda r: f\"{prefix}{int(r):0{width}d}\")\n",
    "\n",
    "# normalize tile_id -> map_id (4-digit folder ids like 0001, 0345)\n",
    "tile_raw = dfu[PATHS.TILE_ID_COL]\n",
    "tile_num = pd.to_numeric(tile_raw, errors=\"coerce\")\n",
    "if tile_num.notna().all():\n",
    "    dfu[\"map_id\"] = tile_num.astype(int).astype(str).str.zfill(4)\n",
    "else:\n",
    "    dfu[\"map_id\"] = tile_raw.astype(str).str.strip().str.zfill(4)\n",
    "\n",
    "labels = dfu[[\n",
    "    \"map_id\",\n",
    "    \"prompt_id\",\n",
    "    PATHS.OPERATOR_COL,\n",
    "    PATHS.PARAM_VALUE_COL,\n",
    "    PATHS.INTENSITY_COL,   # kept for diagnostics (not used in training)\n",
    "]].copy()\n",
    "\n",
    "df = pairs_df.merge(labels, on=[\"map_id\", \"prompt_id\"], how=\"left\")\n",
    "\n",
    "OP_COL    = PATHS.OPERATOR_COL       # \"operator\"\n",
    "PARAM_COL = PATHS.PARAM_VALUE_COL    # \"param_value\"\n",
    "\n",
    "# clean target columns\n",
    "df[OP_COL] = df[OP_COL].astype(str).str.strip().str.lower()\n",
    "df[PARAM_COL] = pd.to_numeric(df[PARAM_COL], errors=\"coerce\")\n",
    "\n",
    "# --- Ensure tile refs exist (added in concat cell) ---\n",
    "assert \"tile_diag_m\" in df.columns and \"tile_area_m2\" in df.columns, \\\n",
    "    \"Missing tile_diag_m/tile_area_m2 in df (check concat cell saved them).\"\n",
    "\n",
    "df[\"tile_diag_m\"]  = pd.to_numeric(df[\"tile_diag_m\"], errors=\"coerce\")\n",
    "df[\"tile_area_m2\"] = pd.to_numeric(df[\"tile_area_m2\"], errors=\"coerce\")\n",
    "\n",
    "# --- Keep only rows that have targets + refs ---\n",
    "mask = (\n",
    "    df[OP_COL].notna() &\n",
    "    df[PARAM_COL].notna() &\n",
    "    df[\"tile_diag_m\"].notna() &\n",
    "    df[\"tile_area_m2\"].notna()\n",
    ")\n",
    "\n",
    "X  = X[mask.values].astype(np.float64, copy=False)\n",
    "df = df.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# --- Solution 1: build normalized regression target ---\n",
    "DIST_OPS_SET = set(DISTANCE_OPS)  # from src.config\n",
    "AREA_OPS_SET = set(AREA_OPS)      # from src.config\n",
    "\n",
    "df[\"param_norm\"] = np.nan\n",
    "\n",
    "m_dist = df[OP_COL].isin(DIST_OPS_SET)\n",
    "m_area = df[OP_COL].isin(AREA_OPS_SET)\n",
    "\n",
    "df.loc[m_dist, \"param_norm\"] = df.loc[m_dist, PARAM_COL] / df.loc[m_dist, \"tile_diag_m\"]\n",
    "df.loc[m_area, \"param_norm\"] = df.loc[m_area, PARAM_COL] / df.loc[m_area, \"tile_area_m2\"]\n",
    "\n",
    "assert df[\"param_norm\"].notna().all(), \"param_norm has NaNs (unknown operator or missing refs).\"\n",
    "\n",
    "print(f\"After cleaning: X={X.shape}, df={df.shape}, ops={sorted(df[OP_COL].unique())}\")\n",
    "print(\"Example rows:\")\n",
    "display(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997cab7",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è 5) Split & Targets ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bc0897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET SUMMARY ===\n",
      "Total rows (prompts): 564\n",
      "Unique maps: 401\n",
      "Multi-prompt maps (>1 prompt): 22\n",
      "Single-prompt maps (=1 prompt): 379\n",
      "\n",
      "Top 10 maps by prompt count:\n",
      "map_id\n",
      "1646    30\n",
      "1304    29\n",
      "1755    26\n",
      "1532    13\n",
      "0127    10\n",
      "0168     8\n",
      "0142     7\n",
      "0078     6\n",
      "0080     6\n",
      "0001     6\n",
      "dtype: int64\n",
      "\n",
      "=== SPLIT SUMMARY ===\n",
      "‚úÖ Split found (seed=42)\n",
      "Train maps: 287  (includes multi-prompt maps: 22)\n",
      "Val maps:   57\n",
      "Test maps:  57\n",
      "Rows -> Train: (450, 1701), Val: (57, 1701), Test: (57, 1701)\n",
      "‚úÖ Verified: no map_id leakage across splits.\n",
      "‚úÖ Verified: all multi-prompt maps are in TRAIN.\n",
      "‚úÖ Saved splits to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/splits.json\n",
      "\n",
      "TRAIN ‚Äî Operator counts\n",
      "operator\n",
      "select       146\n",
      "aggregate    134\n",
      "simplify     109\n",
      "displace      61\n",
      "Name: count, dtype: int64\n",
      "\n",
      "VAL ‚Äî Operator counts\n",
      "operator\n",
      "select       19\n",
      "aggregate    16\n",
      "simplify     13\n",
      "displace      9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "TEST ‚Äî Operator counts\n",
      "operator\n",
      "select       19\n",
      "aggregate    16\n",
      "simplify     13\n",
      "displace      9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "TRAIN ‚Äî Operator √ó Intensity table (counts)\n",
      "intensity  high  low  medium\n",
      "operator                    \n",
      "aggregate    37   38      59\n",
      "displace     13   20      28\n",
      "select       35   35      76\n",
      "simplify     25   26      58\n",
      "\n",
      "TRAIN ‚Äî Operator totals (row sums)\n",
      "operator\n",
      "aggregate    134\n",
      "displace      61\n",
      "select       146\n",
      "simplify     109\n",
      "dtype: int64\n",
      "\n",
      "TRAIN ‚Äî Intensity totals (col sums)\n",
      "intensity\n",
      "high      110\n",
      "low       119\n",
      "medium    221\n",
      "dtype: int64\n",
      "\n",
      "VAL ‚Äî Operator √ó Intensity table (counts)\n",
      "intensity  high  low  medium\n",
      "operator                    \n",
      "aggregate     5    6       5\n",
      "displace      3    3       3\n",
      "select        6    6       7\n",
      "simplify      4    4       5\n",
      "\n",
      "VAL ‚Äî Operator totals (row sums)\n",
      "operator\n",
      "aggregate    16\n",
      "displace      9\n",
      "select       19\n",
      "simplify     13\n",
      "dtype: int64\n",
      "\n",
      "VAL ‚Äî Intensity totals (col sums)\n",
      "intensity\n",
      "high      18\n",
      "low       19\n",
      "medium    20\n",
      "dtype: int64\n",
      "\n",
      "TEST ‚Äî Operator √ó Intensity table (counts)\n",
      "intensity  high  low  medium\n",
      "operator                    \n",
      "aggregate     5    6       5\n",
      "displace      3    3       3\n",
      "select        6    6       7\n",
      "simplify      4    4       5\n",
      "\n",
      "TEST ‚Äî Operator totals (row sums)\n",
      "operator\n",
      "aggregate    16\n",
      "displace      9\n",
      "select       19\n",
      "simplify     13\n",
      "dtype: int64\n",
      "\n",
      "TEST ‚Äî Intensity totals (col sums)\n",
      "intensity\n",
      "high      18\n",
      "low       19\n",
      "medium    20\n",
      "dtype: int64\n",
      "\n",
      "‚úÖ TRAIN: All operator√óintensity combos present.\n",
      "\n",
      "‚úÖ VAL: All operator√óintensity combos present.\n",
      "\n",
      "‚úÖ TEST: All operator√óintensity combos present.\n",
      "\n",
      "TRAIN ‚Äî prompts per map statistics\n",
      "TRAIN ‚Äî #maps with >1 prompt: 22\n",
      "\n",
      "VAL ‚Äî prompts per map statistics\n",
      "VAL ‚Äî #maps with >1 prompt: 0\n",
      "\n",
      "TEST ‚Äî prompts per map statistics\n",
      "TEST ‚Äî #maps with >1 prompt: 0\n",
      "\n",
      "TRAIN ‚Äî Top multi-prompt maps (forced to train):\n",
      "map_id\n",
      "1646    30\n",
      "1304    29\n",
      "1755    26\n",
      "1532    13\n",
      "0127    10\n",
      "0168     8\n",
      "0142     7\n",
      "0080     6\n",
      "0001     6\n",
      "0078     6\n",
      "0073     6\n",
      "0159     5\n",
      "0074     5\n",
      "0079     4\n",
      "0081     4\n",
      "0077     4\n",
      "0025     3\n",
      "0120     3\n",
      "0240     3\n",
      "0118     3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import json\n",
    "\n",
    "FIXED_CLASSES = [\"simplify\", \"select\", \"aggregate\", \"displace\"]\n",
    "USE_INTENSITY_FOR_STRAT = True  # will fall back to operator-only if needed\n",
    "\n",
    "OP_COL   = PATHS.OPERATOR_COL        # \"operator\"\n",
    "PARAM_COL = PATHS.PARAM_VALUE_COL    # \"param_value\"\n",
    "INT_COL  = PATHS.INTENSITY_COL       # \"intensity\"\n",
    "\n",
    "df = df.copy()\n",
    "df[OP_COL] = df[OP_COL].astype(str).str.strip().str.lower()\n",
    "if INT_COL in df.columns:\n",
    "    df[INT_COL] = df[INT_COL].astype(str).str.strip().str.lower()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Group stats: prompts per map_id\n",
    "# ------------------------------------------------------------\n",
    "prompt_counts = df.groupby(\"map_id\").size()\n",
    "multi_map_ids = prompt_counts[prompt_counts > 1].index.tolist()\n",
    "single_map_ids = prompt_counts[prompt_counts == 1].index.tolist()\n",
    "\n",
    "print(\"=== DATASET SUMMARY ===\")\n",
    "print(f\"Total rows (prompts): {len(df)}\")\n",
    "print(f\"Unique maps: {prompt_counts.shape[0]}\")\n",
    "print(f\"Multi-prompt maps (>1 prompt): {len(multi_map_ids)}\")\n",
    "print(f\"Single-prompt maps (=1 prompt): {len(single_map_ids)}\")\n",
    "print(\"\\nTop 10 maps by prompt count:\")\n",
    "print(prompt_counts.sort_values(ascending=False).head(10))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Map-level table for single maps (one row per map_id)\n",
    "# ------------------------------------------------------------\n",
    "df_single = df[df[\"map_id\"].isin(single_map_ids)].copy()\n",
    "map_level = df_single.groupby(\"map_id\").first().reset_index()\n",
    "\n",
    "# Build strat label: operator√óintensity if feasible, else operator only\n",
    "if USE_INTENSITY_FOR_STRAT and INT_COL in map_level.columns:\n",
    "    map_level[\"_strat\"] = map_level[OP_COL] + \"__\" + map_level[INT_COL]\n",
    "    vc = map_level[\"_strat\"].value_counts()\n",
    "    if (vc < 2).any():\n",
    "        print(\"\\n‚ö†Ô∏è Some operator√óintensity groups too rare (<2 single-maps). Falling back to operator-only stratification.\")\n",
    "        map_level[\"_strat\"] = map_level[OP_COL]\n",
    "else:\n",
    "    map_level[\"_strat\"] = map_level[OP_COL]\n",
    "\n",
    "def has_all_ops(dfx: pd.DataFrame) -> bool:\n",
    "    return set(dfx[OP_COL].unique()) >= set(FIXED_CLASSES)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Split single maps into train/val/test with retries\n",
    "# ------------------------------------------------------------\n",
    "test_ratio = CFG.TEST_RATIO\n",
    "val_ratio = CFG.VAL_RATIO\n",
    "val_rel = val_ratio / (1.0 - test_ratio)\n",
    "\n",
    "X_idx = np.arange(len(map_level))\n",
    "y_strat = map_level[\"_strat\"].to_numpy()\n",
    "map_ids_arr = map_level[\"map_id\"].to_numpy()\n",
    "\n",
    "best = None\n",
    "for attempt in range(500):\n",
    "    rs = CFG.SEED + attempt\n",
    "\n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=test_ratio, random_state=rs)\n",
    "    trainval_i, test_i = next(sss1.split(X_idx, y_strat))\n",
    "\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_rel, random_state=rs + 999)\n",
    "    train_i, val_i = next(sss2.split(trainval_i, y_strat[trainval_i]))\n",
    "\n",
    "    single_train_maps = set(map_ids_arr[trainval_i[train_i]])\n",
    "    single_val_maps   = set(map_ids_arr[trainval_i[val_i]])\n",
    "    single_test_maps  = set(map_ids_arr[test_i])\n",
    "\n",
    "    train_maps = set(multi_map_ids) | single_train_maps\n",
    "    val_maps   = single_val_maps\n",
    "    test_maps  = single_test_maps\n",
    "\n",
    "    # leakage check\n",
    "    if (train_maps & val_maps) or (train_maps & test_maps) or (val_maps & test_maps):\n",
    "        continue\n",
    "\n",
    "    df_train_tmp = df[df[\"map_id\"].isin(train_maps)]\n",
    "    df_val_tmp   = df[df[\"map_id\"].isin(val_maps)]\n",
    "    df_test_tmp  = df[df[\"map_id\"].isin(test_maps)]\n",
    "\n",
    "    # must contain all operators in each split\n",
    "    if not (has_all_ops(df_train_tmp) and has_all_ops(df_val_tmp) and has_all_ops(df_test_tmp)):\n",
    "        continue\n",
    "\n",
    "    best = (train_maps, val_maps, test_maps, rs)\n",
    "    break\n",
    "\n",
    "if best is None:\n",
    "    raise RuntimeError(\n",
    "        \"Could not find a leakage-safe split with operator coverage in all splits \"\n",
    "        \"and multi-prompt maps forced to TRAIN. \"\n",
    "        \"Try: USE_INTENSITY_FOR_STRAT=False, or adjust VAL/TEST ratios.\"\n",
    "    )\n",
    "\n",
    "train_maps, val_maps, test_maps, used_seed = best\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Build row-level splits (no leakage)\n",
    "# ------------------------------------------------------------\n",
    "train_idx = df.index[df[\"map_id\"].isin(train_maps)].to_numpy()\n",
    "val_idx   = df.index[df[\"map_id\"].isin(val_maps)].to_numpy()\n",
    "test_idx  = df.index[df[\"map_id\"].isin(test_maps)].to_numpy()\n",
    "\n",
    "X_train, X_val, X_test = X[train_idx], X[val_idx], X[test_idx]\n",
    "df_train = df.loc[train_idx].reset_index(drop=True)\n",
    "df_val   = df.loc[val_idx].reset_index(drop=True)\n",
    "df_test  = df.loc[test_idx].reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== SPLIT SUMMARY ===\")\n",
    "print(f\"‚úÖ Split found (seed={used_seed})\")\n",
    "print(f\"Train maps: {len(train_maps)}  (includes multi-prompt maps: {len(set(multi_map_ids))})\")\n",
    "print(f\"Val maps:   {len(val_maps)}\")\n",
    "print(f\"Test maps:  {len(test_maps)}\")\n",
    "print(f\"Rows -> Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Hard guarantees\n",
    "assert set(df_train[\"map_id\"]).isdisjoint(df_val[\"map_id\"])\n",
    "assert set(df_train[\"map_id\"]).isdisjoint(df_test[\"map_id\"])\n",
    "assert set(df_val[\"map_id\"]).isdisjoint(df_test[\"map_id\"])\n",
    "assert set(multi_map_ids).issubset(train_maps)\n",
    "print(\"‚úÖ Verified: no map_id leakage across splits.\")\n",
    "print(\"‚úÖ Verified: all multi-prompt maps are in TRAIN.\")\n",
    "\n",
    "split_path = PATHS.TRAIN_OUT / \"splits.json\"\n",
    "json.dump(\n",
    "    {\n",
    "        \"train_idx\": train_idx.tolist(),\n",
    "        \"val_idx\": val_idx.tolist(),\n",
    "        \"test_idx\": test_idx.tolist(),\n",
    "        \"seed_used\": int(used_seed),\n",
    "        \"use_intensity_for_strat\": bool(USE_INTENSITY_FOR_STRAT),\n",
    "    },\n",
    "    open(split_path, \"w\"),\n",
    "    indent=2\n",
    ")\n",
    "print(\"‚úÖ Saved splits to\", split_path)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Detailed diagnostics: operator + intensity coverage\n",
    "# ------------------------------------------------------------\n",
    "def print_operator_counts(dfx, name):\n",
    "    print(f\"\\n{name} ‚Äî Operator counts\")\n",
    "    print(dfx[OP_COL].value_counts())\n",
    "\n",
    "def print_op_intensity_table(dfx, name):\n",
    "    if INT_COL not in dfx.columns:\n",
    "        print(f\"\\n{name} ‚Äî intensity column missing; skipping op√óintensity table.\")\n",
    "        return\n",
    "    print(f\"\\n{name} ‚Äî Operator √ó Intensity table (counts)\")\n",
    "    tab = (\n",
    "        dfx.groupby([OP_COL, INT_COL]).size()\n",
    "        .unstack(fill_value=0)\n",
    "        .sort_index()\n",
    "    )\n",
    "    print(tab)\n",
    "    print(f\"\\n{name} ‚Äî Operator totals (row sums)\")\n",
    "    print(tab.sum(axis=1))\n",
    "    print(f\"\\n{name} ‚Äî Intensity totals (col sums)\")\n",
    "    print(tab.sum(axis=0))\n",
    "\n",
    "def report_missing_combos(dfx, name, all_ops, all_ints):\n",
    "    if INT_COL not in dfx.columns:\n",
    "        return\n",
    "    present = set(zip(dfx[OP_COL], dfx[INT_COL]))\n",
    "    missing = [(op, it) for op in all_ops for it in all_ints if (op, it) not in present]\n",
    "    if missing:\n",
    "        print(f\"\\n‚ö†Ô∏è {name}: Missing operator√óintensity combos ({len(missing)}):\")\n",
    "        print(missing[:40], \"...\" if len(missing) > 40 else \"\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ {name}: All operator√óintensity combos present.\")\n",
    "\n",
    "ALL_OPS = FIXED_CLASSES[:]  # enforce fixed order\n",
    "ALL_INTS = sorted(df[INT_COL].unique()) if INT_COL in df.columns else []\n",
    "\n",
    "print_operator_counts(df_train, \"TRAIN\")\n",
    "print_operator_counts(df_val,   \"VAL\")\n",
    "print_operator_counts(df_test,  \"TEST\")\n",
    "\n",
    "print_op_intensity_table(df_train, \"TRAIN\")\n",
    "print_op_intensity_table(df_val,   \"VAL\")\n",
    "print_op_intensity_table(df_test,  \"TEST\")\n",
    "\n",
    "report_missing_combos(df_train, \"TRAIN\", ALL_OPS, ALL_INTS)\n",
    "report_missing_combos(df_val,   \"VAL\",   ALL_OPS, ALL_INTS)\n",
    "report_missing_combos(df_test,  \"TEST\",  ALL_OPS, ALL_INTS)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) Map-level prompt multiplicity info per split\n",
    "# ------------------------------------------------------------\n",
    "def map_prompt_stats(map_set, name):\n",
    "    sub_counts = prompt_counts.loc[list(map_set)]\n",
    "    print(f\"\\n{name} ‚Äî prompts per map statistics\")\n",
    "    print(f\"{name} ‚Äî #maps with >1 prompt:\", int((sub_counts > 1).sum()))\n",
    "\n",
    "map_prompt_stats(train_maps, \"TRAIN\")\n",
    "map_prompt_stats(val_maps,   \"VAL\")\n",
    "map_prompt_stats(test_maps,  \"TEST\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) Optional: show top multi-prompt maps in TRAIN\n",
    "# ------------------------------------------------------------\n",
    "top_multi = prompt_counts.loc[multi_map_ids].sort_values(ascending=False).head(20)\n",
    "print(\"\\nTRAIN ‚Äî Top multi-prompt maps (forced to train):\")\n",
    "print(top_multi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187473d",
   "metadata": {},
   "source": [
    "## üßº 6) Modality-Aware Preprocessing (map only) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41942b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modality-aware preprocessing complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/preproc.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === MODALITY-AWARE PREPROCESSING ===\n",
    "def split_blocks(X):\n",
    "    X_map    = X[:, :MAP_DIM].astype(np.float64, copy=True)\n",
    "    X_prompt = X[:, MAP_DIM:MAP_DIM+PROMPT_DIM].astype(np.float64, copy=True)\n",
    "    return X_map, X_prompt\n",
    "\n",
    "def l2_normalize_rows(A, eps=1e-12):\n",
    "    nrm = np.sqrt((A * A).sum(axis=1, keepdims=True))\n",
    "    return A / np.maximum(nrm, eps)\n",
    "\n",
    "# split\n",
    "Xm_tr, Xp_tr = split_blocks(X_train)\n",
    "Xm_va, Xp_va = split_blocks(X_val)\n",
    "Xm_te, Xp_te = split_blocks(X_test)\n",
    "\n",
    "# prompts: L2 only\n",
    "Xp_tr = l2_normalize_rows(Xp_tr)\n",
    "Xp_va = l2_normalize_rows(Xp_va)\n",
    "Xp_te = l2_normalize_rows(Xp_te)\n",
    "\n",
    "# maps: inf‚ÜíNaN\n",
    "for A in (Xm_tr, Xm_va, Xm_te):\n",
    "    A[~np.isfinite(A)] = np.nan\n",
    "\n",
    "# impute (train)\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "Xm_tr_imp = imp.fit_transform(Xm_tr)\n",
    "Xm_va_imp = imp.transform(Xm_va)\n",
    "Xm_te_imp = imp.transform(Xm_te)\n",
    "\n",
    "# clip (5‚Äì95%) train thresholds\n",
    "q_lo = np.nanpercentile(Xm_tr_imp, 5, axis=0)\n",
    "q_hi = np.nanpercentile(Xm_tr_imp, 95, axis=0)\n",
    "def clip_to_q(A, lo, hi): return np.clip(A, lo, hi)\n",
    "\n",
    "Xm_tr_imp = clip_to_q(Xm_tr_imp, q_lo, q_hi)\n",
    "Xm_va_imp = clip_to_q(Xm_va_imp, q_lo, q_hi)\n",
    "Xm_te_imp = clip_to_q(Xm_te_imp, q_lo, q_hi)\n",
    "\n",
    "# drop zero-variance cols on train\n",
    "stds = np.nanstd(Xm_tr_imp, axis=0)\n",
    "keep_mask = stds > 1e-12\n",
    "\n",
    "# scale kept columns (train fit)\n",
    "scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5, 95))\n",
    "Xm_tr_kept = scaler.fit_transform(Xm_tr_imp[:, keep_mask])\n",
    "Xm_va_kept = scaler.transform(Xm_va_imp[:, keep_mask])\n",
    "Xm_te_kept = scaler.transform(Xm_te_imp[:, keep_mask])\n",
    "\n",
    "# rebuild full map dim (dropped cols = 0)\n",
    "Xm_tr_s = np.zeros_like(Xm_tr_imp, dtype=np.float64)\n",
    "Xm_va_s = np.zeros_like(Xm_va_imp, dtype=np.float64)\n",
    "Xm_te_s = np.zeros_like(Xm_te_imp, dtype=np.float64)\n",
    "Xm_tr_s[:, keep_mask] = Xm_tr_kept.astype(np.float64)\n",
    "Xm_va_s[:, keep_mask] = Xm_va_kept.astype(np.float64)\n",
    "Xm_te_s[:, keep_mask] = Xm_te_kept.astype(np.float64)\n",
    "\n",
    "# fuse back\n",
    "X_train_s = np.concatenate([Xm_tr_s, Xp_tr], axis=1).astype(np.float64)\n",
    "X_val_s   = np.concatenate([Xm_va_s, Xp_va], axis=1).astype(np.float64)\n",
    "X_test_s  = np.concatenate([Xm_te_s, Xp_te], axis=1).astype(np.float64)\n",
    "\n",
    "assert np.isfinite(X_train_s).all() and np.isfinite(X_val_s).all() and np.isfinite(X_test_s).all(), \"Non-finite after preprocessing.\"\n",
    "print(\"‚úÖ Modality-aware preprocessing complete.\")\n",
    "\n",
    "# save preprocessing bundle\n",
    "joblib.dump({\n",
    "    \"imp\": imp, \"q_lo\": q_lo, \"q_hi\": q_hi,\n",
    "    \"keep_mask\": keep_mask, \"scaler\": scaler,\n",
    "    \"map_dim\": MAP_DIM, \"prompt_dim\": PROMPT_DIM,\n",
    "    \"tile_diag_m\": CFG.TILE_DIAG_M,\"tile_area_m2\": CFG.TILE_AREA_M2,\n",
    "}, PATHS.TRAIN_OUT / \"preproc.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c32c2",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è 7) Class Weights ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0df2b2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator classes: ['aggregate', 'displace', 'select', 'simplify']\n",
      "Class weights: {'aggregate': np.float64(0.8395522388059702), 'displace': np.float64(1.8442622950819672), 'select': np.float64(0.7705479452054794), 'simplify': np.float64(1.0321100917431192)}\n",
      "Sample weight summary: {'min': 0.025684931506849314, 'max': 1.8442622950819672, 'mean': 0.6500530407829777}\n"
     ]
    }
   ],
   "source": [
    "# === BUILD y_train_cls + CLASS + MAP-AWARE SAMPLE WEIGHTS ===\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "OP_COL = PATHS.OPERATOR_COL  # \"operator\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Build y_train_cls directly from df_train\n",
    "# ------------------------------------------------------------\n",
    "# factorize gives stable integer labels starting at 0\n",
    "y_train_cls, class_names = pd.factorize(df_train[OP_COL], sort=True)\n",
    "n_classes = len(class_names)\n",
    "\n",
    "print(\"Operator classes:\", list(class_names))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Class weights (operator imbalance)\n",
    "# ------------------------------------------------------------\n",
    "classes = np.arange(n_classes)\n",
    "\n",
    "cls_w = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=y_train_cls\n",
    ")\n",
    "cls_w = np.asarray(cls_w, dtype=\"float64\")\n",
    "\n",
    "class_weight_map = dict(zip(class_names, cls_w))\n",
    "print(\"Class weights:\", class_weight_map)\n",
    "\n",
    "# per-sample class weight\n",
    "w_class = np.array([cls_w[c] for c in y_train_cls], dtype=\"float64\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Map-level weighting (prompt multiplicity correction)\n",
    "# ------------------------------------------------------------\n",
    "map_counts = df_train[\"map_id\"].value_counts()\n",
    "\n",
    "# each map contributes ~1 total weight\n",
    "w_map = df_train[\"map_id\"].map(lambda m: 1.0 / map_counts[m]).to_numpy(dtype=\"float64\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Final sample weights\n",
    "# ------------------------------------------------------------\n",
    "sample_w = w_class * w_map\n",
    "\n",
    "print(\n",
    "    \"Sample weight summary:\",\n",
    "    {\n",
    "        \"min\": float(sample_w.min()),\n",
    "        \"max\": float(sample_w.max()),\n",
    "        \"mean\": float(sample_w.mean()),\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf3c9cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['aggregate', 'displace', 'select', 'simplify']\n",
      "y_train_cls shape: (450,)\n",
      "y_val_cls shape: (57,)\n",
      "y_test_cls shape: (57,)\n"
     ]
    }
   ],
   "source": [
    "# === BUILD CLASS LABELS (train / val / test) ===\n",
    "\n",
    "OP_COL = PATHS.OPERATOR_COL  # \"operator\"\n",
    "\n",
    "# Train labels + class names\n",
    "y_train_cls, class_names = pd.factorize(df_train[OP_COL], sort=True)\n",
    "\n",
    "# Val/Test labels must use SAME class order\n",
    "y_val_cls = pd.Categorical(df_val[OP_COL], categories=class_names).codes\n",
    "y_test_cls = pd.Categorical(df_test[OP_COL], categories=class_names).codes\n",
    "\n",
    "# Safety checks\n",
    "assert (y_val_cls >= 0).all(), \"VAL contains unseen operator labels\"\n",
    "assert (y_test_cls >= 0).all(), \"TEST contains unseen operator labels\"\n",
    "\n",
    "print(\"Classes:\", list(class_names))\n",
    "print(\"y_train_cls shape:\", y_train_cls.shape)\n",
    "print(\"y_val_cls shape:\", y_val_cls.shape)\n",
    "print(\"y_test_cls shape:\", y_test_cls.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988f274",
   "metadata": {},
   "source": [
    "## üß† 8) Train MLP ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95133825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching 50 MLP configs...\n",
      "[01/50] cvF1=0.714¬±0.026 | VAL F1=0.755 acc=0.754 | (128, 64), Œ±=2.02e-02, lr=1.2e-03, bs=16\n",
      "[02/50] cvF1=0.864¬±0.062 | VAL F1=0.913 acc=0.912 | (256, 128), Œ±=3.49e-05, lr=1.7e-04, bs=64\n",
      "[03/50] cvF1=0.791¬±0.046 | VAL F1=0.819 acc=0.825 | (256,), Œ±=1.03e-02, lr=7.7e-04, bs=128\n",
      "[04/50] cvF1=0.833¬±0.045 | VAL F1=0.889 acc=0.895 | (256,), Œ±=1.18e-05, lr=2.7e-03, bs=128\n",
      "[05/50] cvF1=0.860¬±0.052 | VAL F1=0.873 acc=0.877 | (256, 128, 64), Œ±=5.47e-05, lr=1.9e-04, bs=16\n",
      "[06/50] cvF1=0.877¬±0.060 | VAL F1=0.844 acc=0.842 | (64,), Œ±=1.14e-04, lr=6.0e-04, bs=128\n",
      "[07/50] cvF1=0.870¬±0.060 | VAL F1=0.881 acc=0.877 | (64,), Œ±=1.03e-04, lr=8.0e-04, bs=32\n",
      "[08/50] cvF1=0.755¬±0.075 | VAL F1=0.842 acc=0.842 | (128, 64), Œ±=2.43e-02, lr=2.2e-04, bs=32\n",
      "[09/50] cvF1=0.840¬±0.028 | VAL F1=0.879 acc=0.877 | (256, 128, 64), Œ±=4.95e-05, lr=5.7e-04, bs=128\n",
      "[10/50] cvF1=0.865¬±0.064 | VAL F1=0.895 acc=0.895 | (64,), Œ±=1.45e-05, lr=7.9e-04, bs=16\n",
      "[11/50] cvF1=0.858¬±0.054 | VAL F1=0.879 acc=0.877 | (64,), Œ±=1.68e-05, lr=2.5e-03, bs=128\n",
      "[12/50] cvF1=0.786¬±0.022 | VAL F1=0.800 acc=0.807 | (256, 128, 64), Œ±=6.47e-03, lr=2.8e-04, bs=16\n",
      "[13/50] cvF1=0.833¬±0.059 | VAL F1=0.839 acc=0.842 | (128,), Œ±=2.39e-03, lr=4.5e-04, bs=64\n",
      "[14/50] cvF1=0.866¬±0.065 | VAL F1=0.895 acc=0.895 | (128, 64), Œ±=5.27e-04, lr=1.1e-04, bs=32\n",
      "[15/50] cvF1=0.868¬±0.062 | VAL F1=0.910 acc=0.912 | (64,), Œ±=7.94e-05, lr=9.5e-04, bs=32\n",
      "[16/50] cvF1=0.817¬±0.033 | VAL F1=0.854 acc=0.860 | (256, 128, 64), Œ±=6.43e-04, lr=6.4e-04, bs=32\n",
      "[17/50] cvF1=0.747¬±0.081 | VAL F1=0.754 acc=0.754 | (256, 128), Œ±=2.35e-02, lr=1.4e-03, bs=32\n",
      "[18/50] cvF1=0.777¬±0.040 | VAL F1=0.779 acc=0.789 | (128,), Œ±=1.29e-02, lr=7.6e-04, bs=128\n",
      "[19/50] cvF1=0.850¬±0.049 | VAL F1=0.862 acc=0.860 | (256, 128, 64), Œ±=4.80e-05, lr=1.2e-04, bs=128\n",
      "[20/50] cvF1=0.839¬±0.060 | VAL F1=0.840 acc=0.842 | (256, 128), Œ±=2.25e-04, lr=2.5e-04, bs=16\n",
      "[21/50] cvF1=0.718¬±0.032 | VAL F1=0.727 acc=0.737 | (128,), Œ±=2.27e-02, lr=7.9e-04, bs=16\n",
      "[22/50] cvF1=0.870¬±0.060 | VAL F1=0.881 acc=0.877 | (64,), Œ±=1.07e-04, lr=1.8e-04, bs=16\n",
      "[23/50] cvF1=0.870¬±0.044 | VAL F1=0.862 acc=0.860 | (64,), Œ±=4.84e-03, lr=2.0e-04, bs=128\n",
      "[24/50] cvF1=0.856¬±0.065 | VAL F1=0.878 acc=0.877 | (256,), Œ±=4.91e-05, lr=1.1e-03, bs=64\n",
      "[25/50] cvF1=0.756¬±0.065 | VAL F1=0.733 acc=0.754 | (64,), Œ±=1.28e-03, lr=2.3e-03, bs=32\n",
      "[26/50] cvF1=0.756¬±0.018 | VAL F1=0.782 acc=0.789 | (64,), Œ±=1.52e-02, lr=1.8e-03, bs=128\n",
      "[27/50] cvF1=0.862¬±0.070 | VAL F1=0.910 acc=0.912 | (128,), Œ±=2.15e-05, lr=3.5e-04, bs=32\n",
      "[28/50] cvF1=0.781¬±0.047 | VAL F1=0.856 acc=0.860 | (256, 128), Œ±=3.44e-03, lr=8.7e-04, bs=64\n",
      "[29/50] cvF1=0.864¬±0.064 | VAL F1=0.858 acc=0.860 | (256,), Œ±=4.38e-04, lr=1.5e-04, bs=32\n",
      "[30/50] cvF1=0.804¬±0.070 | VAL F1=0.822 acc=0.825 | (256,), Œ±=4.42e-03, lr=6.7e-04, bs=64\n",
      "[31/50] cvF1=0.844¬±0.028 | VAL F1=0.872 acc=0.877 | (256, 128, 64), Œ±=5.21e-04, lr=5.9e-04, bs=64\n",
      "[32/50] cvF1=0.873¬±0.061 | VAL F1=0.889 acc=0.895 | (128,), Œ±=1.23e-05, lr=1.4e-04, bs=64\n",
      "[33/50] cvF1=0.872¬±0.062 | VAL F1=0.910 acc=0.912 | (64,), Œ±=1.24e-04, lr=5.6e-04, bs=32\n",
      "[34/50] cvF1=0.867¬±0.056 | VAL F1=0.909 acc=0.912 | (256, 128), Œ±=7.36e-05, lr=4.0e-04, bs=64\n",
      "[35/50] cvF1=0.868¬±0.058 | VAL F1=0.882 acc=0.877 | (256, 128), Œ±=6.25e-05, lr=1.3e-04, bs=64\n",
      "[36/50] cvF1=0.812¬±0.030 | VAL F1=0.911 acc=0.912 | (256,), Œ±=3.64e-05, lr=2.4e-03, bs=16\n",
      "[37/50] cvF1=0.807¬±0.029 | VAL F1=0.840 acc=0.842 | (256, 128, 64), Œ±=1.59e-03, lr=1.9e-03, bs=128\n",
      "[38/50] cvF1=0.846¬±0.029 | VAL F1=0.874 acc=0.877 | (128, 64), Œ±=4.45e-05, lr=2.1e-03, bs=64\n",
      "[39/50] cvF1=0.860¬±0.071 | VAL F1=0.895 acc=0.895 | (128,), Œ±=2.66e-05, lr=3.4e-04, bs=32\n",
      "[40/50] cvF1=0.854¬±0.062 | VAL F1=0.858 acc=0.860 | (64,), Œ±=8.84e-05, lr=9.1e-04, bs=16\n",
      "[41/50] cvF1=0.865¬±0.066 | VAL F1=0.895 acc=0.895 | (128, 64), Œ±=1.68e-04, lr=2.8e-04, bs=32\n",
      "[42/50] cvF1=0.865¬±0.062 | VAL F1=0.875 acc=0.877 | (256,), Œ±=2.83e-04, lr=2.1e-04, bs=64\n",
      "[43/50] cvF1=0.850¬±0.036 | VAL F1=0.878 acc=0.877 | (128,), Œ±=1.49e-04, lr=2.5e-03, bs=32\n",
      "[44/50] cvF1=0.876¬±0.060 | VAL F1=0.861 acc=0.860 | (64,), Œ±=2.54e-04, lr=1.2e-04, bs=32\n",
      "[45/50] cvF1=0.860¬±0.066 | VAL F1=0.895 acc=0.895 | (128, 64), Œ±=7.22e-05, lr=1.1e-03, bs=64\n",
      "[46/50] cvF1=0.873¬±0.067 | VAL F1=0.808 acc=0.807 | (64,), Œ±=3.27e-05, lr=3.0e-03, bs=64\n",
      "[47/50] cvF1=0.776¬±0.053 | VAL F1=0.757 acc=0.754 | (64,), Œ±=2.49e-02, lr=4.0e-04, bs=64\n",
      "[48/50] cvF1=0.865¬±0.056 | VAL F1=0.891 acc=0.895 | (256, 128), Œ±=1.58e-04, lr=8.6e-04, bs=32\n",
      "[49/50] cvF1=0.841¬±0.067 | VAL F1=0.858 acc=0.860 | (128,), Œ±=7.02e-04, lr=4.6e-04, bs=32\n",
      "[50/50] cvF1=0.871¬±0.061 | VAL F1=0.895 acc=0.895 | (128, 64), Œ±=1.91e-05, lr=3.5e-04, bs=16\n",
      "\n",
      "=== Top candidates (by VAL macro-F1) ===\n",
      "VAL F1=0.913 (acc=0.912) | cvF1=0.864¬±0.062 | params={'hidden_layer_sizes': (256, 128), 'alpha': 3.487351559952691e-05, 'learning_rate_init': 0.00016998978382700761, 'batch_size': 64, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.911 (acc=0.912) | cvF1=0.812¬±0.030 | params={'hidden_layer_sizes': (256,), 'alpha': 3.6356826243945064e-05, 'learning_rate_init': 0.0023619797107806524, 'batch_size': 16, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.910 (acc=0.912) | cvF1=0.872¬±0.062 | params={'hidden_layer_sizes': (64,), 'alpha': 0.00012389502377355922, 'learning_rate_init': 0.0005639239991667559, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.910 (acc=0.912) | cvF1=0.868¬±0.062 | params={'hidden_layer_sizes': (64,), 'alpha': 7.939796552655359e-05, 'learning_rate_init': 0.0009519754482692684, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.910 (acc=0.912) | cvF1=0.862¬±0.070 | params={'hidden_layer_sizes': (128,), 'alpha': 2.1466070134458155e-05, 'learning_rate_init': 0.00035297465462888, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "\n",
      "üèÜ Selected params:\n",
      "{'activation': 'relu',\n",
      " 'alpha': 3.487351559952691e-05,\n",
      " 'batch_size': 64,\n",
      " 'early_stopping': False,\n",
      " 'hidden_layer_sizes': (256, 128),\n",
      " 'learning_rate_init': 0.00016998978382700761,\n",
      " 'max_iter': 800,\n",
      " 'random_state': 42,\n",
      " 'solver': 'adam',\n",
      " 'tol': 0.0001,\n",
      " 'verbose': False}\n",
      "\n",
      "===== VAL =====\n",
      "VAL: acc=0.9123  f1_macro=0.9132\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   aggregate       0.88      0.94      0.91        16\n",
      "    displace       0.89      0.89      0.89         9\n",
      "      select       0.89      0.89      0.89        19\n",
      "    simplify       1.00      0.92      0.96        13\n",
      "\n",
      "    accuracy                           0.91        57\n",
      "   macro avg       0.92      0.91      0.91        57\n",
      "weighted avg       0.91      0.91      0.91        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[15  1  0  0]\n",
      " [ 0  8  1  0]\n",
      " [ 2  0 17  0]\n",
      " [ 0  0  1 12]]\n",
      "\n",
      "===== TEST =====\n",
      "TEST: acc=0.8947  f1_macro=0.9039\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   aggregate       0.81      0.81      0.81        16\n",
      "    displace       1.00      1.00      1.00         9\n",
      "      select       0.90      0.95      0.92        19\n",
      "    simplify       0.92      0.85      0.88        13\n",
      "\n",
      "    accuracy                           0.89        57\n",
      "   macro avg       0.91      0.90      0.90        57\n",
      "weighted avg       0.90      0.89      0.89        57\n",
      "\n",
      "Confusion matrix:\n",
      " [[13  0  2  1]\n",
      " [ 0  9  0  0]\n",
      " [ 1  0 18  0]\n",
      " [ 2  0  0 11]]\n",
      "\n",
      "‚úÖ Saved final MLP (trained on ALL TRAIN) to: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/best_mlp_fulltrain.joblib\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# MLP search where each model trains on ALL training data\n",
    "# =========================\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# ---- numerics: keep float64 everywhere ----\n",
    "X_train_s = X_train_s.astype(np.float64, copy=False)\n",
    "X_val_s   = X_val_s.astype(np.float64, copy=False)\n",
    "X_test_s  = X_test_s.astype(np.float64, copy=False)\n",
    "sample_w  = sample_w.astype(np.float64, copy=False)\n",
    "\n",
    "# ---- group by map_id (maps can repeat; prompts don't) ----\n",
    "assert \"map_id\" in df_train.columns, \"df_train must contain 'map_id' for grouped CV.\"\n",
    "groups_tr = df_train[\"map_id\"].astype(str).values\n",
    "\n",
    "# ---- CV splitter (for scoring only) ----\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ---- search space helpers ----\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "def draw_params(n):\n",
    "    sizes = [(64,), (128,), (256,), (128, 64), (256, 128), (256, 128, 64)]\n",
    "    batches = [16, 32, 64, 128]\n",
    "    for _ in range(n):\n",
    "        yield {\n",
    "            \"hidden_layer_sizes\": sizes[rng.randint(len(sizes))],\n",
    "            \"alpha\": 10**rng.uniform(-5, np.log10(3e-2)),          # loguniform(1e-5, 3e-2)\n",
    "            \"learning_rate_init\": 10**rng.uniform(-4, np.log10(3e-3)),  # loguniform(1e-4, 3e-3)\n",
    "            \"batch_size\": batches[rng.randint(len(batches))],\n",
    "            \"activation\": \"relu\",\n",
    "            \"solver\": \"adam\",\n",
    "            \"max_iter\": 800,            # allow convergence w/o early stopping\n",
    "            \"early_stopping\": False,    # <‚Äî IMPORTANT: use ALL training samples\n",
    "            \"random_state\": 42,\n",
    "            \"verbose\": False,\n",
    "            \"tol\": 1e-4\n",
    "        }\n",
    "\n",
    "# ---- CV scorer using grouped folds; model sees only its fold-train here (for the score only) ----\n",
    "def cv_macro_f1(params):\n",
    "    scores = []\n",
    "    for tr_idx, va_idx in cv.split(X_train_s, y_train_cls, groups_tr):\n",
    "        clf = MLPClassifier(**params)\n",
    "        clf.fit(X_train_s[tr_idx], y_train_cls[tr_idx], sample_weight=sample_w[tr_idx])\n",
    "        pred = clf.predict(X_train_s[va_idx])\n",
    "        scores.append(f1_score(y_train_cls[va_idx], pred, average=\"macro\"))\n",
    "    return float(np.mean(scores)), float(np.std(scores))\n",
    "\n",
    "@dataclass\n",
    "class Candidate:\n",
    "    params: dict\n",
    "    cv_mean: float\n",
    "    cv_std: float\n",
    "    val_f1: float\n",
    "    val_acc: float\n",
    "\n",
    "# ---- run search ----\n",
    "N_ITER = 50   # tune this for time/quality tradeoff\n",
    "candidates = []\n",
    "\n",
    "print(f\"\\nSearching {N_ITER} MLP configs...\")\n",
    "for i, params in enumerate(draw_params(N_ITER), 1):\n",
    "    cv_mean, cv_std = cv_macro_f1(params)\n",
    "\n",
    "    # IMPORTANT PART: refit SAME PARAMS on FULL TRAIN (no early_stopping) so the model sees ALL training data\n",
    "    clf_full = MLPClassifier(**params)\n",
    "    clf_full.fit(X_train_s, y_train_cls, sample_weight=sample_w)\n",
    "\n",
    "    # evaluate on external VAL (never used for training)\n",
    "    val_pred = clf_full.predict(X_val_s)\n",
    "    val_f1 = f1_score(y_val_cls, val_pred, average=\"macro\")\n",
    "    val_acc = accuracy_score(y_val_cls, val_pred)\n",
    "\n",
    "    candidates.append(Candidate(params, cv_mean, cv_std, val_f1, val_acc))\n",
    "    print(f\"[{i:02d}/{N_ITER}] cvF1={cv_mean:.3f}¬±{cv_std:.3f} | VAL F1={val_f1:.3f} acc={val_acc:.3f} | {params['hidden_layer_sizes']}, Œ±={params['alpha']:.2e}, lr={params['learning_rate_init']:.1e}, bs={params['batch_size']}\")\n",
    "\n",
    "# ---- pick winner by external VAL macro-F1 (tie-breaker: VAL acc, then CV mean) ----\n",
    "candidates.sort(key=lambda c: (c.val_f1, c.val_acc, c.cv_mean), reverse=True)\n",
    "best = candidates[0]\n",
    "print(\"\\n=== Top candidates (by VAL macro-F1) ===\")\n",
    "for c in candidates[:5]:\n",
    "    print(f\"VAL F1={c.val_f1:.3f} (acc={c.val_acc:.3f}) | cvF1={c.cv_mean:.3f}¬±{c.cv_std:.3f} | params={c.params}\")\n",
    "\n",
    "print(\"\\nüèÜ Selected params:\")\n",
    "pprint(best.params)\n",
    "\n",
    "# ---- train final model on FULL TRAIN (no early_stopping so it uses 100% of train) ----\n",
    "final_mlp = MLPClassifier(**best.params)\n",
    "final_mlp.fit(X_train_s, y_train_cls, sample_weight=sample_w)\n",
    "\n",
    "# ---- evaluate on VAL & TEST ----\n",
    "for name, Xs, ys in [(\"VAL\", X_val_s, y_val_cls), (\"TEST\", X_test_s, y_test_cls)]:\n",
    "    yhat = final_mlp.predict(Xs)\n",
    "    acc  = accuracy_score(ys, yhat)\n",
    "    f1m  = f1_score(ys, yhat, average=\"macro\")\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    print(f\"{name}: acc={acc:.4f}  f1_macro={f1m:.4f}\")\n",
    "    print(classification_report(ys, yhat, target_names=list(class_names)))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(ys, yhat))\n",
    "\n",
    "# ---- save final model ----\n",
    "out_dir = Path(PATHS.TRAIN_OUT); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "import joblib\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"model\": final_mlp,\n",
    "        \"class_names\": list(class_names),\n",
    "        \"best_params\": best.params,\n",
    "    },\n",
    "    out_dir / \"best_mlp_fulltrain.joblib\"\n",
    ")\n",
    "print(f\"\\n‚úÖ Saved final MLP (trained on ALL TRAIN) to: {out_dir / 'best_mlp_fulltrain.joblib'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea7905b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'aggregate' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 1.0968500666661787\n",
      "best CV RMSE (param_norm units): 0.0038100191568649975\n",
      "best params:\n",
      "{'alpha': np.float64(0.0041619125396912095),\n",
      " 'hidden_layer_sizes': (64,),\n",
      " 'learning_rate_init': np.float64(0.00010558059144381523)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'displace' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 1.0912611307335696\n",
      "best CV RMSE (param_norm units): 0.0034785623694100435\n",
      "best params:\n",
      "{'alpha': np.float64(2.1453931225439485e-06),\n",
      " 'hidden_layer_sizes': (128,),\n",
      " 'learning_rate_init': np.float64(0.0001483039268456802)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'select' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 0.775985044970275\n",
      "best CV RMSE (param_norm units): 0.00017890435794292944\n",
      "best params:\n",
      "{'alpha': np.float64(3.11927680501103e-05),\n",
      " 'hidden_layer_sizes': (256,),\n",
      " 'learning_rate_init': np.float64(0.00010725209743172001)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'simplify' (predicting param_norm) ===\n",
      "best CV RMSE (scaled): 1.0006631121656742\n",
      "best CV RMSE (param_norm units): 0.004189178331297992\n",
      "best params:\n",
      "{'alpha': np.float64(2.1453931225439485e-06),\n",
      " 'hidden_layer_sizes': (128,),\n",
      " 'learning_rate_init': np.float64(0.0001483039268456802)}\n",
      "\n",
      "--- Regression with predicted classes (realistic) ---\n",
      "VAL: MAE=11.0798  RMSE=21.4469\n",
      "TEST: MAE=21.9618  RMSE=89.8775\n",
      "\n",
      "--- Regression with TRUE classes (oracle routing) ---\n",
      "VAL-oracle: MAE=7.9370  RMSE=13.0077\n",
      "TEST-oracle: MAE=18.5313  RMSE=88.0549\n",
      "\n",
      "‚úÖ Saved classification+regression bundle to: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/cls_plus_regressors.joblib\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Regression branch (one MLPRegressor per operator) ‚Äî Solution 1 (normalize by tile diag/area)\n",
    "# =========================\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import joblib\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "# -------------------------\n",
    "# 0) Targets: use param_norm (created earlier) + keep raw param_value for reporting\n",
    "# -------------------------\n",
    "OP_COL    = PATHS.OPERATOR_COL        # \"operator\"\n",
    "PARAM_COL = PATHS.PARAM_VALUE_COL     # \"param_value\"\n",
    "\n",
    "assert \"param_norm\" in df_train.columns and \"param_norm\" in df_val.columns and \"param_norm\" in df_test.columns, \\\n",
    "    \"Missing param_norm in df_* (make sure you added Solution 1 param_norm in the earlier cell).\"\n",
    "assert \"tile_diag_m\" in df_val.columns and \"tile_area_m2\" in df_val.columns, \\\n",
    "    \"Missing tile refs in df_val (check concat saved tile_diag_m/tile_area_m2).\"\n",
    "assert \"tile_diag_m\" in df_test.columns and \"tile_area_m2\" in df_test.columns, \\\n",
    "    \"Missing tile refs in df_test (check concat saved tile_diag_m/tile_area_m2).\"\n",
    "\n",
    "y_train_norm = pd.to_numeric(df_train[\"param_norm\"], errors=\"coerce\").to_numpy()\n",
    "y_val_norm   = pd.to_numeric(df_val[\"param_norm\"], errors=\"coerce\").to_numpy()\n",
    "y_test_norm  = pd.to_numeric(df_test[\"param_norm\"], errors=\"coerce\").to_numpy()\n",
    "\n",
    "y_val_raw  = pd.to_numeric(df_val[PARAM_COL], errors=\"coerce\").to_numpy()\n",
    "y_test_raw = pd.to_numeric(df_test[PARAM_COL], errors=\"coerce\").to_numpy()\n",
    "\n",
    "assert np.isfinite(y_train_norm).all() and np.isfinite(y_val_norm).all() and np.isfinite(y_test_norm).all(), \\\n",
    "    \"Non-finite values found in param_norm. Check your normalization step.\"\n",
    "\n",
    "# Optional: log1p on normalized values (usually not needed after normalization)\n",
    "USE_LOG1P = False\n",
    "if USE_LOG1P:\n",
    "    assert (y_train_norm >= 0).all() and (y_val_norm >= 0).all() and (y_test_norm >= 0).all(), \\\n",
    "        \"log1p selected but param_norm has negatives.\"\n",
    "    ytr_t = np.log1p(y_train_norm)\n",
    "    yva_t = np.log1p(y_val_norm)\n",
    "    yte_t = np.log1p(y_test_norm)\n",
    "    def inv_t(x): return np.expm1(x)\n",
    "else:\n",
    "    ytr_t = y_train_norm.copy()\n",
    "    yva_t = y_val_norm.copy()\n",
    "    yte_t = y_test_norm.copy()\n",
    "    def inv_t(x): return x\n",
    "\n",
    "# -------------------------\n",
    "# 1) Grouped CV by map_id (no leakage)\n",
    "# -------------------------\n",
    "assert \"map_id\" in df_train.columns\n",
    "gk = GroupKFold(n_splits=5)\n",
    "groups_tr = df_train[\"map_id\"].astype(str).values\n",
    "\n",
    "# -------------------------\n",
    "# 2) Search space for MLPRegressor\n",
    "# -------------------------\n",
    "base_reg = MLPRegressor(\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    learning_rate=\"adaptive\",\n",
    "    early_stopping=False,\n",
    "    max_iter=2000,\n",
    "    tol=1e-3,\n",
    "    random_state=42,\n",
    "    verbose=False,\n",
    "    batch_size=\"auto\"\n",
    ")\n",
    "\n",
    "param_dist_reg = {\n",
    "    \"hidden_layer_sizes\": [(64,), (128,), (256,), (128, 64), (256, 128)],\n",
    "    \"alpha\": loguniform(1e-6, 3e-2),\n",
    "    \"learning_rate_init\": loguniform(1e-4, 3e-3),\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# 3) Fit one regressor per operator (on normalized target)\n",
    "# -------------------------\n",
    "regressors = {}\n",
    "search_summaries = {}\n",
    "\n",
    "for cls_idx, cls_name in enumerate(class_names):\n",
    "    m_tr = (y_train_cls == cls_idx)\n",
    "\n",
    "    Xk = X_train_s[m_tr]\n",
    "    yk = ytr_t[m_tr]\n",
    "    gk_tr = groups_tr[m_tr]\n",
    "    wk = sample_w[m_tr]  # keep your weighting\n",
    "\n",
    "    if Xk.shape[0] < 10:\n",
    "        print(f\"‚ö†Ô∏è Skipping class '{cls_name}' (too few samples: {Xk.shape[0]}).\")\n",
    "        continue\n",
    "\n",
    "    # scale target for easier optimization (still OK)\n",
    "    t_scaler = StandardScaler()\n",
    "    yk_s = t_scaler.fit_transform(yk.reshape(-1, 1)).ravel()\n",
    "\n",
    "    splits = list(gk.split(Xk, yk_s, groups=gk_tr))\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base_reg,\n",
    "        param_distributions=param_dist_reg,\n",
    "        n_iter=40,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        cv=splits,\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    search.fit(Xk, yk_s, sample_weight=wk)\n",
    "\n",
    "    rmse_scaled = -search.best_score_\n",
    "    # Convert CV RMSE back to *normalized target units* (undo StandardScaler)\n",
    "    rmse_norm_units = rmse_scaled * float(t_scaler.scale_[0])\n",
    "\n",
    "    print(f\"\\n=== Regressor for class '{cls_name}' (predicting param_norm) ===\")\n",
    "    print(\"best CV RMSE (scaled):\", rmse_scaled)\n",
    "    print(\"best CV RMSE (param_norm units):\", rmse_norm_units)\n",
    "    print(\"best params:\"); pprint(search.best_params_)\n",
    "\n",
    "    search_summaries[cls_name] = {\n",
    "        \"rmse_scaled\": float(rmse_scaled),\n",
    "        \"rmse_param_norm\": float(rmse_norm_units),\n",
    "        \"params\": search.best_params_\n",
    "    }\n",
    "\n",
    "    reg_full = MLPRegressor(\n",
    "        **{**search.best_estimator_.get_params(), \"early_stopping\": False, \"max_iter\": 2000, \"random_state\": 42}\n",
    "    )\n",
    "    reg_full.fit(Xk, yk_s, sample_weight=wk)\n",
    "\n",
    "    regressors[cls_name] = (reg_full, t_scaler)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Routing + prediction: output REAL param_value (meters or m¬≤)\n",
    "# -------------------------\n",
    "DIST_OPS_SET = set(DISTANCE_OPS)\n",
    "AREA_OPS_SET = set(AREA_OPS)\n",
    "\n",
    "def route_and_predict_param_value(Xs, df_s, pred_cls_idx):\n",
    "    \"\"\"\n",
    "    Predict param_value in original units.\n",
    "    The regressor predicts param_norm (possibly log-transformed), then we:\n",
    "      - inverse StandardScaler\n",
    "      - inverse log1p if used\n",
    "      - unnormalize by tile ref: diag for distance ops, area for select\n",
    "    \"\"\"\n",
    "    yhat = np.full(len(pred_cls_idx), np.nan, dtype=float)\n",
    "\n",
    "    tile_diag = pd.to_numeric(df_s[\"tile_diag_m\"], errors=\"coerce\").to_numpy()\n",
    "    tile_area = pd.to_numeric(df_s[\"tile_area_m2\"], errors=\"coerce\").to_numpy()\n",
    "\n",
    "    for i, cidx in enumerate(pred_cls_idx):\n",
    "        cname = class_names[int(cidx)]\n",
    "        pack = regressors.get(cname)\n",
    "        if pack is None:\n",
    "            continue\n",
    "\n",
    "        reg, t_scaler = pack\n",
    "        pred_scaled = reg.predict(Xs[i:i+1])[0]\n",
    "        pred_t = t_scaler.inverse_transform([[pred_scaled]])[0, 0]  # back to ytr_t units\n",
    "        pred_norm = inv_t(pred_t)  # undo log1p if used\n",
    "\n",
    "        op = cname  # operator name\n",
    "\n",
    "        if op in DIST_OPS_SET:\n",
    "            yhat[i] = pred_norm * tile_diag[i]\n",
    "        elif op in AREA_OPS_SET:\n",
    "            yhat[i] = pred_norm * tile_area[i]\n",
    "        else:\n",
    "            # unknown operator group\n",
    "            yhat[i] = np.nan\n",
    "\n",
    "    return yhat\n",
    "\n",
    "def print_reg_metrics(name, y_true_raw, y_pred_raw):\n",
    "    mask = np.isfinite(y_true_raw) & np.isfinite(y_pred_raw)\n",
    "    if mask.sum() == 0:\n",
    "        print(f\"{name}: no finite pairs to evaluate.\")\n",
    "        return np.nan, np.nan\n",
    "    if mask.sum() < len(y_true_raw):\n",
    "        print(f\"{name}: dropped {len(y_true_raw) - mask.sum()} samples with NaNs.\")\n",
    "\n",
    "    yt = y_true_raw[mask]\n",
    "    yp = y_pred_raw[mask]\n",
    "\n",
    "    mae = mean_absolute_error(yt, yp)\n",
    "    rmse = float(np.sqrt(mean_squared_error(yt, yp)))\n",
    "    print(f\"{name}: MAE={mae:.4f}  RMSE={rmse:.4f}\")\n",
    "    return mae, rmse\n",
    "\n",
    "# Classification predictions (already trained classifier)\n",
    "clf_cls = final_mlp\n",
    "val_pred_cls  = clf_cls.predict(X_val_s)\n",
    "test_pred_cls = clf_cls.predict(X_test_s)\n",
    "\n",
    "# Predict param_value (real units)\n",
    "yhat_val  = route_and_predict_param_value(X_val_s,  df_val,  val_pred_cls)\n",
    "yhat_test = route_and_predict_param_value(X_test_s, df_test, test_pred_cls)\n",
    "\n",
    "print(\"\\n--- Regression with predicted classes (realistic) ---\")\n",
    "print_reg_metrics(\"VAL\",  y_val_raw,  yhat_val)\n",
    "print_reg_metrics(\"TEST\", y_test_raw, yhat_test)\n",
    "\n",
    "# Oracle routing (true operator)\n",
    "yhat_val_or  = route_and_predict_param_value(X_val_s,  df_val,  y_val_cls)\n",
    "yhat_test_or = route_and_predict_param_value(X_test_s, df_test, y_test_cls)\n",
    "\n",
    "print(\"\\n--- Regression with TRUE classes (oracle routing) ---\")\n",
    "print_reg_metrics(\"VAL-oracle\",  y_val_raw,  yhat_val_or)\n",
    "print_reg_metrics(\"TEST-oracle\", y_test_raw, yhat_test_or)\n",
    "\n",
    "# -------------------------\n",
    "# 5) Save bundle (include normalization metadata)\n",
    "# -------------------------\n",
    "bundle = {\n",
    "    \"classifier\": clf_cls,\n",
    "    \"regressors_by_class\": regressors,\n",
    "    \"class_names\": list(class_names),\n",
    "    \"use_log1p\": USE_LOG1P,\n",
    "    \"target\": \"param_norm\",\n",
    "    \"normalization\": {\n",
    "        \"distance_ops\": list(DISTANCE_OPS),\n",
    "        \"area_ops\": list(AREA_OPS),\n",
    "        \"tile_width_m\": float(CFG.TILE_WIDTH_M),\n",
    "        \"tile_height_m\": float(CFG.TILE_HEIGHT_M),\n",
    "        \"tile_diag_m\": float(CFG.TILE_DIAG_M),\n",
    "        \"tile_area_m2\": float(CFG.TILE_AREA_M2),\n",
    "    },\n",
    "    \"cv_summary\": search_summaries,\n",
    "}\n",
    "\n",
    "out_dir = Path(PATHS.TRAIN_OUT)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(bundle, out_dir / \"cls_plus_regressors.joblib\")\n",
    "print(f\"\\n‚úÖ Saved classification+regression bundle to: {out_dir / 'cls_plus_regressors.joblib'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54e54be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Rebuilt df + X\n",
      "X: (564, 1701) df: (564, 8)\n",
      "operators: ['aggregate', 'displace', 'select', 'simplify']\n",
      "\n",
      "=== SPLIT SUMMARY (reconstructed) ===\n",
      "seed used: 42\n",
      "rows: 564\n",
      "train/val/test rows: 450 38 76\n",
      "multi-prompt maps forced to train: 22\n",
      "\n",
      "=== ALIGNMENT CHECK ===\n",
      "df_train vs X_train_s: 450 450\n",
      "df_val   vs X_val_s  : 38 38\n",
      "df_test  vs X_test_s : 76 76\n",
      "\n",
      "=== Bundle loaded ===\n",
      "classifier: <class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "regressors keys: ['aggregate', 'displace', 'select', 'simplify']\n",
      "class_names: ['aggregate', 'displace', 'select', 'simplify']\n",
      "use_log1p: False\n",
      "\n",
      "=== Target distributions (param_value) ===\n",
      "VAL: n=38 min=0 p50=6.215 p95=101.126 p99=110.406 max=112.407 std=36.858\n",
      "TEST: n=76 min=0 p50=5.6315 p95=90 p99=110.537 max=114.021 std=33.1914\n",
      "\n",
      "=== Why CV RMSE looked ~1.0 in your search ===\n",
      "You scored RMSE on StandardScaler(y). So RMSE‚âà1 means ‚âà1 * std(y) in original units.\n",
      "\n",
      "aggregate  std(y)‚âà0.0034736  =>  RMSE_scaled * 0.0034736 ‚âà RMSE_original\n",
      "displace   std(y)‚âà0.00318765  =>  RMSE_scaled * 0.00318765 ‚âà RMSE_original\n",
      "select     std(y)‚âà0.000230551  =>  RMSE_scaled * 0.000230551 ‚âà RMSE_original\n",
      "simplify   std(y)‚âà0.0041864  =>  RMSE_scaled * 0.0041864 ‚âà RMSE_original\n",
      "\n",
      "=== Regression (realistic routing) ===\n",
      "\n",
      "VAL-real: MAE=25.3271 RMSE=44.7224 n=38\n",
      "Top RMSE contributors:\n",
      "01. y=112.407 yhat=0.000579769 err=112.406 sq=12635.2\n",
      "02. y=107 yhat=0.000372035 err=107 sq=11448.9\n",
      "03. y=100.09 yhat=0.000553987 err=100.089 sq=10017.9\n",
      "04. y=92.537 yhat=0.000484757 err=92.5365 sq=8563.01\n",
      "05. y=92 yhat=0.000455888 err=91.9995 sq=8463.92\n",
      "06. y=89.721 yhat=0.000456709 err=89.7205 sq=8049.78\n",
      "07. y=86.752 yhat=0.000581538 err=86.7514 sq=7525.81\n",
      "08. y=65 yhat=0.000331606 err=64.9997 sq=4224.96\n",
      "09. y=55 yhat=0.00035457 err=54.9996 sq=3024.96\n",
      "10. y=24.832 yhat=0.000220151 err=24.8318 sq=616.617\n",
      "\n",
      "TEST-real: MAE=24.6265 RMSE=41.3309 n=76\n",
      "Top RMSE contributors:\n",
      "01. y=114.021 yhat=0.000464426 err=114.021 sq=13000.7\n",
      "02. y=109.375 yhat=0.000598462 err=109.374 sq=11962.8\n",
      "03. y=92 yhat=0.000657164 err=91.9993 sq=8463.88\n",
      "04. y=90 yhat=0.000448628 err=89.9996 sq=8099.92\n",
      "05. y=90 yhat=0.000580089 err=89.9994 sq=8099.9\n",
      "06. y=89 yhat=0.000394057 err=88.9996 sq=7920.93\n",
      "07. y=88.494 yhat=0.000545105 err=88.4935 sq=7831.09\n",
      "08. y=81.429 yhat=0.000517163 err=81.4285 sq=6630.6\n",
      "09. y=76.349 yhat=0.000405992 err=76.3486 sq=5829.11\n",
      "10. y=75 yhat=0.000528971 err=74.9995 sq=5624.92\n",
      "\n",
      "=== Regression (oracle routing) ===\n",
      "\n",
      "VAL-oracle: MAE=25.3272 RMSE=44.7224 n=38\n",
      "Top RMSE contributors:\n",
      "01. y=112.407 yhat=0.000579769 err=112.406 sq=12635.2\n",
      "02. y=107 yhat=0.000372035 err=107 sq=11448.9\n",
      "03. y=100.09 yhat=0.000553987 err=100.089 sq=10017.9\n",
      "04. y=92.537 yhat=0.000484757 err=92.5365 sq=8563.01\n",
      "05. y=92 yhat=0.000455888 err=91.9995 sq=8463.92\n",
      "06. y=89.721 yhat=0.000456709 err=89.7205 sq=8049.78\n",
      "07. y=86.752 yhat=0.000581538 err=86.7514 sq=7525.81\n",
      "08. y=65 yhat=0.000331606 err=64.9997 sq=4224.96\n",
      "09. y=55 yhat=0.00035457 err=54.9996 sq=3024.96\n",
      "10. y=24.832 yhat=0.000220151 err=24.8318 sq=616.617\n",
      "\n",
      "TEST-oracle: MAE=24.6264 RMSE=41.3309 n=76\n",
      "Top RMSE contributors:\n",
      "01. y=114.021 yhat=0.000464426 err=114.021 sq=13000.7\n",
      "02. y=109.375 yhat=0.000598462 err=109.374 sq=11962.8\n",
      "03. y=92 yhat=0.000657164 err=91.9993 sq=8463.88\n",
      "04. y=90 yhat=0.000448628 err=89.9996 sq=8099.92\n",
      "05. y=90 yhat=0.000580089 err=89.9994 sq=8099.9\n",
      "06. y=89 yhat=0.000394057 err=88.9996 sq=7920.93\n",
      "07. y=88.494 yhat=0.000545105 err=88.4935 sq=7831.09\n",
      "08. y=81.429 yhat=0.000517163 err=81.4285 sq=6630.6\n",
      "09. y=76.349 yhat=0.000405992 err=76.3486 sq=5829.11\n",
      "10. y=75 yhat=0.000528971 err=74.9995 sq=5624.92\n",
      "\n",
      "=== Per-class oracle breakdown ===\n",
      "\n",
      "VAL-oracle/aggregate: MAE=1.4817 RMSE=2.9156 n=11\n",
      "Top RMSE contributors:\n",
      "01. y=7.031 yhat=0.0109339 err=7.02007 sq=49.2813\n",
      "02. y=5.463 yhat=0.00721604 err=5.45578 sq=29.7656\n",
      "03. y=3.806 yhat=0.00340902 err=3.80259 sq=14.4597\n",
      "04. y=0 yhat=0.00813159 err=-0.00813159 sq=6.61227e-05\n",
      "05. y=0 yhat=0.0052406 err=-0.0052406 sq=2.74638e-05\n",
      "06. y=0 yhat=0.00212294 err=-0.00212294 sq=4.50688e-06\n",
      "07. y=0 yhat=0.00206188 err=-0.00206188 sq=4.25135e-06\n",
      "08. y=0 yhat=0.00115203 err=-0.00115203 sq=1.32718e-06\n",
      "09. y=0 yhat=0.000907084 err=-0.000907084 sq=8.22801e-07\n",
      "10. y=0 yhat=-0.000300888 err=0.000300888 sq=9.05334e-08\n",
      "\n",
      "TEST-oracle/aggregate: MAE=1.3553 RMSE=2.8193 n=22\n",
      "Top RMSE contributors:\n",
      "01. y=10.094 yhat=0.0107531 err=10.0832 sq=101.672\n",
      "02. y=5.362 yhat=0.00552019 err=5.35648 sq=28.6919\n",
      "03. y=4.255 yhat=0.00144515 err=4.25355 sq=18.0927\n",
      "04. y=3.053 yhat=0.00378086 err=3.04922 sq=9.29774\n",
      "05. y=2.947 yhat=0.00232072 err=2.94468 sq=8.67114\n",
      "06. y=2.077 yhat=0.00115051 err=2.07585 sq=4.30915\n",
      "07. y=2.036 yhat=0.00204972 err=2.03395 sq=4.13695\n",
      "08. y=0 yhat=0.00426183 err=-0.00426183 sq=1.81632e-05\n",
      "09. y=0 yhat=0.00292137 err=-0.00292137 sq=8.53439e-06\n",
      "10. y=0 yhat=0.0024043 err=-0.0024043 sq=5.78065e-06\n",
      "\n",
      "VAL-oracle/displace: MAE=6.3753 RMSE=6.3863 n=6\n",
      "Top RMSE contributors:\n",
      "01. y=7.159 yhat=0.00840602 err=7.15059 sq=51.131\n",
      "02. y=6.412 yhat=0.0100452 err=6.40195 sq=40.985\n",
      "03. y=6.341 yhat=0.0112926 err=6.32971 sq=40.0652\n",
      "04. y=6.257 yhat=0.00843939 err=6.24856 sq=39.0445\n",
      "05. y=6.173 yhat=0.0103087 err=6.16269 sq=37.9788\n",
      "06. y=5.968 yhat=0.00956622 err=5.95843 sq=35.5029\n",
      "\n",
      "TEST-oracle/displace: MAE=4.3420 RMSE=4.5959 n=12\n",
      "Top RMSE contributors:\n",
      "01. y=7.789 yhat=0.00849251 err=7.78051 sq=60.5363\n",
      "02. y=6.285 yhat=0.00619721 err=6.2788 sq=39.4234\n",
      "03. y=5.634 yhat=0.009831 err=5.62417 sq=31.6313\n",
      "04. y=4.728 yhat=0.00935993 err=4.71864 sq=22.2656\n",
      "05. y=4.5 yhat=0.00878952 err=4.49121 sq=20.171\n",
      "06. y=4.055 yhat=0.0078826 err=4.04712 sq=16.3792\n",
      "07. y=3.959 yhat=0.0074801 err=3.95152 sq=15.6145\n",
      "08. y=3.733 yhat=0.0114426 err=3.72156 sq=13.85\n",
      "09. y=3.5 yhat=0.00788888 err=3.49211 sq=12.1948\n",
      "10. y=2.882 yhat=0.00758921 err=2.87441 sq=8.26224\n",
      "\n",
      "VAL-oracle/select: MAE=72.2778 RMSE=79.2964 n=12\n",
      "Top RMSE contributors:\n",
      "01. y=112.407 yhat=0.000579769 err=112.406 sq=12635.2\n",
      "02. y=107 yhat=0.000372035 err=107 sq=11448.9\n",
      "03. y=100.09 yhat=0.000553987 err=100.089 sq=10017.9\n",
      "04. y=92.537 yhat=0.000484757 err=92.5365 sq=8563.01\n",
      "05. y=92 yhat=0.000455888 err=91.9995 sq=8463.92\n",
      "06. y=89.721 yhat=0.000456709 err=89.7205 sq=8049.78\n",
      "07. y=86.752 yhat=0.000581538 err=86.7514 sq=7525.81\n",
      "08. y=65 yhat=0.000331606 err=64.9997 sq=4224.96\n",
      "09. y=55 yhat=0.00035457 err=54.9996 sq=3024.96\n",
      "10. y=24.832 yhat=0.000220151 err=24.8318 sq=616.617\n",
      "\n",
      "TEST-oracle/select: MAE=67.1510 RMSE=71.7078 n=25\n",
      "Top RMSE contributors:\n",
      "01. y=114.021 yhat=0.000464426 err=114.021 sq=13000.7\n",
      "02. y=109.375 yhat=0.000598462 err=109.374 sq=11962.8\n",
      "03. y=92 yhat=0.000657164 err=91.9993 sq=8463.88\n",
      "04. y=90 yhat=0.000448628 err=89.9996 sq=8099.92\n",
      "05. y=90 yhat=0.000580089 err=89.9994 sq=8099.9\n",
      "06. y=89 yhat=0.000394057 err=88.9996 sq=7920.93\n",
      "07. y=88.494 yhat=0.000545105 err=88.4935 sq=7831.09\n",
      "08. y=81.429 yhat=0.000517163 err=81.4285 sq=6630.6\n",
      "09. y=76.349 yhat=0.000405992 err=76.3486 sq=5829.11\n",
      "10. y=75 yhat=0.000528971 err=74.9995 sq=5624.92\n",
      "\n",
      "VAL-oracle/simplify: MAE=4.5055 RMSE=4.8349 n=9\n",
      "Top RMSE contributors:\n",
      "01. y=7 yhat=0.0125643 err=6.98744 sq=48.8243\n",
      "02. y=6.769 yhat=0.0115284 err=6.75747 sq=45.6634\n",
      "03. y=5.867 yhat=0.0102525 err=5.85675 sq=34.3015\n",
      "04. y=5.448 yhat=0.00939076 err=5.43861 sq=29.5785\n",
      "05. y=4.557 yhat=0.00676199 err=4.55024 sq=20.7047\n",
      "06. y=3.573 yhat=0.00626382 err=3.56674 sq=12.7216\n",
      "07. y=2.947 yhat=0.00544863 err=2.94155 sq=8.65272\n",
      "08. y=2.361 yhat=0.00555825 err=2.35544 sq=5.54811\n",
      "09. y=2.1 yhat=0.00508252 err=2.09492 sq=4.38868\n",
      "\n",
      "TEST-oracle/simplify: MAE=6.5243 RMSE=7.0634 n=17\n",
      "Top RMSE contributors:\n",
      "01. y=13.925 yhat=0.0213393 err=13.9037 sq=193.312\n",
      "02. y=10.124 yhat=0.0092756 err=10.1147 sq=102.308\n",
      "03. y=8.783 yhat=0.0129124 err=8.77009 sq=76.9144\n",
      "04. y=8 yhat=0.0126974 err=7.9873 sq=63.797\n",
      "05. y=7.295 yhat=0.0124276 err=7.28257 sq=53.0359\n",
      "06. y=7.135 yhat=0.0113898 err=7.12361 sq=50.7458\n",
      "07. y=6.957 yhat=0.0150613 err=6.94194 sq=48.1905\n",
      "08. y=6.928 yhat=0.0113497 err=6.91665 sq=47.8401\n",
      "09. y=6.242 yhat=0.0102285 err=6.23177 sq=38.835\n",
      "10. y=5.629 yhat=0.00610609 err=5.62289 sq=31.6169\n"
     ]
    }
   ],
   "source": [
    "import json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# =========================\n",
    "# CONFIG (EDIT IF NEEDED)\n",
    "# =========================\n",
    "PROJ_ROOT = Path(\"/Users/amirdonyadide/Documents/GitHub/Thesis\")\n",
    "\n",
    "TRAIN_OUT   = PROJ_ROOT / \"data/output/train_out\"\n",
    "PROMPT_OUT  = PROJ_ROOT / \"data/output/prompt_out\"\n",
    "\n",
    "X_CONCAT_NPY   = TRAIN_OUT / \"X_concat.npy\"\n",
    "TRAIN_PAIRS_PQ = TRAIN_OUT / \"train_pairs.parquet\"\n",
    "BUNDLE_JOBLIB  = TRAIN_OUT / \"cls_plus_regressors.joblib\"\n",
    "PREPROC_JOBLIB = TRAIN_OUT / \"preproc.joblib\"  # optional (we re-run anyway)\n",
    "\n",
    "PROMPTS_PQ   = PROMPT_OUT / \"prompts.parquet\"\n",
    "\n",
    "USER_STUDY_XLSX = PROJ_ROOT / \"data/userstudy/UserStudy.xlsx\"\n",
    "RESPONSES_SHEET = \"Responses\"\n",
    "\n",
    "# These must match your config.py values:\n",
    "TILE_ID_COL    = \"tile_id\"\n",
    "COMPLETE_COL   = \"complete\"\n",
    "REMOVE_COL     = \"remove\"\n",
    "TEXT_COL       = \"cleaned_text\"\n",
    "OPERATOR_COL   = \"operator\"\n",
    "PARAM_VALUE_COL= \"param_value\"\n",
    "INTENSITY_COL  = \"intensity\"   # present but not used for training\n",
    "\n",
    "PROMPT_ID_PREFIX = \"r\"\n",
    "PROMPT_ID_WIDTH  = 8\n",
    "\n",
    "# Split behavior (must match your CFG)\n",
    "SEED = 42\n",
    "TEST_RATIO = 0.20\n",
    "VAL_RATIO  = 0.10\n",
    "\n",
    "# Dims (read from meta.json if present)\n",
    "META_JSON = TRAIN_OUT / \"meta.json\"\n",
    "\n",
    "FIXED_CLASSES = [\"simplify\", \"select\", \"aggregate\", \"displace\"]  # same as your code\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def safe_numeric(s: pd.Series) -> np.ndarray:\n",
    "    ss = s.copy()\n",
    "    if ss.dtype == object:\n",
    "        ss = ss.astype(str).str.replace(\",\", \".\", regex=False).str.strip()\n",
    "    return pd.to_numeric(ss, errors=\"coerce\").to_numpy()\n",
    "\n",
    "def describe(name, y):\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    y = y[np.isfinite(y)]\n",
    "    if len(y) == 0:\n",
    "        return f\"{name}: EMPTY\"\n",
    "    qs = np.percentile(y, [0, 1, 5, 25, 50, 75, 95, 99, 100])\n",
    "    return (f\"{name}: n={len(y)} min={qs[0]:.6g} p50={qs[4]:.6g} \"\n",
    "            f\"p95={qs[6]:.6g} p99={qs[7]:.6g} max={qs[8]:.6g} std={np.std(y):.6g}\")\n",
    "\n",
    "def load_dims():\n",
    "    if META_JSON.exists():\n",
    "        meta = json.loads(META_JSON.read_text())\n",
    "        return int(meta[\"map_dim\"]), int(meta[\"prompt_dim\"])\n",
    "    raise FileNotFoundError(f\"Missing {META_JSON}. Can't infer MAP_DIM/PROMPT_DIM.\")\n",
    "\n",
    "def make_prompt_ids_from_excel_rowindex(dfu):\n",
    "    dfu = dfu.reset_index(drop=False).rename(columns={\"index\": \"_row\"})\n",
    "    dfu[\"prompt_id\"] = dfu[\"_row\"].apply(lambda r: f\"{PROMPT_ID_PREFIX}{int(r):0{PROMPT_ID_WIDTH}d}\")\n",
    "    return dfu\n",
    "\n",
    "def normalize_tile_to_map_id(tile_raw: pd.Series) -> pd.Series:\n",
    "    tile_num = pd.to_numeric(tile_raw, errors=\"coerce\")\n",
    "    if tile_num.notna().all():\n",
    "        return tile_num.astype(int).astype(str).str.zfill(4)\n",
    "    return tile_raw.astype(str).str.strip().str.zfill(4)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Rebuild df exactly like your pipeline\n",
    "# =========================\n",
    "def rebuild_df_and_X():\n",
    "    assert X_CONCAT_NPY.exists(), f\"Missing {X_CONCAT_NPY}\"\n",
    "    assert TRAIN_PAIRS_PQ.exists(), f\"Missing {TRAIN_PAIRS_PQ}\"\n",
    "    assert USER_STUDY_XLSX.exists(), f\"Missing {USER_STUDY_XLSX}\"\n",
    "\n",
    "    X = np.load(X_CONCAT_NPY).astype(np.float64, copy=False)\n",
    "    pairs_df = pd.read_parquet(TRAIN_PAIRS_PQ)\n",
    "    assert len(pairs_df) == X.shape[0], \"X_concat.npy and train_pairs.parquet row mismatch!\"\n",
    "\n",
    "    # Load Excel and filter (complete/remove)\n",
    "    dfu = pd.read_excel(USER_STUDY_XLSX, sheet_name=RESPONSES_SHEET)\n",
    "    dfu[COMPLETE_COL] = dfu[COMPLETE_COL].astype(bool)\n",
    "    dfu[REMOVE_COL]   = dfu[REMOVE_COL].astype(bool)\n",
    "\n",
    "    mask = pd.Series(True, index=dfu.index)\n",
    "    mask &= (dfu[COMPLETE_COL] == True)\n",
    "    mask &= (dfu[REMOVE_COL] == False)\n",
    "    dfu = dfu[mask].copy()\n",
    "\n",
    "    # prompt_id consistent with prompt_embeddings.py (row index in ORIGINAL excel)\n",
    "    dfu = make_prompt_ids_from_excel_rowindex(dfu)\n",
    "\n",
    "    # map_id normalization\n",
    "    dfu[\"map_id\"] = normalize_tile_to_map_id(dfu[TILE_ID_COL])\n",
    "\n",
    "    labels = dfu[[\"map_id\", \"prompt_id\", OPERATOR_COL, PARAM_VALUE_COL]].copy()\n",
    "    labels[OPERATOR_COL] = labels[OPERATOR_COL].astype(str).str.strip().str.lower()\n",
    "    labels[PARAM_VALUE_COL] = pd.to_numeric(labels[PARAM_VALUE_COL], errors=\"coerce\")\n",
    "\n",
    "    df = pairs_df.merge(labels, on=[\"map_id\", \"prompt_id\"], how=\"left\")\n",
    "\n",
    "    # clean & filter\n",
    "    df[OPERATOR_COL] = df[OPERATOR_COL].astype(str).str.strip().str.lower()\n",
    "    df[PARAM_VALUE_COL] = pd.to_numeric(df[PARAM_VALUE_COL], errors=\"coerce\")\n",
    "    keep = df[OPERATOR_COL].notna() & df[PARAM_VALUE_COL].notna()\n",
    "    X = X[keep.values]\n",
    "    df = df.loc[keep].reset_index(drop=True)\n",
    "\n",
    "    print(\"‚úÖ Rebuilt df + X\")\n",
    "    print(\"X:\", X.shape, \"df:\", df.shape)\n",
    "    print(\"operators:\", sorted(df[OPERATOR_COL].unique()))\n",
    "    return df, X\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) Recreate split indices exactly like your pipeline\n",
    "# =========================\n",
    "def make_split(df: pd.DataFrame):\n",
    "    OP_COL = OPERATOR_COL\n",
    "    INT_COL = INTENSITY_COL  # ignored; but kept for compatibility\n",
    "\n",
    "    # prompts per map\n",
    "    prompt_counts = df.groupby(\"map_id\").size()\n",
    "    multi_map_ids = prompt_counts[prompt_counts > 1].index.tolist()\n",
    "    single_map_ids = prompt_counts[prompt_counts == 1].index.tolist()\n",
    "\n",
    "    df_single = df[df[\"map_id\"].isin(single_map_ids)].copy()\n",
    "    map_level = df_single.groupby(\"map_id\").first().reset_index()\n",
    "\n",
    "    # stratify by operator ONLY (since you don't want intensity)\n",
    "    map_level[\"_strat\"] = map_level[OP_COL].astype(str)\n",
    "\n",
    "    def has_all_ops(dfx):\n",
    "        return set(dfx[OP_COL].unique()) >= set(FIXED_CLASSES)\n",
    "\n",
    "    test_ratio = TEST_RATIO\n",
    "    val_ratio = VAL_RATIO\n",
    "    val_rel = val_ratio / (1.0 - test_ratio)\n",
    "\n",
    "    X_idx = np.arange(len(map_level))\n",
    "    y_strat = map_level[\"_strat\"].to_numpy()\n",
    "    map_ids_arr = map_level[\"map_id\"].to_numpy()\n",
    "\n",
    "    best = None\n",
    "    for attempt in range(500):\n",
    "        rs = SEED + attempt\n",
    "\n",
    "        sss1 = StratifiedShuffleSplit(n_splits=1, test_size=test_ratio, random_state=rs)\n",
    "        trainval_i, test_i = next(sss1.split(X_idx, y_strat))\n",
    "\n",
    "        sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_rel, random_state=rs + 999)\n",
    "        train_i, val_i = next(sss2.split(trainval_i, y_strat[trainval_i]))\n",
    "\n",
    "        single_train_maps = set(map_ids_arr[trainval_i[train_i]])\n",
    "        single_val_maps   = set(map_ids_arr[trainval_i[val_i]])\n",
    "        single_test_maps  = set(map_ids_arr[test_i])\n",
    "\n",
    "        train_maps = set(multi_map_ids) | single_train_maps\n",
    "        val_maps   = single_val_maps\n",
    "        test_maps  = single_test_maps\n",
    "\n",
    "        if (train_maps & val_maps) or (train_maps & test_maps) or (val_maps & test_maps):\n",
    "            continue\n",
    "\n",
    "        df_train_tmp = df[df[\"map_id\"].isin(train_maps)]\n",
    "        df_val_tmp   = df[df[\"map_id\"].isin(val_maps)]\n",
    "        df_test_tmp  = df[df[\"map_id\"].isin(test_maps)]\n",
    "\n",
    "        if not (has_all_ops(df_train_tmp) and has_all_ops(df_val_tmp) and has_all_ops(df_test_tmp)):\n",
    "            continue\n",
    "\n",
    "        best = (train_maps, val_maps, test_maps, rs, multi_map_ids, prompt_counts)\n",
    "        break\n",
    "\n",
    "    if best is None:\n",
    "        raise RuntimeError(\"Could not find leakage-safe split with operator coverage.\")\n",
    "\n",
    "    train_maps, val_maps, test_maps, used_seed, multi_map_ids, prompt_counts = best\n",
    "\n",
    "    train_idx = df.index[df[\"map_id\"].isin(train_maps)].to_numpy()\n",
    "    val_idx   = df.index[df[\"map_id\"].isin(val_maps)].to_numpy()\n",
    "    test_idx  = df.index[df[\"map_id\"].isin(test_maps)].to_numpy()\n",
    "\n",
    "    return train_idx, val_idx, test_idx, used_seed, multi_map_ids, prompt_counts\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) Re-run your preprocessing to produce X_*_s\n",
    "# =========================\n",
    "def preprocess_like_pipeline(X_train, X_val, X_test, MAP_DIM, PROMPT_DIM):\n",
    "    def split_blocks(X):\n",
    "        Xm = X[:, :MAP_DIM].astype(np.float64, copy=True)\n",
    "        Xp = X[:, MAP_DIM:MAP_DIM+PROMPT_DIM].astype(np.float64, copy=True)\n",
    "        return Xm, Xp\n",
    "\n",
    "    def l2_normalize_rows(A, eps=1e-12):\n",
    "        nrm = np.sqrt((A * A).sum(axis=1, keepdims=True))\n",
    "        return A / np.maximum(nrm, eps)\n",
    "\n",
    "    Xm_tr, Xp_tr = split_blocks(X_train)\n",
    "    Xm_va, Xp_va = split_blocks(X_val)\n",
    "    Xm_te, Xp_te = split_blocks(X_test)\n",
    "\n",
    "    # prompts l2\n",
    "    Xp_tr = l2_normalize_rows(Xp_tr)\n",
    "    Xp_va = l2_normalize_rows(Xp_va)\n",
    "    Xp_te = l2_normalize_rows(Xp_te)\n",
    "\n",
    "    # maps: inf -> nan\n",
    "    for A in (Xm_tr, Xm_va, Xm_te):\n",
    "        A[~np.isfinite(A)] = np.nan\n",
    "\n",
    "    imp = SimpleImputer(strategy=\"median\")\n",
    "    Xm_tr_imp = imp.fit_transform(Xm_tr)\n",
    "    Xm_va_imp = imp.transform(Xm_va)\n",
    "    Xm_te_imp = imp.transform(Xm_te)\n",
    "\n",
    "    q_lo = np.nanpercentile(Xm_tr_imp, 5, axis=0)\n",
    "    q_hi = np.nanpercentile(Xm_tr_imp, 95, axis=0)\n",
    "\n",
    "    Xm_tr_imp = np.clip(Xm_tr_imp, q_lo, q_hi)\n",
    "    Xm_va_imp = np.clip(Xm_va_imp, q_lo, q_hi)\n",
    "    Xm_te_imp = np.clip(Xm_te_imp, q_lo, q_hi)\n",
    "\n",
    "    stds = np.nanstd(Xm_tr_imp, axis=0)\n",
    "    keep_mask = stds > 1e-12\n",
    "\n",
    "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5, 95))\n",
    "    Xm_tr_kept = scaler.fit_transform(Xm_tr_imp[:, keep_mask])\n",
    "    Xm_va_kept = scaler.transform(Xm_va_imp[:, keep_mask])\n",
    "    Xm_te_kept = scaler.transform(Xm_te_imp[:, keep_mask])\n",
    "\n",
    "    Xm_tr_s = np.zeros_like(Xm_tr_imp, dtype=np.float64)\n",
    "    Xm_va_s = np.zeros_like(Xm_va_imp, dtype=np.float64)\n",
    "    Xm_te_s = np.zeros_like(Xm_te_imp, dtype=np.float64)\n",
    "    Xm_tr_s[:, keep_mask] = Xm_tr_kept\n",
    "    Xm_va_s[:, keep_mask] = Xm_va_kept\n",
    "    Xm_te_s[:, keep_mask] = Xm_te_kept\n",
    "\n",
    "    X_train_s = np.concatenate([Xm_tr_s, Xp_tr], axis=1)\n",
    "    X_val_s   = np.concatenate([Xm_va_s, Xp_va], axis=1)\n",
    "    X_test_s  = np.concatenate([Xm_te_s, Xp_te], axis=1)\n",
    "\n",
    "    return X_train_s, X_val_s, X_test_s\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) Evaluate bundle regression + pinpoint issues\n",
    "# =========================\n",
    "def eval_bundle(df_train, df_val, df_test, X_train_s, X_val_s, X_test_s):\n",
    "    assert BUNDLE_JOBLIB.exists(), f\"Missing {BUNDLE_JOBLIB}\"\n",
    "    bundle = joblib.load(BUNDLE_JOBLIB)\n",
    "\n",
    "    clf = bundle.get(\"classifier\")\n",
    "    regs = bundle.get(\"regressors_by_class\")\n",
    "    class_names = list(bundle.get(\"class_names\"))\n",
    "    use_log1p = bool(bundle.get(\"use_log1p\", False))\n",
    "\n",
    "    print(\"\\n=== Bundle loaded ===\")\n",
    "    print(\"classifier:\", type(clf))\n",
    "    print(\"regressors keys:\", list(regs.keys())[:10])\n",
    "    print(\"class_names:\", class_names)\n",
    "    print(\"use_log1p:\", use_log1p)\n",
    "\n",
    "    def inv_t(x):\n",
    "        return np.expm1(x) if use_log1p else x\n",
    "\n",
    "    # Labels for routing\n",
    "    y_val_cls = pd.Categorical(df_val[OPERATOR_COL], categories=class_names).codes\n",
    "    y_test_cls = pd.Categorical(df_test[OPERATOR_COL], categories=class_names).codes\n",
    "    assert (y_val_cls >= 0).all() and (y_test_cls >= 0).all(), \"Unseen operator in val/test\"\n",
    "\n",
    "    # true y\n",
    "    y_val = safe_numeric(df_val[PARAM_VALUE_COL])\n",
    "    y_te  = safe_numeric(df_test[PARAM_VALUE_COL])\n",
    "\n",
    "    # quick distribution checks\n",
    "    print(\"\\n=== Target distributions (param_value) ===\")\n",
    "    print(describe(\"VAL\", y_val))\n",
    "    print(describe(\"TEST\", y_te))\n",
    "\n",
    "    # IMPORTANT: show what ‚ÄúCV RMSE ‚âà 1‚Äù actually means (std of each class)\n",
    "    print(\"\\n=== Why CV RMSE looked ~1.0 in your search ===\")\n",
    "    print(\"You scored RMSE on StandardScaler(y). So RMSE‚âà1 means ‚âà1 * std(y) in original units.\\n\")\n",
    "    for op in class_names:\n",
    "        if op in regs:\n",
    "            _, t_scaler = regs[op]\n",
    "            scale = float(np.ravel(t_scaler.scale_)[0])\n",
    "            print(f\"{op:10s} std(y)‚âà{scale:.6g}  =>  RMSE_scaled * {scale:.6g} ‚âà RMSE_original\")\n",
    "\n",
    "    def route_predict(Xs, cls_idx_arr):\n",
    "        yhat = np.full(len(cls_idx_arr), np.nan, dtype=float)\n",
    "        for i, cidx in enumerate(cls_idx_arr):\n",
    "            op = class_names[int(cidx)]\n",
    "            pack = regs.get(op)\n",
    "            if pack is None:\n",
    "                continue\n",
    "            reg, t_scaler = pack\n",
    "            pred_s = reg.predict(Xs[i:i+1])[0]                   # scaled y\n",
    "            pred_t = t_scaler.inverse_transform([[pred_s]])[0,0] # back to transformed/original\n",
    "            yhat[i] = pred_t\n",
    "        return yhat\n",
    "\n",
    "    def metrics(name, y_true, y_pred_t):\n",
    "        y_pred = inv_t(y_pred_t)\n",
    "        m = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "        y_true = y_true[m]\n",
    "        y_pred = y_pred[m]\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r = rmse(y_true, y_pred)\n",
    "\n",
    "        # Show top RMSE contributors\n",
    "        err = y_true - y_pred\n",
    "        sq = err**2\n",
    "        worst = np.argsort(-sq)[:min(10, len(sq))]\n",
    "        print(f\"\\n{name}: MAE={mae:.4f} RMSE={r:.4f} n={len(y_true)}\")\n",
    "        print(\"Top RMSE contributors:\")\n",
    "        for j, k in enumerate(worst, 1):\n",
    "            print(f\"{j:02d}. y={y_true[k]:.6g} yhat={y_pred[k]:.6g} err={err[k]:.6g} sq={sq[k]:.6g}\")\n",
    "        return mae, r\n",
    "\n",
    "    # Routing: predicted vs oracle\n",
    "    val_pred_cls  = clf.predict(X_val_s)\n",
    "    test_pred_cls = clf.predict(X_test_s)\n",
    "\n",
    "    yhat_val_real_t  = route_predict(X_val_s,  val_pred_cls)\n",
    "    yhat_te_real_t   = route_predict(X_test_s, test_pred_cls)\n",
    "    yhat_val_or_t    = route_predict(X_val_s,  y_val_cls)\n",
    "    yhat_te_or_t     = route_predict(X_test_s, y_test_cls)\n",
    "\n",
    "    print(\"\\n=== Regression (realistic routing) ===\")\n",
    "    metrics(\"VAL-real\",  y_val, yhat_val_real_t)\n",
    "    metrics(\"TEST-real\", y_te,  yhat_te_real_t)\n",
    "\n",
    "    print(\"\\n=== Regression (oracle routing) ===\")\n",
    "    metrics(\"VAL-oracle\",  y_val, yhat_val_or_t)\n",
    "    metrics(\"TEST-oracle\", y_te,  yhat_te_or_t)\n",
    "\n",
    "    # Per-class oracle breakdown (pinpoints which operator causes blow-up)\n",
    "    print(\"\\n=== Per-class oracle breakdown ===\")\n",
    "    for op_idx, op in enumerate(class_names):\n",
    "        m_val = (y_val_cls == op_idx)\n",
    "        m_te  = (y_test_cls == op_idx)\n",
    "        if m_val.sum() >= 5:\n",
    "            metrics(f\"VAL-oracle/{op}\", y_val[m_val], yhat_val_or_t[m_val])\n",
    "        if m_te.sum() >= 5:\n",
    "            metrics(f\"TEST-oracle/{op}\", y_te[m_te], yhat_te_or_t[m_te])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# RUN\n",
    "# =========================\n",
    "df, X = rebuild_df_and_X()\n",
    "MAP_DIM, PROMPT_DIM = load_dims()\n",
    "\n",
    "train_idx, val_idx, test_idx, used_seed, multi_map_ids, prompt_counts = make_split(df)\n",
    "\n",
    "print(\"\\n=== SPLIT SUMMARY (reconstructed) ===\")\n",
    "print(\"seed used:\", used_seed)\n",
    "print(\"rows:\", len(df))\n",
    "print(\"train/val/test rows:\", len(train_idx), len(val_idx), len(test_idx))\n",
    "print(\"multi-prompt maps forced to train:\", len(multi_map_ids))\n",
    "\n",
    "df_train = df.loc[train_idx].reset_index(drop=True)\n",
    "df_val   = df.loc[val_idx].reset_index(drop=True)\n",
    "df_test  = df.loc[test_idx].reset_index(drop=True)\n",
    "\n",
    "X_train, X_val, X_test = X[train_idx], X[val_idx], X[test_idx]\n",
    "\n",
    "# Preprocess exactly like your pipeline\n",
    "X_train_s, X_val_s, X_test_s = preprocess_like_pipeline(X_train, X_val, X_test, MAP_DIM, PROMPT_DIM)\n",
    "\n",
    "# Quick alignment sanity\n",
    "print(\"\\n=== ALIGNMENT CHECK ===\")\n",
    "print(\"df_train vs X_train_s:\", len(df_train), X_train_s.shape[0])\n",
    "print(\"df_val   vs X_val_s  :\", len(df_val), X_val_s.shape[0])\n",
    "print(\"df_test  vs X_test_s :\", len(df_test), X_test_s.shape[0])\n",
    "\n",
    "# Evaluate and diagnose\n",
    "eval_bundle(df_train, df_val, df_test, X_train_s, X_val_s, X_test_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfdda6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
