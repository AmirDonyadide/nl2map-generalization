{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f56a2c",
   "metadata": {},
   "source": [
    "## üß© 0) Setup & Imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f89f6ac45439b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:21:53.302406Z",
     "start_time": "2025-10-27T11:21:53.298709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFIG SUMMARY ===\n",
      "PROJ_ROOT  : /Users/amirdonyadide/Documents/GitHub/Thesis\n",
      "DATA_DIR   : /Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
      "INPUT_DIR  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input\n",
      "OUTPUT_DIR : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output\n",
      "MAPS_ROOT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/samples/pairs\n",
      "INPUT PAT. : *_input.geojson\n",
      "PROMPTS_CSV: /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/prompts.csv\n",
      "PAIRS_CSV  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/pairs.csv\n",
      "PROMPT_OUT : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "MAP_OUT    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n",
      "TRAIN_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out\n",
      "MODEL_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models\n",
      "SPLIT_OUT  : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/splits\n",
      "PRM_NPZ    : /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/prompts_embeddings.npz\n",
      "--- Model ---\n",
      "USE_MODEL  : dan\n",
      "MAP_DIM    : 165\n",
      "PROMPT_DIM : 512\n",
      "FUSED_DIM  : 677\n",
      "BATCH_SIZE : 512\n",
      "VAL/TEST   : 0.15 0.15\n",
      "SEED       : 42\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out\n",
      "üßπ Removing old directory: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/models\n",
      "‚úÖ All output folders cleaned and recreated fresh.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================== PARAMETERS / IMPORTS =====================\n",
    "from pathlib import Path\n",
    "import sys, subprocess, numpy as np, pandas as pd, joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, make_scorer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from scipy.stats import loguniform, randint\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Project config\n",
    "PROJ_ROOT = Path(\"../\").resolve()\n",
    "SRC_DIR   = PROJ_ROOT / \"src\"\n",
    "if str(PROJ_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJ_ROOT))\n",
    "\n",
    "from src.config import PATHS, CFG, print_summary\n",
    "print_summary()\n",
    "\n",
    "# Dims (fallbacks if CFG unset)\n",
    "MAP_DIM     = CFG.MAP_DIM or 165\n",
    "PROMPT_DIM  = CFG.PROMPT_DIM or 512\n",
    "FUSED_DIM   = CFG.FUSED_DIM or (MAP_DIM + PROMPT_DIM)\n",
    "BATCH_SIZE  = CFG.BATCH_SIZE\n",
    "\n",
    "# Clean outputs for a fresh run\n",
    "PATHS.clean_outputs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a2350e",
   "metadata": {},
   "source": [
    "## üìö 1) Build Prompt Embeddings (USE) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9ed0df45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:21:55.572701Z",
     "start_time": "2025-10-27T11:21:55.570071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMD: /opt/anaconda3/envs/thesis/bin/python -m src.mapvec.prompts.prompt_embeddings --input /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/prompts.csv --model dan --l2 --out_dir /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out -v\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      2\u001b[39m cmd = [\n\u001b[32m      3\u001b[39m     sys.executable, \u001b[33m\"\u001b[39m\u001b[33m-m\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msrc.mapvec.prompts.prompt_embeddings\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m--input\u001b[39m\u001b[33m\"\u001b[39m,    \u001b[38;5;28mstr\u001b[39m(PATHS.PROMPTS_CSV),\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m-v\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m ]\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCMD:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(cmd))\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m res = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPATHS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPROJ_ROOT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res.returncode != \u001b[32m0\u001b[39m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSystemExit\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPrompt embedding step failed.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/thesis/lib/python3.11/subprocess.py:550\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Popen(*popenargs, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m         stdout, stderr = \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    552\u001b[39m         process.kill()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/thesis/lib/python3.11/subprocess.py:1201\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1199\u001b[39m         stderr = \u001b[38;5;28mself\u001b[39m.stderr.read()\n\u001b[32m   1200\u001b[39m         \u001b[38;5;28mself\u001b[39m.stderr.close()\n\u001b[32m-> \u001b[39m\u001b[32m1201\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1203\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/thesis/lib/python3.11/subprocess.py:1264\u001b[39m, in \u001b[36mPopen.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m     endtime = _time() + timeout\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1266\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1267\u001b[39m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[32m   1268\u001b[39m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[32m   1269\u001b[39m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[32m   1270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/thesis/lib/python3.11/subprocess.py:2053\u001b[39m, in \u001b[36mPopen._wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   2051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.returncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2052\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2053\u001b[39m (pid, sts) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[32m   2055\u001b[39m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[32m   2056\u001b[39m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[32m   2057\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pid == \u001b[38;5;28mself\u001b[39m.pid:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/thesis/lib/python3.11/subprocess.py:2011\u001b[39m, in \u001b[36mPopen._try_wait\u001b[39m\u001b[34m(self, wait_flags)\u001b[39m\n\u001b[32m   2009\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[32m   2010\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2011\u001b[39m     (pid, sts) = os.waitpid(\u001b[38;5;28mself\u001b[39m.pid, wait_flags)\n\u001b[32m   2012\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[32m   2013\u001b[39m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[32m   2014\u001b[39m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[32m   2015\u001b[39m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[32m   2016\u001b[39m     pid = \u001b[38;5;28mself\u001b[39m.pid\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === PROMPT EMBEDDINGS ===\n",
    "cmd = [\n",
    "    sys.executable, \"-m\", \"src.mapvec.prompts.prompt_embeddings\",\n",
    "    \"--input\",    str(PATHS.PROMPTS_CSV),\n",
    "    \"--model\",    str(CFG.USE_MODEL),\n",
    "    \"--l2\",\n",
    "    \"--out_dir\",  str(PATHS.PROMPT_OUT),\n",
    "    \"-v\",\n",
    "]\n",
    "print(\"CMD:\", \" \".join(cmd))\n",
    "res = subprocess.run(cmd, cwd=str(PATHS.PROJ_ROOT))\n",
    "if res.returncode != 0:\n",
    "    raise SystemExit(\"Prompt embedding step failed.\")\n",
    "print(\"‚úÖ Prompt embeddings completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c0b51",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è 2) Build Map Embeddings (geometric) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca0c3d8b71fc70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T11:26:59.687350Z",
     "start_time": "2025-10-27T11:26:19.901557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMD: /opt/anaconda3/envs/thesis/bin/python -m src.mapvec.maps.map_embeddings --root /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/samples/pairs --pattern *_input.geojson --out_dir /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out --norm fixed --norm-wh 400x400 -v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:07:06 | DEBUG | PROJECT_ROOT=/Users/amirdonyadide/Documents/GitHub/Thesis\n",
      "17:07:06 | DEBUG | DATA_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
      "17:07:06 | INFO | Scanning /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/samples/pairs (pattern=*_input.geojson)‚Ä¶\n",
      "17:07:06 | INFO | First pass: counting polygons to normalize poly_count‚Ä¶\n",
      "17:07:11 | INFO | Max polygons across dataset: 789\n",
      "17:07:12 | INFO | OK  map_id=0073  -> vector[165]\n",
      "17:07:14 | INFO | OK  map_id=0080  -> vector[165]\n",
      "17:07:14 | INFO | OK  map_id=0093  -> vector[165]\n",
      "17:07:18 | INFO | OK  map_id=0122  -> vector[165]\n",
      "17:07:19 | INFO | OK  map_id=0123  -> vector[165]\n",
      "17:07:20 | INFO | OK  map_id=0127  -> vector[165]\n",
      "17:07:21 | INFO | OK  map_id=0158  -> vector[165]\n",
      "17:07:23 | INFO | OK  map_id=0159  -> vector[165]\n",
      "17:07:24 | INFO | OK  map_id=0160  -> vector[165]\n",
      "17:07:25 | INFO | OK  map_id=0165  -> vector[165]\n",
      "17:07:26 | INFO | OK  map_id=0167  -> vector[165]\n",
      "17:07:27 | INFO | OK  map_id=0168  -> vector[165]\n",
      "17:07:28 | INFO | OK  map_id=0171  -> vector[165]\n",
      "17:07:31 | INFO | OK  map_id=0208  -> vector[165]\n",
      "17:07:34 | INFO | OK  map_id=0209  -> vector[165]\n",
      "17:07:35 | INFO | OK  map_id=0215  -> vector[165]\n",
      "17:07:36 | INFO | OK  map_id=0240  -> vector[165]\n",
      "17:07:37 | INFO | OK  map_id=0256  -> vector[165]\n",
      "17:07:38 | INFO | OK  map_id=0257  -> vector[165]\n",
      "17:07:39 | INFO | OK  map_id=0262  -> vector[165]\n",
      "17:07:41 | INFO | OK  map_id=0285  -> vector[165]\n",
      "17:07:42 | INFO | OK  map_id=0286  -> vector[165]\n",
      "17:07:46 | INFO | OK  map_id=0288  -> vector[165]\n",
      "17:07:48 | INFO | OK  map_id=0289  -> vector[165]\n",
      "17:07:49 | INFO | OK  map_id=0313  -> vector[165]\n",
      "17:07:50 | INFO | OK  map_id=0341  -> vector[165]\n",
      "17:07:51 | INFO | OK  map_id=0362  -> vector[165]\n",
      "17:07:53 | INFO | OK  map_id=0363  -> vector[165]\n",
      "17:07:55 | INFO | OK  map_id=0364  -> vector[165]\n",
      "17:07:56 | INFO | OK  map_id=0379  -> vector[165]\n",
      "17:07:57 | INFO | OK  map_id=0389  -> vector[165]\n",
      "17:07:59 | INFO | OK  map_id=0390  -> vector[165]\n",
      "17:08:00 | INFO | OK  map_id=0409  -> vector[165]\n",
      "17:08:02 | INFO | OK  map_id=0410  -> vector[165]\n",
      "17:08:03 | INFO | OK  map_id=0412  -> vector[165]\n",
      "17:08:04 | INFO | OK  map_id=0413  -> vector[165]\n",
      "17:08:06 | INFO | OK  map_id=0414  -> vector[165]\n",
      "17:08:07 | INFO | OK  map_id=0417  -> vector[165]\n",
      "17:08:08 | INFO | OK  map_id=0421  -> vector[165]\n",
      "17:08:09 | INFO | OK  map_id=0426  -> vector[165]\n",
      "17:08:10 | INFO | OK  map_id=0427  -> vector[165]\n",
      "17:08:11 | INFO | OK  map_id=0432  -> vector[165]\n",
      "17:08:12 | INFO | OK  map_id=0433  -> vector[165]\n",
      "17:08:13 | INFO | OK  map_id=0437  -> vector[165]\n",
      "17:08:16 | INFO | OK  map_id=0438  -> vector[165]\n",
      "17:08:17 | INFO | OK  map_id=0439  -> vector[165]\n",
      "17:08:19 | INFO | OK  map_id=0454  -> vector[165]\n",
      "17:08:20 | INFO | OK  map_id=0458  -> vector[165]\n",
      "17:08:21 | INFO | OK  map_id=0459  -> vector[165]\n",
      "17:08:23 | INFO | OK  map_id=0460  -> vector[165]\n",
      "17:08:24 | INFO | OK  map_id=0466  -> vector[165]\n",
      "17:08:25 | INFO | OK  map_id=0469  -> vector[165]\n",
      "17:08:27 | INFO | OK  map_id=0471  -> vector[165]\n",
      "17:08:29 | INFO | OK  map_id=0472  -> vector[165]\n",
      "17:08:32 | INFO | OK  map_id=0474  -> vector[165]\n",
      "17:08:33 | INFO | OK  map_id=0475  -> vector[165]\n",
      "17:08:35 | INFO | OK  map_id=0479  -> vector[165]\n",
      "17:08:36 | INFO | OK  map_id=0480  -> vector[165]\n",
      "17:08:37 | INFO | OK  map_id=0481  -> vector[165]\n",
      "17:08:37 | INFO | OK  map_id=0482  -> vector[165]\n",
      "17:08:38 | INFO | OK  map_id=0508  -> vector[165]\n",
      "17:08:39 | INFO | OK  map_id=0509  -> vector[165]\n",
      "17:08:41 | INFO | OK  map_id=0518  -> vector[165]\n",
      "17:08:47 | INFO | OK  map_id=0520  -> vector[165]\n",
      "17:08:48 | INFO | OK  map_id=0521  -> vector[165]\n",
      "17:08:50 | INFO | OK  map_id=0523  -> vector[165]\n",
      "17:08:52 | INFO | OK  map_id=0527  -> vector[165]\n",
      "17:08:53 | INFO | OK  map_id=0528  -> vector[165]\n",
      "17:08:54 | INFO | OK  map_id=0529  -> vector[165]\n",
      "17:08:56 | INFO | OK  map_id=0530  -> vector[165]\n",
      "17:08:56 | INFO | OK  map_id=0553  -> vector[165]\n",
      "17:08:57 | INFO | OK  map_id=0557  -> vector[165]\n",
      "17:08:58 | INFO | OK  map_id=0575  -> vector[165]\n",
      "17:08:59 | INFO | OK  map_id=0576  -> vector[165]\n",
      "17:09:01 | INFO | OK  map_id=0594  -> vector[165]\n",
      "17:09:02 | INFO | OK  map_id=0595  -> vector[165]\n",
      "17:09:03 | INFO | OK  map_id=0600  -> vector[165]\n",
      "17:09:05 | INFO | OK  map_id=0605  -> vector[165]\n",
      "17:09:06 | INFO | OK  map_id=0606  -> vector[165]\n",
      "17:09:07 | INFO | OK  map_id=0608  -> vector[165]\n",
      "17:09:09 | INFO | OK  map_id=0609  -> vector[165]\n",
      "17:09:10 | INFO | OK  map_id=0611  -> vector[165]\n",
      "17:09:12 | INFO | OK  map_id=0614  -> vector[165]\n",
      "17:09:13 | INFO | OK  map_id=0615  -> vector[165]\n",
      "17:09:14 | INFO | OK  map_id=0618  -> vector[165]\n",
      "17:09:15 | INFO | OK  map_id=0623  -> vector[165]\n",
      "17:09:16 | INFO | OK  map_id=0624  -> vector[165]\n",
      "17:09:17 | INFO | OK  map_id=0645  -> vector[165]\n",
      "17:09:18 | INFO | OK  map_id=0646  -> vector[165]\n",
      "17:09:19 | INFO | OK  map_id=0655  -> vector[165]\n",
      "17:09:20 | INFO | OK  map_id=0656  -> vector[165]\n",
      "17:09:21 | INFO | OK  map_id=0657  -> vector[165]\n",
      "17:09:22 | INFO | OK  map_id=0658  -> vector[165]\n",
      "17:09:24 | INFO | OK  map_id=0659  -> vector[165]\n",
      "17:09:25 | INFO | OK  map_id=0667  -> vector[165]\n",
      "17:09:26 | INFO | OK  map_id=0672  -> vector[165]\n",
      "17:09:27 | INFO | OK  map_id=0699  -> vector[165]\n",
      "17:09:29 | INFO | OK  map_id=0700  -> vector[165]\n",
      "17:09:30 | INFO | OK  map_id=0701  -> vector[165]\n",
      "17:09:32 | INFO | OK  map_id=0706  -> vector[165]\n",
      "17:09:33 | INFO | OK  map_id=0707  -> vector[165]\n",
      "17:09:34 | INFO | OK  map_id=0715  -> vector[165]\n",
      "17:09:35 | INFO | OK  map_id=0721  -> vector[165]\n",
      "17:09:36 | INFO | OK  map_id=0747  -> vector[165]\n",
      "17:09:38 | INFO | OK  map_id=0748  -> vector[165]\n",
      "17:09:39 | INFO | OK  map_id=0749  -> vector[165]\n",
      "17:09:40 | INFO | OK  map_id=0755  -> vector[165]\n",
      "17:09:41 | INFO | OK  map_id=0758  -> vector[165]\n",
      "17:09:42 | INFO | OK  map_id=0759  -> vector[165]\n",
      "17:09:43 | INFO | OK  map_id=0762  -> vector[165]\n",
      "17:09:44 | INFO | OK  map_id=0770  -> vector[165]\n",
      "17:09:46 | INFO | OK  map_id=0804  -> vector[165]\n",
      "17:09:47 | INFO | OK  map_id=0807  -> vector[165]\n",
      "17:09:48 | INFO | OK  map_id=0808  -> vector[165]\n",
      "17:09:50 | INFO | OK  map_id=0809  -> vector[165]\n",
      "17:09:52 | INFO | OK  map_id=0819  -> vector[165]\n",
      "17:09:53 | INFO | OK  map_id=0848  -> vector[165]\n",
      "17:09:55 | INFO | OK  map_id=0853  -> vector[165]\n",
      "17:09:57 | INFO | OK  map_id=0854  -> vector[165]\n",
      "17:09:58 | INFO | OK  map_id=0856  -> vector[165]\n",
      "17:09:59 | INFO | OK  map_id=0857  -> vector[165]\n",
      "17:10:05 | INFO | OK  map_id=0858  -> vector[165]\n",
      "17:10:06 | INFO | OK  map_id=0859  -> vector[165]\n",
      "17:10:07 | INFO | OK  map_id=0867  -> vector[165]\n",
      "17:10:11 | INFO | OK  map_id=0868  -> vector[165]\n",
      "17:10:11 | INFO | OK  map_id=0869  -> vector[165]\n",
      "17:10:12 | INFO | OK  map_id=0901  -> vector[165]\n",
      "17:10:14 | INFO | OK  map_id=0903  -> vector[165]\n",
      "17:10:15 | INFO | OK  map_id=0904  -> vector[165]\n",
      "17:10:16 | INFO | OK  map_id=0905  -> vector[165]\n",
      "17:10:22 | INFO | OK  map_id=0906  -> vector[165]\n",
      "17:10:23 | INFO | OK  map_id=0907  -> vector[165]\n",
      "17:10:25 | INFO | OK  map_id=0908  -> vector[165]\n",
      "17:10:28 | INFO | OK  map_id=0917  -> vector[165]\n",
      "17:10:29 | INFO | OK  map_id=0918  -> vector[165]\n",
      "17:10:32 | INFO | OK  map_id=0926  -> vector[165]\n",
      "17:10:33 | INFO | OK  map_id=0947  -> vector[165]\n",
      "17:10:35 | INFO | OK  map_id=0948  -> vector[165]\n",
      "17:10:38 | INFO | OK  map_id=0949  -> vector[165]\n",
      "17:10:39 | INFO | OK  map_id=0950  -> vector[165]\n",
      "17:10:40 | INFO | OK  map_id=0951  -> vector[165]\n",
      "17:10:41 | INFO | OK  map_id=0952  -> vector[165]\n",
      "17:10:43 | INFO | OK  map_id=0966  -> vector[165]\n",
      "17:10:46 | INFO | OK  map_id=0967  -> vector[165]\n",
      "17:10:47 | INFO | OK  map_id=0970  -> vector[165]\n",
      "17:10:48 | INFO | OK  map_id=0971  -> vector[165]\n",
      "17:10:50 | INFO | OK  map_id=0974  -> vector[165]\n",
      "17:10:51 | INFO | OK  map_id=0975  -> vector[165]\n",
      "17:10:52 | INFO | OK  map_id=0976  -> vector[165]\n",
      "17:10:53 | INFO | OK  map_id=0994  -> vector[165]\n",
      "17:10:55 | INFO | OK  map_id=0995  -> vector[165]\n",
      "17:10:56 | INFO | OK  map_id=0997  -> vector[165]\n",
      "17:10:58 | INFO | OK  map_id=0998  -> vector[165]\n",
      "17:11:00 | INFO | OK  map_id=1019  -> vector[165]\n",
      "17:11:03 | INFO | OK  map_id=1020  -> vector[165]\n",
      "17:11:05 | INFO | OK  map_id=1052  -> vector[165]\n",
      "17:11:07 | INFO | OK  map_id=1053  -> vector[165]\n",
      "17:11:08 | INFO | OK  map_id=1054  -> vector[165]\n",
      "17:11:09 | INFO | OK  map_id=1055  -> vector[165]\n",
      "17:11:11 | INFO | OK  map_id=1056  -> vector[165]\n",
      "17:11:12 | INFO | OK  map_id=1057  -> vector[165]\n",
      "17:11:14 | INFO | OK  map_id=1069  -> vector[165]\n",
      "17:11:15 | INFO | OK  map_id=1070  -> vector[165]\n",
      "17:11:17 | INFO | OK  map_id=1090  -> vector[165]\n",
      "17:11:18 | INFO | OK  map_id=1091  -> vector[165]\n",
      "17:11:19 | INFO | OK  map_id=1092  -> vector[165]\n",
      "17:11:20 | INFO | OK  map_id=1100  -> vector[165]\n",
      "17:11:21 | INFO | OK  map_id=1103  -> vector[165]\n",
      "17:11:22 | INFO | OK  map_id=1105  -> vector[165]\n",
      "17:11:24 | INFO | OK  map_id=1106  -> vector[165]\n",
      "17:11:25 | INFO | OK  map_id=1118  -> vector[165]\n",
      "17:11:27 | INFO | OK  map_id=1119  -> vector[165]\n",
      "17:11:29 | INFO | OK  map_id=1120  -> vector[165]\n",
      "17:11:31 | INFO | OK  map_id=1139  -> vector[165]\n",
      "17:11:33 | INFO | OK  map_id=1140  -> vector[165]\n",
      "17:11:33 | INFO | OK  map_id=1148  -> vector[165]\n",
      "17:11:34 | INFO | OK  map_id=1155  -> vector[165]\n",
      "17:11:36 | INFO | OK  map_id=1157  -> vector[165]\n",
      "17:11:38 | INFO | OK  map_id=1168  -> vector[165]\n",
      "17:11:40 | INFO | OK  map_id=1169  -> vector[165]\n",
      "17:11:42 | INFO | OK  map_id=1170  -> vector[165]\n",
      "17:11:43 | INFO | OK  map_id=1197  -> vector[165]\n",
      "17:11:45 | INFO | OK  map_id=1198  -> vector[165]\n",
      "17:11:46 | INFO | OK  map_id=1202  -> vector[165]\n",
      "17:11:49 | INFO | OK  map_id=1203  -> vector[165]\n",
      "17:11:51 | INFO | OK  map_id=1204  -> vector[165]\n",
      "17:11:52 | INFO | OK  map_id=1217  -> vector[165]\n",
      "17:11:54 | INFO | OK  map_id=1218  -> vector[165]\n",
      "17:11:55 | INFO | OK  map_id=1219  -> vector[165]\n",
      "17:11:56 | INFO | OK  map_id=1221  -> vector[165]\n",
      "17:11:58 | INFO | OK  map_id=1222  -> vector[165]\n",
      "17:11:58 | INFO | OK  map_id=1231  -> vector[165]\n",
      "17:12:00 | INFO | OK  map_id=1233  -> vector[165]\n",
      "17:12:01 | INFO | OK  map_id=1234  -> vector[165]\n",
      "17:12:03 | INFO | OK  map_id=1261  -> vector[165]\n",
      "17:12:04 | INFO | OK  map_id=1269  -> vector[165]\n",
      "17:12:08 | INFO | OK  map_id=1270  -> vector[165]\n",
      "17:12:09 | INFO | OK  map_id=1271  -> vector[165]\n",
      "17:12:10 | INFO | OK  map_id=1276  -> vector[165]\n",
      "17:12:12 | INFO | OK  map_id=1277  -> vector[165]\n",
      "17:12:13 | INFO | OK  map_id=1283  -> vector[165]\n",
      "17:12:14 | INFO | OK  map_id=1284  -> vector[165]\n",
      "17:12:15 | INFO | OK  map_id=1285  -> vector[165]\n",
      "17:12:17 | INFO | OK  map_id=1295  -> vector[165]\n",
      "17:12:20 | INFO | OK  map_id=1296  -> vector[165]\n",
      "17:12:21 | INFO | OK  map_id=1297  -> vector[165]\n",
      "17:12:22 | INFO | OK  map_id=1303  -> vector[165]\n",
      "17:12:24 | INFO | OK  map_id=1304  -> vector[165]\n",
      "17:12:25 | INFO | OK  map_id=1310  -> vector[165]\n",
      "17:12:28 | INFO | OK  map_id=1319  -> vector[165]\n",
      "17:12:29 | INFO | OK  map_id=1333  -> vector[165]\n",
      "17:12:30 | INFO | OK  map_id=1334  -> vector[165]\n",
      "17:12:33 | INFO | OK  map_id=1344  -> vector[165]\n",
      "17:12:33 | INFO | OK  map_id=1349  -> vector[165]\n",
      "17:12:35 | INFO | OK  map_id=1364  -> vector[165]\n",
      "17:12:38 | INFO | OK  map_id=1365  -> vector[165]\n",
      "17:12:41 | INFO | OK  map_id=1366  -> vector[165]\n",
      "17:12:45 | INFO | OK  map_id=1367  -> vector[165]\n",
      "17:12:46 | INFO | OK  map_id=1368  -> vector[165]\n",
      "17:12:47 | INFO | OK  map_id=1369  -> vector[165]\n",
      "17:12:48 | INFO | OK  map_id=1377  -> vector[165]\n",
      "17:12:49 | INFO | OK  map_id=1378  -> vector[165]\n",
      "17:12:50 | INFO | OK  map_id=1385  -> vector[165]\n",
      "17:12:52 | INFO | OK  map_id=1386  -> vector[165]\n",
      "17:12:53 | INFO | OK  map_id=1399  -> vector[165]\n",
      "17:12:54 | INFO | OK  map_id=1401  -> vector[165]\n",
      "17:12:55 | INFO | OK  map_id=1408  -> vector[165]\n",
      "17:12:58 | INFO | OK  map_id=1409  -> vector[165]\n",
      "17:13:01 | INFO | OK  map_id=1410  -> vector[165]\n",
      "17:13:02 | INFO | OK  map_id=1413  -> vector[165]\n",
      "17:13:04 | INFO | OK  map_id=1414  -> vector[165]\n",
      "17:13:06 | INFO | OK  map_id=1415  -> vector[165]\n",
      "17:13:08 | INFO | OK  map_id=1416  -> vector[165]\n",
      "17:13:09 | INFO | OK  map_id=1417  -> vector[165]\n",
      "17:13:11 | INFO | OK  map_id=1418  -> vector[165]\n",
      "17:13:12 | INFO | OK  map_id=1434  -> vector[165]\n",
      "17:13:13 | INFO | OK  map_id=1438  -> vector[165]\n",
      "17:13:15 | INFO | OK  map_id=1439  -> vector[165]\n",
      "17:13:16 | INFO | OK  map_id=1450  -> vector[165]\n",
      "17:13:18 | INFO | OK  map_id=1451  -> vector[165]\n",
      "17:13:21 | INFO | OK  map_id=1458  -> vector[165]\n",
      "17:13:22 | INFO | OK  map_id=1459  -> vector[165]\n",
      "17:13:24 | INFO | OK  map_id=1460  -> vector[165]\n",
      "17:13:25 | INFO | OK  map_id=1465  -> vector[165]\n",
      "17:13:27 | INFO | OK  map_id=1466  -> vector[165]\n",
      "17:13:29 | INFO | OK  map_id=1467  -> vector[165]\n",
      "17:13:32 | INFO | OK  map_id=1473  -> vector[165]\n",
      "17:13:32 | INFO | OK  map_id=1474  -> vector[165]\n",
      "17:13:33 | INFO | OK  map_id=1476  -> vector[165]\n",
      "17:13:35 | INFO | OK  map_id=1479  -> vector[165]\n",
      "17:13:36 | INFO | OK  map_id=1486  -> vector[165]\n",
      "17:13:38 | INFO | OK  map_id=1487  -> vector[165]\n",
      "17:13:39 | INFO | OK  map_id=1496  -> vector[165]\n",
      "17:13:41 | INFO | OK  map_id=1500  -> vector[165]\n",
      "17:13:42 | INFO | OK  map_id=1501  -> vector[165]\n",
      "17:13:44 | INFO | OK  map_id=1507  -> vector[165]\n",
      "17:13:45 | INFO | OK  map_id=1508  -> vector[165]\n",
      "17:13:47 | INFO | OK  map_id=1509  -> vector[165]\n",
      "17:13:49 | INFO | OK  map_id=1514  -> vector[165]\n",
      "17:13:51 | INFO | OK  map_id=1515  -> vector[165]\n",
      "17:13:51 | INFO | OK  map_id=1557  -> vector[165]\n",
      "17:13:53 | INFO | OK  map_id=1563  -> vector[165]\n",
      "17:13:55 | INFO | OK  map_id=1564  -> vector[165]\n",
      "17:13:57 | INFO | OK  map_id=1565  -> vector[165]\n",
      "17:13:58 | INFO | OK  map_id=1570  -> vector[165]\n",
      "17:13:59 | INFO | OK  map_id=1579  -> vector[165]\n",
      "17:14:00 | INFO | OK  map_id=1580  -> vector[165]\n",
      "17:14:01 | INFO | OK  map_id=1583  -> vector[165]\n",
      "17:14:02 | INFO | OK  map_id=1584  -> vector[165]\n",
      "17:14:03 | INFO | OK  map_id=1598  -> vector[165]\n",
      "17:14:05 | INFO | OK  map_id=1613  -> vector[165]\n",
      "17:14:06 | INFO | OK  map_id=1614  -> vector[165]\n",
      "17:14:07 | INFO | OK  map_id=1618  -> vector[165]\n",
      "17:14:08 | INFO | OK  map_id=1619  -> vector[165]\n",
      "17:14:09 | INFO | OK  map_id=1629  -> vector[165]\n",
      "17:14:12 | INFO | OK  map_id=1630  -> vector[165]\n",
      "17:14:13 | INFO | OK  map_id=1631  -> vector[165]\n",
      "17:14:14 | INFO | OK  map_id=1647  -> vector[165]\n",
      "17:14:15 | INFO | OK  map_id=1649  -> vector[165]\n",
      "17:14:16 | INFO | OK  map_id=1650  -> vector[165]\n",
      "17:14:17 | INFO | OK  map_id=1653  -> vector[165]\n",
      "17:14:18 | INFO | OK  map_id=1666  -> vector[165]\n",
      "17:14:19 | INFO | OK  map_id=1667  -> vector[165]\n",
      "17:14:22 | INFO | OK  map_id=1672  -> vector[165]\n",
      "17:14:23 | INFO | OK  map_id=1673  -> vector[165]\n",
      "17:14:24 | INFO | OK  map_id=1679  -> vector[165]\n",
      "17:14:25 | INFO | OK  map_id=1691  -> vector[165]\n",
      "17:14:27 | INFO | OK  map_id=1696  -> vector[165]\n",
      "17:14:30 | INFO | OK  map_id=1700  -> vector[165]\n",
      "17:14:32 | INFO | OK  map_id=1702  -> vector[165]\n",
      "17:14:35 | INFO | OK  map_id=1703  -> vector[165]\n",
      "17:14:36 | INFO | OK  map_id=1709  -> vector[165]\n",
      "17:14:37 | INFO | OK  map_id=1710  -> vector[165]\n",
      "17:14:39 | INFO | OK  map_id=1748  -> vector[165]\n",
      "17:14:41 | INFO | OK  map_id=1749  -> vector[165]\n",
      "17:14:43 | INFO | OK  map_id=1750  -> vector[165]\n",
      "17:14:44 | INFO | OK  map_id=1751  -> vector[165]\n",
      "17:14:45 | INFO | OK  map_id=1752  -> vector[165]\n",
      "17:14:46 | INFO | OK  map_id=1755  -> vector[165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Map embeddings completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:14:47 | INFO | OK  map_id=1757  -> vector[165]\n",
      "17:14:47 | INFO | Saved 300 vectors (failed=0) to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out\n"
     ]
    }
   ],
   "source": [
    "# === MAP EMBEDDINGS ===\n",
    "cmd = [\n",
    "    sys.executable, \"-m\", \"src.mapvec.maps.map_embeddings\",\n",
    "    \"--root\", str(PATHS.MAPS_ROOT),\n",
    "    \"--pattern\", PATHS.INPUT_MAPS_PATTERN,\n",
    "    \"--out_dir\", str(PATHS.MAP_OUT),\n",
    "    \"--norm\", \"fixed\",\n",
    "    \"--norm-wh\", \"400x400\",\n",
    "    \"-v\",\n",
    "]\n",
    "print(\"CMD:\", \" \".join(cmd))\n",
    "res = subprocess.run(cmd, cwd=str(PATHS.PROJ_ROOT))\n",
    "if res.returncode != 0:\n",
    "    raise SystemExit(\"Map embedding step failed.\")\n",
    "print(\"‚úÖ Map embeddings completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd186319e89f445",
   "metadata": {},
   "source": [
    "## üîó 3) Concatenate (pairs ‚Üí fused rows) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2b07a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMD: /opt/anaconda3/envs/thesis/bin/python -m src.mapvec.concat.concat_embeddings --pairs /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/pairs.csv --map_npz /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/maps_embeddings.npz --prompt_npz /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/prompts_embeddings.npz --out_dir /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out --drop_dupes\n",
      "‚úÖ Concatenation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:15:10 | INFO | Map  embeddings: (300, 165) from /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/map_out/maps_embeddings.npz\n",
      "17:15:10 | INFO | Prompt embeddings: (500, 512) from /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/prompts_embeddings.npz\n",
      "17:15:10 | INFO | X shape = (450, 677)  (map_dim=165, prompt_dim=512)\n",
      "17:15:10 | INFO | Saved to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out in 0.02s\n"
     ]
    }
   ],
   "source": [
    "# === CONCATENATION ===\n",
    "cmd = [\n",
    "    sys.executable, \"-m\", \"src.mapvec.concat.concat_embeddings\",\n",
    "    \"--pairs\",      str(PATHS.PAIRS_CSV),\n",
    "    \"--map_npz\",    str(PATHS.MAP_OUT / \"maps_embeddings.npz\"),\n",
    "    \"--prompt_npz\", str(PATHS.PROMPT_OUT / \"prompts_embeddings.npz\"),\n",
    "    \"--out_dir\",    str(PATHS.TRAIN_OUT),\n",
    "    \"--drop_dupes\",\n",
    "    # \"--l2-prompt\",     # safety net if you want L2 here as well\n",
    "    # \"--fail_on_missing\"\n",
    "    # \"--save-blocks\"\n",
    "]\n",
    "print(\"CMD:\", \" \".join(cmd))\n",
    "res = subprocess.run(cmd, cwd=str(PATHS.PROJ_ROOT))\n",
    "if res.returncode != 0:\n",
    "    raise SystemExit(\"Concatenation step failed.\")\n",
    "print(\"‚úÖ Concatenation completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5142d4b68c273d37",
   "metadata": {},
   "source": [
    "## üì• 4) Load & Basic Cleaning ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a494fd27dfe7681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded X: (450, 677), pairs: (450, 4)\n",
      "After cleaning: X=(450, 677), df=(450, 4), ops=['aggregate', 'displace', 'select', 'simplify']\n"
     ]
    }
   ],
   "source": [
    "# === LOAD FUSED DATA ===\n",
    "X = np.load(PATHS.TRAIN_OUT / \"X_concat.npy\")\n",
    "pairs_df = pd.read_parquet(PATHS.TRAIN_OUT / \"train_pairs.parquet\")\n",
    "print(f\"Loaded X: {X.shape}, pairs: {pairs_df.shape}\")\n",
    "\n",
    "OP_COL = \"operator\"\n",
    "PARAM_COLS = [\"param\"]\n",
    "\n",
    "df = pairs_df.copy()\n",
    "df[OP_COL] = df[OP_COL].astype(str).str.strip().str.lower()\n",
    "\n",
    "mask = df[OP_COL].notna()\n",
    "for c in PARAM_COLS:\n",
    "    mask &= df[c].notna()\n",
    "\n",
    "X  = X[mask.values].astype(\"float64\", copy=False)\n",
    "df = df.loc[mask].reset_index(drop=True)\n",
    "print(f\"After cleaning: X={X.shape}, df={df.shape}, ops={sorted(df[OP_COL].unique())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997cab7",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è 5) Split & Targets ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc0897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (315, 677), Val: (67, 677), Test: (68, 677)\n"
     ]
    }
   ],
   "source": [
    "# === SPLIT ===\n",
    "FIXED_CLASSES = [\"simplify\", \"select\", \"aggregate\", \"displace\"]\n",
    "\n",
    "X_train, X_temp, df_train, df_temp = train_test_split(\n",
    "    X, df,\n",
    "    test_size=CFG.VAL_RATIO + CFG.TEST_RATIO,\n",
    "    random_state=CFG.SEED,\n",
    "    shuffle=True,\n",
    "    stratify=df[OP_COL] if df[OP_COL].nunique() > 1 else None\n",
    ")\n",
    "rel_test = CFG.TEST_RATIO / (CFG.VAL_RATIO + CFG.TEST_RATIO)\n",
    "X_val, X_test, df_val, df_test = train_test_split(\n",
    "    X_temp, df_temp,\n",
    "    test_size=rel_test,\n",
    "    random_state=CFG.SEED,\n",
    "    shuffle=True,\n",
    "    stratify=df_temp[OP_COL] if df_temp[OP_COL].nunique() > 1 else None\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# === TARGETS ===\n",
    "le = LabelEncoder().fit(FIXED_CLASSES)\n",
    "y_train_cls = le.transform(df_train[OP_COL])\n",
    "y_val_cls   = le.transform(df_val[OP_COL])\n",
    "y_test_cls  = le.transform(df_test[OP_COL])\n",
    "\n",
    "y_train_reg = df_train[PARAM_COLS].to_numpy(dtype=\"float64\")\n",
    "y_val_reg   = df_val[PARAM_COLS].to_numpy(dtype=\"float64\")\n",
    "y_test_reg  = df_test[PARAM_COLS].to_numpy(dtype=\"float64\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187473d",
   "metadata": {},
   "source": [
    "## üßº 6) Modality-Aware Preprocessing (map only) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41942b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modality-aware preprocessing complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/preproc.joblib']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === MODALITY-AWARE PREPROCESSING ===\n",
    "MAP_DIM     = CFG.MAP_DIM or 165       # set to true map dim\n",
    "PROMPT_DIM  = CFG.PROMPT_DIM or 512\n",
    "\n",
    "def split_blocks(X):\n",
    "    X_map    = X[:, :MAP_DIM].astype(np.float64, copy=True)\n",
    "    X_prompt = X[:, MAP_DIM:MAP_DIM+PROMPT_DIM].astype(np.float64, copy=True)\n",
    "    return X_map, X_prompt\n",
    "\n",
    "def l2_normalize_rows(A, eps=1e-12):\n",
    "    nrm = np.sqrt((A * A).sum(axis=1, keepdims=True))\n",
    "    return A / np.maximum(nrm, eps)\n",
    "\n",
    "# split\n",
    "Xm_tr, Xp_tr = split_blocks(X_train)\n",
    "Xm_va, Xp_va = split_blocks(X_val)\n",
    "Xm_te, Xp_te = split_blocks(X_test)\n",
    "\n",
    "# prompts: L2 only\n",
    "Xp_tr = l2_normalize_rows(Xp_tr)\n",
    "Xp_va = l2_normalize_rows(Xp_va)\n",
    "Xp_te = l2_normalize_rows(Xp_te)\n",
    "\n",
    "# maps: inf‚ÜíNaN\n",
    "for A in (Xm_tr, Xm_va, Xm_te):\n",
    "    A[~np.isfinite(A)] = np.nan\n",
    "\n",
    "# impute (train)\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "Xm_tr_imp = imp.fit_transform(Xm_tr)\n",
    "Xm_va_imp = imp.transform(Xm_va)\n",
    "Xm_te_imp = imp.transform(Xm_te)\n",
    "\n",
    "# clip (5‚Äì95%) train thresholds\n",
    "q_lo = np.nanpercentile(Xm_tr_imp, 5, axis=0)\n",
    "q_hi = np.nanpercentile(Xm_tr_imp, 95, axis=0)\n",
    "def clip_to_q(A, lo, hi): return np.clip(A, lo, hi)\n",
    "\n",
    "Xm_tr_imp = clip_to_q(Xm_tr_imp, q_lo, q_hi)\n",
    "Xm_va_imp = clip_to_q(Xm_va_imp, q_lo, q_hi)\n",
    "Xm_te_imp = clip_to_q(Xm_te_imp, q_lo, q_hi)\n",
    "\n",
    "# drop zero-variance cols on train\n",
    "stds = np.nanstd(Xm_tr_imp, axis=0)\n",
    "keep_mask = stds > 1e-12\n",
    "\n",
    "# scale kept columns (train fit)\n",
    "scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5, 95))\n",
    "Xm_tr_kept = scaler.fit_transform(Xm_tr_imp[:, keep_mask])\n",
    "Xm_va_kept = scaler.transform(Xm_va_imp[:, keep_mask])\n",
    "Xm_te_kept = scaler.transform(Xm_te_imp[:, keep_mask])\n",
    "\n",
    "# rebuild full map dim (dropped cols = 0)\n",
    "Xm_tr_s = np.zeros_like(Xm_tr_imp, dtype=np.float64)\n",
    "Xm_va_s = np.zeros_like(Xm_va_imp, dtype=np.float64)\n",
    "Xm_te_s = np.zeros_like(Xm_te_imp, dtype=np.float64)\n",
    "Xm_tr_s[:, keep_mask] = Xm_tr_kept.astype(np.float64)\n",
    "Xm_va_s[:, keep_mask] = Xm_va_kept.astype(np.float64)\n",
    "Xm_te_s[:, keep_mask] = Xm_te_kept.astype(np.float64)\n",
    "\n",
    "# fuse back\n",
    "X_train_s = np.concatenate([Xm_tr_s, Xp_tr], axis=1).astype(np.float64)\n",
    "X_val_s   = np.concatenate([Xm_va_s, Xp_va], axis=1).astype(np.float64)\n",
    "X_test_s  = np.concatenate([Xm_te_s, Xp_te], axis=1).astype(np.float64)\n",
    "\n",
    "assert np.isfinite(X_train_s).all() and np.isfinite(X_val_s).all() and np.isfinite(X_test_s).all(), \"Non-finite after preprocessing.\"\n",
    "print(\"‚úÖ Modality-aware preprocessing complete.\")\n",
    "\n",
    "# save preprocessing bundle\n",
    "joblib.dump({\n",
    "    \"imp\": imp, \"q_lo\": q_lo, \"q_hi\": q_hi,\n",
    "    \"keep_mask\": keep_mask, \"scaler\": scaler,\n",
    "    \"map_dim\": MAP_DIM, \"prompt_dim\": PROMPT_DIM\n",
    "}, PATHS.TRAIN_OUT / \"preproc.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c32c2",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è 7) Class Weights ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df2b2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {np.str_('aggregate'): np.float64(0.9264705882352942), np.str_('displace'): np.float64(1.0361842105263157), np.str_('select'): np.float64(0.984375), np.str_('simplify'): np.float64(1.0641891891891893)}\n"
     ]
    }
   ],
   "source": [
    "classes  = list(le.classes_)\n",
    "n_classes = len(classes)\n",
    "cls_w    = compute_class_weight(class_weight=\"balanced\",\n",
    "                                classes=np.arange(n_classes),\n",
    "                                y=y_train_cls)\n",
    "sample_w = np.array([cls_w[c] for c in y_train_cls], dtype=\"float64\")\n",
    "print(\"Class weights:\", dict(zip(classes, cls_w)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988f274",
   "metadata": {},
   "source": [
    "## üß† 8) Train MLP ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "95133825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching 50 MLP configs...\n",
      "[01/50] cvF1=0.230¬±0.078 | VAL F1=0.758 acc=0.761 | (128, 64), Œ±=2.02e-02, lr=1.2e-03, bs=16\n",
      "[02/50] cvF1=0.241¬±0.014 | VAL F1=0.674 acc=0.687 | (256, 128), Œ±=3.49e-05, lr=1.7e-04, bs=64\n",
      "[03/50] cvF1=0.212¬±0.037 | VAL F1=0.696 acc=0.701 | (256,), Œ±=1.03e-02, lr=7.7e-04, bs=128\n",
      "[04/50] cvF1=0.225¬±0.034 | VAL F1=0.665 acc=0.672 | (256,), Œ±=1.18e-05, lr=2.7e-03, bs=128\n",
      "[05/50] cvF1=0.218¬±0.052 | VAL F1=0.690 acc=0.701 | (256, 128, 64), Œ±=5.47e-05, lr=1.9e-04, bs=16\n",
      "[06/50] cvF1=0.224¬±0.036 | VAL F1=0.696 acc=0.701 | (64,), Œ±=1.14e-04, lr=6.0e-04, bs=128\n",
      "[07/50] cvF1=0.236¬±0.047 | VAL F1=0.695 acc=0.701 | (64,), Œ±=1.03e-04, lr=8.0e-04, bs=32\n",
      "[08/50] cvF1=0.241¬±0.058 | VAL F1=0.773 acc=0.776 | (128, 64), Œ±=2.43e-02, lr=2.2e-04, bs=32\n",
      "[09/50] cvF1=0.206¬±0.038 | VAL F1=0.677 acc=0.687 | (256, 128, 64), Œ±=4.95e-05, lr=5.7e-04, bs=128\n",
      "[10/50] cvF1=0.243¬±0.034 | VAL F1=0.681 acc=0.687 | (64,), Œ±=1.45e-05, lr=7.9e-04, bs=16\n",
      "[11/50] cvF1=0.229¬±0.030 | VAL F1=0.677 acc=0.687 | (64,), Œ±=1.68e-05, lr=2.5e-03, bs=128\n",
      "[12/50] cvF1=0.231¬±0.051 | VAL F1=0.726 acc=0.731 | (256, 128, 64), Œ±=6.47e-03, lr=2.8e-04, bs=16\n",
      "[13/50] cvF1=0.209¬±0.020 | VAL F1=0.618 acc=0.627 | (128,), Œ±=2.39e-03, lr=4.5e-04, bs=64\n",
      "[14/50] cvF1=0.218¬±0.033 | VAL F1=0.678 acc=0.687 | (128, 64), Œ±=5.27e-04, lr=1.1e-04, bs=32\n",
      "[15/50] cvF1=0.233¬±0.030 | VAL F1=0.695 acc=0.701 | (64,), Œ±=7.94e-05, lr=9.5e-04, bs=32\n",
      "[16/50] cvF1=0.209¬±0.046 | VAL F1=0.661 acc=0.672 | (256, 128, 64), Œ±=6.43e-04, lr=6.4e-04, bs=32\n",
      "[17/50] cvF1=0.220¬±0.042 | VAL F1=0.673 acc=0.687 | (256, 128), Œ±=2.35e-02, lr=1.4e-03, bs=32\n",
      "[18/50] cvF1=0.198¬±0.040 | VAL F1=0.667 acc=0.672 | (128,), Œ±=1.29e-02, lr=7.6e-04, bs=128\n",
      "[19/50] cvF1=0.210¬±0.042 | VAL F1=0.665 acc=0.672 | (256, 128, 64), Œ±=4.80e-05, lr=1.2e-04, bs=128\n",
      "[20/50] cvF1=0.243¬±0.011 | VAL F1=0.672 acc=0.687 | (256, 128), Œ±=2.25e-04, lr=2.5e-04, bs=16\n",
      "[21/50] cvF1=0.255¬±0.031 | VAL F1=0.755 acc=0.761 | (128,), Œ±=2.27e-02, lr=7.9e-04, bs=16\n",
      "[22/50] cvF1=0.250¬±0.037 | VAL F1=0.679 acc=0.687 | (64,), Œ±=1.07e-04, lr=1.8e-04, bs=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/50] cvF1=0.217¬±0.031 | VAL F1=0.678 acc=0.687 | (64,), Œ±=4.84e-03, lr=2.0e-04, bs=128\n",
      "[24/50] cvF1=0.226¬±0.046 | VAL F1=0.648 acc=0.657 | (256,), Œ±=4.91e-05, lr=1.1e-03, bs=64\n",
      "[25/50] cvF1=0.226¬±0.043 | VAL F1=0.691 acc=0.701 | (64,), Œ±=1.28e-03, lr=2.3e-03, bs=32\n",
      "[26/50] cvF1=0.225¬±0.028 | VAL F1=0.709 acc=0.716 | (64,), Œ±=1.52e-02, lr=1.8e-03, bs=128\n",
      "[27/50] cvF1=0.213¬±0.021 | VAL F1=0.618 acc=0.627 | (128,), Œ±=2.15e-05, lr=3.5e-04, bs=32\n",
      "[28/50] cvF1=0.236¬±0.022 | VAL F1=0.672 acc=0.687 | (256, 128), Œ±=3.44e-03, lr=8.7e-04, bs=64\n",
      "[29/50] cvF1=0.222¬±0.031 | VAL F1=0.646 acc=0.657 | (256,), Œ±=4.38e-04, lr=1.5e-04, bs=32\n",
      "[30/50] cvF1=0.210¬±0.034 | VAL F1=0.678 acc=0.687 | (256,), Œ±=4.42e-03, lr=6.7e-04, bs=64\n",
      "[31/50] cvF1=0.206¬±0.046 | VAL F1=0.676 acc=0.687 | (256, 128, 64), Œ±=5.21e-04, lr=5.9e-04, bs=64\n",
      "[32/50] cvF1=0.205¬±0.035 | VAL F1=0.618 acc=0.627 | (128,), Œ±=1.23e-05, lr=1.4e-04, bs=64\n",
      "[33/50] cvF1=0.248¬±0.044 | VAL F1=0.677 acc=0.687 | (64,), Œ±=1.24e-04, lr=5.6e-04, bs=32\n",
      "[34/50] cvF1=0.237¬±0.025 | VAL F1=0.674 acc=0.687 | (256, 128), Œ±=7.36e-05, lr=4.0e-04, bs=64\n",
      "[35/50] cvF1=0.230¬±0.009 | VAL F1=0.662 acc=0.672 | (256, 128), Œ±=6.25e-05, lr=1.3e-04, bs=64\n",
      "[36/50] cvF1=0.224¬±0.058 | VAL F1=0.696 acc=0.701 | (256,), Œ±=3.64e-05, lr=2.4e-03, bs=16\n",
      "[37/50] cvF1=0.222¬±0.051 | VAL F1=0.676 acc=0.687 | (256, 128, 64), Œ±=1.59e-03, lr=1.9e-03, bs=128\n",
      "[38/50] cvF1=0.189¬±0.032 | VAL F1=0.667 acc=0.672 | (128, 64), Œ±=4.45e-05, lr=2.1e-03, bs=64\n",
      "[39/50] cvF1=0.219¬±0.021 | VAL F1=0.618 acc=0.627 | (128,), Œ±=2.66e-05, lr=3.4e-04, bs=32\n",
      "[40/50] cvF1=0.240¬±0.037 | VAL F1=0.681 acc=0.687 | (64,), Œ±=8.84e-05, lr=9.1e-04, bs=16\n",
      "[41/50] cvF1=0.211¬±0.036 | VAL F1=0.694 acc=0.701 | (128, 64), Œ±=1.68e-04, lr=2.8e-04, bs=32\n",
      "[42/50] cvF1=0.225¬±0.035 | VAL F1=0.646 acc=0.657 | (256,), Œ±=2.83e-04, lr=2.1e-04, bs=64\n",
      "[43/50] cvF1=0.223¬±0.031 | VAL F1=0.604 acc=0.612 | (128,), Œ±=1.49e-04, lr=2.5e-03, bs=32\n",
      "[44/50] cvF1=0.237¬±0.054 | VAL F1=0.679 acc=0.687 | (64,), Œ±=2.54e-04, lr=1.2e-04, bs=32\n",
      "[45/50] cvF1=0.190¬±0.038 | VAL F1=0.679 acc=0.687 | (128, 64), Œ±=7.22e-05, lr=1.1e-03, bs=64\n",
      "[46/50] cvF1=0.236¬±0.033 | VAL F1=0.691 acc=0.701 | (64,), Œ±=3.27e-05, lr=3.0e-03, bs=64\n",
      "[47/50] cvF1=0.248¬±0.061 | VAL F1=0.696 acc=0.701 | (64,), Œ±=2.49e-02, lr=4.0e-04, bs=64\n",
      "[48/50] cvF1=0.238¬±0.031 | VAL F1=0.672 acc=0.687 | (256, 128), Œ±=1.58e-04, lr=8.6e-04, bs=32\n",
      "[49/50] cvF1=0.199¬±0.024 | VAL F1=0.616 acc=0.627 | (128,), Œ±=7.02e-04, lr=4.6e-04, bs=32\n",
      "[50/50] cvF1=0.205¬±0.041 | VAL F1=0.678 acc=0.687 | (128, 64), Œ±=1.91e-05, lr=3.5e-04, bs=16\n",
      "\n",
      "=== Top candidates (by VAL macro-F1) ===\n",
      "VAL F1=0.773 (acc=0.776) | cvF1=0.241¬±0.058 | params={'hidden_layer_sizes': (128, 64), 'alpha': 0.024314537412227038, 'learning_rate_init': 0.00022071481969101166, 'batch_size': 32, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.758 (acc=0.761) | cvF1=0.230¬±0.078 | params={'hidden_layer_sizes': (128, 64), 'alpha': 0.020218499516556746, 'learning_rate_init': 0.0012057126287443765, 'batch_size': 16, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.755 (acc=0.761) | cvF1=0.255¬±0.031 | params={'hidden_layer_sizes': (128,), 'alpha': 0.02271484329844196, 'learning_rate_init': 0.0007882485664694609, 'batch_size': 16, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.726 (acc=0.731) | cvF1=0.231¬±0.051 | params={'hidden_layer_sizes': (256, 128, 64), 'alpha': 0.006469870699850825, 'learning_rate_init': 0.0002818068029184724, 'batch_size': 16, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "VAL F1=0.709 (acc=0.716) | cvF1=0.225¬±0.028 | params={'hidden_layer_sizes': (64,), 'alpha': 0.015185382476939244, 'learning_rate_init': 0.0018013995527645626, 'batch_size': 128, 'activation': 'relu', 'solver': 'adam', 'max_iter': 800, 'early_stopping': False, 'random_state': 42, 'verbose': False, 'tol': 0.0001}\n",
      "\n",
      "üèÜ Selected params:\n",
      "{'activation': 'relu',\n",
      " 'alpha': 0.024314537412227038,\n",
      " 'batch_size': 32,\n",
      " 'early_stopping': False,\n",
      " 'hidden_layer_sizes': (128, 64),\n",
      " 'learning_rate_init': 0.00022071481969101166,\n",
      " 'max_iter': 800,\n",
      " 'random_state': 42,\n",
      " 'solver': 'adam',\n",
      " 'tol': 0.0001,\n",
      " 'verbose': False}\n",
      "\n",
      "===== VAL =====\n",
      "VAL: acc=0.7761  f1_macro=0.7730\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   aggregate       0.78      0.78      0.78        18\n",
      "    displace       0.74      0.88      0.80        16\n",
      "      select       0.91      0.59      0.71        17\n",
      "    simplify       0.74      0.88      0.80        16\n",
      "\n",
      "    accuracy                           0.78        67\n",
      "   macro avg       0.79      0.78      0.77        67\n",
      "weighted avg       0.79      0.78      0.77        67\n",
      "\n",
      "Confusion matrix:\n",
      " [[14  1  1  2]\n",
      " [ 1 14  0  1]\n",
      " [ 2  3 10  2]\n",
      " [ 1  1  0 14]]\n",
      "\n",
      "===== TEST =====\n",
      "TEST: acc=0.6765  f1_macro=0.6546\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   aggregate       0.67      0.56      0.61        18\n",
      "    displace       0.65      0.76      0.70        17\n",
      "      select       0.71      0.94      0.81        18\n",
      "    simplify       0.67      0.40      0.50        15\n",
      "\n",
      "    accuracy                           0.68        68\n",
      "   macro avg       0.67      0.67      0.65        68\n",
      "weighted avg       0.67      0.68      0.66        68\n",
      "\n",
      "Confusion matrix:\n",
      " [[10  3  3  2]\n",
      " [ 1 13  2  1]\n",
      " [ 1  0 17  0]\n",
      " [ 3  4  2  6]]\n",
      "\n",
      "‚úÖ Saved final MLP (trained on ALL TRAIN) to: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/best_mlp_fulltrain.joblib\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# MLP search where each model trains on ALL training data\n",
    "# =========================\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# ---- numerics: keep float64 everywhere ----\n",
    "X_train_s = X_train_s.astype(np.float64, copy=False)\n",
    "X_val_s   = X_val_s.astype(np.float64, copy=False)\n",
    "X_test_s  = X_test_s.astype(np.float64, copy=False)\n",
    "sample_w  = sample_w.astype(np.float64, copy=False)\n",
    "\n",
    "# ---- group by map_id (maps can repeat; prompts don't) ----\n",
    "assert \"map_id\" in df_train.columns, \"df_train must contain 'map_id' for grouped CV.\"\n",
    "groups_tr = df_train[\"map_id\"].astype(str).values\n",
    "\n",
    "# ---- CV splitter (for scoring only) ----\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# ---- search space helpers ----\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "def draw_params(n):\n",
    "    sizes = [(64,), (128,), (256,), (128, 64), (256, 128), (256, 128, 64)]\n",
    "    batches = [16, 32, 64, 128]\n",
    "    for _ in range(n):\n",
    "        yield {\n",
    "            \"hidden_layer_sizes\": sizes[rng.randint(len(sizes))],\n",
    "            \"alpha\": 10**rng.uniform(-5, np.log10(3e-2)),          # loguniform(1e-5, 3e-2)\n",
    "            \"learning_rate_init\": 10**rng.uniform(-4, np.log10(3e-3)),  # loguniform(1e-4, 3e-3)\n",
    "            \"batch_size\": batches[rng.randint(len(batches))],\n",
    "            \"activation\": \"relu\",\n",
    "            \"solver\": \"adam\",\n",
    "            \"max_iter\": 800,            # allow convergence w/o early stopping\n",
    "            \"early_stopping\": False,    # <‚Äî IMPORTANT: use ALL training samples\n",
    "            \"random_state\": 42,\n",
    "            \"verbose\": False,\n",
    "            \"tol\": 1e-4\n",
    "        }\n",
    "\n",
    "# ---- CV scorer using grouped folds; model sees only its fold-train here (for the score only) ----\n",
    "def cv_macro_f1(params):\n",
    "    scores = []\n",
    "    for tr_idx, va_idx in cv.split(X_train_s, y_train_cls, groups_tr):\n",
    "        clf = MLPClassifier(**params)\n",
    "        clf.fit(X_train_s[tr_idx], y_train_cls[tr_idx], sample_weight=sample_w[tr_idx])\n",
    "        pred = clf.predict(X_train_s[va_idx])\n",
    "        scores.append(f1_score(y_train_cls[va_idx], pred, average=\"macro\"))\n",
    "    return float(np.mean(scores)), float(np.std(scores))\n",
    "\n",
    "@dataclass\n",
    "class Candidate:\n",
    "    params: dict\n",
    "    cv_mean: float\n",
    "    cv_std: float\n",
    "    val_f1: float\n",
    "    val_acc: float\n",
    "\n",
    "# ---- run search ----\n",
    "N_ITER = 50   # tune this for time/quality tradeoff\n",
    "candidates = []\n",
    "\n",
    "print(f\"\\nSearching {N_ITER} MLP configs...\")\n",
    "for i, params in enumerate(draw_params(N_ITER), 1):\n",
    "    cv_mean, cv_std = cv_macro_f1(params)\n",
    "\n",
    "    # IMPORTANT PART: refit SAME PARAMS on FULL TRAIN (no early_stopping) so the model sees ALL training data\n",
    "    clf_full = MLPClassifier(**params)\n",
    "    clf_full.fit(X_train_s, y_train_cls, sample_weight=sample_w)\n",
    "\n",
    "    # evaluate on external VAL (never used for training)\n",
    "    val_pred = clf_full.predict(X_val_s)\n",
    "    val_f1 = f1_score(y_val_cls, val_pred, average=\"macro\")\n",
    "    val_acc = accuracy_score(y_val_cls, val_pred)\n",
    "\n",
    "    candidates.append(Candidate(params, cv_mean, cv_std, val_f1, val_acc))\n",
    "    print(f\"[{i:02d}/{N_ITER}] cvF1={cv_mean:.3f}¬±{cv_std:.3f} | VAL F1={val_f1:.3f} acc={val_acc:.3f} | {params['hidden_layer_sizes']}, Œ±={params['alpha']:.2e}, lr={params['learning_rate_init']:.1e}, bs={params['batch_size']}\")\n",
    "\n",
    "# ---- pick winner by external VAL macro-F1 (tie-breaker: VAL acc, then CV mean) ----\n",
    "candidates.sort(key=lambda c: (c.val_f1, c.val_acc, c.cv_mean), reverse=True)\n",
    "best = candidates[0]\n",
    "print(\"\\n=== Top candidates (by VAL macro-F1) ===\")\n",
    "for c in candidates[:5]:\n",
    "    print(f\"VAL F1={c.val_f1:.3f} (acc={c.val_acc:.3f}) | cvF1={c.cv_mean:.3f}¬±{c.cv_std:.3f} | params={c.params}\")\n",
    "\n",
    "print(\"\\nüèÜ Selected params:\")\n",
    "pprint(best.params)\n",
    "\n",
    "# ---- train final model on FULL TRAIN (no early_stopping so it uses 100% of train) ----\n",
    "final_mlp = MLPClassifier(**best.params)\n",
    "final_mlp.fit(X_train_s, y_train_cls, sample_weight=sample_w)\n",
    "\n",
    "# ---- evaluate on VAL & TEST ----\n",
    "for name, Xs, ys in [(\"VAL\", X_val_s, y_val_cls), (\"TEST\", X_test_s, y_test_cls)]:\n",
    "    yhat = final_mlp.predict(Xs)\n",
    "    acc  = accuracy_score(ys, yhat)\n",
    "    f1m  = f1_score(ys, yhat, average=\"macro\")\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    print(f\"{name}: acc={acc:.4f}  f1_macro={f1m:.4f}\")\n",
    "    print(classification_report(ys, yhat, target_names=list(le.classes_)))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(ys, yhat))\n",
    "\n",
    "# ---- save final model ----\n",
    "out_dir = Path(PATHS.TRAIN_OUT); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "import joblib\n",
    "joblib.dump({\"model\": final_mlp, \"label_encoder\": le, \"best_params\": best.params}, out_dir / \"best_mlp_fulltrain.joblib\")\n",
    "print(f\"\\n‚úÖ Saved final MLP (trained on ALL TRAIN) to: {out_dir / 'best_mlp_fulltrain.joblib'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ea7905b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'aggregate' ===\n",
      "best CV RMSE: 3.776095454330027\n",
      "best params:\n",
      "{'alpha': np.float64(2.861167865082196e-05),\n",
      " 'hidden_layer_sizes': (256, 128),\n",
      " 'learning_rate_init': np.float64(0.0002516607127550297)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'displace' ===\n",
      "best CV RMSE: 0.5718840359495102\n",
      "best params:\n",
      "{'alpha': np.float64(0.0005248702648435531),\n",
      " 'hidden_layer_sizes': (256, 128),\n",
      " 'learning_rate_init': np.float64(0.0004628518674713464)}\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regressor for class 'select' ===\n",
      "best CV RMSE: 59.038668859579765\n",
      "best params:\n",
      "{'alpha': np.float64(2.1453931225439485e-06),\n",
      " 'hidden_layer_sizes': (128,),\n",
      " 'learning_rate_init': np.float64(0.0001483039268456802)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "\n",
      "=== Regressor for class 'simplify' ===\n",
      "best CV RMSE: 2.7372283634097707\n",
      "best params:\n",
      "{'alpha': np.float64(0.000522114714225509),\n",
      " 'hidden_layer_sizes': (256, 128),\n",
      " 'learning_rate_init': np.float64(0.00016149614799999194)}\n",
      "\n",
      "--- Regression with predicted classes (realistic) ---\n",
      "VAL: MAE=21.7280  RMSE=42.3725\n",
      "TEST: MAE=23.2668  RMSE=41.8655\n",
      "\n",
      "--- Regression with TRUE classes (oracle routing) ---\n",
      "VAL-oracle: MAE=9.8681  RMSE=24.4868\n",
      "TEST-oracle: MAE=10.0799  RMSE=23.9558\n",
      "\n",
      "‚úÖ Saved classification+regression bundle to: /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out/cls_plus_regressors.joblib\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Regression branch (one MLPRegressor per operator)\n",
    "# =========================\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import joblib\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import StratifiedGroupKFold, GroupKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "# ---- 1) Prepare numeric regression targets\n",
    "def _coerce_param_to_float(s):\n",
    "    # Try robust parse; you can customize if your 'param' has units or JSON.\n",
    "    try:\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "y_train_reg = df_train[\"param\"].apply(_coerce_param_to_float).to_numpy()\n",
    "y_val_reg   = df_val[\"param\"].apply(_coerce_param_to_float).to_numpy()\n",
    "y_test_reg  = df_test[\"param\"].apply(_coerce_param_to_float).to_numpy()\n",
    "\n",
    "# Guard: drop if any NaNs (or you can filter rows; here we assert)\n",
    "assert np.isfinite(y_train_reg).all() and np.isfinite(y_val_reg).all() and np.isfinite(y_test_reg).all(), \\\n",
    "    \"Non-finite values found in regression target 'param'. Clean/parse them first.\"\n",
    "\n",
    "# Optional: log1p transform if param is positive and skewed\n",
    "USE_LOG1P = False\n",
    "if USE_LOG1P:\n",
    "    assert (y_train_reg >= 0).all() and (y_val_reg >= 0).all() and (y_test_reg >= 0).all(), \\\n",
    "        \"log1p selected but param has negatives.\"\n",
    "    ytr_reg_t = np.log1p(y_train_reg)\n",
    "    yva_reg_t = np.log1p(y_val_reg)\n",
    "    yte_reg_t = np.log1p(y_test_reg)\n",
    "    def inv_t(x): return np.expm1(x)\n",
    "else:\n",
    "    ytr_reg_t = y_train_reg.copy()\n",
    "    yva_reg_t = y_val_reg.copy()\n",
    "    yte_reg_t = y_test_reg.copy()\n",
    "    def inv_t(x): return x\n",
    "\n",
    "# ---- 2) Grouped CV by map_id for *regression* (no stratification needed on a numeric target)\n",
    "assert \"map_id\" in df_train.columns\n",
    "gk = GroupKFold(n_splits=5)\n",
    "groups_tr = df_train[\"map_id\"].astype(str).values\n",
    "\n",
    "# ---- 3) Search space for MLPRegressor (kept modest; widen n_iter to search more)\n",
    "base_reg = MLPRegressor(\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    learning_rate=\"adaptive\",   # <‚Äî helps convergence on tough subsets\n",
    "    early_stopping=False,       # keep OFF during search so it uses all class data\n",
    "    max_iter=2000,              # <‚Äî more runway\n",
    "    tol=1e-3,                   # <‚Äî slightly easier convergence threshold\n",
    "    random_state=42,\n",
    "    verbose=False,\n",
    "    batch_size=\"auto\"           # <‚Äî avoids clipping warnings\n",
    ")\n",
    "param_dist_reg = {\n",
    "    \"hidden_layer_sizes\": [(64,), (128,), (256,), (128, 64), (256, 128)],\n",
    "    \"alpha\": loguniform(1e-6, 3e-2),        # widen upper range for stronger regularization\n",
    "    \"learning_rate_init\": loguniform(1e-4, 3e-3),\n",
    "    # \"batch_size\": [\"auto\"]  # not tuning batch size anymore\n",
    "}\n",
    "\n",
    "# ---- 4) Fit one regressor per class\n",
    "class_names = list(le.classes_)\n",
    "n_classes = len(class_names)\n",
    "regressors = {}\n",
    "search_summaries = {}\n",
    "\n",
    "for cls_idx, cls_name in enumerate(class_names):\n",
    "    # mask for this class in TRAIN\n",
    "    m_tr = (y_train_cls == cls_idx)\n",
    "    Xk, yk, gk_tr = X_train_s[m_tr], ytr_reg_t[m_tr], groups_tr[m_tr]\n",
    "    if Xk.shape[0] < 10:\n",
    "        print(f\"‚ö†Ô∏è Skipping class '{cls_name}' (too few samples: {Xk.shape[0]}).\")\n",
    "        continue\n",
    "\n",
    "    # grouped CV splits for this class only\n",
    "    splits = list(gk.split(Xk, yk, groups=gk_tr))\n",
    "\n",
    "    # negative RMSE is a good search objective\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base_reg,\n",
    "        param_distributions=param_dist_reg,\n",
    "        n_iter=40,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        cv=splits,\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(Xk, yk)\n",
    "\n",
    "    print(f\"\\n=== Regressor for class '{cls_name}' ===\")\n",
    "    print(\"best CV RMSE:\", -search.best_score_)\n",
    "    print(\"best params:\"); pprint(search.best_params_)\n",
    "    search_summaries[cls_name] = {\"neg_rmse_cv\": search.best_score_, \"params\": search.best_params_}\n",
    "\n",
    "    # Refit on the FULL class-specific TRAIN subset\n",
    "    reg_full = MLPRegressor(\n",
    "        **{**search.best_estimator_.get_params(), \"early_stopping\": False, \"max_iter\": 2000, \"random_state\": 42}\n",
    "    )\n",
    "    reg_full.fit(Xk, yk)\n",
    "    regressors[cls_name] = reg_full\n",
    "\n",
    "# ---- 5) Evaluate on VAL & TEST using your classifier's prediction to route to regressors\n",
    "def route_and_predict(Xs, pred_cls_idx):\n",
    "    yhat_reg = np.zeros(len(pred_cls_idx), dtype=float)\n",
    "    for i, cidx in enumerate(pred_cls_idx):\n",
    "        cname = class_names[cidx]\n",
    "        reg = regressors.get(cname, None)\n",
    "        if reg is None:\n",
    "            # Fallback: if a regressor is missing for a rare class, you can use a global mean or nearest regressor\n",
    "            yhat_reg[i] = np.nan\n",
    "        else:\n",
    "            yhat_reg[i] = reg.predict(Xs[i:i+1])[0]\n",
    "    return yhat_reg\n",
    "\n",
    "# helper to print metrics (older sklearn: no squared=False)\n",
    "def print_reg_metrics(name, y_true, y_pred_transformed):\n",
    "    # inverse-transform predictions if you used log1p\n",
    "    y_pred = inv_t(y_pred_transformed)\n",
    "\n",
    "    # guard against NaNs (e.g., missing regressor for a class)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    if mask.sum() == 0:\n",
    "        print(f\"{name}: no finite pairs to evaluate.\")\n",
    "        return np.nan, np.nan\n",
    "    if mask.sum() < len(y_true):\n",
    "        print(f\"{name}: dropped {len(y_true) - mask.sum()} samples with NaNs.\")\n",
    "\n",
    "    y_true_m = y_true[mask]\n",
    "    y_pred_m = y_pred[mask]\n",
    "\n",
    "    mae = mean_absolute_error(y_true_m, y_pred_m)\n",
    "    mse = mean_squared_error(y_true_m, y_pred_m)   # older sklearn doesn't support squared=False\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"{name}: MAE={mae:.4f}  RMSE={rmse:.4f}\")\n",
    "    return mae, rmse\n",
    "\n",
    "\n",
    "# Classification predictions (already trained classifier)\n",
    "clf_cls = clf  # <- ensure this is your trained best classifier\n",
    "val_pred_cls = clf_cls.predict(X_val_s)\n",
    "test_pred_cls = clf_cls.predict(X_test_s)\n",
    "\n",
    "# route to per-class regressors\n",
    "yhat_val_reg_t  = route_and_predict(X_val_s,  val_pred_cls)\n",
    "yhat_test_reg_t = route_and_predict(X_test_s, test_pred_cls)\n",
    "\n",
    "print(\"\\n--- Regression with predicted classes (realistic) ---\")\n",
    "print_reg_metrics(\"VAL\",  y_val_reg,  yhat_val_reg_t)\n",
    "print_reg_metrics(\"TEST\", y_test_reg, yhat_test_reg_t)\n",
    "\n",
    "# ---- 6) Optional: 'oracle' evaluation to isolate regressor quality (use TRUE class for routing)\n",
    "yhat_val_oracle_t  = route_and_predict(X_val_s,  y_val_cls)\n",
    "yhat_test_oracle_t = route_and_predict(X_test_s, y_test_cls)\n",
    "\n",
    "print(\"\\n--- Regression with TRUE classes (oracle routing) ---\")\n",
    "print_reg_metrics(\"VAL-oracle\",  y_val_reg,  yhat_val_oracle_t)\n",
    "print_reg_metrics(\"TEST-oracle\", y_test_reg, yhat_test_oracle_t)\n",
    "\n",
    "# ---- 7) Save bundle\n",
    "bundle = {\n",
    "    \"classifier\": clf_cls,\n",
    "    \"regressors_by_class\": regressors,\n",
    "    \"label_encoder\": le,\n",
    "    \"use_log1p\": USE_LOG1P\n",
    "}\n",
    "out_dir = Path(PATHS.TRAIN_OUT)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(bundle, out_dir / \"cls_plus_regressors.joblib\")\n",
    "print(f\"\\n‚úÖ Saved classification+regression bundle to: {out_dir / 'cls_plus_regressors.joblib'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb8b74d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
