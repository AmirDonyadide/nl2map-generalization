{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c1f56a2c",
      "metadata": {},
      "source": [
        "# MapVec end-to-end pipeline üìí\n",
        "\n",
        "This notebook runs the **entire pipeline**:\n",
        "1. Prompt embeddings (Universal Sentence Encoder)\n",
        "2. Map embeddings (handcrafted polygon features)\n",
        "3. Concatenation into a training matrix\n",
        "4. Helper cells to inspect vectors by `prompt_id` or `map_id`\n",
        "\n",
        "**Edit the Parameters** in the next cell to match your project layout.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================== PARAMETERS =====================\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shlex\n",
        "import joblib\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, mean_squared_error, mean_absolute_error\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "9ed0df45",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- PROJECT ROOTS ----------\n",
        "# Project root that contains `src/` and `data/`\n",
        "PROJ_ROOT = Path(\"../\")\n",
        "\n",
        "# Data locations\n",
        "DATA_DIR    = PROJ_ROOT / \"data\"\n",
        "PROMPTS_CSV = DATA_DIR / \"input\" / \"prompts.csv\"   # columns: prompt_id,text (or id,text)\n",
        "PAIRS_CSV   = DATA_DIR / \"input\" / \"pairs.csv\"     # columns: map_id,prompt_id\n",
        "MAPS_ROOT   = DATA_DIR / \"input\" / \"samples\" / \"pairs\"\n",
        "\n",
        "# File patterns\n",
        "INPUT_MAPS_PATTERN  = \"*_input.geojson\"\n",
        "OUTPUT_MAPS_PATTERN = \"*_generalized.geojson\"\n",
        "\n",
        "# Output directories\n",
        "PROMPT_OUT   = DATA_DIR / \"output\" / \"prompt_out\"\n",
        "MAP_OUT      = DATA_DIR / \"output\" / \"map_out\"\n",
        "PAIR_MAP_OUT = DATA_DIR / \"output\" / \"pair_map_out\"\n",
        "TRAIN_OUT    = DATA_DIR / \"output\" / \"train_out\"\n",
        "MODEL_OUT    = DATA_DIR / \"output\" / \"models\"\n",
        "\n",
        "SPLIT_OUT    = TRAIN_OUT / \"splits\"\n",
        "\n",
        "# Paths to precomputed embeddings (for concatenation)\n",
        "MAP_NPZ = PAIR_MAP_OUT / \"embeddings.npz\"\n",
        "PRM_NPZ = PROMPT_OUT / \"embeddings.npz\"\n",
        "\n",
        "# ---------- EMBEDDINGS / MODEL CHOICES ----------\n",
        "# USE model variant for prompt embeddings: 'dan' or 'transformer'\n",
        "USE_MODEL  = \"dan\"\n",
        "\n",
        "# Expected dims\n",
        "MAP_DIM     = 996    # map embedding dim\n",
        "PROMPT_DIM  = 512    # prompt (USE) embedding dim\n",
        "FUSED_DIM   = MAP_DIM + PROMPT_DIM  # 1508\n",
        "BATCH_SIZE  = 512\n",
        "\n",
        "# ---------- DATA SPLITS ----------\n",
        "VAL_RATIO = 0.15   # 15% validation\n",
        "TEST_RATIO = 0.15  # 15% test (remaining 70% train)\n",
        "SEED = 42          # reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1e8f95c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßπ Removing old directory: ../data/output/prompt_out\n",
            "üßπ Removing old directory: ../data/output/map_out\n",
            "üßπ Removing old directory: ../data/output/train_out\n",
            "üßπ Removing old directory: ../data/models\n",
            "üßπ Removing old directory: ../data/output/pair_map_out\n",
            "‚úÖ All output folders cleaned and recreated fresh.\n"
          ]
        }
      ],
      "source": [
        "# ===================== CLEAN PREVIOUS OUTPUTS =====================\n",
        "for d in [PROMPT_OUT, MAP_OUT, TRAIN_OUT, MODEL_OUT, PAIR_MAP_OUT, SPLIT_OUT]:\n",
        "    if d.exists():\n",
        "        print(f\"üßπ Removing old directory: {d}\")\n",
        "        shutil.rmtree(d)\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ All output folders cleaned and recreated fresh.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All output folders created.\n"
          ]
        }
      ],
      "source": [
        "# Make sure Python can import your local modules (src/)\n",
        "sys.path.insert(0, str(PROJ_ROOT))\n",
        "PROMPT_OUT.mkdir(parents=True, exist_ok=True)\n",
        "MAP_OUT.mkdir(parents=True, exist_ok=True)\n",
        "TRAIN_OUT.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_OUT.mkdir(parents=True, exist_ok=True)\n",
        "PAIR_MAP_OUT.mkdir(parents=True, exist_ok=True)\n",
        "SPLIT_OUT.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_OUT.mkdir(parents=True, exist_ok=True)\n",
        "print(\"‚úÖ All output folders created.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Dependency check (Parquet engine)\n",
        "We ensure `pyarrow` or `fastparquet` is available for `pandas.to_parquet`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Prompt embeddings\n",
        "Runs `src/mapvec/prompts/prompt_embeddings.py` using your chosen USE model and saves artifacts to `PROMPT_OUT`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python ../src/mapvec/prompts/prompt_embeddings.py --input ../data/input/prompts.csv --model dan --l2 --out_dir ../data/output/prompt_out -v\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "14:54:18 | DEBUG | FILE_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/src/mapvec/prompts\n",
            "14:54:18 | DEBUG | PROJECT_ROOT=/Users/amirdonyadide/Documents/GitHub/Thesis\n",
            "14:54:18 | DEBUG | DEFAULT_DATA_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
            "14:54:18 | INFO | DATA_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
            "14:54:18 | INFO | INPUT=/Users/amirdonyadide/Documents/GitHub/Thesis/data/input/prompts.csv\n",
            "14:54:18 | INFO | OUT_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
            "14:54:18 | INFO | Reading CSV: /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/prompts.csv\n",
            "14:54:18 | INFO | Loaded 500 prompts (id_col=prompt_id). Sample IDs: p001, p002, p003‚Ä¶\n",
            "14:54:18 | INFO | Using local USE-dan at /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/model_dan\n",
            "14:54:18 | INFO | Loading USE-dan from local path: /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/model_dan ‚Ä¶\n",
            "14:54:21 | INFO | Fingerprint not found. Saved model loading will continue.\n",
            "14:54:21 | INFO | path_and_singleprint metric could not be logged. Saved model loading will continue.\n",
            "14:54:21 | INFO | Model loaded in 3.44s\n",
            "14:54:21 | INFO | Embedding 500 prompts (batch_size=512, l2=True)‚Ä¶\n",
            "14:54:21 | DEBUG |   embedded rows [1:500)\n",
            "14:54:21 | INFO | Done embedding in 0.12s (dim=512).\n",
            "14:54:21 | INFO | Writing outputs to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out\n",
            "14:54:21 | INFO |   saved embeddings.npz (shape=(500, 512))\n",
            "14:54:22 | INFO |   saved prompts.parquet (rows=500)\n",
            "14:54:22 | INFO |   saved meta.json\n",
            "14:54:22 | INFO | All done ‚úÖ\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt embeddings completed.\n"
          ]
        }
      ],
      "source": [
        "cmd = (\n",
        "    f\"python {shlex.quote(str(PROJ_ROOT / 'src' / 'mapvec' / 'prompts' / 'prompt_embeddings.py'))} \"\n",
        "    f\"--input {shlex.quote(str(PROMPTS_CSV))} --model {shlex.quote(str(USE_MODEL))} --l2 --out_dir {shlex.quote(str(PROMPT_OUT))} -v\"\n",
        ")\n",
        "print(cmd)\n",
        "res = subprocess.run(cmd, shell=True)\n",
        "if res.returncode != 0:\n",
        "    raise SystemExit('Prompt embedding step failed.')\n",
        "print('Prompt embeddings completed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Map embeddings\n",
        "Runs the map embedding module on the GeoJSON inputs. Skips problematic features, logs warnings, and writes `embeddings.npz` to `PAIR_MAP_OUT`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fa2b07a9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CMD: /opt/anaconda3/envs/thesis/bin/python -m src.mapvec.maps.pair_map_embeddings --root ../data/input/samples/pairs --input_pattern *_input.geojson --gen_pattern *_generalized.geojson --out_dir ../data/output/pair_map_out -v\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "14:54:33 | DEBUG | PROJECT_ROOT=/Users/amirdonyadide/Documents/GitHub/Thesis\n",
            "14:54:33 | DEBUG | DATA_DIR=/Users/amirdonyadide/Documents/GitHub/Thesis/data\n",
            "14:54:33 | INFO | Scanning /Users/amirdonyadide/Documents/GitHub/Thesis/data/input/samples/pairs (in=*_input.geojson, gen=*_generalized.geojson)‚Ä¶\n",
            "14:54:38 | INFO | OK  map_id=0073  -> pair_vec[996] (per_map_dim=249)\n",
            "14:54:39 | INFO | OK  map_id=0080  -> pair_vec[996] (per_map_dim=249)\n",
            "14:54:40 | INFO | OK  map_id=0093  -> pair_vec[996] (per_map_dim=249)\n",
            "14:54:44 | INFO | OK  map_id=0122  -> pair_vec[996] (per_map_dim=249)\n",
            "14:54:46 | INFO | OK  map_id=0123  -> pair_vec[996] (per_map_dim=249)\n",
            "14:54:47 | INFO | OK  map_id=0127  -> pair_vec[996] (per_map_dim=249)\n",
            "14:54:48 | INFO | OK  map_id=0158  -> pair_vec[996] (per_map_dim=249)\n",
            "14:54:52 | INFO | OK  map_id=0159  -> pair_vec[996] (per_map_dim=249)\n",
            "14:54:53 | INFO | OK  map_id=0160  -> pair_vec[996] (per_map_dim=249)\n",
            "14:54:55 | INFO | OK  map_id=0165  -> pair_vec[996] (per_map_dim=249)\n",
            "14:54:58 | INFO | OK  map_id=0167  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:00 | INFO | OK  map_id=0168  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:01 | INFO | OK  map_id=0171  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:04 | INFO | OK  map_id=0208  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:11 | INFO | OK  map_id=0209  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:12 | INFO | OK  map_id=0215  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:14 | INFO | OK  map_id=0240  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:15 | INFO | OK  map_id=0256  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:16 | INFO | OK  map_id=0257  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:17 | INFO | OK  map_id=0262  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:20 | INFO | OK  map_id=0285  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:22 | INFO | OK  map_id=0286  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:25 | INFO | OK  map_id=0288  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:30 | INFO | OK  map_id=0289  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:32 | INFO | OK  map_id=0313  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:33 | INFO | OK  map_id=0341  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:34 | INFO | OK  map_id=0362  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:38 | INFO | OK  map_id=0363  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:41 | INFO | OK  map_id=0364  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:43 | INFO | OK  map_id=0379  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:46 | INFO | OK  map_id=0389  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:50 | INFO | OK  map_id=0390  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:51 | INFO | OK  map_id=0409  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:54 | INFO | OK  map_id=0410  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:56 | INFO | OK  map_id=0412  -> pair_vec[996] (per_map_dim=249)\n",
            "14:55:58 | INFO | OK  map_id=0413  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:00 | INFO | OK  map_id=0414  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:01 | INFO | OK  map_id=0417  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:02 | INFO | OK  map_id=0421  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:05 | INFO | OK  map_id=0426  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:06 | INFO | OK  map_id=0427  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:07 | INFO | OK  map_id=0432  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:08 | INFO | OK  map_id=0433  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:11 | INFO | OK  map_id=0437  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:14 | INFO | OK  map_id=0438  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:17 | INFO | OK  map_id=0439  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:18 | INFO | OK  map_id=0454  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:20 | INFO | OK  map_id=0458  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:21 | INFO | OK  map_id=0459  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:23 | INFO | OK  map_id=0460  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:26 | INFO | OK  map_id=0466  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:28 | INFO | OK  map_id=0469  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:31 | INFO | OK  map_id=0471  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:33 | INFO | OK  map_id=0472  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:39 | INFO | OK  map_id=0474  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:40 | INFO | OK  map_id=0475  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:43 | INFO | OK  map_id=0479  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:45 | INFO | OK  map_id=0480  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:46 | INFO | OK  map_id=0481  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:47 | INFO | OK  map_id=0482  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:49 | INFO | OK  map_id=0508  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:52 | INFO | OK  map_id=0509  -> pair_vec[996] (per_map_dim=249)\n",
            "14:56:54 | INFO | OK  map_id=0518  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:02 | INFO | OK  map_id=0520  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:05 | INFO | OK  map_id=0521  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:07 | INFO | OK  map_id=0523  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:10 | INFO | OK  map_id=0527  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:13 | INFO | OK  map_id=0528  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:15 | INFO | OK  map_id=0529  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:18 | INFO | OK  map_id=0530  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:19 | INFO | OK  map_id=0553  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:21 | INFO | OK  map_id=0557  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:23 | INFO | OK  map_id=0575  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:24 | INFO | OK  map_id=0576  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:26 | INFO | OK  map_id=0594  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:29 | INFO | OK  map_id=0595  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:31 | INFO | OK  map_id=0600  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:35 | INFO | OK  map_id=0605  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:38 | INFO | OK  map_id=0606  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:39 | INFO | OK  map_id=0608  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:42 | INFO | OK  map_id=0609  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:43 | INFO | OK  map_id=0611  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:47 | INFO | OK  map_id=0614  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:49 | INFO | OK  map_id=0615  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:51 | INFO | OK  map_id=0618  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:54 | INFO | OK  map_id=0623  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:55 | INFO | OK  map_id=0624  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:57 | INFO | OK  map_id=0645  -> pair_vec[996] (per_map_dim=249)\n",
            "14:57:59 | INFO | OK  map_id=0646  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:00 | INFO | OK  map_id=0655  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:01 | INFO | OK  map_id=0656  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:03 | INFO | OK  map_id=0657  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:07 | INFO | OK  map_id=0658  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:08 | INFO | OK  map_id=0659  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:10 | INFO | OK  map_id=0667  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:12 | INFO | OK  map_id=0672  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:14 | INFO | OK  map_id=0699  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:17 | INFO | OK  map_id=0700  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:20 | INFO | OK  map_id=0701  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:22 | INFO | OK  map_id=0706  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:23 | INFO | OK  map_id=0707  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:25 | INFO | OK  map_id=0715  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:27 | INFO | OK  map_id=0721  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:29 | INFO | OK  map_id=0747  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:31 | INFO | OK  map_id=0748  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:33 | INFO | OK  map_id=0749  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:34 | INFO | OK  map_id=0755  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:35 | INFO | OK  map_id=0758  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:37 | INFO | OK  map_id=0759  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:39 | INFO | OK  map_id=0762  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:40 | INFO | OK  map_id=0770  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:42 | INFO | OK  map_id=0804  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:44 | INFO | OK  map_id=0807  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:45 | INFO | OK  map_id=0808  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:48 | INFO | OK  map_id=0809  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:50 | INFO | OK  map_id=0819  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:51 | INFO | OK  map_id=0848  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:56 | INFO | OK  map_id=0853  -> pair_vec[996] (per_map_dim=249)\n",
            "14:58:59 | INFO | OK  map_id=0854  -> pair_vec[996] (per_map_dim=249)\n",
            "14:59:01 | INFO | OK  map_id=0856  -> pair_vec[996] (per_map_dim=249)\n",
            "14:59:02 | INFO | OK  map_id=0857  -> pair_vec[996] (per_map_dim=249)\n",
            "14:59:13 | INFO | OK  map_id=0858  -> pair_vec[996] (per_map_dim=249)\n",
            "14:59:16 | INFO | OK  map_id=0859  -> pair_vec[996] (per_map_dim=249)\n",
            "14:59:18 | INFO | OK  map_id=0867  -> pair_vec[996] (per_map_dim=249)\n",
            "14:59:24 | INFO | OK  map_id=0868  -> pair_vec[996] (per_map_dim=249)\n",
            "14:59:24 | INFO | OK  map_id=0869  -> pair_vec[996] (per_map_dim=249)\n",
            "14:59:27 | INFO | OK  map_id=0901  -> pair_vec[996] (per_map_dim=249)\n",
            "14:59:30 | INFO | OK  map_id=0903  -> pair_vec[996] (per_map_dim=249)\n",
            "14:59:32 | INFO | OK  map_id=0904  -> pair_vec[996] (per_map_dim=249)\n",
            "14:59:34 | INFO | OK  map_id=0905  -> pair_vec[996] (per_map_dim=249)\n",
            "14:59:45 | INFO | OK  map_id=0906  -> pair_vec[996] (per_map_dim=249)\n",
            "14:59:46 | INFO | OK  map_id=0907  -> pair_vec[996] (per_map_dim=249)\n",
            "14:59:48 | INFO | OK  map_id=0908  -> pair_vec[996] (per_map_dim=249)\n",
            "14:59:51 | INFO | OK  map_id=0917  -> pair_vec[996] (per_map_dim=249)\n",
            "14:59:53 | INFO | OK  map_id=0918  -> pair_vec[996] (per_map_dim=249)\n",
            "14:59:57 | INFO | OK  map_id=0926  -> pair_vec[996] (per_map_dim=249)\n",
            "14:59:59 | INFO | OK  map_id=0947  -> pair_vec[996] (per_map_dim=249)\n",
            "15:00:05 | INFO | OK  map_id=0948  -> pair_vec[996] (per_map_dim=249)\n",
            "15:00:09 | INFO | OK  map_id=0949  -> pair_vec[996] (per_map_dim=249)\n",
            "15:00:10 | INFO | OK  map_id=0950  -> pair_vec[996] (per_map_dim=249)\n",
            "15:00:11 | INFO | OK  map_id=0951  -> pair_vec[996] (per_map_dim=249)\n",
            "15:00:14 | INFO | OK  map_id=0952  -> pair_vec[996] (per_map_dim=249)\n",
            "15:00:17 | INFO | OK  map_id=0966  -> pair_vec[996] (per_map_dim=249)\n",
            "15:00:22 | INFO | OK  map_id=0967  -> pair_vec[996] (per_map_dim=249)\n",
            "15:00:23 | INFO | OK  map_id=0970  -> pair_vec[996] (per_map_dim=249)\n",
            "15:00:26 | INFO | OK  map_id=0971  -> pair_vec[996] (per_map_dim=249)\n",
            "15:00:28 | INFO | OK  map_id=0974  -> pair_vec[996] (per_map_dim=249)\n",
            "15:00:30 | INFO | OK  map_id=0975  -> pair_vec[996] (per_map_dim=249)\n",
            "15:00:32 | INFO | OK  map_id=0976  -> pair_vec[996] (per_map_dim=249)\n",
            "15:00:33 | INFO | OK  map_id=0994  -> pair_vec[996] (per_map_dim=249)\n",
            "15:00:36 | INFO | OK  map_id=0995  -> pair_vec[996] (per_map_dim=249)\n",
            "15:00:38 | INFO | OK  map_id=0997  -> pair_vec[996] (per_map_dim=249)\n",
            "15:00:43 | INFO | OK  map_id=0998  -> pair_vec[996] (per_map_dim=249)\n",
            "15:00:44 | INFO | OK  map_id=1019  -> pair_vec[996] (per_map_dim=249)\n",
            "15:00:50 | INFO | OK  map_id=1020  -> pair_vec[996] (per_map_dim=249)\n",
            "15:00:54 | INFO | OK  map_id=1052  -> pair_vec[996] (per_map_dim=249)\n",
            "15:00:58 | INFO | OK  map_id=1053  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:01 | INFO | OK  map_id=1054  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:02 | INFO | OK  map_id=1055  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:04 | INFO | OK  map_id=1056  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:06 | INFO | OK  map_id=1057  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:11 | INFO | OK  map_id=1069  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:12 | INFO | OK  map_id=1070  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:14 | INFO | OK  map_id=1090  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:17 | INFO | OK  map_id=1091  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:18 | INFO | OK  map_id=1092  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:19 | INFO | OK  map_id=1100  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:21 | INFO | OK  map_id=1103  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:24 | INFO | OK  map_id=1105  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:27 | INFO | OK  map_id=1106  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:29 | INFO | OK  map_id=1118  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:31 | INFO | OK  map_id=1119  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:33 | INFO | OK  map_id=1120  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:35 | INFO | OK  map_id=1139  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:37 | INFO | OK  map_id=1140  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:39 | INFO | OK  map_id=1148  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:40 | INFO | OK  map_id=1155  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:44 | INFO | OK  map_id=1157  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:48 | INFO | OK  map_id=1168  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:50 | INFO | OK  map_id=1169  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:52 | INFO | OK  map_id=1170  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:54 | INFO | OK  map_id=1197  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:57 | INFO | OK  map_id=1198  -> pair_vec[996] (per_map_dim=249)\n",
            "15:01:59 | INFO | OK  map_id=1202  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:05 | INFO | OK  map_id=1203  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:06 | INFO | OK  map_id=1204  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:08 | INFO | OK  map_id=1217  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:13 | INFO | OK  map_id=1218  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:16 | INFO | OK  map_id=1219  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:17 | INFO | OK  map_id=1221  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:18 | INFO | OK  map_id=1222  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:20 | INFO | OK  map_id=1231  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:21 | INFO | OK  map_id=1233  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:23 | INFO | OK  map_id=1234  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:25 | INFO | OK  map_id=1261  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:27 | INFO | OK  map_id=1269  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:36 | INFO | OK  map_id=1270  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:37 | INFO | OK  map_id=1271  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:39 | INFO | OK  map_id=1276  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:41 | INFO | OK  map_id=1277  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:44 | INFO | OK  map_id=1283  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:46 | INFO | OK  map_id=1284  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:48 | INFO | OK  map_id=1285  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:52 | INFO | OK  map_id=1295  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:57 | INFO | OK  map_id=1296  -> pair_vec[996] (per_map_dim=249)\n",
            "15:02:59 | INFO | OK  map_id=1297  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:00 | INFO | OK  map_id=1303  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:03 | INFO | OK  map_id=1304  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:04 | INFO | OK  map_id=1310  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:09 | INFO | OK  map_id=1319  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:12 | INFO | OK  map_id=1333  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:13 | INFO | OK  map_id=1334  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:19 | INFO | OK  map_id=1344  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:20 | INFO | OK  map_id=1349  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:22 | INFO | OK  map_id=1364  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:28 | INFO | OK  map_id=1365  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:33 | INFO | OK  map_id=1366  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:36 | INFO | OK  map_id=1367  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:38 | INFO | OK  map_id=1368  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:40 | INFO | OK  map_id=1369  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:41 | INFO | OK  map_id=1377  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:43 | INFO | OK  map_id=1378  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:45 | INFO | OK  map_id=1385  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:48 | INFO | OK  map_id=1386  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:50 | INFO | OK  map_id=1399  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:51 | INFO | OK  map_id=1401  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:53 | INFO | OK  map_id=1408  -> pair_vec[996] (per_map_dim=249)\n",
            "15:03:57 | INFO | OK  map_id=1409  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:00 | INFO | OK  map_id=1410  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:01 | INFO | OK  map_id=1413  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:07 | INFO | OK  map_id=1414  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:08 | INFO | OK  map_id=1415  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:10 | INFO | OK  map_id=1416  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:11 | INFO | OK  map_id=1417  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:16 | INFO | OK  map_id=1418  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:17 | INFO | OK  map_id=1434  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:18 | INFO | OK  map_id=1438  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:23 | INFO | OK  map_id=1439  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:25 | INFO | OK  map_id=1450  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:27 | INFO | OK  map_id=1451  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:33 | INFO | OK  map_id=1458  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:35 | INFO | OK  map_id=1459  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:37 | INFO | OK  map_id=1460  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:38 | INFO | OK  map_id=1465  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:43 | INFO | OK  map_id=1466  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:45 | INFO | OK  map_id=1467  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:48 | INFO | OK  map_id=1473  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:49 | INFO | OK  map_id=1474  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:51 | INFO | OK  map_id=1476  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:54 | INFO | OK  map_id=1479  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:55 | INFO | OK  map_id=1486  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:57 | INFO | OK  map_id=1487  -> pair_vec[996] (per_map_dim=249)\n",
            "15:04:59 | INFO | OK  map_id=1496  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:01 | INFO | OK  map_id=1500  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:03 | INFO | OK  map_id=1501  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:06 | INFO | OK  map_id=1507  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:07 | INFO | OK  map_id=1508  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:12 | INFO | OK  map_id=1509  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:13 | INFO | OK  map_id=1514  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:17 | INFO | OK  map_id=1515  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:18 | INFO | OK  map_id=1557  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:20 | INFO | OK  map_id=1563  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:25 | INFO | OK  map_id=1564  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:28 | INFO | OK  map_id=1565  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:30 | INFO | OK  map_id=1570  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:31 | INFO | OK  map_id=1579  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:35 | INFO | OK  map_id=1580  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:38 | INFO | OK  map_id=1583  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:39 | INFO | OK  map_id=1584  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:40 | INFO | OK  map_id=1598  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:43 | INFO | OK  map_id=1613  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:44 | INFO | OK  map_id=1614  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:46 | INFO | OK  map_id=1618  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:47 | INFO | OK  map_id=1619  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:49 | INFO | OK  map_id=1629  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:55 | INFO | OK  map_id=1630  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:56 | INFO | OK  map_id=1631  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:57 | INFO | OK  map_id=1647  -> pair_vec[996] (per_map_dim=249)\n",
            "15:05:59 | INFO | OK  map_id=1649  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:01 | INFO | OK  map_id=1650  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:03 | INFO | OK  map_id=1653  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:04 | INFO | OK  map_id=1666  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:07 | INFO | OK  map_id=1667  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:09 | INFO | OK  map_id=1672  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:11 | INFO | OK  map_id=1673  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:13 | INFO | OK  map_id=1679  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:15 | INFO | OK  map_id=1691  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:17 | INFO | OK  map_id=1696  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:22 | INFO | OK  map_id=1700  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:28 | INFO | OK  map_id=1702  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:31 | INFO | OK  map_id=1703  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:33 | INFO | OK  map_id=1709  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:34 | INFO | OK  map_id=1710  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:36 | INFO | OK  map_id=1748  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:38 | INFO | OK  map_id=1749  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:40 | INFO | OK  map_id=1750  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:43 | INFO | OK  map_id=1751  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:44 | INFO | OK  map_id=1752  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:46 | INFO | OK  map_id=1755  -> pair_vec[996] (per_map_dim=249)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pair map embeddings completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "15:06:47 | INFO | OK  map_id=1757  -> pair_vec[996] (per_map_dim=249)\n",
            "15:06:48 | INFO | Saved 300 pair vectors (failed=0) to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/pair_map_out\n"
          ]
        }
      ],
      "source": [
        "# notebook snippet\n",
        "cmd = [\n",
        "    sys.executable, \"-m\", \"src.mapvec.maps.pair_map_embeddings\",\n",
        "    \"--root\", str(MAPS_ROOT),\n",
        "    \"--input_pattern\", str(INPUT_MAPS_PATTERN),\n",
        "    \"--gen_pattern\", str(OUTPUT_MAPS_PATTERN),\n",
        "    \"--out_dir\", str(PAIR_MAP_OUT),\n",
        "    \"-v\"\n",
        "]\n",
        "print(\"CMD:\", \" \".join(cmd))\n",
        "res = subprocess.run(cmd, cwd=str(PROJ_ROOT))\n",
        "if res.returncode != 0:\n",
        "    raise SystemExit(\"Pair map embedding step failed.\")\n",
        "print(\"Pair map embeddings completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Concatenate\n",
        "Joins map & prompt vectors using `pairs.csv` and writes `X_concat.npy` and `train_pairs.parquet` to `TRAIN_OUT`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CMD: /opt/anaconda3/envs/thesis/bin/python -m src.mapvec.concat.concat_embeddings --pairs ../data/input/pairs.csv --map_npz ../data/output/pair_map_out/embeddings.npz --prompt_npz ../data/output/prompt_out/embeddings.npz --out_dir ../data/output/train_out --drop_dupes\n",
            "Concatenation completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "19:01:58 | INFO | Map  embeddings: (300, 996) from /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/pair_map_out/embeddings.npz\n",
            "19:01:58 | INFO | Prompt embeddings: (500, 512) from /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/prompt_out/embeddings.npz\n",
            "19:01:58 | INFO | X shape = (450, 1508)  (map_dim=996, prompt_dim=512)\n",
            "19:01:58 | INFO | Saved to /Users/amirdonyadide/Documents/GitHub/Thesis/data/output/train_out in 0.05s\n"
          ]
        }
      ],
      "source": [
        "cmd = [\n",
        "    sys.executable, \"-m\", \"src.mapvec.concat.concat_embeddings\",\n",
        "    \"--pairs\",      str(PAIRS_CSV),\n",
        "    \"--map_npz\",    str(PAIR_MAP_OUT / \"embeddings.npz\"),   # from pair_map_out\n",
        "    \"--prompt_npz\", str(PROMPT_OUT / \"embeddings.npz\"),\n",
        "    \"--out_dir\",    str(TRAIN_OUT),\n",
        "    \"--drop_dupes\",                                   # optional: drop duplicate (map_id,prompt_id)\n",
        "    # \"--fail_on_missing\",                            # optional: stop instead of skipping missing IDs\n",
        "]\n",
        "print(\"CMD:\", \" \".join(cmd))\n",
        "\n",
        "# Run from the project root so src/ is importable\n",
        "res = subprocess.run(cmd, cwd=str(PROJ_ROOT))\n",
        "if res.returncode != 0:\n",
        "    raise SystemExit(\"Concatenation step failed.\")\n",
        "print(\"Concatenation completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b997cab7",
      "metadata": {},
      "source": [
        "## 4) Split dataset  \n",
        "Splits the concatenated feature matrix `X_concat.npy` and its metadata `train_pairs.parquet` into separate **training**, **validation**, and **test** subsets.  \n",
        "Each split preserves row alignment between features and metadata, and the resulting files are saved under `TRAIN_OUT/splits/` as:  \n",
        "\n",
        "- `X_train.npy`, `pairs_train.parquet`  \n",
        "- `X_val.npy`, `pairs_val.parquet`  \n",
        "- `X_test.npy`, `pairs_test.parquet`  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3bc0897d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded X: (450, 1508), pairs: (450, 4)\n",
            "Train: (315, 1508), Val: (67, 1508), Test: (68, 1508)\n",
            "Saved splits to ../data/output/train_out/splits\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "X = np.load(TRAIN_OUT / \"X_concat.npy\")\n",
        "pairs_df = pd.read_parquet(TRAIN_OUT / \"train_pairs.parquet\")\n",
        "\n",
        "print(f\"Loaded X: {X.shape}, pairs: {pairs_df.shape}\")\n",
        "\n",
        "# --- Step 1: Train/Test split\n",
        "X_train, X_temp, df_train, df_temp = train_test_split(\n",
        "    X, pairs_df, test_size=VAL_RATIO + TEST_RATIO, random_state=SEED, shuffle=True\n",
        ")\n",
        "\n",
        "# --- Step 2: Split temp into Val/Test\n",
        "relative_test_ratio = TEST_RATIO / (VAL_RATIO + TEST_RATIO)\n",
        "X_val, X_test, df_val, df_test = train_test_split(\n",
        "    X_temp, df_temp, test_size=relative_test_ratio, random_state=SEED, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "# --- Save splits\n",
        "split_dir = TRAIN_OUT / \"splits\"\n",
        "split_dir.mkdir(exist_ok=True)\n",
        "\n",
        "np.save(split_dir / \"X_train.npy\", X_train)\n",
        "np.save(split_dir / \"X_val.npy\",   X_val)\n",
        "np.save(split_dir / \"X_test.npy\",  X_test)\n",
        "\n",
        "df_train.to_parquet(split_dir / \"pairs_train.parquet\", index=False)\n",
        "df_val.to_parquet(split_dir / \"pairs_val.parquet\", index=False)\n",
        "df_test.to_parquet(split_dir / \"pairs_test.parquet\", index=False)\n",
        "\n",
        "print(f\"Saved splits to {split_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "41942b1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================== MODEL TRAINING PARAMETERS =====================\n",
        "# --------- CONFIG ----------\n",
        "OP_COL         = \"operator\"\n",
        "PARAM_COLS     = [\"param\"]                     # keep list so y has shape (n,1)\n",
        "FIXED_CLASSES  = [\"simplify\", \"select\", \"aggregate\", \"displace\"]\n",
        "CLIP_ABS = None  # set to 8.0 to enable, or None to disable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "bb7d5a8b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_X_and_df(X, df):\n",
        "    \"\"\"Filter rows that have operator and param; normalize operator; keep X aligned.\"\"\"\n",
        "    df2 = df.copy()\n",
        "    df2[OP_COL] = df2[OP_COL].astype(str).str.strip().str.lower()\n",
        "    mask = df2[OP_COL].notna()\n",
        "    for c in PARAM_COLS:\n",
        "        mask &= df2[c].notna()\n",
        "    mask = mask.values  # numpy bool array aligned with X\n",
        "    X2   = X[mask]\n",
        "    df2  = df2.loc[mask].reset_index(drop=True)\n",
        "    return X2.astype(\"float32\", copy=False), df2\n",
        "\n",
        "# Apply to your splits produced earlier\n",
        "X_train, df_train = filter_X_and_df(X_train, df_train)\n",
        "X_val,   df_val   = filter_X_and_df(X_val,   df_val)\n",
        "X_test,  df_test  = filter_X_and_df(X_test,  df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "25b55b49",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes: ['aggregate', 'displace', 'select', 'simplify']\n"
          ]
        }
      ],
      "source": [
        "# Fixed class order (even if a class is missing in train)\n",
        "le = LabelEncoder()\n",
        "le.fit(FIXED_CLASSES)\n",
        "classes = le.classes_.tolist()\n",
        "print(\"Classes:\", classes)\n",
        "\n",
        "y_train_cls = le.transform(df_train[OP_COL])\n",
        "y_val_cls   = le.transform(df_val[OP_COL])\n",
        "y_test_cls  = le.transform(df_test[OP_COL])\n",
        "\n",
        "# Single-parameter targets -> 2D arrays (n,1)\n",
        "y_train_reg = df_train[PARAM_COLS].to_numpy(dtype=\"float32\")\n",
        "y_val_reg   = df_val[PARAM_COLS].to_numpy(dtype=\"float32\")\n",
        "y_test_reg  = df_test[PARAM_COLS].to_numpy(dtype=\"float32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "144ee7e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace inf with NaN so imputer can handle them\n",
        "for X in (X_train, X_val, X_test):\n",
        "    X[~np.isfinite(X)] = np.nan\n",
        "\n",
        "imp = SimpleImputer(strategy=\"median\")\n",
        "X_train_imp = imp.fit_transform(X_train)\n",
        "X_val_imp   = imp.transform(X_val)\n",
        "X_test_imp  = imp.transform(X_test)\n",
        "\n",
        "scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(5, 95))\n",
        "X_train_s = scaler.fit_transform(X_train_imp)\n",
        "X_val_s   = scaler.transform(X_val_imp)\n",
        "X_test_s  = scaler.transform(X_test_imp)\n",
        "\n",
        "# Clip extremes to stabilize optimizer\n",
        "if CLIP_ABS is not None:\n",
        "    X_train_s = np.clip(X_train_s, -CLIP_ABS, CLIP_ABS)\n",
        "    X_val_s   = np.clip(X_val_s,   -CLIP_ABS, CLIP_ABS)\n",
        "    X_test_s  = np.clip(X_test_s,  -CLIP_ABS, CLIP_ABS)\n",
        "\n",
        "assert np.isfinite(X_train_s).all() and np.isfinite(X_val_s).all() and np.isfinite(X_test_s).all(), \"Non-finite after scaling.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "229a52d7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class weights: {'aggregate': np.float64(0.9375), 'displace': np.float64(0.9603658536585366), 'select': np.float64(1.0227272727272727), 'simplify': np.float64(1.09375)}\n",
            "Iteration 1, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 2, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 3, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 4, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 5, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 6, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 7, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 8, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 9, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 10, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 11, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 12, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 13, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 14, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 15, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 16, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 17, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 18, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 19, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 20, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 21, loss = nan\n",
            "Validation score: 0.415531\n",
            "Iteration 22, loss = nan\n",
            "Validation score: 0.415531\n",
            "Validation score did not improve more than tol=0.000100 for 20 consecutive epochs. Stopping.\n",
            "‚ö†Ô∏è Adam crashed (Solver produced non-finite parameter weights. The input data may contain large values and need to be preprocessed.). Falling back to lbfgs (no sample_weight).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:602: ConvergenceWarning: lbfgs failed to converge after 0 iteration(s) (status=2):\n",
            "ABNORMAL: \n",
            "\n",
            "You might also want to scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        }
      ],
      "source": [
        "# Class weights for imbalance\n",
        "cls_w    = compute_class_weight(\"balanced\", classes=np.arange(len(classes)), y=y_train_cls)\n",
        "sample_w = np.array([cls_w[c] for c in y_train_cls], dtype=\"float32\")\n",
        "print(\"Class weights:\", dict(zip(classes, cls_w)))\n",
        "\n",
        "clf = MLPClassifier(\n",
        "    hidden_layer_sizes=(256, 128),\n",
        "    activation=\"relu\",\n",
        "    solver=\"adam\",\n",
        "    alpha=1e-3,\n",
        "    learning_rate_init=1e-4,\n",
        "    batch_size=128,\n",
        "    max_iter=400,\n",
        "    early_stopping=True,\n",
        "    n_iter_no_change=20,\n",
        "    random_state=SEED,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "try:\n",
        "    clf.fit(X_train_s, y_train_cls, sample_weight=sample_w)\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Adam crashed ({e}). Falling back to lbfgs (no sample_weight).\")\n",
        "    clf = MLPClassifier(\n",
        "        hidden_layer_sizes=(256, 128),\n",
        "        activation=\"relu\",\n",
        "        solver=\"lbfgs\",\n",
        "        alpha=1e-3,\n",
        "        max_iter=500,\n",
        "        random_state=SEED,\n",
        "        verbose=True\n",
        "    )\n",
        "    clf.fit(X_train_s, y_train_cls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "44b82400",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- X_train ---\n",
            "shape: (315, 1508) dtype: float32\n",
            "finite: True\n",
            "min/max: -552.0 4054873900.0\n",
            "mean/std: 24875.152 8824922.0\n",
            "all-NaN cols: 0 zero-variance cols: 58\n",
            "--- X_val ---\n",
            "shape: (67, 1508) dtype: float32\n",
            "finite: True\n",
            "min/max: -505.0 96049.9\n",
            "mean/std: 22.502472 675.2408\n",
            "all-NaN cols: 0 zero-variance cols: 72\n",
            "--- X_test ---\n",
            "shape: (68, 1508) dtype: float32\n",
            "finite: True\n",
            "min/max: -552.0 4054873900.0\n",
            "mean/std: 115146.09 18993522.0\n",
            "all-NaN cols: 0 zero-variance cols: 67\n",
            "--- X_train_imp ---\n",
            "shape: (315, 1508) dtype: float32\n",
            "finite: True\n",
            "min/max: -552.0 4054873900.0\n",
            "mean/std: 24875.152 8824922.0\n",
            "all-NaN cols: 0 zero-variance cols: 58\n",
            "--- X_train_s ---\n",
            "shape: (315, 1508) dtype: float32\n",
            "finite: True\n",
            "min/max: -20.723267 142484560.0\n",
            "mean/std: 1179.2317 357503.12\n",
            "all-NaN cols: 0 zero-variance cols: 58\n",
            "y_train_cls has NaN? False\n",
            "classes present in train: ['aggregate', 'displace', 'select', 'simplify']\n"
          ]
        }
      ],
      "source": [
        "def check_matrix(name, X):\n",
        "    print(f\"--- {name} ---\")\n",
        "    print(\"shape:\", X.shape, \"dtype:\", X.dtype)\n",
        "    print(\"finite:\", np.isfinite(X).all())\n",
        "    print(\"min/max:\", np.nanmin(X), np.nanmax(X))\n",
        "    print(\"mean/std:\", np.nanmean(X), np.nanstd(X))\n",
        "    # any all-NaN or all-constant columns before scaling?\n",
        "    col_nan = np.isnan(X).all(axis=0).sum()\n",
        "    col_zero_var = (np.nanstd(X, axis=0) == 0).sum()\n",
        "    print(\"all-NaN cols:\", col_nan, \"zero-variance cols:\", col_zero_var)\n",
        "\n",
        "check_matrix(\"X_train\", X_train)\n",
        "check_matrix(\"X_val\",   X_val)\n",
        "check_matrix(\"X_test\",  X_test)\n",
        "\n",
        "check_matrix(\"X_train_imp\", X_train_imp)\n",
        "check_matrix(\"X_train_s\",   X_train_s)\n",
        "\n",
        "print(\"y_train_cls has NaN?\", np.isnan(y_train_cls).any() if hasattr(y_train_cls, \"__len__\") else False)\n",
        "print(\"classes present in train:\", sorted(set(df_train[OP_COL])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "13fca12f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL] acc=0.224  f1_macro=0.191\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   aggregate       0.00      0.00      0.00        17\n",
            "    displace       0.42      0.42      0.42        12\n",
            "      select       0.50      0.04      0.08        24\n",
            "    simplify       0.17      0.64      0.27        14\n",
            "\n",
            "    accuracy                           0.22        67\n",
            "   macro avg       0.27      0.28      0.19        67\n",
            "weighted avg       0.29      0.22      0.16        67\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        }
      ],
      "source": [
        "y_val_pred  = clf.predict(X_val_s)\n",
        "val_acc     = accuracy_score(y_val_cls, y_val_pred)\n",
        "val_f1m     = f1_score(y_val_cls, y_val_pred, average=\"macro\")\n",
        "print(f\"[VAL] acc={val_acc:.3f}  f1_macro={val_f1m:.3f}\")\n",
        "print(classification_report(y_val_cls, y_val_pred, target_names=classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "1980b8b6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1771: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:214: RuntimeWarning: invalid value encountered in add\n",
            "  activation += self.intercepts_[i]\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (600) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1771: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Regressor trained for 'aggregate' on 84 samples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (600) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1771: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Regressor trained for 'displace' on 82 samples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (600) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1771: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Regressor trained for 'select' on 77 samples.\n",
            "‚úÖ Regressor trained for 'simplify' on 72 samples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (600) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "regressors = {}\n",
        "for idx, name in enumerate(classes):\n",
        "    sel = (y_train_cls == idx)\n",
        "    n = int(sel.sum())\n",
        "    if n < 5:\n",
        "        print(f\"‚ö†Ô∏è  Skipping regressor for '{name}' (only {n} samples).\")\n",
        "        continue\n",
        "\n",
        "    reg = MLPRegressor(\n",
        "        hidden_layer_sizes=(128, 64),\n",
        "        activation=\"relu\",\n",
        "        solver=\"adam\",\n",
        "        alpha=1e-3,\n",
        "        learning_rate_init=5e-4,\n",
        "        max_iter=600,\n",
        "        early_stopping=True,\n",
        "        n_iter_no_change=30,\n",
        "        random_state=SEED,\n",
        "        verbose=False\n",
        "    )\n",
        "    reg.fit(X_train_s[sel], y_train_reg[sel])  # (n,d) -> (n,1)\n",
        "    regressors[name] = reg\n",
        "    print(f\"‚úÖ Regressor trained for '{name}' on {n} samples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "b5603571",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TEST] acc=0.279  f1_macro=0.214\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   aggregate       0.00      0.00      0.00        20\n",
            "    displace       0.42      0.33      0.37        15\n",
            "      select       1.00      0.07      0.13        14\n",
            "    simplify       0.24      0.68      0.35        19\n",
            "\n",
            "    accuracy                           0.28        68\n",
            "   macro avg       0.41      0.27      0.21        68\n",
            "weighted avg       0.36      0.28      0.21        68\n",
            "\n",
            "Parameter evaluation on 19/68 samples with correct operator prediction.\n",
            "[TEST Param | correct-ops] MSE=390.9869  MAE=7.3307\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "/opt/anaconda3/envs/thesis/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        }
      ],
      "source": [
        "# Classification\n",
        "y_test_pred = clf.predict(X_test_s)\n",
        "test_acc    = accuracy_score(y_test_cls, y_test_pred)\n",
        "test_f1m    = f1_score(y_test_cls, y_test_pred, average=\"macro\")\n",
        "print(f\"[TEST] acc={test_acc:.3f}  f1_macro={test_f1m:.3f}\")\n",
        "print(classification_report(y_test_cls, y_test_pred, target_names=classes))\n",
        "\n",
        "# Parameter regression (conditioned on correct operator)\n",
        "mask = (y_test_pred == y_test_cls)\n",
        "print(f\"Parameter evaluation on {int(mask.sum())}/{len(mask)} samples with correct operator prediction.\")\n",
        "y_pred_params = np.full_like(y_test_reg, np.nan, dtype=\"float32\")  # (n,1)\n",
        "\n",
        "for i, ok in enumerate(mask):\n",
        "    if not ok:\n",
        "        continue\n",
        "    cls_name = classes[y_test_pred[i]]\n",
        "    reg = regressors.get(cls_name)\n",
        "    if reg is None:\n",
        "        continue\n",
        "    pred = reg.predict(X_test_s[i:i+1])[0]\n",
        "    y_pred_params[i] = pred if hasattr(pred, \"__len__\") else [float(pred)]\n",
        "\n",
        "valid = np.isfinite(y_pred_params).all(axis=1) & mask\n",
        "if valid.any():\n",
        "    mse = mean_squared_error(y_test_reg[valid], y_pred_params[valid])\n",
        "    mae = mean_absolute_error(y_test_reg[valid], y_pred_params[valid])\n",
        "    print(f\"[TEST Param | correct-ops] MSE={mse:.4f}  MAE={mae:.4f}\")\n",
        "else:\n",
        "    print(\"No valid parameter predictions to score.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "50f905d8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models saved to ../data/output/models\n"
          ]
        }
      ],
      "source": [
        "joblib.dump(imp,        MODEL_OUT / \"imputer.joblib\")\n",
        "joblib.dump(scaler,     MODEL_OUT / \"scaler.joblib\")\n",
        "joblib.dump(le,         MODEL_OUT / \"label_encoder.joblib\")\n",
        "joblib.dump(clf,        MODEL_OUT / \"mlp_classifier.joblib\")\n",
        "joblib.dump(regressors, MODEL_OUT / \"per_class_regressors.joblib\")\n",
        "print(f\"Models saved to {MODEL_OUT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "10fd82f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_pipeline(X_batch):\n",
        "    \"\"\"\n",
        "    X_batch: (n, 1508) raw concatenated features\n",
        "    Returns: dict with operator_probs, operator_label, params_pred\n",
        "    \"\"\"\n",
        "    Xb = np.asarray(X_batch, dtype=\"float32\")\n",
        "    Xb[~np.isfinite(Xb)] = np.nan\n",
        "    Xb = imp.transform(Xb)\n",
        "    Xb = scaler.transform(Xb)\n",
        "    Xb = np.clip(Xb, -CLIP_ABS, CLIP_ABS)\n",
        "\n",
        "    proba = clf.predict_proba(Xb)                    # (n, 4)\n",
        "    pred_idx = np.argmax(proba, axis=1)\n",
        "    pred_labels = le.inverse_transform(pred_idx).tolist()\n",
        "\n",
        "    params = []\n",
        "    for i, lbl in enumerate(pred_labels):\n",
        "        reg = regressors.get(lbl)\n",
        "        if reg is None:\n",
        "            params.append([np.nan])                 # single param\n",
        "        else:\n",
        "            pred = reg.predict(Xb[i:i+1])[0]\n",
        "            params.append(pred.tolist() if hasattr(pred, \"tolist\") else [float(pred)])\n",
        "\n",
        "    return {\"operator_probs\": proba, \"operator_label\": pred_labels, \"params_pred\": np.array(params)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d21771b",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "thesis",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
